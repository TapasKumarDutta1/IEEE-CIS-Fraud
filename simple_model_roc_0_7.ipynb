{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_0.7",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_0_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a0129dee-d063-4160-b6df-a11e32c15a6f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b4244b66-2a8d-4222-f469-545e7ae58468"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 107MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 63% 33.0M/52.2M [00:00<00:00, 32.4MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 82.2MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 158MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 84% 49.0M/58.3M [00:00<00:00, 34.6MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 70.7MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 221MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv80v8W_Ko2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7bd3dbd6-8859-4534-d7af-d94b2172d394"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jt2pcxPije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a8b8c94a-75b4-4d7c-f774-edcb5b18d9af"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvEaxp9jhbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "9902bd17-e9a7-4af3-d6d2-e91cf3ec0fd3"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38009488-078d-43cf-e603-1657cd99a53d"
      },
      "source": [
        "\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkLEBcl6LHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 0.7\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGXOzNdXS9DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67aa427b-ee87-4fa3-e54b-aa915e6ca61a"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "# trn=trn.drop(['isFraud_isna'],1)\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Adam(0.001,decay=1e-3),loss=rac,metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.00001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 28602.8965 - accuracy: 0.6934 - val_loss: 23533.6406 - val_accuracy: 0.5614\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 23170.6816 - accuracy: 0.7519 - val_loss: 21631.7383 - val_accuracy: 0.6334\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 20868.9746 - accuracy: 0.7758 - val_loss: 20010.1504 - val_accuracy: 0.6432\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 19397.7500 - accuracy: 0.7953 - val_loss: 19169.9453 - val_accuracy: 0.6898\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 18094.3828 - accuracy: 0.8052 - val_loss: 19761.6738 - val_accuracy: 0.6578\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 17334.0781 - accuracy: 0.8121 - val_loss: 18684.7891 - val_accuracy: 0.7147\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 16491.6484 - accuracy: 0.8216 - val_loss: 17889.9473 - val_accuracy: 0.7239\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 15803.4941 - accuracy: 0.8266 - val_loss: 19024.6074 - val_accuracy: 0.7351\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 15337.3965 - accuracy: 0.8313 - val_loss: 18811.8164 - val_accuracy: 0.7044\n",
            "Epoch 10/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 14946.4854 - accuracy: 0.8361roc-auc_val: 0.7959\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 14912.4492 - accuracy: 0.8358 - val_loss: 18436.3359 - val_accuracy: 0.6961\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14556.7754 - accuracy: 0.8377 - val_loss: 18033.3379 - val_accuracy: 0.7465\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14120.0869 - accuracy: 0.8416 - val_loss: 17855.7773 - val_accuracy: 0.7549\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13940.8525 - accuracy: 0.8435 - val_loss: 17498.8672 - val_accuracy: 0.7566\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13743.1924 - accuracy: 0.8459 - val_loss: 17851.1777 - val_accuracy: 0.7628\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13410.9150 - accuracy: 0.8514 - val_loss: 17758.8242 - val_accuracy: 0.7641\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13085.5000 - accuracy: 0.8525 - val_loss: 18178.9688 - val_accuracy: 0.7239\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12883.0957 - accuracy: 0.8541 - val_loss: 17711.4004 - val_accuracy: 0.7573\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12696.6279 - accuracy: 0.8545 - val_loss: 18049.4727 - val_accuracy: 0.7512\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12649.2129 - accuracy: 0.8537 - val_loss: 17287.0293 - val_accuracy: 0.7576\n",
            "Epoch 20/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 12462.8105 - accuracy: 0.8544roc-auc_val: 0.8127\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 12434.5469 - accuracy: 0.8544 - val_loss: 17214.8320 - val_accuracy: 0.7532\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12144.0713 - accuracy: 0.8574 - val_loss: 17379.7520 - val_accuracy: 0.7771\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12077.8945 - accuracy: 0.8594 - val_loss: 17238.3340 - val_accuracy: 0.7759\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12036.3730 - accuracy: 0.8603 - val_loss: 17211.3281 - val_accuracy: 0.7744\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11692.1953 - accuracy: 0.8620 - val_loss: 17192.4180 - val_accuracy: 0.7687\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11702.4570 - accuracy: 0.8612 - val_loss: 17380.4297 - val_accuracy: 0.7680\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11606.7324 - accuracy: 0.8630 - val_loss: 17348.2773 - val_accuracy: 0.7956\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11496.0342 - accuracy: 0.8632 - val_loss: 17415.5215 - val_accuracy: 0.7836\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11428.6748 - accuracy: 0.8629 - val_loss: 17563.4082 - val_accuracy: 0.7668\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11177.6787 - accuracy: 0.8641 - val_loss: 17235.5547 - val_accuracy: 0.7792\n",
            "Epoch 30/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 11011.7178 - accuracy: 0.8646roc-auc_val: 0.8065\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 11017.8320 - accuracy: 0.8645 - val_loss: 17638.3242 - val_accuracy: 0.7486\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11122.3945 - accuracy: 0.8639 - val_loss: 17219.6348 - val_accuracy: 0.7654\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10941.9062 - accuracy: 0.8659 - val_loss: 17239.2266 - val_accuracy: 0.7852\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10874.9395 - accuracy: 0.8664 - val_loss: 17067.5684 - val_accuracy: 0.7776\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10749.7363 - accuracy: 0.8652 - val_loss: 17084.2812 - val_accuracy: 0.7787\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10650.0713 - accuracy: 0.8667 - val_loss: 16891.0762 - val_accuracy: 0.7775\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10670.5605 - accuracy: 0.8653 - val_loss: 17037.1621 - val_accuracy: 0.7922\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10619.8125 - accuracy: 0.8693 - val_loss: 16862.3105 - val_accuracy: 0.8092\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10592.7744 - accuracy: 0.8702 - val_loss: 16693.6816 - val_accuracy: 0.7917\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10475.1221 - accuracy: 0.8686 - val_loss: 17002.1797 - val_accuracy: 0.7844\n",
            "Epoch 40/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 10366.0381 - accuracy: 0.8675roc-auc_val: 0.8182\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 10366.0381 - accuracy: 0.8675 - val_loss: 16904.2266 - val_accuracy: 0.8032\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10395.8721 - accuracy: 0.8704 - val_loss: 16847.6914 - val_accuracy: 0.7856\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10301.9629 - accuracy: 0.8716 - val_loss: 17101.7852 - val_accuracy: 0.7859\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10232.5586 - accuracy: 0.8704 - val_loss: 17071.6816 - val_accuracy: 0.7829\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10091.2305 - accuracy: 0.8721 - val_loss: 16809.1035 - val_accuracy: 0.8132\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10017.7432 - accuracy: 0.8727 - val_loss: 16656.0508 - val_accuracy: 0.7985\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10055.7080 - accuracy: 0.8723 - val_loss: 17328.3105 - val_accuracy: 0.7801\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9928.6680 - accuracy: 0.8728 - val_loss: 16922.3691 - val_accuracy: 0.7932\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9925.3232 - accuracy: 0.8738 - val_loss: 16759.9062 - val_accuracy: 0.7967\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9911.5967 - accuracy: 0.8713 - val_loss: 17031.0273 - val_accuracy: 0.8097\n",
            "Epoch 50/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 9967.3271 - accuracy: 0.8742 roc-auc_val: 0.8158\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 9911.4609 - accuracy: 0.8742 - val_loss: 17019.5781 - val_accuracy: 0.8083\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9808.5098 - accuracy: 0.8734 - val_loss: 16967.3613 - val_accuracy: 0.8164\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9804.3594 - accuracy: 0.8747 - val_loss: 16783.3320 - val_accuracy: 0.8068\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9701.6211 - accuracy: 0.8744 - val_loss: 16872.7676 - val_accuracy: 0.8155\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9693.4033 - accuracy: 0.8761 - val_loss: 16721.0625 - val_accuracy: 0.8265\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9723.5830 - accuracy: 0.8766 - val_loss: 17009.9824 - val_accuracy: 0.8087\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9654.5645 - accuracy: 0.8767 - val_loss: 16964.5332 - val_accuracy: 0.8231\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9553.6104 - accuracy: 0.8778 - val_loss: 16862.4336 - val_accuracy: 0.8165\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9603.1182 - accuracy: 0.8750 - val_loss: 16837.3613 - val_accuracy: 0.8074\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9459.8086 - accuracy: 0.8776 - val_loss: 16790.7461 - val_accuracy: 0.8248\n",
            "Epoch 60/1000\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 9549.7549 - accuracy: 0.8754roc-auc_val: 0.8195\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 9537.2441 - accuracy: 0.8753 - val_loss: 16793.9570 - val_accuracy: 0.8097\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9443.8809 - accuracy: 0.8753 - val_loss: 16955.3516 - val_accuracy: 0.8020\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9444.4180 - accuracy: 0.8777 - val_loss: 16921.4238 - val_accuracy: 0.8051\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9453.8096 - accuracy: 0.8758 - val_loss: 16913.3301 - val_accuracy: 0.8173\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9407.7607 - accuracy: 0.8766 - val_loss: 16989.1992 - val_accuracy: 0.8155\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9310.0322 - accuracy: 0.8774 - val_loss: 16973.1621 - val_accuracy: 0.8049\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9184.9980 - accuracy: 0.8791 - val_loss: 16925.4453 - val_accuracy: 0.8181\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9265.5947 - accuracy: 0.8783 - val_loss: 16986.8047 - val_accuracy: 0.8129\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9290.5371 - accuracy: 0.8786 - val_loss: 17023.9102 - val_accuracy: 0.8159\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9216.3789 - accuracy: 0.8802 - val_loss: 17028.9844 - val_accuracy: 0.8230\n",
            "Epoch 70/1000\n",
            "218/225 [============================>.] - ETA: 0s - loss: 9217.4229 - accuracy: 0.8801roc-auc_val: 0.818\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 9202.8486 - accuracy: 0.8801 - val_loss: 16930.2930 - val_accuracy: 0.8186\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9136.1289 - accuracy: 0.8781 - val_loss: 16993.0273 - val_accuracy: 0.8104\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9107.1689 - accuracy: 0.8812 - val_loss: 16892.3164 - val_accuracy: 0.8219\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9030.1689 - accuracy: 0.8820 - val_loss: 16800.3418 - val_accuracy: 0.8285\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8960.4209 - accuracy: 0.8817 - val_loss: 17058.2070 - val_accuracy: 0.8085\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8912.1475 - accuracy: 0.8820 - val_loss: 16811.5332 - val_accuracy: 0.8309\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9076.7070 - accuracy: 0.8804 - val_loss: 16891.7285 - val_accuracy: 0.8188\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8972.5371 - accuracy: 0.8826 - val_loss: 16859.6387 - val_accuracy: 0.8210\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8982.6162 - accuracy: 0.8800 - val_loss: 16909.8848 - val_accuracy: 0.8174\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9026.1953 - accuracy: 0.8818 - val_loss: 16835.4316 - val_accuracy: 0.8193\n",
            "Epoch 80/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 8940.3750 - accuracy: 0.8802roc-auc_val: 0.82\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 8912.3887 - accuracy: 0.8805 - val_loss: 16777.9395 - val_accuracy: 0.8283\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8904.2656 - accuracy: 0.8829 - val_loss: 16766.2383 - val_accuracy: 0.8262\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8912.8340 - accuracy: 0.8815 - val_loss: 16924.0332 - val_accuracy: 0.8181\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8807.0684 - accuracy: 0.8827 - val_loss: 16871.4883 - val_accuracy: 0.8253\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8825.7314 - accuracy: 0.8839 - val_loss: 16870.0469 - val_accuracy: 0.8184\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8820.1650 - accuracy: 0.8817 - val_loss: 16695.8848 - val_accuracy: 0.8279\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8896.4316 - accuracy: 0.8838 - val_loss: 16728.1113 - val_accuracy: 0.8193\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8706.5781 - accuracy: 0.8830 - val_loss: 16884.6289 - val_accuracy: 0.8139\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8758.4141 - accuracy: 0.8817 - val_loss: 16789.8457 - val_accuracy: 0.8213\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8666.0430 - accuracy: 0.8825 - val_loss: 16810.7988 - val_accuracy: 0.8219\n",
            "Epoch 90/1000\n",
            "218/225 [============================>.] - ETA: 0s - loss: 8713.7422 - accuracy: 0.8827roc-auc_val: 0.8185\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 8709.4316 - accuracy: 0.8830 - val_loss: 16888.2891 - val_accuracy: 0.8197\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8709.0752 - accuracy: 0.8828 - val_loss: 16813.8848 - val_accuracy: 0.8187\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8587.2275 - accuracy: 0.8816 - val_loss: 16823.3086 - val_accuracy: 0.8246\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8682.7842 - accuracy: 0.8850 - val_loss: 16708.4883 - val_accuracy: 0.8290\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8598.0068 - accuracy: 0.8828 - val_loss: 16857.8770 - val_accuracy: 0.8211\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8600.3574 - accuracy: 0.8847 - val_loss: 16912.0801 - val_accuracy: 0.8230\n",
            "Epoch 1/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 28515.3418 - accuracy: 0.6899 - val_loss: 24565.4688 - val_accuracy: 0.6417\n",
            "Epoch 2/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 23068.8691 - accuracy: 0.7447 - val_loss: 22523.3965 - val_accuracy: 0.6919\n",
            "Epoch 3/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 20857.9238 - accuracy: 0.7726 - val_loss: 21815.5977 - val_accuracy: 0.6805\n",
            "Epoch 4/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 19294.2500 - accuracy: 0.7916 - val_loss: 21243.1602 - val_accuracy: 0.7315\n",
            "Epoch 5/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 17990.0391 - accuracy: 0.8050 - val_loss: 21437.2637 - val_accuracy: 0.7304\n",
            "Epoch 6/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 17035.0195 - accuracy: 0.8146 - val_loss: 21278.8770 - val_accuracy: 0.7374\n",
            "Epoch 7/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 16056.9971 - accuracy: 0.8219 - val_loss: 20478.0176 - val_accuracy: 0.7618\n",
            "Epoch 8/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 15550.3506 - accuracy: 0.8264 - val_loss: 21611.7266 - val_accuracy: 0.7706\n",
            "Epoch 9/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14986.7656 - accuracy: 0.8292 - val_loss: 21314.1543 - val_accuracy: 0.7648\n",
            "Epoch 10/1000\n",
            "170/181 [===========================>..] - ETA: 0s - loss: 14463.6680 - accuracy: 0.8390roc-auc_val: 0.8109\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 14362.7109 - accuracy: 0.8395 - val_loss: 21157.8730 - val_accuracy: 0.8006\n",
            "Epoch 11/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14012.5479 - accuracy: 0.8410 - val_loss: 20781.7539 - val_accuracy: 0.7954\n",
            "Epoch 12/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13549.5420 - accuracy: 0.8450 - val_loss: 20687.7305 - val_accuracy: 0.7914\n",
            "Epoch 13/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13291.1484 - accuracy: 0.8476 - val_loss: 20565.2344 - val_accuracy: 0.8017\n",
            "Epoch 14/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12946.1250 - accuracy: 0.8508 - val_loss: 21191.2734 - val_accuracy: 0.7892\n",
            "Epoch 15/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12718.1250 - accuracy: 0.8521 - val_loss: 20962.5820 - val_accuracy: 0.7979\n",
            "Epoch 16/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12477.2305 - accuracy: 0.8546 - val_loss: 20635.7051 - val_accuracy: 0.8113\n",
            "Epoch 17/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12272.3301 - accuracy: 0.8554 - val_loss: 20644.5898 - val_accuracy: 0.7901\n",
            "Epoch 18/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12183.5879 - accuracy: 0.8576 - val_loss: 21088.8438 - val_accuracy: 0.8078\n",
            "Epoch 19/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11824.1348 - accuracy: 0.8602 - val_loss: 21062.1758 - val_accuracy: 0.8041\n",
            "Epoch 20/1000\n",
            "177/181 [============================>.] - ETA: 0s - loss: 11359.1201 - accuracy: 0.8631roc-auc_val: 0.8181\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 11337.5762 - accuracy: 0.8632 - val_loss: 20599.5254 - val_accuracy: 0.8265\n",
            "Epoch 21/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11436.2822 - accuracy: 0.8662 - val_loss: 20736.2949 - val_accuracy: 0.8283\n",
            "Epoch 22/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11452.1289 - accuracy: 0.8633 - val_loss: 20643.8223 - val_accuracy: 0.8200\n",
            "Epoch 23/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11262.8242 - accuracy: 0.8654 - val_loss: 20539.2051 - val_accuracy: 0.8122\n",
            "Epoch 24/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11027.8721 - accuracy: 0.8663 - val_loss: 20753.8008 - val_accuracy: 0.8269\n",
            "Epoch 25/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10926.5449 - accuracy: 0.8685 - val_loss: 20982.3926 - val_accuracy: 0.8440\n",
            "Epoch 26/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10828.6533 - accuracy: 0.8677 - val_loss: 20900.6445 - val_accuracy: 0.8295\n",
            "Epoch 27/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10686.9531 - accuracy: 0.8688 - val_loss: 20800.0762 - val_accuracy: 0.8315\n",
            "Epoch 28/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10503.5293 - accuracy: 0.8724 - val_loss: 20993.9434 - val_accuracy: 0.8242\n",
            "Epoch 29/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10505.7500 - accuracy: 0.8720 - val_loss: 20990.8613 - val_accuracy: 0.8194\n",
            "Epoch 30/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 10406.5410 - accuracy: 0.8720roc-auc_val: 0.8187\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 10362.8555 - accuracy: 0.8723 - val_loss: 20572.2617 - val_accuracy: 0.8323\n",
            "Epoch 31/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10178.5498 - accuracy: 0.8730 - val_loss: 20821.9316 - val_accuracy: 0.8391\n",
            "Epoch 32/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10167.0078 - accuracy: 0.8721 - val_loss: 20717.4375 - val_accuracy: 0.8308\n",
            "Epoch 33/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9918.3213 - accuracy: 0.8753 - val_loss: 20992.6816 - val_accuracy: 0.8242\n",
            "Epoch 34/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9876.0508 - accuracy: 0.8736 - val_loss: 20929.5586 - val_accuracy: 0.8326\n",
            "Epoch 35/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9924.9600 - accuracy: 0.8763 - val_loss: 21044.6055 - val_accuracy: 0.8340\n",
            "Epoch 36/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9811.3652 - accuracy: 0.8768 - val_loss: 20562.3555 - val_accuracy: 0.8370\n",
            "Epoch 37/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9745.4805 - accuracy: 0.8771 - val_loss: 20739.8906 - val_accuracy: 0.8326\n",
            "Epoch 38/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9587.8438 - accuracy: 0.8777 - val_loss: 20663.7578 - val_accuracy: 0.8597\n",
            "Epoch 39/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9540.7490 - accuracy: 0.8781 - val_loss: 20549.6113 - val_accuracy: 0.8324\n",
            "Epoch 40/1000\n",
            "174/181 [===========================>..] - ETA: 0s - loss: 9542.6143 - accuracy: 0.8787roc-auc_val: 0.8193\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 9479.9238 - accuracy: 0.8789 - val_loss: 20698.5586 - val_accuracy: 0.8498\n",
            "Epoch 41/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9313.4238 - accuracy: 0.8797 - val_loss: 20636.2461 - val_accuracy: 0.8360\n",
            "Epoch 42/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9319.1055 - accuracy: 0.8791 - val_loss: 21073.6953 - val_accuracy: 0.8446\n",
            "Epoch 43/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9290.5527 - accuracy: 0.8804 - val_loss: 20808.7285 - val_accuracy: 0.8495\n",
            "Epoch 44/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9249.4365 - accuracy: 0.8796 - val_loss: 20711.0527 - val_accuracy: 0.8512\n",
            "Epoch 45/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9175.2930 - accuracy: 0.8819 - val_loss: 20923.9355 - val_accuracy: 0.8531\n",
            "Epoch 46/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9063.7666 - accuracy: 0.8824 - val_loss: 20725.1836 - val_accuracy: 0.8439\n",
            "Epoch 47/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8981.9043 - accuracy: 0.8815 - val_loss: 20806.7383 - val_accuracy: 0.8477\n",
            "Epoch 48/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8998.4043 - accuracy: 0.8825 - val_loss: 20956.1973 - val_accuracy: 0.8527\n",
            "Epoch 49/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8863.4990 - accuracy: 0.8832 - val_loss: 20835.5312 - val_accuracy: 0.8535\n",
            "Epoch 50/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 8967.2246 - accuracy: 0.8827roc-auc_val: 0.818\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 8932.9834 - accuracy: 0.8827 - val_loss: 20788.0977 - val_accuracy: 0.8532\n",
            "Epoch 51/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8853.2285 - accuracy: 0.8838 - val_loss: 20866.2695 - val_accuracy: 0.8572\n",
            "Epoch 52/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8758.3496 - accuracy: 0.8839 - val_loss: 21040.2070 - val_accuracy: 0.8623\n",
            "Epoch 53/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8883.2900 - accuracy: 0.8828 - val_loss: 21025.8047 - val_accuracy: 0.8590\n",
            "Epoch 54/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8640.4980 - accuracy: 0.8842 - val_loss: 20984.0195 - val_accuracy: 0.8451\n",
            "Epoch 55/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8712.0791 - accuracy: 0.8838 - val_loss: 21053.9492 - val_accuracy: 0.8572\n",
            "Epoch 56/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8586.5361 - accuracy: 0.8853 - val_loss: 21110.9980 - val_accuracy: 0.8530\n",
            "Epoch 57/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8608.4443 - accuracy: 0.8855 - val_loss: 21084.9824 - val_accuracy: 0.8639\n",
            "Epoch 1/1000\n",
            "136/136 [==============================] - 1s 11ms/step - loss: 28723.6855 - accuracy: 0.6783 - val_loss: 26319.9902 - val_accuracy: 0.6813\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 23110.5234 - accuracy: 0.7418 - val_loss: 24765.0078 - val_accuracy: 0.7130\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 20823.2734 - accuracy: 0.7648 - val_loss: 23612.9082 - val_accuracy: 0.7136\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 19129.6445 - accuracy: 0.7843 - val_loss: 23241.7578 - val_accuracy: 0.7529\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 17917.7773 - accuracy: 0.7998 - val_loss: 22558.2480 - val_accuracy: 0.7601\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 16724.0039 - accuracy: 0.8109 - val_loss: 22403.1074 - val_accuracy: 0.7547\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 15930.9609 - accuracy: 0.8185 - val_loss: 22425.3711 - val_accuracy: 0.7691\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 15230.5273 - accuracy: 0.8246 - val_loss: 22925.6367 - val_accuracy: 0.7669\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14666.5967 - accuracy: 0.8309 - val_loss: 22856.2461 - val_accuracy: 0.7944\n",
            "Epoch 10/1000\n",
            "135/136 [============================>.] - ETA: 0s - loss: 14097.6328 - accuracy: 0.8309roc-auc_val: 0.8132\n",
            "136/136 [==============================] - 9s 66ms/step - loss: 14066.1904 - accuracy: 0.8310 - val_loss: 22612.4414 - val_accuracy: 0.7648\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13692.7666 - accuracy: 0.8378 - val_loss: 22575.6523 - val_accuracy: 0.7767\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13232.3760 - accuracy: 0.8438 - val_loss: 22569.5039 - val_accuracy: 0.8072\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12911.9004 - accuracy: 0.8475 - val_loss: 22427.1543 - val_accuracy: 0.7847\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12463.5928 - accuracy: 0.8503 - val_loss: 22186.8496 - val_accuracy: 0.8018\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12056.8896 - accuracy: 0.8547 - val_loss: 22386.0117 - val_accuracy: 0.8075\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12009.8018 - accuracy: 0.8566 - val_loss: 22163.1387 - val_accuracy: 0.7888\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11662.8291 - accuracy: 0.8592 - val_loss: 22525.2031 - val_accuracy: 0.7988\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11478.9482 - accuracy: 0.8580 - val_loss: 22262.7051 - val_accuracy: 0.7897\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11188.2773 - accuracy: 0.8609 - val_loss: 22667.6660 - val_accuracy: 0.7898\n",
            "Epoch 20/1000\n",
            "127/136 [===========================>..] - ETA: 0s - loss: 11153.5186 - accuracy: 0.8627roc-auc_val: 0.8153\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 11068.0322 - accuracy: 0.8627 - val_loss: 22503.6602 - val_accuracy: 0.8026\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10845.5771 - accuracy: 0.8661 - val_loss: 23075.4043 - val_accuracy: 0.8221\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10758.8037 - accuracy: 0.8658 - val_loss: 22826.7637 - val_accuracy: 0.8014\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10442.1201 - accuracy: 0.8681 - val_loss: 23142.3477 - val_accuracy: 0.7927\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10374.9922 - accuracy: 0.8710 - val_loss: 22625.6328 - val_accuracy: 0.8183\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10301.7500 - accuracy: 0.8700 - val_loss: 22814.5742 - val_accuracy: 0.7961\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10079.4609 - accuracy: 0.8723 - val_loss: 22927.2695 - val_accuracy: 0.7906\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9881.4229 - accuracy: 0.8736 - val_loss: 22809.0000 - val_accuracy: 0.8095\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9728.4697 - accuracy: 0.8766 - val_loss: 23046.9453 - val_accuracy: 0.8099\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9745.0781 - accuracy: 0.8735 - val_loss: 22691.1484 - val_accuracy: 0.7759\n",
            "Epoch 30/1000\n",
            "126/136 [==========================>...] - ETA: 0s - loss: 9535.1846 - accuracy: 0.8749roc-auc_val: 0.8118\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 9549.7168 - accuracy: 0.8754 - val_loss: 22923.1465 - val_accuracy: 0.8033\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9416.7207 - accuracy: 0.8796 - val_loss: 22899.1895 - val_accuracy: 0.7954\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9334.2744 - accuracy: 0.8793 - val_loss: 23086.2324 - val_accuracy: 0.7895\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9317.3359 - accuracy: 0.8802 - val_loss: 23275.1562 - val_accuracy: 0.8000\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9135.8945 - accuracy: 0.8806 - val_loss: 23206.8066 - val_accuracy: 0.8069\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9107.0518 - accuracy: 0.8789 - val_loss: 23445.9219 - val_accuracy: 0.8111\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8948.8623 - accuracy: 0.8809 - val_loss: 23365.0098 - val_accuracy: 0.7991\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8876.7930 - accuracy: 0.8818 - val_loss: 23325.9375 - val_accuracy: 0.7918\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8901.6641 - accuracy: 0.8829 - val_loss: 23605.2617 - val_accuracy: 0.7897\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8811.2236 - accuracy: 0.8849 - val_loss: 23352.5957 - val_accuracy: 0.7792\n",
            "Epoch 40/1000\n",
            "131/136 [===========================>..] - ETA: 0s - loss: 8821.1719 - accuracy: 0.8849roc-auc_val: 0.8065\n",
            "136/136 [==============================] - 9s 66ms/step - loss: 8762.0586 - accuracy: 0.8848 - val_loss: 23355.5098 - val_accuracy: 0.7947\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8617.5273 - accuracy: 0.8872 - val_loss: 23740.4961 - val_accuracy: 0.7897\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8570.9111 - accuracy: 0.8837 - val_loss: 23610.5098 - val_accuracy: 0.7873\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 8438.1943 - accuracy: 0.8884 - val_loss: 23498.2188 - val_accuracy: 0.7924\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8444.3076 - accuracy: 0.8849 - val_loss: 23425.7207 - val_accuracy: 0.7916\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 8395.8105 - accuracy: 0.8887 - val_loss: 24148.6367 - val_accuracy: 0.8028\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8280.8184 - accuracy: 0.8876 - val_loss: 23719.4961 - val_accuracy: 0.7970\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8434.7686 - accuracy: 0.8847 - val_loss: 23520.4531 - val_accuracy: 0.7834\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8187.6206 - accuracy: 0.8888 - val_loss: 23619.5898 - val_accuracy: 0.8002\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8112.4111 - accuracy: 0.8881 - val_loss: 23658.0391 - val_accuracy: 0.7922\n",
            "Epoch 50/1000\n",
            "128/136 [===========================>..] - ETA: 0s - loss: 8162.1069 - accuracy: 0.8878roc-auc_val: 0.8027\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 8078.9419 - accuracy: 0.8886 - val_loss: 23850.5977 - val_accuracy: 0.8142\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7898.2012 - accuracy: 0.8922 - val_loss: 23634.1914 - val_accuracy: 0.7905\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7944.9692 - accuracy: 0.8901 - val_loss: 23747.4922 - val_accuracy: 0.8049\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8001.8403 - accuracy: 0.8901 - val_loss: 23571.9590 - val_accuracy: 0.7836\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7908.6978 - accuracy: 0.8919 - val_loss: 24123.9512 - val_accuracy: 0.8079\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7812.4116 - accuracy: 0.8911 - val_loss: 23915.6992 - val_accuracy: 0.7949\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7648.8906 - accuracy: 0.8964 - val_loss: 23914.8770 - val_accuracy: 0.7992\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7725.6021 - accuracy: 0.8914 - val_loss: 24020.4414 - val_accuracy: 0.8115\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7658.0537 - accuracy: 0.8928 - val_loss: 23850.3418 - val_accuracy: 0.7998\n",
            "Epoch 59/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7645.0679 - accuracy: 0.8916 - val_loss: 24004.9121 - val_accuracy: 0.8048\n",
            "Epoch 60/1000\n",
            "127/136 [===========================>..] - ETA: 0s - loss: 7576.8813 - accuracy: 0.8931roc-auc_val: 0.8029\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 7607.4106 - accuracy: 0.8934 - val_loss: 23795.0566 - val_accuracy: 0.7875\n",
            "Epoch 61/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7497.9131 - accuracy: 0.8922 - val_loss: 24115.4277 - val_accuracy: 0.8125\n",
            "Epoch 62/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7456.0059 - accuracy: 0.8950 - val_loss: 23862.1719 - val_accuracy: 0.7958\n",
            "Epoch 63/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7246.4453 - accuracy: 0.8973 - val_loss: 23862.3555 - val_accuracy: 0.8000\n",
            "Epoch 64/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7445.1787 - accuracy: 0.8952 - val_loss: 24132.6211 - val_accuracy: 0.7945\n",
            "Epoch 65/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7362.0532 - accuracy: 0.8963 - val_loss: 24065.8320 - val_accuracy: 0.8075\n",
            "Epoch 66/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7290.1689 - accuracy: 0.8975 - val_loss: 23995.2363 - val_accuracy: 0.8117\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 17ms/step - loss: 29029.6973 - accuracy: 0.6620 - val_loss: 27236.3633 - val_accuracy: 0.6945\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 22933.2871 - accuracy: 0.7256 - val_loss: 26358.0957 - val_accuracy: 0.6716\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 20511.0957 - accuracy: 0.7507 - val_loss: 25747.6250 - val_accuracy: 0.7132\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 18881.9434 - accuracy: 0.7657 - val_loss: 25414.6445 - val_accuracy: 0.7494\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 17508.2910 - accuracy: 0.7850 - val_loss: 25172.6719 - val_accuracy: 0.7489\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 16535.1191 - accuracy: 0.7953 - val_loss: 24326.6719 - val_accuracy: 0.7797\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 15394.5635 - accuracy: 0.8047 - val_loss: 24600.5820 - val_accuracy: 0.7915\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 14652.8311 - accuracy: 0.8138 - val_loss: 24576.1504 - val_accuracy: 0.8101\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 14210.6484 - accuracy: 0.8137 - val_loss: 24425.1289 - val_accuracy: 0.8153\n",
            "Epoch 10/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 13205.6309 - accuracy: 0.8315roc-auc_val: 0.8128\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 13247.3896 - accuracy: 0.8310 - val_loss: 24441.4258 - val_accuracy: 0.8138\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12744.9082 - accuracy: 0.8341 - val_loss: 24139.0449 - val_accuracy: 0.8213\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12302.3350 - accuracy: 0.8373 - val_loss: 24348.3184 - val_accuracy: 0.8473\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11950.4375 - accuracy: 0.8404 - val_loss: 24834.7949 - val_accuracy: 0.8416\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11572.7637 - accuracy: 0.8510 - val_loss: 24113.9551 - val_accuracy: 0.8374\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11366.2324 - accuracy: 0.8484 - val_loss: 24259.8223 - val_accuracy: 0.8451\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11094.7012 - accuracy: 0.8526 - val_loss: 24361.4512 - val_accuracy: 0.8323\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10730.5176 - accuracy: 0.8541 - val_loss: 24924.8398 - val_accuracy: 0.8555\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10581.9551 - accuracy: 0.8580 - val_loss: 25300.9824 - val_accuracy: 0.8655\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10078.4541 - accuracy: 0.8663 - val_loss: 24831.3203 - val_accuracy: 0.8769\n",
            "Epoch 20/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 9932.7363 - accuracy: 0.8621roc-auc_val: 0.8135\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 9979.4697 - accuracy: 0.8632 - val_loss: 25227.2539 - val_accuracy: 0.8786\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9695.8955 - accuracy: 0.8670 - val_loss: 24690.5645 - val_accuracy: 0.8754\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9591.0781 - accuracy: 0.8691 - val_loss: 24574.9883 - val_accuracy: 0.8696\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9430.9580 - accuracy: 0.8664 - val_loss: 24821.8887 - val_accuracy: 0.8760\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9404.3740 - accuracy: 0.8713 - val_loss: 25018.6328 - val_accuracy: 0.8679\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8981.0947 - accuracy: 0.8733 - val_loss: 24970.7148 - val_accuracy: 0.8730\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8825.4453 - accuracy: 0.8720 - val_loss: 25060.8926 - val_accuracy: 0.8583\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8600.1191 - accuracy: 0.8753 - val_loss: 25187.2754 - val_accuracy: 0.8747\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8468.9434 - accuracy: 0.8745 - val_loss: 24779.6816 - val_accuracy: 0.8648\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8410.2510 - accuracy: 0.8784 - val_loss: 24911.1777 - val_accuracy: 0.8793\n",
            "Epoch 30/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 8360.9316 - accuracy: 0.8797roc-auc_val: 0.811\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 8325.0762 - accuracy: 0.8794 - val_loss: 25205.8516 - val_accuracy: 0.8794\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8111.6357 - accuracy: 0.8796 - val_loss: 24949.2930 - val_accuracy: 0.8717\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7977.3730 - accuracy: 0.8801 - val_loss: 25534.8438 - val_accuracy: 0.8757\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7892.0996 - accuracy: 0.8825 - val_loss: 25427.5586 - val_accuracy: 0.8799\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7811.2510 - accuracy: 0.8862 - val_loss: 25645.2695 - val_accuracy: 0.8826\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7686.0352 - accuracy: 0.8827 - val_loss: 25679.8281 - val_accuracy: 0.8803\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7523.5786 - accuracy: 0.8860 - val_loss: 25547.5605 - val_accuracy: 0.8751\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7571.3516 - accuracy: 0.8877 - val_loss: 26332.0469 - val_accuracy: 0.8812\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7393.5293 - accuracy: 0.8873 - val_loss: 25634.0957 - val_accuracy: 0.8774\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7265.1538 - accuracy: 0.8912 - val_loss: 25665.0254 - val_accuracy: 0.8802\n",
            "Epoch 40/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 7141.6797 - accuracy: 0.8899roc-auc_val: 0.8044\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 7229.1387 - accuracy: 0.8903 - val_loss: 26246.8750 - val_accuracy: 0.8917\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7137.7710 - accuracy: 0.8904 - val_loss: 26230.8750 - val_accuracy: 0.8892\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7088.9775 - accuracy: 0.8953 - val_loss: 26458.4043 - val_accuracy: 0.8925\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6928.3325 - accuracy: 0.8929 - val_loss: 26174.6152 - val_accuracy: 0.8816\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6799.7222 - accuracy: 0.8946 - val_loss: 25668.7344 - val_accuracy: 0.8660\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6866.8389 - accuracy: 0.8937 - val_loss: 26244.1113 - val_accuracy: 0.8796\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6753.6919 - accuracy: 0.8933 - val_loss: 26143.9199 - val_accuracy: 0.8787\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6736.6084 - accuracy: 0.8946 - val_loss: 26169.3242 - val_accuracy: 0.8793\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6683.3901 - accuracy: 0.8954 - val_loss: 26239.1426 - val_accuracy: 0.8749\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6697.6816 - accuracy: 0.8962 - val_loss: 26744.9766 - val_accuracy: 0.8747\n",
            "Epoch 50/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 6464.5840 - accuracy: 0.8980roc-auc_val: 0.7991\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 6503.5527 - accuracy: 0.8982 - val_loss: 26672.9062 - val_accuracy: 0.8826\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6444.9360 - accuracy: 0.8973 - val_loss: 26964.6426 - val_accuracy: 0.8966\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6315.0054 - accuracy: 0.9012 - val_loss: 26924.0039 - val_accuracy: 0.8930\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6396.6880 - accuracy: 0.8976 - val_loss: 26717.9453 - val_accuracy: 0.8849\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6369.3501 - accuracy: 0.8996 - val_loss: 26707.9453 - val_accuracy: 0.8941\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6413.7988 - accuracy: 0.9024 - val_loss: 26636.6758 - val_accuracy: 0.8879\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6211.7461 - accuracy: 0.9004 - val_loss: 26796.0215 - val_accuracy: 0.8919\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6167.4512 - accuracy: 0.9019 - val_loss: 26824.9863 - val_accuracy: 0.8837\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6037.7935 - accuracy: 0.9069 - val_loss: 26791.4492 - val_accuracy: 0.8848\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6050.8564 - accuracy: 0.9062 - val_loss: 27181.5469 - val_accuracy: 0.8924\n",
            "Epoch 60/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 6181.1279 - accuracy: 0.9029roc-auc_val: 0.7986\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 6095.0815 - accuracy: 0.9024 - val_loss: 26827.5547 - val_accuracy: 0.8798\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5979.2891 - accuracy: 0.9000 - val_loss: 26675.8848 - val_accuracy: 0.8669\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5913.5200 - accuracy: 0.9048 - val_loss: 27218.2070 - val_accuracy: 0.8827\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5757.4819 - accuracy: 0.9059 - val_loss: 26787.8398 - val_accuracy: 0.8749\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5799.0928 - accuracy: 0.9057 - val_loss: 27385.8574 - val_accuracy: 0.8884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5PePWLxWIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cfc2124-fe74-4a8b-9f2f-79e3ccb97b46"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAe7UlEQVR4nO3deZRcZ3nn8e9TW++bututllqbjex4lTA9tgFDzObYiscmhAQbmABjopjtDDMc5pCTSeAkc2aY4RDOEJN4BPYYAjgmCXZ8gjF4GAfFu9u2ZGRjy9rVra1bvan3rqpn/qjbcqnVLZW6qpe69fucU+5b975173Ml+Ve333rfW+buiIhIeEUWuwAREZlfCnoRkZBT0IuIhJyCXkQk5BT0IiIhF1vsAmbS1NTka9euXewyRESKxvPPP9/j7s0zbVuSQb927Vo6OjoWuwwRkaJhZvtn26auGxGRkFPQi4iEnIJeRCTkFPQiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJyCnoRkZBbkjNjF8sPnzkw4/oPX716gSsRESkcXdGLiIScgl5EJOQU9CIiIaegFxEJubN+GGtm9wA3Acfc/bJg3f3ARUGTeqDf3TfO8Np9wAkgBSTdvb1AdYuISI5yGXVzL3An8L2pFe7+oallM/s6MHCG17/L3XvmWqCIiOTnrEHv7lvNbO1M28zMgN8H3l3YskREpFDy7aN/B3DU3V+fZbsDPzez581sc57HEhGROch3wtRtwH1n2H6tu3eZ2XnAo2b2qrtvnalh8EawGWD1ak1QEhEplDlf0ZtZDPgAcP9sbdy9K/h5DHgAuOoMbbe4e7u7tzc3z/j9tiIiMgf5dN28F3jV3Ttn2mhmVWZWM7UMXA/syON4IiIyB2cNejO7D3gKuMjMOs3s9mDTrUzrtjGzFWb2cPC0BXjczLYDzwI/cfdHCle6iIjkIpdRN7fNsv7jM6w7BGwKlvcAG/KsT0RE8qSZsSIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iEnIJeRCTkFPQiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJyCnoRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQu6sQW9m95jZMTPbkbXuK2bWZWbbgsemWV57g5m9Zma7zOxLhSxcRERyk8sV/b3ADTOs/4a7bwweD0/faGZR4FvAjcAlwG1mdkk+xYqIyLk7a9C7+1agdw77vgrY5e573H0C+DvgljnsR0RE8pBPH/1nzeyloGunYYbtK4GDWc87g3UzMrPNZtZhZh3d3d15lCUiItnmGvR/A1wAbAQOA1/PtxB33+Lu7e7e3tzcnO/uREQkMKegd/ej7p5y9zTwbTLdNNN1AauynrcF60REZAHNKejNrDXr6e8AO2Zo9hyw3szWmVkCuBV4aC7HExGRuYudrYGZ3QdcBzSZWSfwZeA6M9sIOLAP+KOg7QrgO+6+yd2TZvZZ4GdAFLjH3V+el7MQEZFZnTXo3f22GVbfPUvbQ8CmrOcPA6cNvRQRkYWjmbEiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJyCnoRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjInTXozeweMztmZjuy1n3NzF41s5fM7AEzq5/ltfvM7Fdmts3MOgpZuIiI5CaXK/p7gRumrXsUuMzdrwB2An98hte/y903unv73EoUEZF8nDXo3X0r0Dtt3c/dPRk8fRpom4faRESkAArRR//vgZ/Oss2Bn5vZ82a2+Uw7MbPNZtZhZh3d3d0FKEtERCDPoDezPwGSwA9maXKtu18J3Ah8xszeOdu+3H2Lu7e7e3tzc3M+ZYmISJY5B72ZfRy4CfiIu/tMbdy9K/h5DHgAuGquxxMRkbmZU9Cb2Q3AfwZudveRWdpUmVnN1DJwPbBjprYiIjJ/chleeR/wFHCRmXWa2e3AnUAN8GgwdPKuoO0KM3s4eGkL8LiZbQeeBX7i7o/My1mIiMisYmdr4O63zbD67lnaHgI2Bct7gA15VSciInnTzFgRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iEnIJeRCTkcgp6M7vHzI6Z2Y6sdcvM7FEzez342TDLaz8WtHndzD5WqMJFRCQ3uV7R3wvcMG3dl4BfuPt64BfB81OY2TLgy8DVwFXAl2d7QxARkfmRU9C7+1agd9rqW4DvBsvfBd4/w0t/C3jU3XvdvQ94lNPfMEREZB7l00ff4u6Hg+UjQMsMbVYCB7OedwbrRERkgRTkw1h3d8Dz2YeZbTazDjPr6O7uLkRZIiJCfkF/1MxaAYKfx2Zo0wWsynreFqw7jbtvcfd2d29vbm7OoywREcmWT9A/BEyNovkY8E8ztPkZcL2ZNQQfwl4frBMRkQWS6/DK+4CngIvMrNPMbge+CrzPzF4H3hs8x8zazew7AO7eC/wF8Fzw+PNgnYiILJBYLo3c/bZZNr1nhrYdwCeznt8D3DOn6kREJG+aGSsiEnIKehGRkFPQi4iEnIJeRCTkFPQiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJyCnoRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgr6LKm08/iuHsYmU4tdiohIwSjos7x+7AQP/+owj+/qWexSREQKRkGfZefRIQCe29tLKu2LXI2ISGHMOejN7CIz25b1GDSzz09rc52ZDWS1+bP8S54/rx89QVVZjBPjSV4+NLDY5YiIFERsri9099eAjQBmFgW6gAdmaPqv7n7TXI+zUA4cH+H48AS/fXkrT+7u4Zm9vVzRVr/YZYmI5K1QXTfvAXa7+/4C7W/B/fL1bgAuaqnh6nWN7O0Z5ujg2CJXJSKSv0IF/a3AfbNse6uZbTezn5rZpbPtwMw2m1mHmXV0d3cXqKzc/fK1bhoq4zRWJ3jLmgZiEeOZvb0LXoeISKHlHfRmlgBuBv5+hs0vAGvcfQPwV8CDs+3H3be4e7u7tzc3N+db1jmZSKZ5ancP61tqMDOqymKsa6riwPHhBa1DRGQ+FOKK/kbgBXc/On2Duw+6+1Cw/DAQN7OmAhyzoJ7f38fwRIoLz6s5ua65poyeoQncNfpGRIpbIYL+NmbptjGz5WZmwfJVwfGOF+CYBfXErh5iEeP85qqT65qqy5hIpRkcSy5iZSIi+ZvzqBsAM6sC3gf8Uda6OwDc/S7gg8CnzCwJjAK3+hK8RN7TM8TqxkrK49GT65prygDoPjG+WGWJiBREXkHv7sNA47R1d2Ut3wncmc8xFkJX3ygr6ytOWddUnQn6niEFvYgUN82MBbr6R2lrODXoa8tjJGIRXdGLSNEr+aAfm0zRMzRx2hW9mdFUndAVvYgUvZIP+kP9owCsmBb0kOm+6VbQi0iRK/mg7wqCfvoVPWQ+kB0YmdRti0WkqCno+4Kgb5gh6KvLcGBvjyZOiUjxKvmgP9Q/SsRgeW35adumRt7s6VbQi0jxKvmg7+wfZXltObHo6X8UbwT90EKXJSJSMCUf9F19ozN22wAkYhHqKuLsUdeNiBQxBX3/6ZOlsjVXl+mKXkSKWkkHfSrtHBkYm3Fo5ZSmmgR7uod1czMRKVolHfTHToyRTPusXTeQ6ac/MZ7UDFkRKVolHfQnh1ae4Yq+sSrzgeyB3pEFqUlEpNBKO+iDyVLT73OTrbEqASjoRaR4KeiZ+fYHU+or45jB/uMKehEpTqUd9H2jNFTGqUzMfrfmWDRCa205B3VFLyJFqrSDvn/2MfTZVjdWsl9BLyJFqqSD/lD/KCvqcgj6ZZXqoxeRolWyQe/uZ5wVm21NYxXdJ8YZmdD3x4pI8SnZoB8aTzI8kaK17vSbmU23alklAAd7R+e7LBGRgss76M1sn5n9ysy2mVnHDNvNzL5pZrvM7CUzuzLfYxbC0cExAFpmuGvldGuCoN9/XPe8EZHik9eXg2d5l7v3zLLtRmB98Lga+Jvg56I6MpCZ6TrT7YmnWx0EvfrpRaQYLUTXzS3A9zzjaaDezFoX4LhndC5X9PWVcWrKYwp6ESlKhQh6B35uZs+b2eYZtq8EDmY97wzWncLMNptZh5l1dHd3F6CsMztyDkFvZhp5IyJFqxBBf627X0mmi+YzZvbOuezE3be4e7u7tzc3NxegrDM7OjhGbXmMikQ0p/YKehEpVnkHvbt3BT+PAQ8AV01r0gWsynreFqxbVEcHx1iew4ibKasbK+nsHSWV1u2KRaS45BX0ZlZlZjVTy8D1wI5pzR4C/iAYfXMNMODuh/M5biEcGRzPqdtmyupllUyk0if79kVEikW+o25agAfMbGpfP3T3R8zsDgB3vwt4GNgE7AJGgE/kecyCODowxvrzmnJuv2ZZFZC5udmZboImIrLU5BX07r4H2DDD+ruylh34TD7HKbRU2ukeGs9paOWU1ScnTY3w1gsa56s0EZGCK8mZsceHxkmlnZbaspxfs6K+nFjE2KtJUyJSZEoy6M9laOWUWDTCuqYqXj+qLwoXkeJSkkF/dDCYFXsOo24ALmyp4fVjJ+ajJBGReVOSQT+XK3qA9S3VHOgdYXQiNR9liYjMi5IM+qMDY0QMmqpz76OHzBW9O+w6pu4bESkepRn0g2M015QRjdg5ve7ClmoAdh5V942IFI+SDPojg2PnNLRyyprGKuJRY6f66UWkiJRk0B8dHDvn/nmAeDTC+U3VGnkjIkWlRIP+3G5/kO3C5TXquhGRolJyQT82mWJgdPKch1ZOufC8ajr7Rhke1/fHikhxKLmgn7op2Xk15zbiZsr6lhoAXtfIGxEpEiUX9EcGMkE/5yt6jbwRkSJTekE/x8lSU9Y0VpGIRXhdQS8iRaLkgr6zbxSAlXO81XA0YlzQXM1OjbwRkSJRgkE/wrKqBFVlc79D80Ut1bx6ZJDMHZhFRJa2kgv6g72jrGrI74tD3rKmgaOD4+w/ru+QFZGlr/SCvm+EtuBLRObqbW/KfDPVE7t7ClGSiMi8KqmgT6WdQ/2jrGrIL+jPb6pieW05T+46XqDKRETmT0kF/ZHBMSZTzqpl+XXdmBlve1MjT+7uIZ1WP72ILG1zDnozW2Vmj5nZK2b2spn9hxnaXGdmA2a2LXj8WX7l5udgb6ZPPd8reoC3X9BE38gkvz4ymPe+RETmUz5fDp4EvuDuL5hZDfC8mT3q7q9Ma/ev7n5THscpmKmgX32OffQ/fObAaeve/RvnAfDkruNcuqIu/+JERObJnK/o3f2wu78QLJ8Afg2sLFRh8+Fg3yhmsGKOY+izLa8r5/zmKn0gKyJLXkH66M1sLfBm4JkZNr/VzLab2U/N7NIz7GOzmXWYWUd3d3chyjpNZ+8IrbXlJGKF+Wji7Rc08ezeXiaS6YLsT0RkPuSdeGZWDfwj8Hl3n95h/QKwxt03AH8FPDjbftx9i7u3u3t7c3NzvmXNqBBDK7O9/U2NjEykeH5/X8H2KSJSaHkFvZnFyYT8D9z9x9O3u/uguw8Fyw8DcTNryueY+chMlipc0L9jfTO15TH+9ul9BduniEih5TPqxoC7gV+7+1/O0mZ50A4zuyo43qIMPh9Ppjh6YizvoZXZqspifOSaNTyy4wj7jw8XbL8iIoWUzxX924F/B7w7a/jkJjO7w8zuCNp8ENhhZtuBbwK3+iLdIKarbxT3wgytzPaJt60lGjHufnxvQfcrIlIocx5e6e6PA3aWNncCd871GIV0MLhr5aoC9tEDnFdbzvs3ruRHHQf5/HsvZFlVoqD7FxHJV8nMjD0wNVmqgF03U/7wneczNpnm3if3FXzfIiL5Kpmg7+wdIRGN0FIzty8cOZMLW2r47ctb+evHdtGxr7fg+xcRyUc+M2OLysG+EVY2VBCJnLG3KWfTZ8teubqBp/cc51M/eIF//ty1c/4GKxGRQiuZK/o93cOsaSxs/3y2ikSUj1yzhuHxJJ/6/vOcGJuct2OJiJyLkgj6kYkkO4+e4Iq2+nk9zvLacr7+exvY3jnA+7/1BHu69XWDIrL4SiLof9U5QNph46r5v/nYjZe38v3br6ZvZJJbvvUED77Ypa8cFJFFVRJBv72zH4AN83xFD5m++709w9x+7TrqKuJ8/v5t/N5dT/FSUIOIyEIrjaA/OMCqZRU0Vpct2DEbKhPc8ZsX8IE3r2RvzzA33/kEH7vnWZ7afVxX+CKyoEoi6Lcd7F+Qq/npIma0r13GY1+8ji/+1kW8fGiA2779NO/7xla2bN3N0cGxBa9JREpP6IP+2IkxuvpH2bhq4YN+yj9vP0xDZYLPvXs9H3jzSiaSaf7bw69yzX//BR/46ye465e72dE1oK8lFJF5Efpx9C8dHABY1KCfEo9GaF+7jPa1y+g+MU40Ao+8fISv/vRVAOor41zRVs8lrbVcuqKWS1bUsq6xqmBj/0WkNIU+6Lcd7CcasSX3dX/NNZnPCz581RoGRyfZ0zPE7u5hek6Mc/fuPUymMlf3FfEobQ0VtDVUsLKhgpX1layoL6epuozG6gTLqhIsq0wQixbfL2fuzvBEir7hCXqHJ+gdmaB/ZILe4Un6hicYmUgxnkwxnkwzkUxPW848n0immUw5k6k0k6k0yZPLTirtRCNGPGokYhHi0QhlsQh1lQkaKuM0VCZoqEzQWJ2gpbac5bXlLK8ro6W2nJry+GL/8cgSN9NXjAJ8+OrVC1zJ2YU+6Ld39nNRSw0ViehilzKr2oo4G1c1sHFVAwDJdJpjg+McHhjjyMAofSOTvHbkBE/v6WV0MjXjPuor4yyrStBUVcayqkx41VbEqUpEqUzEqC6LUVkWpSoRozIRpTweJRoxohEjFjEiUz/NiEUz66NmxCIRotHMcjRiRAySaWc8mQnWieDnZCoTvkNjSU6MJTkxPpn5OZZkcGySwdEkA6MT9A1P0jeSCfa+kYmTb2jTRSzzG1AsGiEW1BaPRjL1Ro14JBKEeITyeFZ9kTdqj5jh7iQ9E/qpdOZNYGQ8Sc+JcYYnkoxMpGb8hrBELEJbQwWtdeWsXlbFmsZK1jZWsnpZFWubKqlMhP5/HQmRUP9rTaed7Qf7uWnDisUu5ZzEIhFW1FcE323bcMq28ckU/aOTDE8kGR5PMTyeZHg8ydB4kuGJFN1D4+w7PszQeJLxyTSpJTDCJxGLUFsew7CTbzarl1VycWstlcEbUVUiSmVZ7OQbU1k8QsQWpstqIpnmxNgkA8Eb0uDoZPDmNMmB4yO8eKCfkYk33mDNYF1jFRevqOWS1szj0pW1nDcP91ESKYRQB/0rhwcZHEuycRFG3MyXsniUlnjuv50k05mr7qnujqmfyVSatEPaPXhklt2ddBpSU8sn27zRNhZcPUejkcxyNHPFHY0YZbEo5fEI5fHMbw3lsciS71ZKxCI0Vpedcfjt2GSK40EX07HBMQ4PjPHU7uP85KXDJ9u01pWzoa2eK1bVsbGtnsva6qhVF5AsAaEO+nuf3EdFPMr1l7YsdimLJhaJEEtEqNRt8vNSHo+ysr6ClfUVsPKNz3vGJlMcHhjjUP8oB/tGeG5fL4+8fOTk9vObq9jYVs8VbXVsWFXPxa21lJ/DG7VIIYQ26I8NjvFP27r48FWrqVfKyTwpj0dZ11TFuqaqk+tGJpJ09Y3S2T9KZ+8Ij75ylB+/2AVALGL8RmsNG9rq2dBWz+VtdaxrqlL4y7wKbdB/76n9JNPOJ96+brFLkRJTmYixvqWG9S01QGZ00eBYks6+ETr7RunsG+EfX+jkB8GoDQNW1FdwfnPmDeP8pirWNVezrrGK1vpy4ku866tUuTudfaPs6h6iq2+UvpEJasvj7Dx6gusuauad65uXzNDoUAb9yESS7z+zn+svaWFt1pWWyGIwM+oq4tRV1J0c5pt25/jQBIcGRukZGuf40AR7uod5dm8v41mjgMzgvJoyWusqWFFfTmtdZiTQ1If1K+oyQ22XSqCUguHxJA9u6+LOx3ZxeCAzu72pOkFjVRkDo5P8qOMg9z65j/ObqvjEtev4UPsqErHFfbMOZdB/e+te+kcm+eQ7zl/sUkRmFDGjuabs5HyKKe7O0HiSnqEJjg+N0z86ycDIJAOjkzy7t4+B0aOnDUmNRiwztLY6s7+m6gTN1WUn51rUVcSprYhTWx4PlmNUxKPYAo1qCoN02tne2c+PX+jigRe7GBpP0lpXzi0bV3DFyvpThm9/8C1tPPyrw/yfJ/bypw/uYMvW3XzhfRfxbzesILpIb8h5Bb2Z3QD8LyAKfMfdvzptexnwPeAtwHHgQ+6+L59jnkkylea//uTX3PvkPm64dDntaxrO/iKRJcTMqCmPU1MeP6Xff4q7MzqRGWI7EDwGxyYZGssMsd3TPcRLBzPLyTPcUiMezRynIh4NhrhmRklNDXctz1pfFo9SFouQiEZIxIJH9nIsQtm059nby6LRk8uLFXS5mupm6xka53D/GK8eGeSVw4Ns3dlDz9A4iViEm65o5aPXrOHXhwZnfLNMxCK8/80ruWXjCv5lZzf/85HX+Pz92/jG/93J7deu43evbKOqbGGvsW2ud1I0syiwE3gf0Ak8B9zm7q9ktfk0cIW732FmtwK/4+4fOtu+29vbvaOj45zqGRpP8ukfvMDWnd188tp1/PGmi8/5H9VsM91Eio17ZlLb8HiS0ckUo5MpxibTjE6kGAuej06mmEymmUhNTX7LTCibSKVPWT/bpLa5iBjBxLvMY2oS3tTkvTfWWda6U18TiYB75gHgwfm+ce5T633GNn7yP5lhxGPBn83UzOvpkdhUXUZrXTkXt9bmNPly+szYdNp55OUjbNm6h20H+0lEI/ybdQ287YIm1jVV0dZQQXk8StqdqNnJz3bOlZk97+7tM23L523lKmCXu+8JDvJ3wC3AK1ltbgG+Eiz/A3CnmZnPw316E9EI7s5XP3A5t1619KYgiywkMzs5lyFf7k7KnVTKSaYzj1TaSabSbyynnWQ6TSrlTKadVDpzO4rTtqcz4ZuZs8EpczV8+k/emMPhWetTycxnF5D5IHvqfE9e1tnUdjulDdltMk8xIF5dRiwaIR4xYtEI5fEINeUxasrjtNSWU53n1XckYmy6vJUbL1vOCwf6+fnLR/jlzm6+9rPXTmvbVF1Gx395b17Hm0k+Z7ASOJj1vBO4erY27p40swGgEeiZvjMz2wxsDp4Omdnpfwo5+D5w21xemNE0U20hoXMrPmE9LwjxuX0kj3PbD9ifzvnQa2bbsGQ+jHX3LcCWxazBzDpm+9Wn2Oncik9Yzwt0bgstnzE/XcCqrOdtwboZ25hZDKgj86GsiIgskHyC/jlgvZmtM7MEcCvw0LQ2DwEfC5Y/CPy/+eifFxGR2c256yboc/8s8DMywyvvcfeXzezPgQ53fwi4G/hbM9sF9JJ5M1jKFrXraJ7p3IpPWM8LdG4Las7DK0VEpDjoJhoiIiGnoBcRCbmSDHozu8HMXjOzXWb2pRm2l5nZ/cH2Z8xs7cJXOTc5nNt/MrNXzOwlM/uFmc069nYpOdt5ZbX7XTNzM1tSw9vOJJdzM7PfD/7eXjazHy50jXOVw7/H1Wb2mJm9GPyb3LQYdZ4rM7vHzI6Z2Y5ZtpuZfTM475fM7MqFrvEUHnyTUKk8yHxwvBs4H0gA24FLprX5NHBXsHwrcP9i113Ac3sXUBksf6oYzi2X8wra1QBbgaeB9sWuu4B/Z+uBF4GG4Pl5i113Ac9tC/CpYPkSYN9i153jub0TuBLYMcv2TcBPyUy+vQZ4ZjHrLcUr+pO3bnD3CWDq1g3ZbgG+Gyz/A/AeK45b/Z313Nz9MXcfCZ4+TWb+w1KXy98ZwF8A/wMYW8ji8pTLuf0h8C137wNw92MLXONc5XJuDtQGy3XAoQWsb87cfSuZkYSzuQX4nmc8DdSbWevCVHe6Ugz6mW7dsHK2Nu6eBKZu3bDU5XJu2W4nc9Wx1J31vIJfjVe5+08WsrACyOXv7ELgQjN7wsyeDu4aWwxyObevAB81s07gYeBzC1PavDvX/xfn1ZK5BYIsLDP7KNAO/OZi15IvM4sAfwl8fJFLmS8xMt0315H5DWyrmV3u7v2LWlVh3Abc6+5fN7O3kpl3c5m7p8/2QsldKV7Rh/nWDbmcG2b2XuBPgJvdfXyBasvH2c6rBrgM+Bcz20emT/ShIvlANpe/s07gIXefdPe9ZG4Pvn6B6stHLud2O/AjAHd/Cignc1OwYpfT/4sLpRSDPsy3bjjruZnZm4H/TSbki6Wv94zn5e4D7t7k7mvdfS2Zzx5udvdz+1KDxZHLv8cHyVzNY2ZNZLpy9ixkkXOUy7kdAN4DYGYXkwn67gWtcn48BPxBMPrmGmDA3Q8vVjEl13Xj4bx1A5DzuX0NqAb+Pvh8+YC737xoRecgx/MqSjme28+A683sFSAFfNHdl/xvmDme2xeAb5vZfyTzwezHi+GiyszuI/Pm2xR8vvBlIA7g7neR+bxhE7ALGAE+sTiVZugWCCIiIVeKXTciIiVFQS8iEnIKehGRkFPQi4iEnIJeRCTkFPQiIiGnoBcRCbn/D2NMvotNeLFSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNDmjro-2ndD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
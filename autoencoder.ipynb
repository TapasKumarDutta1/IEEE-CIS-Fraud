{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "trn_trans=pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\n",
    "trn=pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\n",
    "tst_trans=pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')\n",
    "tst=pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')\n",
    "fnl_trn=trn.merge(trn_trans,on='TransactionID',how='left')\n",
    "fnl_tst=tst.merge(tst_trans,on='TransactionID',how='left')\n",
    "dk={}\n",
    "for i in range(1,39):\n",
    "    if i<10:\n",
    "        dk['id-0'+str(i)]='id_0'+str(i)\n",
    "    else:\n",
    "        dk['id-'+str(i)]='id_'+str(i)\n",
    "fnl_tst=fnl_tst.rename(columns=dk)\n",
    "import gc\n",
    "del([trn_trans,trn,tst_trans,tst])\n",
    "gc.collect()\n",
    "fnl=pd.concat([fnl_trn,fnl_tst],0)\n",
    "del([fnl_trn,fnl_tst])\n",
    "gc.collect()\n",
    "ids=list(fnl.filter(regex='id_'))\n",
    "fnl[ids]=fnl[ids].fillna(-1)\n",
    "fnl[['addr1','addr2']]=fnl[['addr1','addr2']].fillna(-1)\n",
    "ids=list(fnl.filter(regex='card'))\n",
    "fnl[ids]=fnl[ids].fillna(-1)\n",
    "ids=list(fnl.filter(regex='V'))\n",
    "fnl[ids]=fnl[ids].fillna(-1)\n",
    "fnl[['addr1','addr2','DeviceInfo','DeviceType','P_emaildomain','R_emaildomain']]=fnl[['addr1','addr2','DeviceInfo','DeviceType','P_emaildomain','R_emaildomain']].fillna('nan')\n",
    "ids=list(fnl.filter(regex='D'))\n",
    "fnl[ids]=fnl[ids].fillna(-1)\n",
    "ids=list(fnl.filter(regex='C'))\n",
    "fnl[ids]=fnl[ids].fillna(0)\n",
    "ids=list(fnl.filter(regex='M'))\n",
    "fnl[ids]=fnl[ids].fillna(-1)\n",
    "fnl[['dist1','dist2']]=fnl[['dist1','dist2']].fillna(-1)\n",
    "import re\n",
    "def number(data):\n",
    "  try:\n",
    "      return  ' '.join(re.findall(r'\\d+',data)) \n",
    "  except:\n",
    "    return -1\n",
    "def charac(data):\n",
    "    try:\n",
    "      return ''.join(re.findall(\"[a-zA-Z]+\", data))\n",
    "    except:\n",
    "        return -1\n",
    "fnl['DeviceInfo_char']=list(map(charac,fnl['DeviceInfo'].fillna('nan')))\n",
    "fnl['DeviceInfo_number']=list(map(number,fnl['DeviceInfo'].fillna('nan')))\n",
    "\n",
    "\n",
    "fnl['DeviceType_char']=list(map(charac,fnl['DeviceType'].fillna('nan')))\n",
    "fnl['DeviceType_number']=list(map(number,fnl['DeviceType'].fillna('nan')))\n",
    "\n",
    "\n",
    "fnl['os_char']=list(map(charac,fnl['id_30'].fillna('nan')))\n",
    "fnl['os_number']=list(map(number,fnl['id_30'].fillna('nan')))\n",
    "\n",
    "\n",
    "fnl['browse_char']=list(map(charac,fnl['id_31'].fillna('nan')))\n",
    "fnl['browse_number']=list(map(number,fnl['id_31'].fillna('nan')))\n",
    "\n",
    "\n",
    "fnl['P_1']=fnl['P_emaildomain'].str.split('.').str[0]\n",
    "fnl['P_2']=fnl['P_emaildomain'].str.split('.').str[1]\n",
    "\n",
    "\n",
    "fnl['R_1']=fnl['R_emaildomain'].str.split('.').str[0]\n",
    "fnl['R_2']=fnl['R_emaildomain'].str.split('.').str[1]\n",
    "z=fnl.loc[~(fnl['isFraud'].isna())]\n",
    "from tqdm import tqdm\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "cols=list(fnl.select_dtypes(include=object))\n",
    "for col in tqdm(cols):\n",
    "    cbe=CatBoostEncoder(sigma=0.01)\n",
    "    cbe.fit(z[col],z['isFraud'])\n",
    "    fnl[col]=cbe.transform(fnl[col],fnl['isFraud'])\n",
    "del(z)\n",
    "gc.collect()\n",
    "fnl.select_dtypes(include=object)\n",
    "# used=['card'+str(i) for i in range(1,7)]\n",
    "# used+=['C'+str(i) for i in range(1,15)]\n",
    "# used+=['D'+str(i) for i in range(1,16)]\n",
    "#fnl[used]=fnl[used].fillna(0)\n",
    "fnl['day']=fnl['TransactionDT']//86400\n",
    "fnl['month']=fnl['day']//30\n",
    "fnl['week']=fnl['day']//7\n",
    "# fnl[used]=fnl[used].fillna('nan')\n",
    "# cats=['ProductCD','DeviceInfo_char','DeviceInfo_number','DeviceType_char','DeviceType_number','os_char','os_number','browse_char','browse_number','P_1','P_2','addr1']\n",
    "# num=fnl\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col]=df[col].fillna(-1)\n",
    "        df[col]=df[col].replace([np.inf,-np.inf],-1)\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "fnl=fnl.drop(['isFraud'],1)\n",
    "fnl=reduce_mem_usage(fnl)\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "ss=StandardScaler()\n",
    "fnl=pd.DataFrame(ss.fit_transform(fnl))\n",
    "mms=MinMaxScaler()\n",
    "fnl=pd.DataFrame(ss.fit_transform(fnl))\n",
    "fnl=reduce_mem_usage(fnl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "gc.collect()\n",
    "import numpy as np\n",
    "import keras \n",
    "from math import ceil\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import Sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Concatenate, Dropout,BatchNormalization\n",
    "from keras.layers import Add\n",
    "import keras.backend as K\n",
    "class DAESequence(Sequence):\n",
    "    def __init__(self,df,batch_size):\n",
    "        self.batch_size=batch_size\n",
    "        self.len_data=df.shape[0]\n",
    "        self.columns=df.shape[1]\n",
    "        self.data=df.values\n",
    "        self.idx=[]\n",
    "    def __len__(self):\n",
    "        return int(ceil(self.len_data/self.batch_size))\n",
    "    def __getitem__(self,idx):\n",
    "        self.idx.append(idx)\n",
    "        last=min((idx+1)*self.batch_size,self.len_data)\n",
    "        idx=idx*self.batch_size\n",
    "        size=last-idx\n",
    "        output_x=self.data[idx:last]\n",
    "        length=last-idx\n",
    "        rnd=np.random.randint(0,self.len_data-size,size)\n",
    "        rnd[rnd>idx]+=size\n",
    "        cols=np.random.randint(0,self.columns,int(self.columns*0.1))\n",
    "        noise_x=output_x.copy()\n",
    "        noise_x[:,cols]=self.data[rnd[:,None],cols]\n",
    "        return noise_x,output_x\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.layers import BatchNormalization\n",
    "#fnl=pd.concat([fnl_trn,fnl_tst],0).select_dtypes(exclude='uint8')\n",
    "batch_size=1024\n",
    "checkpoint_path = \"training_1.hdf5\" \n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback =ModelCheckpoint(checkpoint_path, \n",
    "monitor='loss',save_best_only=True,save_weights_only=True,verbose=1)\n",
    "len_input_columns=fnl.shape[1]\n",
    "kernel_initializer_1=keras.initializers.RandomNormal(mean=-4.74564e-04, stddev=0.031814238148788886, seed=None)\n",
    "kernel_initializer_0=keras.initializers.RandomNormal(mean=-4.74564e-04, stddev=0.04499212706658476, seed=None)\n",
    "kernel_initializer_2=keras.initializers.RandomNormal(mean=-4.74564e-04, stddev=0.031814238148788886, seed=None)\n",
    "model_dae = Sequential()\n",
    "model_dae.add(Dense(units=len_input_columns,input_shape=(len_input_columns,), activation='relu', dtype='float32', name='Input'))\n",
    "model_dae.add(Dense(units=int(300), activation='relu', dtype='float32',name='Hidden_1', kernel_initializer=kernel_initializer_0))\n",
    "model_dae.add(BatchNormalization())\n",
    "model_dae.add(Dense(units=int(200), activation='relu', dtype='float32',name='Hidden_2', kernel_initializer=kernel_initializer_1))\n",
    "model_dae.add(BatchNormalization())\n",
    "model_dae.add(Dense(units=int(100), activation='relu', dtype='float32',name='Hidden_3', kernel_initializer=kernel_initializer_2))\n",
    "model_dae.add(BatchNormalization())\n",
    "model_dae.add(Dense(units=int(25), activation='relu', dtype='float32',name='Hidden_4', kernel_initializer=kernel_initializer_2))\n",
    "model_dae.add(BatchNormalization())\n",
    "model_dae.add(Dense(units=int(2), activation='relu', dtype='float32',name='Hidden_5', kernel_initializer=kernel_initializer_2))\n",
    "model_dae.add(BatchNormalization())\n",
    "model_dae.add(Dense(units=int(25), activation='relu', dtype='float32',name='Hidden_6', kernel_initializer=kernel_initializer_2))\n",
    "model_dae.add(BatchNormalization())\n",
    "model_dae.add(Dense(units=int(100), activation='relu', dtype='float32',name='Hidden_7', kernel_initializer=kernel_initializer_2))\n",
    "model_dae.add(BatchNormalization())\n",
    "model_dae.add(Dense(units=int(200), activation='relu', dtype='float32',name='Hidden_8', kernel_initializer=kernel_initializer_2))\n",
    "model_dae.add(BatchNormalization())\n",
    "model_dae.add(Dense(units=int(300), activation='relu', dtype='float32',name='Hidden_9', kernel_initializer=kernel_initializer_2))\n",
    "model_dae.add(BatchNormalization())\n",
    "model_dae.add(Dense(units=len_input_columns, activation='linear', dtype='float32', name='Output'))\n",
    "model_opt = keras.optimizers.SGD(lr=0.001, decay=1-0.9, momentum=0.7, nesterov=False)\n",
    "model_dae.compile(loss='mean_squared_error',optimizer=model_opt)\n",
    "hist=model_dae.fit_generator(DAESequence(fnl, batch_size=2048),steps_per_epoch=int(ceil(fnl.shape[0]/batch_size)),epochs=1000,workers=2)\n",
    "model_dae.save_weights('autoenc.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

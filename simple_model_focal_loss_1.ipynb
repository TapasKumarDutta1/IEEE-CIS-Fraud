{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_1",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "8e356095-ecbc-4401-af4d-3e6bdb05bd7d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "f58b530c-0ba7-42f7-b5e8-1e5a8e754bdf"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train_identity.csv.zip to /content\n",
            "\r  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 110MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 94% 49.0M/52.2M [00:01<00:00, 45.9MB/s]\n",
            "100% 52.2M/52.2M [00:01<00:00, 45.0MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 98% 57.0M/58.3M [00:01<00:00, 46.2MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 43.2MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 220MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 159MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDrCIAqHzl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "f8f48dd4-f0f5-415e-e123-6596d22429e8"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWJ-hzcznam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5OQjb1zo6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "f99e1831-c65b-4eb5-f895-beec7d1559e7"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d49b3ba8-338b-45d0-e6a3-87a71a972be3"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0r3SuH1K97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQ20JqWATak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "171d02be-bd87-4973-b66e-454a3c239a01"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=0.2\n",
        "        alpha=0.936\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss\n",
        "dk={}\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate([(4,5),(3,4),(3,5)]):\n",
        "  train=trn.loc[trn['month']>=month[1]]\n",
        "  test=trn.loc[trn['month']<=month[0]]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.0001,decay=1e-3),loss=fl())\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 30ms/step - loss: 134.3649 - val_loss: 109.9633\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 122.2639 - val_loss: 107.0835\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 115.7023 - val_loss: 105.3010\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 111.5497 - val_loss: 103.8119\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 108.1074 - val_loss: 101.9235\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 105.7208 - val_loss: 101.0244\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 103.5981 - val_loss: 99.3398\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 101.4704 - val_loss: 98.7177\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 99.1990 - val_loss: 97.7610\n",
            "Epoch 10/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 98.6709roc-auc_val: 0.7798\n",
            "47/47 [==============================] - 17s 356ms/step - loss: 98.6210 - val_loss: 96.5220\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 95.7758 - val_loss: 95.8464\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 94.6831 - val_loss: 94.9956\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 93.8276 - val_loss: 94.4537\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 92.1009 - val_loss: 93.8127\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 91.4890 - val_loss: 93.5604\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 90.1916 - val_loss: 92.9409\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 89.0570 - val_loss: 92.5272\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 88.1569 - val_loss: 92.2860\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 87.8480 - val_loss: 91.6995\n",
            "Epoch 20/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 87.1191roc-auc_val: 0.7912\n",
            "47/47 [==============================] - 17s 362ms/step - loss: 86.6037 - val_loss: 91.4450\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 86.4109 - val_loss: 90.8588\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 85.8301 - val_loss: 90.6763\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 85.2027 - val_loss: 90.1538\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 84.7439 - val_loss: 89.9509\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 83.9567 - val_loss: 89.9727\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 83.5921 - val_loss: 89.5749\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 82.3327 - val_loss: 89.4163\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 82.1603 - val_loss: 89.4687\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 81.8182 - val_loss: 89.2596\n",
            "Epoch 30/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 81.4770roc-auc_val: 0.796\n",
            "47/47 [==============================] - 17s 356ms/step - loss: 81.1541 - val_loss: 89.1398\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 80.9509 - val_loss: 89.0529\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 80.2310 - val_loss: 88.7551\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 79.4813 - val_loss: 88.8391\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 79.5082 - val_loss: 88.7239\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 78.2584 - val_loss: 88.5570\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 79.8661 - val_loss: 88.4725\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 78.3957 - val_loss: 88.4379\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 79.1901 - val_loss: 88.4805\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 78.4780 - val_loss: 88.3824\n",
            "Epoch 40/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 77.6184roc-auc_val: 0.7993\n",
            "47/47 [==============================] - 17s 357ms/step - loss: 77.3326 - val_loss: 88.2732\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 76.1512 - val_loss: 88.1222\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 76.5760 - val_loss: 88.0168\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 76.2037 - val_loss: 88.0607\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 76.4906 - val_loss: 88.2241\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 75.0828 - val_loss: 88.2431\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 75.6289 - val_loss: 88.1736\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 75.3192 - val_loss: 88.1478\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 76.1398 - val_loss: 88.2515\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 74.8360 - val_loss: 88.1922\n",
            "Epoch 50/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 75.0275roc-auc_val: 0.8004\n",
            "47/47 [==============================] - 17s 358ms/step - loss: 74.7787 - val_loss: 88.1467\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 74.5909 - val_loss: 88.1298\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 73.4567 - val_loss: 88.0383\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 74.0382 - val_loss: 88.0823\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 74.0601 - val_loss: 88.1239\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 73.4307 - val_loss: 88.0935\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 73.9596 - val_loss: 88.1354\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 72.3053 - val_loss: 88.1235\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 73.1848 - val_loss: 88.1623\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 73.4171 - val_loss: 88.1458\n",
            "Epoch 60/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 72.6961roc-auc_val: 0.8018\n",
            "47/47 [==============================] - 17s 357ms/step - loss: 72.5596 - val_loss: 88.3673\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 72.5946 - val_loss: 88.2908\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 72.5732 - val_loss: 88.3362\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 71.5822 - val_loss: 88.3375\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 72.1468 - val_loss: 88.2869\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 72.6472 - val_loss: 88.3152\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 71.6736 - val_loss: 88.3150\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 71.5986 - val_loss: 88.3079\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 70.9069 - val_loss: 88.3624\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 71.2912 - val_loss: 88.3553\n",
            "Epoch 70/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 71.3745roc-auc_val: 0.803\n",
            "47/47 [==============================] - 17s 361ms/step - loss: 71.1657 - val_loss: 88.4294\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 70.9394 - val_loss: 88.5390\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 71.1628 - val_loss: 88.5274\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 70.5375 - val_loss: 88.5479\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 69.9232 - val_loss: 88.5547\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 70.5579 - val_loss: 88.6198\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 70.1035 - val_loss: 88.7152\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 70.3544 - val_loss: 88.7556\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 70.2938 - val_loss: 88.7367\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 70.2325 - val_loss: 88.8087\n",
            "Epoch 80/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 70.6121roc-auc_val: 0.8036\n",
            "47/47 [==============================] - 17s 356ms/step - loss: 70.0044 - val_loss: 88.8011\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 69.7290 - val_loss: 88.8264\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 69.3112 - val_loss: 88.9615\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 70.1600 - val_loss: 89.0285\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 69.2122 - val_loss: 89.0049\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 68.4003 - val_loss: 89.0182\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 68.7322 - val_loss: 89.1240\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 68.9058 - val_loss: 89.2085\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 69.5944 - val_loss: 89.2189\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 68.1137 - val_loss: 89.2728\n",
            "Epoch 90/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 68.8407roc-auc_val: 0.8034\n",
            "47/47 [==============================] - 17s 359ms/step - loss: 68.4317 - val_loss: 89.3423\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 68.2993 - val_loss: 89.3968\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 68.5727 - val_loss: 89.4308\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 18ms/step - loss: 131.2803 - val_loss: 117.8587\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 116.1179 - val_loss: 111.7433\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 109.9661 - val_loss: 108.4385\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 104.6231 - val_loss: 104.6688\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 101.1370 - val_loss: 102.2325\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 97.6462 - val_loss: 100.2816\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 95.4442 - val_loss: 99.1659\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 93.3511 - val_loss: 97.6996\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 92.5700 - val_loss: 96.4202\n",
            "Epoch 10/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 89.7449roc-auc_val: 0.7991\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 89.7449 - val_loss: 95.1782\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.1075 - val_loss: 94.9596\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.3231 - val_loss: 93.8339\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.1838 - val_loss: 92.7898\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.3456 - val_loss: 92.5658\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.4811 - val_loss: 92.3043\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.6068 - val_loss: 91.8376\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 83.3920 - val_loss: 90.6799\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 82.8386 - val_loss: 90.6599\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 82.1522 - val_loss: 90.2197\n",
            "Epoch 20/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 81.8540roc-auc_val: 0.8096\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 81.7430 - val_loss: 89.9139\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 81.0264 - val_loss: 89.7374\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 80.7901 - val_loss: 89.3710\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 79.9958 - val_loss: 89.2510\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 80.3682 - val_loss: 89.0242\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 79.6006 - val_loss: 88.8607\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 79.3128 - val_loss: 88.3798\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 78.8510 - val_loss: 88.4410\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 78.6275 - val_loss: 88.2042\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 78.5504 - val_loss: 88.1223\n",
            "Epoch 30/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 78.3523roc-auc_val: 0.8149\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 78.1974 - val_loss: 88.1211\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 77.8330 - val_loss: 87.8709\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 77.7363 - val_loss: 87.6451\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 77.2175 - val_loss: 87.6862\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 77.3755 - val_loss: 87.6256\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 77.0657 - val_loss: 87.4290\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 76.2114 - val_loss: 87.3265\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 76.2845 - val_loss: 87.2999\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 76.6992 - val_loss: 87.2323\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 75.6456 - val_loss: 87.1558\n",
            "Epoch 40/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 75.9421roc-auc_val: 0.8174\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 75.8001 - val_loss: 87.0234\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 74.9606 - val_loss: 87.0472\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 75.3804 - val_loss: 87.0666\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 74.9177 - val_loss: 86.8283\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 75.2469 - val_loss: 86.9052\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 74.4843 - val_loss: 86.7462\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 74.8181 - val_loss: 86.7320\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 74.3089 - val_loss: 86.7269\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 74.2499 - val_loss: 86.6500\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 74.6364 - val_loss: 86.6200\n",
            "Epoch 50/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 73.7251roc-auc_val: 0.819\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 73.6007 - val_loss: 86.4687\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 73.4669 - val_loss: 86.3806\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 73.5819 - val_loss: 86.2208\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 73.3378 - val_loss: 86.2281\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 73.4872 - val_loss: 86.1801\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 72.9114 - val_loss: 86.1495\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 72.9212 - val_loss: 86.1272\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 73.0764 - val_loss: 86.1516\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 73.0329 - val_loss: 86.0362\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 72.5744 - val_loss: 86.0400\n",
            "Epoch 60/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 72.3816roc-auc_val: 0.8203\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 72.3407 - val_loss: 85.9969\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 72.2574 - val_loss: 85.9982\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 72.7402 - val_loss: 86.0240\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 72.0366 - val_loss: 85.8674\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.9722 - val_loss: 85.8985\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.9270 - val_loss: 85.8451\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.6933 - val_loss: 85.7300\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 72.1439 - val_loss: 85.8062\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.2071 - val_loss: 85.7380\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.4866 - val_loss: 85.7200\n",
            "Epoch 70/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 71.0346roc-auc_val: 0.8214\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 70.8475 - val_loss: 85.6126\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.8918 - val_loss: 85.6666\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.3322 - val_loss: 85.5747\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.2550 - val_loss: 85.5613\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.2757 - val_loss: 85.5812\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 71.2673 - val_loss: 85.4949\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.8085 - val_loss: 85.4386\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.3596 - val_loss: 85.4206\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.5793 - val_loss: 85.3855\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.7391 - val_loss: 85.3208\n",
            "Epoch 80/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.7940roc-auc_val: 0.8219\n",
            "88/88 [==============================] - 14s 161ms/step - loss: 70.7940 - val_loss: 85.3517\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.0569 - val_loss: 85.3421\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.4292 - val_loss: 85.2392\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.6012 - val_loss: 85.2869\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.7449 - val_loss: 85.2507\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.2792 - val_loss: 85.1718\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.3433 - val_loss: 85.1543\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.8442 - val_loss: 85.1682\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.0417 - val_loss: 85.1456\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.8658 - val_loss: 85.1819\n",
            "Epoch 90/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3061roc-auc_val: 0.8226\n",
            "88/88 [==============================] - 14s 161ms/step - loss: 69.3061 - val_loss: 85.1722\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.3775 - val_loss: 85.0905\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.7397 - val_loss: 85.0452\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.6345 - val_loss: 85.0458\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.7409 - val_loss: 85.0448\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.4376 - val_loss: 85.0517\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 70.3215 - val_loss: 85.0489\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.7728 - val_loss: 85.0308\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.6408 - val_loss: 85.0914\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.2089 - val_loss: 85.0565\n",
            "Epoch 100/1000\n",
            "85/88 [===========================>..] - ETA: 0s - loss: 68.9422roc-auc_val: 0.8231\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 68.7483 - val_loss: 85.0066\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.1975 - val_loss: 85.0015\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.2319 - val_loss: 84.9585\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 69.2316 - val_loss: 84.9444\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.2263 - val_loss: 84.8956\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.6380 - val_loss: 84.9010\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.9347 - val_loss: 84.9209\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.4733 - val_loss: 84.9443\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.9511 - val_loss: 84.9618\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.3577 - val_loss: 84.9436\n",
            "Epoch 110/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 68.6640roc-auc_val: 0.8233\n",
            "88/88 [==============================] - 15s 173ms/step - loss: 68.4595 - val_loss: 84.9112\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.4187 - val_loss: 84.8527\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.2025 - val_loss: 84.8523\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.6199 - val_loss: 84.8962\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.2791 - val_loss: 84.8687\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.3439 - val_loss: 84.8447\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.4176 - val_loss: 84.8562\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.4177 - val_loss: 84.8623\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.8473 - val_loss: 84.8136\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.9684 - val_loss: 84.8407\n",
            "Epoch 120/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 67.6825roc-auc_val: 0.8237\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 67.6825 - val_loss: 84.8118\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.0689 - val_loss: 84.7836\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.3845 - val_loss: 84.7299\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.7841 - val_loss: 84.7820\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.5817 - val_loss: 84.8137\n",
            "Epoch 125/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.9728 - val_loss: 84.7911\n",
            "Epoch 126/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.2947 - val_loss: 84.7799\n",
            "Epoch 127/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.3977 - val_loss: 84.7569\n",
            "Epoch 128/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.6875 - val_loss: 84.7361\n",
            "Epoch 129/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.6353 - val_loss: 84.7141\n",
            "Epoch 130/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 67.2495roc-auc_val: 0.8244\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 67.1613 - val_loss: 84.6771\n",
            "Epoch 131/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 68.0069 - val_loss: 84.6603\n",
            "Epoch 132/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.1026 - val_loss: 84.6850\n",
            "Epoch 133/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.0240 - val_loss: 84.7003\n",
            "Epoch 134/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.0057 - val_loss: 84.7144\n",
            "Epoch 135/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.9142 - val_loss: 84.6750\n",
            "Epoch 136/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.6683 - val_loss: 84.6663\n",
            "Epoch 137/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.3027 - val_loss: 84.6418\n",
            "Epoch 138/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.6361 - val_loss: 84.6296\n",
            "Epoch 139/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.1269 - val_loss: 84.6382\n",
            "Epoch 140/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 66.6254roc-auc_val: 0.8247\n",
            "88/88 [==============================] - 14s 160ms/step - loss: 66.6256 - val_loss: 84.6175\n",
            "Epoch 141/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 66.7483 - val_loss: 84.6445\n",
            "Epoch 142/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.8038 - val_loss: 84.6111\n",
            "Epoch 143/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 67.0230 - val_loss: 84.6272\n",
            "Epoch 144/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.8810 - val_loss: 84.5994\n",
            "Epoch 145/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.7798 - val_loss: 84.5669\n",
            "Epoch 146/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.8099 - val_loss: 84.5796\n",
            "Epoch 147/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.8555 - val_loss: 84.5737\n",
            "Epoch 148/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.7713 - val_loss: 84.5785\n",
            "Epoch 149/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.3046 - val_loss: 84.6022\n",
            "Epoch 150/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 66.3104roc-auc_val: 0.8248\n",
            "88/88 [==============================] - 14s 164ms/step - loss: 66.3104 - val_loss: 84.5870\n",
            "Epoch 151/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.4891 - val_loss: 84.6600\n",
            "Epoch 152/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.4286 - val_loss: 84.6219\n",
            "Epoch 153/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.6267 - val_loss: 84.5683\n",
            "Epoch 154/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.3037 - val_loss: 84.5630\n",
            "Epoch 155/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.4159 - val_loss: 84.5558\n",
            "Epoch 156/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.1458 - val_loss: 84.5757\n",
            "Epoch 157/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.4433 - val_loss: 84.5976\n",
            "Epoch 158/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.7372 - val_loss: 84.5805\n",
            "Epoch 159/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.0926 - val_loss: 84.5997\n",
            "Epoch 160/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 66.0862roc-auc_val: 0.8249\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 66.0862 - val_loss: 84.5999\n",
            "Epoch 161/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.2114 - val_loss: 84.6189\n",
            "Epoch 162/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 66.2395 - val_loss: 84.6515\n",
            "Epoch 163/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.5051 - val_loss: 84.6496\n",
            "Epoch 164/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.0814 - val_loss: 84.6106\n",
            "Epoch 165/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.1053 - val_loss: 84.6098\n",
            "Epoch 166/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.4499 - val_loss: 84.5891\n",
            "Epoch 167/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.7388 - val_loss: 84.5705\n",
            "Epoch 168/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.9276 - val_loss: 84.5646\n",
            "Epoch 169/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.0990 - val_loss: 84.5400\n",
            "Epoch 170/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 65.6280roc-auc_val: 0.8252\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 65.5623 - val_loss: 84.5331\n",
            "Epoch 171/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 66.1106 - val_loss: 84.5641\n",
            "Epoch 172/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.1758 - val_loss: 84.5860\n",
            "Epoch 173/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.9784 - val_loss: 84.5378\n",
            "Epoch 174/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.8462 - val_loss: 84.5201\n",
            "Epoch 175/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.2486 - val_loss: 84.5188\n",
            "Epoch 176/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.6174 - val_loss: 84.4999\n",
            "Epoch 177/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.8167 - val_loss: 84.5341\n",
            "Epoch 178/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.3708 - val_loss: 84.5220\n",
            "Epoch 179/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.8147 - val_loss: 84.5087\n",
            "Epoch 180/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 65.2926roc-auc_val: 0.8253\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 65.2200 - val_loss: 84.5209\n",
            "Epoch 181/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.2039 - val_loss: 84.5298\n",
            "Epoch 182/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.2021 - val_loss: 84.5165\n",
            "Epoch 183/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.3818 - val_loss: 84.5456\n",
            "Epoch 184/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.0445 - val_loss: 84.5627\n",
            "Epoch 185/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.4043 - val_loss: 84.5814\n",
            "Epoch 186/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.3273 - val_loss: 84.5886\n",
            "Epoch 187/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.6616 - val_loss: 84.5361\n",
            "Epoch 188/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.4187 - val_loss: 84.5306\n",
            "Epoch 189/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.4946 - val_loss: 84.5339\n",
            "Epoch 190/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 64.9332roc-auc_val: 0.8256\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 64.9048 - val_loss: 84.5250\n",
            "Epoch 191/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.5852 - val_loss: 84.5081\n",
            "Epoch 192/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.7818 - val_loss: 84.5051\n",
            "Epoch 193/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.0490 - val_loss: 84.5593\n",
            "Epoch 194/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.3090 - val_loss: 84.5399\n",
            "Epoch 195/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.2418 - val_loss: 84.5445\n",
            "Epoch 196/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 64.5433 - val_loss: 84.5118\n",
            "Epoch 197/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.0386 - val_loss: 84.5296\n",
            "Epoch 198/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.1013 - val_loss: 84.5434\n",
            "Epoch 199/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.5759 - val_loss: 84.5764\n",
            "Epoch 200/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 64.9551roc-auc_val: 0.8256\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 64.9842 - val_loss: 84.5709\n",
            "Epoch 201/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.8108 - val_loss: 84.5279\n",
            "Epoch 202/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.8609 - val_loss: 84.5531\n",
            "Epoch 203/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.0708 - val_loss: 84.5690\n",
            "Epoch 204/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.5547 - val_loss: 84.5690\n",
            "Epoch 205/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.7467 - val_loss: 84.5544\n",
            "Epoch 206/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.3173 - val_loss: 84.5572\n",
            "Epoch 207/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.5405 - val_loss: 84.6133\n",
            "Epoch 208/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.8882 - val_loss: 84.6341\n",
            "Epoch 209/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.3766 - val_loss: 84.6040\n",
            "Epoch 210/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 64.6633roc-auc_val: 0.8257\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 64.5115 - val_loss: 84.6049\n",
            "Epoch 211/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.1594 - val_loss: 84.6035\n",
            "Epoch 212/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 65.1357 - val_loss: 84.6316\n",
            "Epoch 213/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.8447 - val_loss: 84.6625\n",
            "Epoch 214/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 63.8963 - val_loss: 84.6785\n",
            "Epoch 215/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.2183 - val_loss: 84.6779\n",
            "Epoch 216/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.2169 - val_loss: 84.6493\n",
            "Epoch 217/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.6312 - val_loss: 84.6765\n",
            "Epoch 218/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.4003 - val_loss: 84.6910\n",
            "Epoch 219/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 63.7207 - val_loss: 84.6632\n",
            "Epoch 220/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 64.6165roc-auc_val: 0.8256\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 64.2831 - val_loss: 84.6717\n",
            "Epoch 221/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.2655 - val_loss: 84.6768\n",
            "Epoch 222/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.2406 - val_loss: 84.6844\n",
            "Epoch 223/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 63.7881 - val_loss: 84.6683\n",
            "Epoch 224/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 63.9453 - val_loss: 84.6988\n",
            "Epoch 225/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 64.2949 - val_loss: 84.6705\n",
            "Epoch 226/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 63.6918 - val_loss: 84.6555\n",
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 30ms/step - loss: 136.8169 - val_loss: 117.5735\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 123.2775 - val_loss: 114.2826\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 118.5438 - val_loss: 111.7042\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 112.4086 - val_loss: 108.5683\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 109.0909 - val_loss: 106.5895\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 105.7530 - val_loss: 105.7483\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 103.8566 - val_loss: 104.3052\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 101.0234 - val_loss: 103.4130\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 99.4098 - val_loss: 102.1840\n",
            "Epoch 10/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 98.8603roc-auc_val: 0.7742\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 98.2793 - val_loss: 100.9348\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 96.4251 - val_loss: 99.8595\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 94.8985 - val_loss: 98.8537\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 93.8691 - val_loss: 98.4257\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 92.1626 - val_loss: 97.8483\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 91.1968 - val_loss: 97.0103\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 89.9629 - val_loss: 96.8058\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 88.4700 - val_loss: 96.2493\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 88.9749 - val_loss: 95.3389\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 87.5357 - val_loss: 94.5716\n",
            "Epoch 20/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 87.0495roc-auc_val: 0.7861\n",
            "47/47 [==============================] - 15s 318ms/step - loss: 86.6558 - val_loss: 94.3564\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 87.4112 - val_loss: 94.0517\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 85.3530 - val_loss: 93.5860\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 85.5612 - val_loss: 93.3195\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 84.2938 - val_loss: 93.3812\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 84.1180 - val_loss: 92.9903\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 83.1867 - val_loss: 92.5925\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 82.8953 - val_loss: 92.1252\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 82.3797 - val_loss: 91.8578\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 82.3908 - val_loss: 91.7760\n",
            "Epoch 30/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 81.7479roc-auc_val: 0.7917\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 81.1460 - val_loss: 91.6545\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 81.3396 - val_loss: 91.5640\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 80.1694 - val_loss: 91.0313\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 80.3793 - val_loss: 91.1046\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 79.8601 - val_loss: 90.9987\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 78.9930 - val_loss: 91.0921\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 79.2651 - val_loss: 90.9487\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 78.4745 - val_loss: 90.7982\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 77.0846 - val_loss: 90.5978\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 77.9380 - val_loss: 90.6469\n",
            "Epoch 40/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 78.7595roc-auc_val: 0.7944\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 78.3961 - val_loss: 90.4777\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 77.6897 - val_loss: 90.4564\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 77.4436 - val_loss: 90.3790\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 76.2441 - val_loss: 90.0511\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 76.2673 - val_loss: 89.9907\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 76.3445 - val_loss: 90.0730\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 74.6555 - val_loss: 89.9865\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 75.4638 - val_loss: 90.0816\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 75.2100 - val_loss: 89.9906\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 75.2804 - val_loss: 89.8798\n",
            "Epoch 50/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 75.9638roc-auc_val: 0.7964\n",
            "47/47 [==============================] - 14s 304ms/step - loss: 75.4896 - val_loss: 90.0316\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 75.3032 - val_loss: 90.0054\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 74.8071 - val_loss: 89.8630\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 75.1324 - val_loss: 89.8269\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 73.8913 - val_loss: 89.8037\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 73.6091 - val_loss: 89.7763\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 73.5844 - val_loss: 89.7624\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 73.2288 - val_loss: 89.7145\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 73.1326 - val_loss: 89.8200\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 73.2411 - val_loss: 89.7034\n",
            "Epoch 60/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 72.3618roc-auc_val: 0.7975\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 72.3250 - val_loss: 89.7953\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 72.7273 - val_loss: 89.7938\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 72.2802 - val_loss: 89.8622\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 72.8920 - val_loss: 89.8430\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 71.8324 - val_loss: 89.8285\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 72.2414 - val_loss: 89.7839\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 71.9179 - val_loss: 89.9526\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 70.9877 - val_loss: 89.9138\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 71.6759 - val_loss: 89.8287\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 71.5686 - val_loss: 89.8905\n",
            "Epoch 70/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 72.0428roc-auc_val: 0.7987\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 71.1992 - val_loss: 89.9314\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 71.3329 - val_loss: 90.0471\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 70.7329 - val_loss: 89.9912\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 70.4400 - val_loss: 90.0016\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 70.6345 - val_loss: 90.0649\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 70.4797 - val_loss: 90.1087\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 70.8129 - val_loss: 90.0547\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 70.5945 - val_loss: 90.0301\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.8165 - val_loss: 90.0777\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.5515 - val_loss: 90.1642\n",
            "Epoch 80/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 69.8548roc-auc_val: 0.799\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 69.5287 - val_loss: 90.1355\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.7685 - val_loss: 90.1849\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.4812 - val_loss: 90.2231\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.8796 - val_loss: 90.2825\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.6060 - val_loss: 90.3330\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.5267 - val_loss: 90.2868\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.3436 - val_loss: 90.3001\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 68.9955 - val_loss: 90.3479\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.0048 - val_loss: 90.3271\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.4441 - val_loss: 90.3478\n",
            "Epoch 90/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 68.1176roc-auc_val: 0.7995\n",
            "47/47 [==============================] - 14s 301ms/step - loss: 67.8546 - val_loss: 90.4791\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 68.3925 - val_loss: 90.5181\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 68.0995 - val_loss: 90.5197\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 68.0566 - val_loss: 90.5913\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 69.0744 - val_loss: 90.5234\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 68.5452 - val_loss: 90.4883\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 68.2203 - val_loss: 90.4789\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 68.3054 - val_loss: 90.5274\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 67.4497 - val_loss: 90.4990\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 67.6598 - val_loss: 90.5792\n",
            "Epoch 100/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 68.1953roc-auc_val: 0.8003\n",
            "47/47 [==============================] - 14s 299ms/step - loss: 68.1531 - val_loss: 90.6321\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 67.5407 - val_loss: 90.7214\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 67.7703 - val_loss: 90.7781\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 66.9210 - val_loss: 90.8611\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 67.1896 - val_loss: 90.9258\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 66.3775 - val_loss: 90.8989\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 66.1492 - val_loss: 90.9993\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 66.9520 - val_loss: 91.0164\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 66.7730 - val_loss: 91.0297\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 68.0697 - val_loss: 91.0860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnpeTPNLkiCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "e38430da-d611-4682-b3c6-ba7333533f8a"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXzcVb3/8ddnluyTfW2SJmmbbpS2lEBBoLQgWEDAhSuIgl7BUpCrV733/rz6u/7c7qLXq17ArQKyCaIsArIoSoGWpSVd6d60pW32pclkncky5/fHTEqICTNpJvOdzHyej0cezHI68/mS9N2TM2cRYwxKKaWmP5vVBSillAoPDXSllIoRGuhKKRUjNNCVUipGaKArpVSMcFj1xrm5uaa8vNyqt1dKqWlpy5YtrcaYvLGesyzQy8vLqa6uturtlVJqWhKRo+M9p0MuSikVI4IGuogkichmEdkhIrtF5NtjtPmsiLSIyPbA181TU65SSqnxhDLk4gUuMsZ0i4gT2Cgizxtj3hzV7lFjzO3hL1EppVQogga68e8N0B246wx86X4BSikVZUIaQxcRu4hsB5qBF40xm8Zo9nER2Skij4lIaVirVEopFVRIgW6MGTLGLAVKgLNFZNGoJs8A5caYxcCLwP1jvY6IrBGRahGpbmlpmUzdSimlRpnQLBdjTAewHlg96vE2Y4w3cPdu4Mxx/vw6Y0yVMaYqL2/MaZRKKaVOUSizXPJEJDNwOxm4BNg3qk3RiLtXAXvDWaRSSqngQpnlUgTcLyJ2/P8A/M4Y80cR+Q5QbYx5GviiiFwFDAIngM9OVcFKKaXGJlYdcFFVVWVieaXow5uOjfn49ctnRrgSpVQsEZEtxpiqsZ7TlaJKKRUjNNCVUipGaKArpVSMsGy3xVgyery80e2hrqOXxSWZOO36b6ZSKjI00MOso7efe147Qo93kL/sbWblvDzOKs/GJmJ1aUqpGKfdxzDqH/Tx0JtHGRzycc2yEjKSnTy1vZ633jlhdWlKqTiggR5GT2yrpcHt4bqzSllWlsUtK2YxMzuFl/e3MDjks7o8pVSM00APk7ZuLztr3Vw4L495hekAiAgXz8/H3TfAlmPtFleolIp1Guhhsru+E4CzyrLf8/ic/DRmZqfwyv4WBn3aS1dKTR0N9DDZXe+mODOZrNSE9zwuIlw0P5+OvgG2Hu2wqDqlVDzQQA8Dd98Ax9v7OG1G+pjPV+anUZyZzKYjbRGuTCkVTzTQw2B3vRuARTMyxnxeRFhSkkGD28PRtp5IlqaUiiMa6GGwq66TfFciua7EcducVuwP++d3NUaqLKVUnNFAn6SWLi9H23pYVDx273xYVkoCxZnJPP92Q4QqU0rFGw30SVq/vxkD446fj7RoRjo7at3UdfRNfWFKqbijgT5Jb9e6SXTYKEhPCtp2eNjlBR12UUpNAQ30Sdpd76YoIzmkvVpy0xKZX+jihV067KKUCj8N9EkY8hn2NHRSnBm8dz7sskVFVB9tp6XLG7yxUkpNgAb6JBxu6cYz4KMoMznkP3PR/HyMgddqWqewMqVUPNJAn4RdgfnnMyYQ6KfNSCcrxcmrB1umqiylVJzSQJ+E3XWdJDps5KWNP/98NJtNOL8yjw0HW7HqgG6lVGzSQJ+EXfVuFhSlY7dN7PCKCypzaenysr+pa4oqU0rFo6CBLiJJIrJZRHaIyG4R+fYYbRJF5FERqRGRTSJSPhXFRhOfz7C7rpNFxcHnn492QWUuABsO6Di6Uip8QjmCzgtcZIzpFhEnsFFEnjfGvDmizU1AuzFmjohcB3wfuHYK6rXUyLND27q9dHkH6fEMndJr5LkS+V31cVIT/d+C65fPDF+hSqm4FLSHbvy6A3edga/Rg79XA/cHbj8GXCwS24do1rs9wMQ+EB2pMj+NI609DOhJRkqpMAlpDF1E7CKyHWgGXjTGbBrVpBg4DmCMGQTcQM4Yr7NGRKpFpLqlZXrP8qjv6MMmUJAe+geiI1XmpzHoM7yjuy8qpcIkpEA3xgwZY5YCJcDZIrLoVN7MGLPOGFNljKnKy8s7lZeIGvUdfRSkJ+Gwn9rnyhW5adhFqGnqDt5YKaVCMKE0MsZ0AOuB1aOeqgNKAUTEAWQAMX2aQ0u3l/z32S43mASHjbKcFA42a6ArpcIjlFkueSKSGbidDFwC7BvV7GngM4Hb1wAvmRieZD3o8+HuHSBnAvPPx1KZn0Zjp4dOz0CYKlNKxbNQeuhFwHoR2Qm8hX8M/Y8i8h0RuSrQ5h4gR0RqgK8AX5uacqNDR88ABsgedX7oRM0pcAFwSHvpSqkwCDpt0RizEzhjjMe/OeK2B/i78JYWvdp6/Btr5Uwy0IsykkhJsOuwi1IqLHSl6Clo6+kHJt9Dt4kwJz+NmuZu3QZAKTVpGuin4ERPPwl2G2mJoazLen+V+S66vYPsa9RtAJRSk6OBfgrauvvJTk0gHGun5uSnAbBBd19USk2SBvopONHTT07a5IZbhmUkO8l3JbLhoO7ropSaHA30CfIZw4ne/kmPn49UmZ/GpiMn6Ouf2L4wSik1kgb6BHX2DTDkM+EN9AIX/YM+Nh2J6bVYSqkppoE+QcMzXHJSJ7eoaKTynFQSHDZe1e10lVKToIE+QSdOBnr4eugJDhvLK7L1g1Gl1KRooE9QW3c/dhEyUpxhfd0LKnM52NxNfUdfWF9XKRU/NNAn6ESPl6xUJ7Ywb/e+Yq5/90ntpSulTpUG+gSd6AnvDJdh8wpcFKQn6ji6UuqUaaBPgDGGtp5+ssP4gegwEeGCyjw21rQy5NNtAJRSE6eBPgE9/UN4B31h/UB0pBVz83D3DbCztmNKXl8pFds00CegPUybco3n/Dm5iKDDLkqpUzL53aXiiLvPfxBFRnJ4Z7gAPLzpGAAzMpJ5fGstea5Erl8+M+zvo5SKXdpDn4Dhk4WmItCHVRakUdveq9sAKKUmTAN9Aty9AzhsQkqCfcreozLfhc/AoRY99EIpNTEa6BPg9gyQnuwMy7a545mZnUKiw6anGCmlJkwDfQLcfQNTOtwCYLcJs/PSONjcpacYKaUmRAN9AjojEOjgH0fv6B3gcGvPlL+XUip2aKCHyOczdPYNRibQ810AbDig2wAopUIXNNBFpFRE1ovIHhHZLSJfGqPNShFxi8j2wNc3p6Zc67T2eBkyhvQIBHp2agI5qQm8ooGulJqAUOahDwJfNcZsFREXsEVEXjTG7BnVboMx5sPhLzE6NLo9AGRGINAB5ha6eP1QG339QyRP4awapVTsCNpDN8Y0GGO2Bm53AXuB4qkuLNrUd/gDPRI9dID5BS68gz7eOKyrRpVSoZnQGLqIlANnAJvGePpcEdkhIs+LyGnj/Pk1IlItItUtLdNrOKHR7d+nPBJj6AAVuamkJNh5aV9zRN5PKTX9hRzoIpIGPA78ozGmc9TTW4EyY8wS4E7gD2O9hjFmnTGmyhhTlZeXd6o1W6Kh04PdJqRGaPjDYbdx3pxc1u9r0emLSqmQhBToIuLEH+a/McY8Mfp5Y0ynMaY7cPs5wCkiuWGt1GKNbg8ZU7yoaLSL5udT19HHgSZdZKSUCi6UWS4C3APsNcb8aJw2hYF2iMjZgdeNqSPsGzo8pCdFZrhl2Kp5+QA67KKUCkkoPfTzgBuAi0ZMS7xcRNaKyNpAm2uAXSKyA7gDuM7E2DhBQ2cfGcmR3ZyyMCOJBUXprNdAV0qFIGhCGWM2Au87zmCMuQu4K1xFRRufz9Dk9lKRkxbx975ofh6/eOUwHb39ZKZMzT7sSqnYoCtFQ3Cit5/+IV/Ee+gAH1xQwJDPsH6/9tKVUu9PAz0EDYE56JGasjjSkpJM8l2J/Hl3U8TfWyk1veiJRSFoCMxBj9SiomHDpxiV56by173N3P/6OzjtNj3JSCk1Ju2hh6Cx07oeOsDConT6h3wc0j3SlVLvQwM9BA1uD067kJpozS80s/JSSXTY2NMwej2XUkq9SwM9BE1uD/muJGwRXFQ0ksNmY16hi70NnfhiazaoUiqMNNBD0NzlJc+VaGkNC4vS6ekf4viJXkvrUEpFLw30EDR3eci3ONDnFriw24RddW5L61BKRS8N9BA0d3nJT7c20JOcduYWuNhZ52bIp8MuSqm/pYEehHdwiI7eAfJdSVaXwpKSDLo8g2w+csLqUpRSUUgDPYiWLi+A5UMuAPML03Hahad31FtdilIqCmmgB9E8HOgWD7kAJDhsLChK5/ldDQwM+awuRykVZTTQg2juHO6hWz/kAv6tADp6B9h4UI+mU0q9lwZ6EC1d/lWi0TDkAlBZkEZ6kkOHXZRSf0MDPYjmLi82gZy06Ah0h83GFYuLeGFXI12eAavLUUpFEQ30IJo7veSkJWK3WbNKdCzXnjWTvoEhntquvXSl1Ls00IOIhkVFoy0pyWBhUToPbzqmB0grpU7SQA+iucsbdYEuInxy+Uz2NHSyo1ZXjiql/DTQg/AHenTMcBnpI0tnkJJg5+FNR60uRSkVJfSAi3E8vOkYPmNo7fLS3OU5edhENBiuZWFROk9uq2N+YTqfO7/C4qqUUlbTHvr76PYOYgBXkjUHWwSzfFYOA0NGtwJQSgEhBLqIlIrIehHZIyK7ReRLY7QREblDRGpEZKeILJuaciOryzMIQHpSdP4iU5yZzJz8NDbUtNLXP2R1OUopi4XSQx8EvmqMWQicA3xBRBaOanMZUBn4WgP8PKxVWmR4nne09tABVs3Lp8c7yG/fip4hIaWUNYIGujGmwRizNXC7C9gLFI9qdjXwgPF7E8gUkaKwVxthwz10V5T20AEqclMpz0nll68cxjuovXSl4tmExtBFpBw4A9g06qli4PiI+7X8begjImtEpFpEqltaWiZWqQWGe+hpURzoAKvm59HY6eGxLbVWl6KUslDIgS4iacDjwD8aY07ptGJjzDpjTJUxpiovL+9UXiKiujyDpCTYcdii+7PjOXlpLJuZyY9fPIi7T7cDUCpehZRUIuLEH+a/McY8MUaTOqB0xP2SwGPTWpdnMKqHW4aJCN++ahEnerz86M/7rS5HKWWRUGa5CHAPsNcY86Nxmj0N3BiY7XIO4DbGNISxTkt0eQai+gPRkU4vyeCGc8p48M2jeu6oUnEqlB76ecANwEUisj3wdbmIrBWRtYE2zwGHgRrgV8BtU1NuZHV5BnElRn8PfdhXLp1Hdmoi3/jDLj13VKk4FDStjDEbgffdatD4d4j6QriKigbGGLq8g9Omhw6Qkezkm1cu5IuPbOOOvx7ky5fMtbokpVQERfenfRbq6x9iyGemxRj6SFctmcHHl5Vwx0sHeb1GTzVSKp5ooI+j0xv9c9DH892PnMas3FS+9Oj2k4dcK6Vi3/RLqwiZDqtERxq9edgVp8/gZy/X8OVHt3P/586OqgM6lFJTQwN9HNG+j0swhRlJXLlkBk9uq2PtQ1tYNS//5HPXL59pYWVKqamiQy7jeHfZ//TooY+lqiyLJSUZ/GVPE0dae6wuRyk1xTTQx9HlGSDRYSPBMX3/F4kIH1laTHZqAr+rPo5nQPd6USqWTd+0mmLTZZVoMIlOO9eeVUpn3wDPvT3t13oppd6HBvo4ptMq0WBKslK4oDKX6qPtHGzqsrocpdQU0UAfR6z00IddvKCA3LREntxWR3dgSqZSKrZooI/BGEOnZ2BaLfsPxmm3cc2yYtx9A/zozwesLkcpNQU00MfQ7R1kYMjEzJDLsJk5qZxVkc19rx/RDbyUikEa6GNoDqyujKUhl2EfWlhIdmoC33jybd3AS6kYo4E+hubO4UCPrR46QHKCnf97xUJ21Lp5eLOeQ6pULNFAH0NzlweIzR46wNVLZ/CB2Tn84IV9J69VKTX9aaCPYXhDq/QY7KGDf8HRdz+yCO+Aj39/dq/V5SilwkQDfQzNXV4cNiHJGbv/e2bnpbF25Wye2l7PxoO6za5SsSB2E2sSmjs9uJIc+E/fi123rZxNWU4K//cPb9Pbr3PTlZruNNDH0NzljckPRIc9vOkYD286xhNb6/jgggKOtvVy4z2brS5LKTVJGuhj8Ad6bH4gOtrsvDRWzM2j+mg7z+yot7ocpdQkaKCPYXjIJV58cEEBpVnJfP2JtznaptvsKjVdaaCP4hkYotMzvQ6Hniy7TbjurJnYbMJN91fj7h2wuiSl1CkIGugicq+INIvIrnGeXykibhHZHvj6ZvjLjJzhKYuxtI9LKLJSE/jlDWdyrK2XNQ9W4x3UvdOVmm5C6aHfB6wO0maDMWZp4Os7ky/LOu8uKoqfHvqwc2bl8N9/t5hNR07wT7/fqVsDKDXNBA10Y8yrwIkI1BIV3l32H189dPDPfunxDvGh0wp5Zkc91617A2M01JWaLsI1hn6uiOwQkedF5LTxGonIGhGpFpHqlpaWML11eDV1+nvo6cnx10MfduHcPFbOzeOtd9r57h/3aqgrNU2Eoxu6FSgzxnSLyOXAH4DKsRoaY9YB6wCqqqqiMiUaO7047UJKgt3qUix1ycICvEM+7n3tCKmJdr566TyrS1JKBTHpHroxptMY0x24/RzgFJHcSVdmkaZOD/muJGwxvko0GBHhw6cXcd1Zpdz5Ug0/e7nG6pKUUkFMuocuIoVAkzHGiMjZ+P+RaJt0ZRZpdHsozEiyuoyoICL8+0dPp29giB+8sJ8Up53PnldhdVlKqXEEDXQReQRYCeSKSC3w/wAngDHmF8A1wK0iMgj0AdeZaTzo2tTpYUFRutVlRI1H3zpOVVk2B5u6+dYze9hZ66aqPJvrl8+0ujSl1ChBA90Y88kgz98F3BW2iixkjKGx08PKeflWlxJV/AuPSnnwzaM8ua0Op8Omga5UFNKVoiN0eQfp7R+iMCPR6lKijsNu41PLyyjLSeX31cd5aV+T1SUppUbRQB+hye2fsliQrmPoY0lw2Ljx3DKKMpK59aGtbD4SN8sTlJoWNNBHaAzMQS/UQB9XktPOZz5QTnFWMjfd9xa7691Wl6SUCtBAH6Ex0EPXWS7vLy3RwYM3LSctycFn7t3MkVbdoVGpaKCBPsLwKlEdcgmuODOZB29ajs/Ap+/edPIfQ6WUdTTQR2js9JCZ4iTJGd+rREPx8KZjbD5yguvOKqWl28uVd27kF68csrospeKaBvoIjW6vjp9PUElWCp85txx33wB3bzisPXWlLKSBPkJTp0eHW05BRW4qf39eOV2eQa5d9wbHT/RaXZJScUkDfYTGTo/20E9RWU4qnzuvgo7eAT76s9fZVaezX5SKNA30gIEhH63dXgp0hsspK81O4fFbzyXRYeMTv3yD9fuarS5JqbiigR7Q0uXFGJ2DPlmbj7Rzw7llZCY7+dx9b3HLg1t0P3WlIkQDPeDkoiJd9j9p6UlO1qyYzaLiDP60u5F/eGQbvf2DVpelVMyLv3PWxqHL/sMrwWHjurNKKc5M5tm3GzjU0sO6G86kNDvF6tKUilnaQw/QZf/hJyKsmJvHrz97FnXtvVx110ZePRCdRw8qFQs00AOaOr0k2G1kpyZYXUrMqe/wcPMFs/ybe927mc/cu5kH3zhqdVlKxRwN9ICmTg/56YlInB89N1Vy0xK59cI5nFWexSsHWvjVhsPUtut8daXCSQM9oNGtc9CnWoLDxkfPKOG6s0pp6vRw+f9u4IVdjVaXpVTM0EAPqOvoozgr2eoy4sLikkxuXzWHspxU1j60hX/+/Q7cfQNWl6XUtKeBDgz5DPUdfZRooEdMTloij9/6Ab6wajZPbKvj0h+/oqcgKTVJGuj4Z7gM+gwlWTqlLpIe21JLcWYKa1fMBuBz91Xzld9tx92rvXWlToUGOlAb2ExKe+jWKM5K5gsr57BqXj5Pba/nkh+/wot7tLeu1EQFDXQRuVdEmkVk1zjPi4jcISI1IrJTRJaFv8ypVdveB6A9dAs57DYuWVjA2gtnYxPh8w9U85GfvsY9G47w8KZjVpen1LQQSg/9PmD1+zx/GVAZ+FoD/HzyZUVWbXsfIjAjU2e5WK04M5nbVs3m4vn57Kzt4M71B3U7XqVCFDTQjTGvAu93vPvVwAPG700gU0SKwlVgJBxv76XAlUSiQ08qigYOm42LFxRwy4rZCPDLVw9x94bDusmXUkGEYwy9GDg+4n5t4LG/ISJrRKRaRKpbWqJnCXhte6+On0eh0uwUbl9VyfzCdL737F4+/8AWOnr7rS5LqagV0c25jDHrgHUAVVVVUdHdenjTMfY3dlGWk6pjtVEoOcHOp5bPxDvo4z+f38sVd2zkzuvPYNnMLKtLUyrqhCPQ64DSEfdLAo9NC0M+g7tvgMwUp9WlqHGICElOO5+/YBaPbD7GNT9/ndWnFfLTTy2b9FYN4/0jfv3ymZN6XaWsEI4hl6eBGwOzXc4B3MaYhjC8bkR0egbwGchK0U25ol1J1rtDMM/tauTGezfrfjBKjRDKtMVHgDeAeSJSKyI3ichaEVkbaPIccBioAX4F3DZl1U6B9h7/mKwG+vQwPARz1ZIZbDnazqU/fpVfv3aEgSGf1aUpZbmgQy7GmE8Ged4AXwhbRRHWHliVmKVDLtOGiHDOrBz+ZfU8vv7kLr79zB5+/do7fPmSSj68eAZOe/BfPH0+Q0Onh/5BHwkOXV+nYkPcn1jU3tuPABka6NPOqwda+dDCAipyUvnznka+/OgOvvHkLj62rJhV8/KZnZdGcVYynoEhOj2D7G/sZPuxDrYd72DH8Q46Pf5j8VxJDuYVuLj89CKSnDp1VU1fcR/oHb39pCc7cdi0lzYdiQjzCl1UFqRxoLGLbcc7eGxLLQ+9OfaHnTaBuQUurlhcxMIZGWw40EJzl5etx9o51NLNJ6pKKctJjfBVKBUecR/o7b06wyUW2ESYX5TO/KJ0rl46g32NnRxq6aG+o4+UBDuuJCcVuamcXpxBauK7P/b2wCyZcyqyebT6OL/acJgbzy236CqUmhwN9J5+KnK1RxZLntpef/J2vsu/nYMxcLilh8MtPWP+mZk5qfzDRZX8asNhHtl8jE+ePZN5ha6I1KtUuMT1OMPAkC8wB11nuChIctq58dxyEhw2PnffW7R0ea0uSakJietAb+jwYNAZLupdGclObjinjLYeL2serMYzMGR1SUqFLK4D/VBrN+A/PUepYSVZKfzk2qVsO9bBPz+2UzcFU9NGfAd6sz/Q810a6Oq9Vi8q4l9Wz+OZHfX8+MUDVpejVEji+kPRmuZuUhLs75n1oNSwWy+czZGWHu54qYYEh43bL6q0uiSl3ldcJ1lNc7f2ztW4RIT//NjpDAz5+OGfDzDoM3zp4spJbwim1FSJ20A3xlDT0k1lvk5NU39r5C6MVeXZHDvRx0/+cpDWbi/f/PBpul2AikpxG+htPf109A5oD10FZRPhY8uKSUu089Cbx9h4sI3rl88kLTBUp1vtqmgRt92MmsAHonka6CoENhFWLyri2qpSatt7+dn6Guo7+qwuS6n3iPtA1x66moglpZncsmI2Bv9ZpztqO6wuSamT4jbQD7V0k+y0k56si4rUxBRnJXPbytnMyEzm0beOc/eGw1aXpBQQx4Fe09zN7PxUbDpjQZ0CV5KTm86vYFFxBt97di8/+csBXYCkLBe3H4oeau7m7Ipsq8tQ05jDZuPaqlLmF7r4yV8O0uMd5OuXL9BpjcoycRnoPd5B6t0e5uSnWV2KmubsNuEHH19MaoKdX204Qrd3iO99ZBF2m4a6iry4HHI51OL/QFQDXYXDb986ztwCFxfOzeORzcf42M9e0zNOlSXiMtCHZ7hooKtwERE+dFohly4sYEetm9t+sxXvoO7UqCIrLgP9YHM3DpvoUWMq7FbOy+fKxUW8uKeJm++vpsszYHVJKo6EFOgislpE9otIjYh8bYznPysiLSKyPfB1c/hLDZ9ddW7mFrhCOh1eqYk6d3YuP/y7JbxW08rqn2xg48FWq0tScSJooomIHfgpcBmwEPikiCwco+mjxpilga+7w1xn2Bhj2FnrZklphtWlqBh2zZklPHbrB0h02vj0PZv48qPb2d/YZXVZKsaFMsvlbKDGGHMYQER+C1wN7JnKwqbK0bZe3H0DLC7JtLoUFcOGN/f6zLnl/HVvE3/cWc+T2+q4oDKXSxcWcOHcfGbmpFhcpYo1oQR6MXB8xP1aYPkY7T4uIiuAA8CXjTHHx2hjqYc3HWP7cf9S7br2vvfsqKfUVHDabaxeVMSKuXlsOnKCfY2d/NtTu4HdFKYnsaQ0A5sI8wpc5LkS3zOHXTf9UhMVrnnozwCPGGO8InILcD9w0ehGIrIGWAMwc6Y1P6x17b047UJBepIl76/iU0qCg1Xz8lk5N4+2nn4ONnVxvL2P6nfaaevp5/ldjWSlOFlekcPZFdkkOe1Wl6ymoVACvQ4oHXG/JPDYScaYthF37wZ+MNYLGWPWAesAqqqqLFknXdveR1FGsi78UJYQEXLTEslNS+TcwGMdvf3sb+ri7Vo3L+xu5OUDzXxgdi5XL52hp2mpCQllmsdbQKWIVIhIAnAd8PTIBiJSNOLuVcDe8JUYPkM+Q727j5KsZKtLUeqkzJQEllfkcPMFs7ht5Wxm5abx0r5mVv3wZX731nGGfLpHjApN0EA3xgwCtwN/wh/UvzPG7BaR74jIVYFmXxSR3SKyA/gi8NmpKngymrs8DAwZSrL0wygVnUqyUvj0OWXcsmIWMzKT+ZfHd/LhOzfyWo1OfVTBhfT7nDHmOeC5UY99c8TtfwX+NbylhV9tu/9AAu2hq2hXlpPK1y6bzzM7G/j+8/v41N2bWFySwdVLi7lkQQElWcnYdNhQjRJXA3S17b0kOW3kpCZYXYpSQYkIVy2ZwaULC3hk8zEe31rLd/+4h+/+cQ9JThvlOankpyeRm5pAc5eXtEQHriQHxZnJ5LoSsYnoTJk4E2eB3kdJVopub6qmhZHTahMddq4/u4zmLg/5riQOtXTzTmsPrd1eDjV3nxxOfLe9jYrcVIaM4aL5+RRn6m+l8SBuAr3TM0BTp4cL5+ZZXYpSpyzf5Z9uOzsvjdl5791crn/QR3tvP3XtfRxr76WmuZt/+8Mu/g1YWprJhxcX8ZEzirctvLwAAAs8SURBVMlN02MXY1XcBPobh9rwGZitOyyqGJXgsFGQnkRBehLLyrIwxtDa3c+ehk7eruvge8/u5T+f38cVpxdx8wUVulo6BsVNoG882EqC3cbMbJ3houKDiJDnSuRCVx4Xzs2judPDpndOsH5fM0/vqOeyRYX804fm/U1PX01f8RPoNa1U5KbisOkOiyo+5acnceXiGVyyoICNNa38dW8zf9rdyJllWVw0v4BbV862ukQ1SXER6LXtvRxp7eGK04uCN1YqxiU57XxwQQHLK7J5eX8Lm4+cYPvxDjr6+lm7YjZZOgts2oqL7urwftR6QpFS73IlOblyyQy+fMlcTpuRwbpXD7PiB+v5nz/vp9Htsbo8dQriItA3HGylID2RfJd+uq/UaNmpCXyiqpQXvrSCD8zJ4a71NZz3/Ze45cFqHt9SS3OXhvt0EfNDLkM+w2uHWrl4foHOP1fqfWw52s6Fc/M5vTiTzUfaeK2mjT/tbgJgRkYSJdkp5LsSMYDPZxjyGXzGkOiwk+dKpDgzmfPm5LKgyKV/1ywS84G+u95NR+8AK+bm0uPVQ3uVCiY7NYHVi4q49LRClpZmsuFgKwebu6g90cee+k5EoMsziE0EERgYMnR5BvAO+gDISnFyxswszp2VQ2qiQ1erRlDMB/pzbzditwnnz8k92dtQSgVnE2FnrZuMZCdVZdlUlb1/+y7PAPsbu9hV7+alfc1sONhCVXk2K+bm6oZ4ERLTgT7kMzy5rZaVc/PI0dVxSk0pV5KTqvJsqsqzae708OrBVjYdbmPlf7/MVUtmcMuFs5lX6LK6zFM23gln0fQbSEwH+saaVpo6vXzryhKrS1EqruSnJ3HNmSV8cEE+bT39PLL5GE9sq+Pi+fn8/XkVLJ+VjdMeF3MyIiqmA/2xLbVkJDu5aEG+1aUoFZcyUxK4bdUcbl81hwfeOMp9rx/hr/uacSU5WFGZxxkzM1lUnMHsvDRyUhN0S+BJitlAd/cN8OfdjVx7VimJDj2fUSmrDA9V5LkS+dLFc6lp7mJvYxdbj7Xz7NsNJ9slOGzkuxJxJTlxBbYCTktykJbo/6//MSdpiQ7Kc1NYWJRBcoL+3R4pZgP92Z0NeAd9fHyZDrcoFS0SHDYWzshg4YwMwP9BaoPbQ1tPP+7efro8g3gGfbR0e6nt6MU74MMzMIR30MfgqKP47DZhYVE6q+blcdGCAhYXZ8R9Dz8mA31wyMcDb7xDZX4ai0syrC5HKTUOV5ITV5IzpLaDQz48g/6Ab+70B/6Rlh7ufKmGO16qITXRwfwCF59fUcH5lXmkTcEB293eQY619dDc5aXHO8jCGRkYY6Jm3n1MBvrDm4+xr7GLn31qWdT8j1ZKTY7DbiPNbiMt0UFuWiILZ6QD0Osd5EBzF/sau9jd4GbtQ1tx2oWzyrM5syyL04szWFKaSUF60im/d01zN09srWXb8Y6Th3bbbcJrh9r4694m/vlD87gsCvaKirlAb+v28sM/7ee8OTlctqjQ6nKUUlMsJdHB0tIslpZmMeQzzC1I46V9zbxyoIWfvXzoZAAXpCeyuCSTJSUZLC7JZHFJBpkp778RWfU7J/jlq4d5cU8TDptQVZbFGTOzKHAlIiLsqnOzp6GTW3+zlZvPr+D/XDbf0tk7MRfo//2n/fT2D/GtK0/T3rlSccZuEw619FCWk8qN56bSP+ijwd1HbXsfdR19bDvWzot73l1gODM7hfmFLkqzUyhMT8Jg6B/0caCpmy1H26nr6CMzxckXL670fzg7ahhnWVkW//Gx0/mP5/Zy98Yj7Kx1c9f1Z5A/id8GJiOmAv3Xrx3h0erj3HReBZUF03cBg1IqPBIcNspyUinLST35WF//EPWBkLcJHGrpZsPBVvoG3t0aJD3JQVlOKlXlWZxRmkWCY/xed4LDxreuOo0zZmbytcff5oo7N/LT65dxdkX2lF7bWEIKdBFZDfwvYAfuNsb816jnE4EHgDOBNuBaY8w74S11fEM+w3f/uIf7Xn+HSxYW8NVL50XqrZVS00xygv09Z7JeUJmHMQbvoA8BbDY5pWGTq5cWM78wnbUPbeGTv3qT68+eyW2rZlOUEbkDuoMGuojYgZ8ClwC1wFsi8rQxZs+IZjcB7caYOSJyHfB94NqpKBigt3+Qpk4vDR19rN/fzLM7G6h3e7j5/Ar+9fIF2ON86pJSamJEhCTn5Oe0zyt08dTt5/Ffz+/jkc3HePSt43xoUSFnl2exuCSTPFcimSlOkp32KRkSDqWHfjZQY4w5DCAivwWuBkYG+tXAtwK3HwPuEhExxrx34mgYPLW9ji/9dvvJ+067cEFlHt+88jRW64egSimLpSc5+Y+Pns6tF87m568c4i97mnhmR/172qxZMYuvX74g7O8twTJXRK4BVhtjbg7cvwFYboy5fUSbXYE2tYH7hwJtWke91hpgTeDuPGB/uC4kRLlAa9BW01MsXxvE9vXF8rWBXl+4lRlj8sZ6IqIfihpj1gHrIvmeI4lItTGmyqr3n0qxfG0Q29cXy9cGen2RFMrIfx1QOuJ+SeCxMduIiAPIwP/hqFJKqQgJJdDfAipFpEJEEoDrgKdHtXka+Ezg9jXAS1Mxfq6UUmp8QYdcjDGDInI78Cf80xbvNcbsFpHvANXGmKeBe4AHRaQGOIE/9KORZcM9ERDL1waxfX2xfG2g1xcxQT8UVUopNT3okSFKKRUjNNCVUipGxFygi8hqEdkvIjUi8rUxnk8UkUcDz28SkfLIV3nqQri+r4jIHhHZKSJ/FZEgZ7VHl2DXN6Ldx0XEiEhUTBcLRSjXJiKfCHz/dovIw5GucTJC+NmcKSLrRWRb4OfzcivqPBUicq+INAfW3Iz1vIjIHYFr3ykiyyJdIwDGmJj5wv+h7SFgFpAA7AAWjmpzG/CLwO3rgEetrjvM17cKSAncvjXWri/QzgW8CrwJVFlddxi/d5XANiArcD/f6rrDfH3rgFsDtxcC71hd9wSubwWwDNg1zvOXA88DApwDbLKizljroZ/cpsAY0w8Mb1Mw0tXA/YHbjwEXy/TZZzfo9Rlj1htjegN338S/bmC6COX7B/Bd/PsFeSJZ3CSFcm2fB35qjGkHMMY0R7jGyQjl+gyQHridAdQzTRhjXsU/g288VwMPGL83gUwRifiJF7EW6MXA8RH3awOPjdnGGDMIuIGciFQ3eaFc30g34e81TBdBry/wq2ypMebZSBYWBqF87+YCc0XkNRF5M7DL6XQRyvV9C/i0iNQCzwH/EJnSImKifzenREzth67eJSKfBqqAC62uJVxExAb8CPisxaVMFQf+YZeV+H+zelVETjfGdFhaVfh8ErjPGPM/InIu/rUri4wxPqsLixWx1kOP9W0KQrk+ROSDwDeAq4wx3gjVFg7Brs8FLAJeFpF38I9VPj1NPhgN5XtXCzxtjBkwxhwBDuAP+OkglOu7CfgdgDHmDSAJ/8ZWsSCkv5tTLdYCPda3KQh6fSJyBvBL/GE+ncZgIcj1GWPcxphcY0y5MaYc/2cEVxljqq0pd0JC+dn8A/7eOSKSi38I5nAki5yEUK7vGHAxgIgswB/oLRGtcuo8DdwYmO1yDuA2xjREvAqrPz2egk+jL8ffszkEfCPw2Hfw/8UH/w/R74EaYDMwy+qaw3x9fwGagO2Br6etrjmc1zeq7ctMk1kuIX7vBP+Q0h7gbeA6q2sO8/UtBF7DPwNmO3Cp1TVP4NoeARqAAfy/Sd0ErAXWjvje/TRw7W9b9XOpS/+VUipGxNqQi1JKxS0NdKWUihEa6EopFSM00JVSKkZooCulVIzQQFdKqRihga6UUjHi/wP5HHpxKwJA9wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TISYtM71A1Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
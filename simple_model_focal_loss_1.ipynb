{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_1",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "8e356095-ecbc-4401-af4d-3e6bdb05bd7d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "f58b530c-0ba7-42f7-b5e8-1e5a8e754bdf"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train_identity.csv.zip to /content\n",
            "\r  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 110MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 94% 49.0M/52.2M [00:01<00:00, 45.9MB/s]\n",
            "100% 52.2M/52.2M [00:01<00:00, 45.0MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 98% 57.0M/58.3M [00:01<00:00, 46.2MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 43.2MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 220MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 159MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDrCIAqHzl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "f8f48dd4-f0f5-415e-e123-6596d22429e8"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWJ-hzcznam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5OQjb1zo6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "f99e1831-c65b-4eb5-f895-beec7d1559e7"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d49b3ba8-338b-45d0-e6a3-87a71a972be3"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0r3SuH1K97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQ20JqWATak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69fa5fe7-8a3e-4b0c-8686-a6b349175f3e"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=0.1\n",
        "        alpha=0.9\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss\n",
        "dk={}\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate([(4,5),(3,4),(3,5)]):\n",
        "  train=trn.loc[trn['month']>=month[1]]\n",
        "  test=trn.loc[trn['month']<=month[0]]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.0001,decay=1e-3),loss=fl())\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 30ms/step - loss: 200.7032 - val_loss: 151.8817\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 182.6906 - val_loss: 144.4351\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 171.9283 - val_loss: 138.5095\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 164.4525 - val_loss: 134.4748\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 158.2912 - val_loss: 132.0022\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 153.7497 - val_loss: 130.1737\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 148.7051 - val_loss: 127.7836\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 144.8179 - val_loss: 126.4736\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 142.1708 - val_loss: 125.9551\n",
            "Epoch 10/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 139.1792roc-auc_val: 0.7702\n",
            "47/47 [==============================] - 17s 354ms/step - loss: 138.7582 - val_loss: 124.9687\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 136.1553 - val_loss: 122.9999\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 133.6962 - val_loss: 122.0350\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 131.3637 - val_loss: 120.8353\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 129.5420 - val_loss: 120.2206\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 128.6618 - val_loss: 119.3302\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 125.7868 - val_loss: 118.3605\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 124.6320 - val_loss: 117.8540\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 122.4499 - val_loss: 116.9924\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 120.3000 - val_loss: 116.5667\n",
            "Epoch 20/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 120.8768roc-auc_val: 0.7821\n",
            "47/47 [==============================] - 17s 356ms/step - loss: 120.3124 - val_loss: 115.9000\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 118.2538 - val_loss: 115.3312\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 116.7603 - val_loss: 114.8354\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 115.0613 - val_loss: 114.5992\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 114.8210 - val_loss: 114.6234\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 114.1282 - val_loss: 114.0669\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 112.7144 - val_loss: 113.6592\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 110.3840 - val_loss: 113.4928\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 111.0855 - val_loss: 113.0966\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 110.0658 - val_loss: 112.8696\n",
            "Epoch 30/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 109.0693roc-auc_val: 0.7894\n",
            "47/47 [==============================] - 17s 362ms/step - loss: 108.5511 - val_loss: 112.5554\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 107.7706 - val_loss: 112.3603\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 107.8057 - val_loss: 112.4268\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 106.8648 - val_loss: 112.2622\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 104.6122 - val_loss: 112.1770\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 105.4040 - val_loss: 112.1167\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 104.7845 - val_loss: 112.1611\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 104.6106 - val_loss: 112.1385\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 103.3556 - val_loss: 112.1567\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 103.8575 - val_loss: 112.1786\n",
            "Epoch 40/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 103.2999roc-auc_val: 0.7934\n",
            "47/47 [==============================] - 17s 358ms/step - loss: 102.8874 - val_loss: 112.2812\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 102.4566 - val_loss: 112.2467\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 101.6071 - val_loss: 112.3303\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 101.4984 - val_loss: 112.2167\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 101.8046 - val_loss: 112.1538\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 100.3499 - val_loss: 112.1458\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 99.6303 - val_loss: 112.1813\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 100.1647 - val_loss: 112.2793\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 99.7754 - val_loss: 112.1657\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 98.6205 - val_loss: 112.1714\n",
            "Epoch 50/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 99.3626roc-auc_val: 0.7961\n",
            "47/47 [==============================] - 17s 363ms/step - loss: 98.8719 - val_loss: 112.0828\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 98.8139 - val_loss: 112.0608\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 98.7724 - val_loss: 112.1073\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 97.5981 - val_loss: 112.0673\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 97.6619 - val_loss: 112.0706\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 96.6914 - val_loss: 112.0473\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 96.5994 - val_loss: 112.0370\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 96.4483 - val_loss: 112.1838\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 95.1735 - val_loss: 112.2422\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 96.8808 - val_loss: 112.2671\n",
            "Epoch 60/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 96.3380roc-auc_val: 0.7988\n",
            "47/47 [==============================] - 17s 356ms/step - loss: 95.9590 - val_loss: 112.3037\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 95.1542 - val_loss: 112.4039\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 94.7057 - val_loss: 112.5010\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 94.3783 - val_loss: 112.5532\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 95.3053 - val_loss: 112.6568\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 95.0475 - val_loss: 112.5776\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 94.9653 - val_loss: 112.6878\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 93.9288 - val_loss: 112.7600\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 93.0613 - val_loss: 112.9425\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 94.3786 - val_loss: 112.9541\n",
            "Epoch 70/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 94.2413roc-auc_val: 0.7993\n",
            "47/47 [==============================] - 17s 353ms/step - loss: 94.0209 - val_loss: 113.0595\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 92.8808 - val_loss: 113.1546\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 92.6884 - val_loss: 113.3444\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 93.2244 - val_loss: 113.2546\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 92.0018 - val_loss: 113.2599\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 93.4283 - val_loss: 113.3675\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 91.5793 - val_loss: 113.4675\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 92.9415 - val_loss: 113.4728\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 91.8743 - val_loss: 113.4568\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 91.5946 - val_loss: 113.6013\n",
            "Epoch 80/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 92.2228roc-auc_val: 0.8001\n",
            "47/47 [==============================] - 17s 356ms/step - loss: 91.7133 - val_loss: 113.6299\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 90.5032 - val_loss: 113.6760\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 91.1534 - val_loss: 113.6055\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 91.1175 - val_loss: 113.6501\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 90.1954 - val_loss: 113.7379\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 91.3969 - val_loss: 113.7023\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 90.5340 - val_loss: 113.7098\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 89.7677 - val_loss: 113.7554\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 90.4023 - val_loss: 113.7980\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 90.7998 - val_loss: 113.7895\n",
            "Epoch 90/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 90.0931roc-auc_val: 0.8016\n",
            "47/47 [==============================] - 17s 365ms/step - loss: 89.6884 - val_loss: 114.0036\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 88.9776 - val_loss: 114.0886\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 89.4139 - val_loss: 114.1730\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 89.2335 - val_loss: 114.2355\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 88.8287 - val_loss: 114.3547\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 89.2670 - val_loss: 114.4626\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 89.5308 - val_loss: 114.5075\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 89.2839 - val_loss: 114.6493\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 88.4677 - val_loss: 114.7858\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 87.8437 - val_loss: 114.8145\n",
            "Epoch 100/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 88.6686roc-auc_val: 0.8014\n",
            "47/47 [==============================] - 17s 356ms/step - loss: 88.4562 - val_loss: 114.9347\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 88.9666 - val_loss: 114.9468\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 87.7344 - val_loss: 114.9888\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 87.8810 - val_loss: 115.0413\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 87.9080 - val_loss: 115.1763\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 88.6523 - val_loss: 115.2587\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 87.0682 - val_loss: 115.2865\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 18ms/step - loss: 190.1314 - val_loss: 151.9402\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 168.8831 - val_loss: 142.8022\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 157.3586 - val_loss: 136.9028\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 150.5761 - val_loss: 132.4761\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 143.4553 - val_loss: 130.0871\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 138.0237 - val_loss: 127.0184\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 133.7076 - val_loss: 124.6819\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 130.0938 - val_loss: 123.4683\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 126.8863 - val_loss: 120.9476\n",
            "Epoch 10/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 123.7040roc-auc_val: 0.7888\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 123.7040 - val_loss: 119.9614\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 121.4016 - val_loss: 118.8263\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 118.8831 - val_loss: 117.6590\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 116.9022 - val_loss: 116.7383\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 115.6922 - val_loss: 115.7843\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 113.7425 - val_loss: 115.2554\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 112.4408 - val_loss: 114.2921\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 111.4495 - val_loss: 113.8703\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 109.6054 - val_loss: 113.7265\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 108.5595 - val_loss: 113.2339\n",
            "Epoch 20/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 107.4298roc-auc_val: 0.8022\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 107.2422 - val_loss: 112.8574\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 107.0874 - val_loss: 112.5458\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 105.5714 - val_loss: 112.1292\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 104.7998 - val_loss: 111.9586\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 104.7345 - val_loss: 111.7970\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 104.3927 - val_loss: 111.6230\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 103.8616 - val_loss: 111.3784\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 102.9446 - val_loss: 111.1131\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 102.7547 - val_loss: 111.0327\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 101.7440 - val_loss: 110.8381\n",
            "Epoch 30/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 101.7442roc-auc_val: 0.8074\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 101.5166 - val_loss: 110.6813\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 100.9437 - val_loss: 110.5573\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 100.1142 - val_loss: 110.4591\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 100.3318 - val_loss: 110.3549\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 99.4911 - val_loss: 110.2585\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 98.6580 - val_loss: 110.0616\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 98.6710 - val_loss: 110.0493\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 98.8687 - val_loss: 110.0275\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 98.1311 - val_loss: 109.9195\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 97.4813 - val_loss: 109.8647\n",
            "Epoch 40/1000\n",
            "85/88 [===========================>..] - ETA: 0s - loss: 98.0454roc-auc_val: 0.8112\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 97.9520 - val_loss: 109.6951\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 97.6928 - val_loss: 109.7961\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 97.2951 - val_loss: 109.6678\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 96.7261 - val_loss: 109.7003\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 96.6999 - val_loss: 109.6383\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 96.7129 - val_loss: 109.5937\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 95.3737 - val_loss: 109.5652\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 95.6740 - val_loss: 109.4699\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 95.5490 - val_loss: 109.4286\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 95.8641 - val_loss: 109.4435\n",
            "Epoch 50/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 95.8083roc-auc_val: 0.8128\n",
            "88/88 [==============================] - 15s 171ms/step - loss: 95.8011 - val_loss: 109.4269\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 94.7915 - val_loss: 109.4641\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 94.8404 - val_loss: 109.5092\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 94.8084 - val_loss: 109.4093\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 94.9164 - val_loss: 109.3498\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 94.0512 - val_loss: 109.3987\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 94.0486 - val_loss: 109.3078\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 93.7055 - val_loss: 109.3328\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 94.0878 - val_loss: 109.2116\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 93.5375 - val_loss: 109.1962\n",
            "Epoch 60/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 93.4702roc-auc_val: 0.8147\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 93.4702 - val_loss: 109.1715\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 93.5761 - val_loss: 109.1661\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 92.9284 - val_loss: 109.1675\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 92.7463 - val_loss: 109.1497\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 92.8791 - val_loss: 109.1769\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 92.9224 - val_loss: 109.1881\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 92.5202 - val_loss: 109.2224\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 92.4463 - val_loss: 109.2492\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 92.4409 - val_loss: 109.2011\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 91.9630 - val_loss: 109.1033\n",
            "Epoch 70/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 91.7925roc-auc_val: 0.8156\n",
            "88/88 [==============================] - 14s 161ms/step - loss: 91.7925 - val_loss: 109.0939\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 91.7138 - val_loss: 108.9880\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 91.7983 - val_loss: 109.0356\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 91.3704 - val_loss: 108.9653\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 91.3982 - val_loss: 108.9583\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 91.8138 - val_loss: 108.9207\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.4226 - val_loss: 109.0209\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 91.0584 - val_loss: 108.9434\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.9179 - val_loss: 108.9851\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.9511 - val_loss: 108.9901\n",
            "Epoch 80/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 91.1429roc-auc_val: 0.8166\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 90.9461 - val_loss: 108.9181\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.4405 - val_loss: 108.9089\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.1242 - val_loss: 108.9317\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.6098 - val_loss: 108.9143\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.3989 - val_loss: 108.9796\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.5489 - val_loss: 108.9834\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.0204 - val_loss: 108.9856\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.3726 - val_loss: 109.0200\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 90.0969 - val_loss: 109.0409\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.9734 - val_loss: 108.9395\n",
            "Epoch 90/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 90.2894roc-auc_val: 0.8172\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 89.8065 - val_loss: 108.9856\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.9953 - val_loss: 108.9960\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.4274 - val_loss: 108.9616\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.2115 - val_loss: 108.8834\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.5605 - val_loss: 108.9090\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.4736 - val_loss: 108.8271\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.0467 - val_loss: 108.8476\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.5986 - val_loss: 108.8799\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.3199 - val_loss: 108.9715\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.7322 - val_loss: 108.9764\n",
            "Epoch 100/1000\n",
            "85/88 [===========================>..] - ETA: 0s - loss: 88.1021roc-auc_val: 0.8181\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 88.1613 - val_loss: 108.9079\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.1054 - val_loss: 108.9267\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 89.0414 - val_loss: 108.8923\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.6378 - val_loss: 108.9214\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.3325 - val_loss: 108.9551\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.1783 - val_loss: 108.9036\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.4282 - val_loss: 108.9456\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.6338 - val_loss: 108.9641\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.1066 - val_loss: 109.0340\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.9521 - val_loss: 108.9711\n",
            "Epoch 110/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 87.8995roc-auc_val: 0.8182\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 87.7281 - val_loss: 108.9116\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.7851 - val_loss: 108.9509\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.8872 - val_loss: 108.8730\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.4843 - val_loss: 108.8945\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.5379 - val_loss: 108.8925\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.6495 - val_loss: 108.9428\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.2488 - val_loss: 108.9598\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.5386 - val_loss: 108.9044\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 88.2429 - val_loss: 108.9320\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.9513 - val_loss: 109.0172\n",
            "Epoch 120/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 87.5011roc-auc_val: 0.8185\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 87.5011 - val_loss: 108.9748\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.5916 - val_loss: 108.8941\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.4810 - val_loss: 108.9022\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.1199 - val_loss: 108.9206\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.1064 - val_loss: 108.8687\n",
            "Epoch 125/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.6509 - val_loss: 108.8099\n",
            "Epoch 126/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.9998 - val_loss: 108.7833\n",
            "Epoch 127/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.5936 - val_loss: 108.7868\n",
            "Epoch 128/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.0873 - val_loss: 108.7996\n",
            "Epoch 129/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.6261 - val_loss: 108.7943\n",
            "Epoch 130/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 87.4504roc-auc_val: 0.8195\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 87.3932 - val_loss: 108.7319\n",
            "Epoch 131/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 87.1191 - val_loss: 108.7008\n",
            "Epoch 132/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.3779 - val_loss: 108.7299\n",
            "Epoch 133/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 86.4481 - val_loss: 108.7065\n",
            "Epoch 134/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.4647 - val_loss: 108.7655\n",
            "Epoch 135/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.0310 - val_loss: 108.8552\n",
            "Epoch 136/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.0601 - val_loss: 108.8574\n",
            "Epoch 137/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.7622 - val_loss: 108.8917\n",
            "Epoch 138/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.4683 - val_loss: 108.8264\n",
            "Epoch 139/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.4772 - val_loss: 108.8444\n",
            "Epoch 140/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 85.4546roc-auc_val: 0.8196\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 85.2452 - val_loss: 108.8349\n",
            "Epoch 141/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.0410 - val_loss: 108.8070\n",
            "Epoch 142/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.9741 - val_loss: 108.8268\n",
            "Epoch 143/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.8906 - val_loss: 108.8809\n",
            "Epoch 144/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.1022 - val_loss: 108.9073\n",
            "Epoch 145/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.0352 - val_loss: 108.8083\n",
            "Epoch 146/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.9333 - val_loss: 108.7834\n",
            "Epoch 147/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.0315 - val_loss: 108.8334\n",
            "Epoch 148/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.2632 - val_loss: 108.7800\n",
            "Epoch 149/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.9564 - val_loss: 108.8106\n",
            "Epoch 150/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 85.8197roc-auc_val: 0.8201\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 85.7321 - val_loss: 108.8028\n",
            "Epoch 151/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.8075 - val_loss: 108.7666\n",
            "Epoch 152/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.4273 - val_loss: 108.8007\n",
            "Epoch 153/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.4387 - val_loss: 108.7524\n",
            "Epoch 154/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.8381 - val_loss: 108.7564\n",
            "Epoch 155/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 86.1192 - val_loss: 108.7977\n",
            "Epoch 156/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.4466 - val_loss: 108.7737\n",
            "Epoch 157/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.3801 - val_loss: 108.8214\n",
            "Epoch 158/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.0044 - val_loss: 108.7573\n",
            "Epoch 159/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.7742 - val_loss: 108.7764\n",
            "Epoch 160/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 85.2369roc-auc_val: 0.8203\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 85.0751 - val_loss: 108.7789\n",
            "Epoch 161/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.2132 - val_loss: 108.7712\n",
            "Epoch 162/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.8937 - val_loss: 108.8287\n",
            "Epoch 163/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.2799 - val_loss: 108.8450\n",
            "Epoch 164/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.8396 - val_loss: 108.8498\n",
            "Epoch 165/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 83.9825 - val_loss: 108.8433\n",
            "Epoch 166/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.0795 - val_loss: 108.8565\n",
            "Epoch 167/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.8881 - val_loss: 108.8374\n",
            "Epoch 168/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.7594 - val_loss: 108.8680\n",
            "Epoch 169/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.6318 - val_loss: 108.8485\n",
            "Epoch 170/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 85.5529roc-auc_val: 0.8205\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 85.1628 - val_loss: 108.8207\n",
            "Epoch 171/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.6983 - val_loss: 108.8362\n",
            "Epoch 172/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.6227 - val_loss: 108.8509\n",
            "Epoch 173/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.1930 - val_loss: 108.8560\n",
            "Epoch 174/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.9264 - val_loss: 108.8940\n",
            "Epoch 175/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.5199 - val_loss: 108.8828\n",
            "Epoch 176/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.7185 - val_loss: 108.8577\n",
            "Epoch 177/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.2548 - val_loss: 108.8451\n",
            "Epoch 178/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 85.0222 - val_loss: 108.8192\n",
            "Epoch 179/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 83.5973 - val_loss: 108.8222\n",
            "Epoch 180/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 84.5902roc-auc_val: 0.8208\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 84.4829 - val_loss: 108.7893\n",
            "Epoch 181/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 84.2425 - val_loss: 108.8406\n",
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 30ms/step - loss: 199.2525 - val_loss: 170.4407\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 181.9532 - val_loss: 161.4684\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 171.5209 - val_loss: 154.8704\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 165.4381 - val_loss: 150.0549\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 159.1331 - val_loss: 147.3325\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 153.6507 - val_loss: 144.6796\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 149.8207 - val_loss: 141.6365\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 146.1452 - val_loss: 139.9362\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 142.7424 - val_loss: 137.4956\n",
            "Epoch 10/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 140.2728roc-auc_val: 0.7652\n",
            "47/47 [==============================] - 14s 301ms/step - loss: 139.3407 - val_loss: 135.9897\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 137.0567 - val_loss: 132.8419\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 133.2368 - val_loss: 131.2413\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 131.2619 - val_loss: 130.0142\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 128.8777 - val_loss: 128.1496\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 126.5355 - val_loss: 127.0775\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 125.6845 - val_loss: 125.8944\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 124.2308 - val_loss: 124.7212\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 121.9364 - val_loss: 123.7310\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 120.3178 - val_loss: 122.9079\n",
            "Epoch 20/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 119.0289roc-auc_val: 0.7821\n",
            "47/47 [==============================] - 14s 299ms/step - loss: 118.7865 - val_loss: 122.2853\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 117.6579 - val_loss: 121.5170\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 116.4681 - val_loss: 120.8005\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 114.7487 - val_loss: 119.5558\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 114.0768 - val_loss: 119.1794\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 113.5363 - val_loss: 118.6523\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 112.7684 - val_loss: 118.0607\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 111.4206 - val_loss: 117.8020\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 111.0838 - val_loss: 117.2671\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 109.7476 - val_loss: 116.7489\n",
            "Epoch 30/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 108.0618roc-auc_val: 0.7889\n",
            "47/47 [==============================] - 14s 299ms/step - loss: 107.9159 - val_loss: 116.4614\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 108.5770 - val_loss: 116.1917\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 107.2394 - val_loss: 115.9238\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 107.3365 - val_loss: 115.8579\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 105.9954 - val_loss: 115.4687\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 105.2613 - val_loss: 115.3626\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 105.6247 - val_loss: 115.1876\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 104.3207 - val_loss: 114.8767\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 103.5628 - val_loss: 114.7307\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 104.8442 - val_loss: 114.5422\n",
            "Epoch 40/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 103.7878roc-auc_val: 0.7922\n",
            "47/47 [==============================] - 14s 299ms/step - loss: 103.0159 - val_loss: 114.7192\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 101.8943 - val_loss: 114.5217\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 102.2911 - val_loss: 114.0946\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 100.9332 - val_loss: 113.9779\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 101.0651 - val_loss: 114.0986\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 100.5146 - val_loss: 113.8781\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 99.7822 - val_loss: 113.9534\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 100.3601 - val_loss: 113.9298\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 99.7957 - val_loss: 113.9754\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 98.1845 - val_loss: 114.0248\n",
            "Epoch 50/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 98.6255roc-auc_val: 0.7938\n",
            "47/47 [==============================] - 14s 303ms/step - loss: 98.1953 - val_loss: 113.9884\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 98.3478 - val_loss: 113.8774\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 97.1735 - val_loss: 114.0862\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 96.1293 - val_loss: 113.5939\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 97.7350 - val_loss: 113.6104\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 97.5903 - val_loss: 113.6759\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 96.3859 - val_loss: 113.7244\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 96.1335 - val_loss: 113.8519\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 96.4279 - val_loss: 113.7234\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 96.3245 - val_loss: 113.8151\n",
            "Epoch 60/1000\n",
            "44/47 [===========================>..] - ETA: 0s - loss: 95.2604roc-auc_val: 0.7955\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 95.1177 - val_loss: 113.8399\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 94.8704 - val_loss: 113.9685\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 94.0783 - val_loss: 113.8718\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 94.7961 - val_loss: 113.9247\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 94.7867 - val_loss: 113.9168\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 94.7645 - val_loss: 113.8201\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 94.3522 - val_loss: 113.7160\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 93.3462 - val_loss: 113.7378\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 93.4969 - val_loss: 113.8196\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 93.3446 - val_loss: 113.8384\n",
            "Epoch 70/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 93.8536roc-auc_val: 0.7975\n",
            "47/47 [==============================] - 14s 303ms/step - loss: 92.9500 - val_loss: 113.9105\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 92.9550 - val_loss: 114.0604\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 93.0783 - val_loss: 114.1564\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 92.4900 - val_loss: 114.2598\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 91.7379 - val_loss: 114.4248\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 92.8395 - val_loss: 114.4212\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 91.2608 - val_loss: 114.4923\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 92.6513 - val_loss: 114.4866\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 90.8658 - val_loss: 114.5477\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 91.0582 - val_loss: 114.5948\n",
            "Epoch 80/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 91.5437roc-auc_val: 0.7966\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 91.1916 - val_loss: 114.7214\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 91.2218 - val_loss: 114.9047\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 90.5068 - val_loss: 114.9612\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 90.6338 - val_loss: 114.9997\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 90.4641 - val_loss: 114.9820\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 89.9767 - val_loss: 115.0956\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 90.4118 - val_loss: 115.1851\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 89.2180 - val_loss: 115.3732\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 89.7600 - val_loss: 115.3795\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 90.1663 - val_loss: 115.3786\n",
            "Epoch 90/1000\n",
            "45/47 [===========================>..] - ETA: 0s - loss: 90.7553roc-auc_val: 0.7982\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 90.5298 - val_loss: 115.4384\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 89.4348 - val_loss: 115.5190\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 88.7677 - val_loss: 115.5826\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 89.0660 - val_loss: 115.5449\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 88.9535 - val_loss: 115.6149\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 89.1008 - val_loss: 115.7449\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 89.0758 - val_loss: 115.6783\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 88.2272 - val_loss: 115.6720\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 87.5122 - val_loss: 115.9451\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 88.5937 - val_loss: 116.0204\n",
            "Epoch 100/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 88.5177roc-auc_val: 0.798\n",
            "47/47 [==============================] - 14s 301ms/step - loss: 88.1843 - val_loss: 116.1002\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 88.0279 - val_loss: 116.2777\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 87.1440 - val_loss: 116.3635\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 86.9333 - val_loss: 116.3044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnpeTPNLkiCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "b023d1f7-b06e-4de7-f548-d38f25da7960"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXzc9X3n8dd3bo1mdI8OW7Zl+QKDuWIwBGggISQhlKRN2iWEtHRpeDRJ07Tbx+6jV7a720cf22zbNCnLtvAAcrQlaXdLEnKVcjtcBnMaML7vS7JkSaNj7u/+MSPhGMsayTPzm/nN+/l46GFJM4+Zzw/Jb77+/L6HsdYiIiLVy+N0ASIicmYKahGRKqegFhGpcgpqEZEqp6AWEalyvnK8aEdHh+3r6yvHS4uIuNJLL7103FobO91jZQnqvr4+Nm/eXI6XFhFxJWPMvtkeU+tDRKTKKahFRKqcglpEpMopqEVEqpyCWkSkyimoRUSqnIJaRKTKKahFRKpcXQZ1OptjIplxugwRkaKUZWVitfvLh7dx/9N7uHJlB+9bHSPk9848dsuGpQ5WJiLybnU5on7i7QH8Xg9PbR/kr/59G0PjSadLEhGZVV2MqB/YtH/m88lkhh0D41y/tosVsQh/99Quthwa5Zo1nQ5WKCIyu7obUe8bngRgWXsjS9rCdDeF2D044XBVIiKzq7+gHprAawy9rQ0A9Mca2Tc8QSabc7gyEZHTq7ug3js0yeLWBvze/KX3d0RIZy0HTkw5XJmIyOnVVVCnszkOnZiirz08873lHY0YYPfguHOFiYicQV0F9cETU2StZVl748z3GgJeFrU0sEt9ahGpUnUV1PuG8mG8rC38c9/v72jkwIlJ0upTi0gVqqug3js0QWc0SDj487MS+2MRsjnLvqFJhyoTEZld3QS1tZb9w5Msaw+/67G+9jAeoz61iFSnugnqqXSWRDpHZzT0rseC/nyfenqOtYhINamboB5L5DdhioZOvxizuynEsbEE1tpKliUiMqf6CeqpNABNIf9pH+9qCjGZynJ8PFXJskRE5lQ3QR1PFIK6YfagBth+LF6xmkREilE3QT1X66OrKQjAtqMKahGpLvUT1FNpGvzemaXjp4oEfYQDXo2oRaTq1E9QJzI0Ncy+q6sxhq6mENsU1CJSZeomqOOJ9Kw3Eqd1N4XYfjSumR8iUlXqJqjHptJE5wjqrqYQE6ksh0a0k56IVI+iTngxxuwF4kAWyFhr15ezqFLLWct48sytD/j5G4q9re9ewSgi4oT5jKivtdZeVGshDTCRzJCzs8+hnjY9RU99ahGpJnXR+hibyk/Na5plat60kN/LouZ8n1pEpFoUG9QW+HdjzEvGmDtO9wRjzB3GmM3GmM2Dg4Olq7AExgqLXebqUQOs7o6y7Zg2ZxKR6lFsUF9lrb0E+AjwBWPML5z6BGvtPdba9dba9bFYrKRFnq2xOVYlnmxNV5RdA+M6Q1FEqkZRQW2tPVT4cwD4HnBZOYsqtXgigyG/qGUuq7qipLI57aQnIlVjzqA2xjQaY6LTnwPXA2+Uu7BSGptKEwn68HrMnM/tj+WP6dqto7lEpEoUMz2vC/ieMWb6+Q9Ya/+trFWV2FgiTXSOqXnTVnREgOlDBLrKWJWISHHmTC9r7W7gwgrUUjbxRIbmIvrTAM1hP+2NAY2oRaRq1Mn0vLlXJZ6sP9bI7uOa+SEi1cH1QZ3K5JhIZedclXiy/o6IRtQiUjVcH9SD40lg7lWJJ1vR2cjQRIrRyXS5yhIRKZrrg/roaAKYe1XiyfoLNxR3qf0hIlXA9UE9MJYP6vn2qEFT9ESkOrg+qI8XWh+zHcF1Okvawvg8pjBFT0TEWcWnV40aKfSZGwLeop7/wKb9ALSEAzy1fXBmu9NbNiwtT4EiInNw/Yh6dCpNwOvB55nfpcYiAQbjyTJVJSJSPNcH9chUuujR9Mk6okGGJ1LkdCyXiDjM/UE9mT99fL5ikSCZnJ1pnYiIOMX1QT06lVrYiDqSP5ZL7Q8RcVodBHWa8AJbH/DOrBEREae4PqgX2vpoDHhp8HtnVjaKiDjF/UG9wJuJxhg6IgGOq/UhIg5zdVAn0llSmRzhBYyoAWLRoEbUIuI4Vwf1O4tdFraupyMSJJ7IkEhnS1mWiMi8uDuop1JA8asSTzU980M3FEXESa4O6ultShdyMxHyrQ9QUIuIs1wd1CNT+aBeyPQ8gPbGAAYYjKdKWJWIyPy4OqjPdkTt83pobQxoRC0ijnJ3UE/Nb+e804lFggpqEXGUq4N6ZCqF12MI+hZ+mR2R/Ig6l9PmTCLiDHcH9WSalgY/xpgFv0ZHNEg6azlSOClGRKTS3B3UU2maG4o/gut0YoUpejrtRUSc4uqgHptK0xw+u6Ce3pxJ5yeKiFNcHdTTrY+zEQ36CPo8GlGLiGPcHdRTqbNufeQ3Zwqy+7hG1CLiDFcH9ehkmpZw4KxfJxYNqvUhIo4pOqiNMV5jzCvGmB+Vs6BSyeYsY4nMWY+oIT9F79DIFFMpbc4kIpU3nxH1l4Ct5Sqk1MYKi11KE9T5G4p71P4QEQcUFdTGmF7go8C95S2ndKZXJbac5awPeGdzpl26oSgiDih2RP014L8AudmeYIy5wxiz2RizeXBwsCTFnY2REgZ1e6Om6ImIc+YMamPMjcCAtfalMz3PWnuPtXa9tXZ9LBYrWYELNTKZ3/GuueHsbyYGfB4WtzSw+7hG1CJSecWMqK8EbjLG7AW+C7zfGPOPZa2qBEZL2KMG6I81akQtIo6YM6ittX9ore211vYBNwOPW2tvLXtlZ6mUPWqA/o5Gdg+OY602ZxKRynLtPOrp8xJLN6KOMJHKMqBTyUWkwuYV1NbaJ621N5armFIanUrTGPDi95bm/0X9sUZAMz9EpPJcPaIuxarEaf2xCKCZHyJSea4N6tES7PNxsp6mECG/R0EtIhXn2qAemTz7vahP5vEY+jsimqInIhXn2qAeS5Q2qEFT9ETEGa4N6ngiQzTkK+lr9sciHDwxSTKjzZlEpHJcG9RjU2maSjyiXhFrJGdh39BkSV9XRORMXBnUmWyOiVSWplCpgzo/82PHMfWpRaRyXBnU48kMQMlbHys7I3gMbDs6VtLXFRE5E1cG9dhUPqhL3foI+b30xyK8dSRe0tcVETmT0g45q8RYIr98vJQj6gc27Qegwe/lpX3DM1/fsmFpyd5DROR03DmiLgR1qXvUAD3NIU5MpkmkNfNDRCrDlUEdT5SnRw3Q3RwC4NhYouSvLSJyOq4M6lKel3iq7qZ8UB8ZVVCLSGW4MqinR9TlaH00N/gJ+T0cVVCLSIW4Mqine9SRMrQ+jDF0NzVwVK0PEakQdwb1VIZI0IfXY8ry+j3NIY6OJcjptBcRqQBXBnU8kS7LjcRp3c0hUpkcJyZSZXsPEZFprgzqsUS6LP3padM3FNX+EJFKcGVQl2PnvJN1NYUwoBuKIlIRrlqZOL1acO/QBNGgf+brUgv4PLRHApqiJyIV4coRdSKdoyHgLet7LG5p4MCJSaxuKIpImbk0qLMEfeW9tCVtYeKJjEbVIlJ2rgtqay2JdJYGf3lH1EtawwC8emCkrO8jIuK6oE5nLTmb35K0nHqaQ3g9RkEtImXnuqCeKuxqV+6g9nk9LGoO8ep+BbWIlJfrgjoxE9Tlv7TetjBbDo2SyebK/l4iUr9cHNTlHVEDLG0NM5XOsu2YTnwRkfJxYVDnR7flvpkI+ZkfAK+o/SEiZTRnUBtjQsaYF4wxrxlj3jTG/PdKFLZQ0yPqYAVaH61hP22NAd1QFJGyKmZlYhJ4v7V23BjjB542xvzUWvt8mWtbkOmbiZUYURtjuGhJi4JaRMpqzmGnzRsvfOkvfFTtcrxkBXvUABcvaWHnwDijk+mKvJ+I1J+i+gPGGK8x5lVgAHjEWrvpNM+5wxiz2RizeXBwsNR1Fm0qncPrMfjKtBf1qS5d3gbAC3uHK/J+IlJ/igpqa23WWnsR0AtcZow5/zTPucdau95auz4Wi5W6zqIlMllCPg/GVCaoL17aQtDn4dldxyvyfiJSf+Z1x81aOwI8AXy4POWcvUQ6W7G2B0DQ52V9XyvP7Rqq2HuKSH0pZtZHzBjTUvi8Afgg8Ha5C1uoSgc1wBX97bx9NM7QeLKi7ysi9aGYEXUP8IQx5nXgRfI96h+Vt6yFS6RzFZnxcbIrVrQDsGmP+tQiUnpzTs+z1r4OXFyBWkpiKp2lqYynu5zqgU37yeYsAa+Hbz27l5HJNLdsWFqx9xcR93PdysSkA60Pr8fQ1xFm9+BERd9XROqD64J6yoGgBujviDA4nmQsofnUIlJargrqbM6SztqK7Jx3qv5YI4BG1SJScq4K6krunHeqRS0NNPi97NBOeiJSYgrqEvEYw5ruKNuOxcnmqnaFvYjUIFcFdSU3ZDqdc7qjTKayvLL/hCPvLyLupKAuoVWdUTwGHt064Mj7i4g7uSuoU4WgDjgT1A0BL33tjTz+9jFH3l9E3MldQe3wiBrgnJ4mth8b58DwpGM1iIi7uCuoHR5RQ75PDfDYVo2qRaQ03BXU6Sw+j8Hvde6yOiJB+mONPPa2+tQiUhruCupU1tHR9LTr13bz3K4hhidSTpciIi7grqBOZx3tT0/7xQt7yOQsP9lyxOlSRMQF3BXUVTKiXtvTxMrOCA+9dtjpUkTEBdwV1FUyojbGcNOFi3hx7zCHR6acLkdEapy7gjpVHUENcNOFi7AWfvS6RtUicnbcFdTpLOEqaH0A9HU0cmFvs9ofInLWKncUSpmlszmSmRyhKgjqBzbtB6C3NcyPtxzh64/uIBYN6uQXEVkQ14yox6byG/ZXS+sD4ILeZjwGXtqnsxRFZOFcE9SjhaCultYHQDTk55zuJl7ad4JMLud0OSJSo1wT1CNVOKIGuGx5GxOpLFuP6EABEVkY1wT1aJUG9crOCC0Nfl7cq/aHiCyMa4J6pkcdqK77ox5jWN/Xys6BcfYPaUc9EZk/1wT1yOR0UFfXiBrgPcvaMMB3XtzvdCkiUoNcE9TV2voAaG7wc25PE999Yf/MVqwiIsVyVVAHfB68HuN0Kad15coOTkymefCVg06XIiI1xjVBPTKZrsrR9LS+9jAX9DZz39N7yOmUchGZhzmD2hizxBjzhDHmLWPMm8aYL1WisPkanaruoDbGcPtVy9k9OMGT23WogIgUr5gRdQb4fWvtWuBy4AvGmLXlLWv+xqbSVXkj8WQ3rOuhpznEvT/b43QpIlJD5gxqa+0Ra+3Lhc/jwFZgcbkLm6+RqVRVj6gB/F4Pt723j2d3DfHSvhNOlyMiNWJePWpjTB9wMbDpNI/dYYzZbIzZPDg4WJrq5mG0BkbUALdevoz2xgB/88h2p0sRkRpRdFAbYyLAvwK/a60dO/Vxa+091tr11tr1sVislDUWpdp71JDfVe8Hrx5mw/I2nt55nD//8daZnfZERGZTVFAbY/zkQ/qfrLUPlrek+UuksyTSuarakOlMNvS3Ew35eOStY1irGSAicmbFzPowwH3AVmvtV8tf0vxNLx8PVfmIeprf6+GaNZ3sHZpg58C40+WISJUrZkR9JfAZ4P3GmFcLHzeUua55mVmVWCMjaoBLl7XS1hjgh68fIZnRakURmV0xsz6ettYaa+0F1tqLCh8/qURxxZrZi7pGRtQAPq+Hmy5cxPHxJPc8tdvpckSkirliZWI1b8h0Jqu7oqxb3MydT+xk7/EJp8sRkSrliqCu5g2Z5vLRdT0EvB7+8MEtZLI6BUZE3s1dQV1jI2qApgY///XGtTy3e4j/+dO3nS5HRKpQde2yv0AjNTbr41S/eukS3joyxn1P72F1V4T/cKlOKxeRd7giqEcnU0RDPjymOrc4LcaffPRcdg2O8yfff4NU1nLrhqWYGr4eESkdVwT18fEUsUjQ6TIWbHp14jWrOzk6muDL33+Dx7Ye4yufuICuppDD1YmI01wR1IPxJLFo7Qb1tIaAl9ve28fze4b56ZYjXPWVx7lyRQdXr4rN9N9v2aC2iEi9ccXNxMFxdwQ15PetvqK/nS99YBXn9jTx5PZBvvrINg6PTDldmog4xB1B7ZIR9cnaI0FuvnQpX7h2JX6vh/ue3sOhEwprkXpU80E9mcownszQGXVnL3dxSwOfvbqfkN/Dfc/s5o1Do06XJCIVVvNBPRhPArhuRH2y1sYAn726n6DPyxceeJl4Iu10SSJSQQrqGtESDnDzpUs4eGKKP/reG9oeVaSOuCeoa3h6XrGWtTfye9et4oevHeZfNh9wuhwRqZCaD+qBQlB3Nrk/qAE+d81KrlzZzpd/8KbOXRSpEzUf1IPxJF6PoTUccLqUivB6DHd+6hJ6mkPc8e3NHBiedLokESkzVwR1e2MAr6d+llu3NQa4/7ZLSWdz/MdvvsjwRMrpkkSkjGo+qAfiibppe0B+ufkDm/azafcwv7J+CXuOT/DBrz7F/iGNrEXcquaDenA8WRc3Ek9nRSzC7VctZzKV5Zf/7hleOzDidEkiUga1H9QuXJU4H8vaG/mt960g5Pdy8z3P8/jbx5wuSURKrKaDOpez+Z3z6jioIT+H/MHPv5eVnRF+81ubZ3bjExF3qOnd84YnU2Rz1rXLx+fj0bcG+OVLFpPMZPmj723hh68f5obze/jMFcucLk1EzlJNj6jrZVVisYI+L5+5vI8rV7Tz3K4hvvHsnpn/RiJSuxTULuP1GD56wSI++Z5e9g9Nct1Xn+K7L+wnl9OSc5Fa5Yqg7lRQv8slS1v57WtXsqY7yh88uIVP/v2zvLBn2OmyRGQBajqop5ePd9Tp9Ly5dDaFuOnCRXziksXsGBjnV+9+jg/9zUatZhSpMTUd1IPxJI0BL43Bmr4nWlYeY3jPsjZ+/4Nr+NB53ewdmuBDX9vIt57dq3aISI2o7aB20RFc5RbweXjf6hhf+sAqLu1r408fepNf/8YLWn4uUgNqeig6GE9oat48tYQDXL+2i9ZwgB+9fpj3//WTfPqyZSxubdDBuSJVas4RtTHmfmPMgDHmjUoUNB8Ddb4qcaGMMVy2vI07fqEfLNy9cReb9+pGo0i1KmZE/U3gfwPfLm8pxZteeXd4ZIquaEgr8RaotzXMF65dyT9vPsCDrxwi6Pfwp794HiG/1+nSROQkc46orbUbgaobbk2lsiTSOVrCfqdLqWmNQR+3vbePa1bH+M4LB/j4Xc+w41jc6bJE5CQlu5lojLnDGLPZGLN5cHCwVC87q+PjmppXKh5juP68br5x26UcH09y451P8w/P7dW5jCJVomRBba29x1q73lq7PhaLleplZzU0kQ/q9sb6ONmlEo6MJvjs1f0saw/z5R+8yUe+/jPu2bjb6bJE6l7NTs87Pp7CkD/tREonGvLza1f08dF1PewYGOfOx3bw7K7jTpclUtdqOKiTtIT9+Lw1ewlVy2MMV67s4PPX5Pe5vvXeTdz1xE4tkBFxSDHT874DPAesMcYcNMbcXv6y5jY0nlJ/usx6mhv4/LUruPGCRfzlw9v49L2b2HN8wumyROpOMbM+PmWt7bHW+q21vdba+ypR2Bw1MTSRpD2itke5BX1evn7zRXzlE+t44/AoH/raRv72sR2MJzNOlyZSN2qybzBRmJrX3qgRdSV854UDZHPwhWtXsrozwlcf2c6Vf/E4X3t0O6OTaafLE3G9mlxCPjQzNU8j6kpqCvm5ZcMyDp6YZMfAOF97dAf3/mwPt16+jN+8erlaUSJlUpNBfXw8v5FQu4LBEb2tYXpbw5zb3cST2we4+6ldfOOZPVx3bhdfv/ki3eAVKbGa/Bs1NJ7EY6A1rBG1k7qbQ9x86VK+dN0qlraF+fGWI9x459M8tX1Qi2VESqgmg/r4RIrWcACvxzhdigCd0RC3vbePT29Yyngyw6/f/wKfvncTT7w9QDKTdbo8kZpXk62PofGk+qFVxhjDeYua+a+/uJYHNu3nzsd38hvffJHGgJdr1nTywbVdXLumk2btzSIybzUX1NZahsZTLO9odLoUOY1/fekQQZ+X3/3AKnYNTrD1yBgbtw/y4y1H8HkMN6zr4bNX97Out9npUkVqRs0F9UA8SSqb043EKufzeljTHWVNd5SbLlrEwRNTbDk4wsNvHuWh1w6zMhbhL3/lAi5e2up0qSJVr+aCenplnKbm1Q6PMSxtC7O0LcwHzu3ixb3DbNw+yC/9n2e57txOfu2KPq5c2aF7DiKzqLmg3jkwDmh701oV8nu5elWMy/raGE9muO+ZPTy6dYBFzSE+fH4PV6/qYEN/G+FAzf1qipRNzf1teP3gCOGAl5YG3ZSqZUG/l6Dfy+9dt5qtR8Z4ef8Jvv3cXu5/Zg9Bn4dr1sS4YV0PHzqvWyfOSN2ruaB+7cAova0NGKN/JruB3+vhgt4WLuhtIZ3NsXdogrePxHl21xAPv3mMBr+XS5a2sGF5O79z3SqnyxVxRE0F9UQyw46BONes6XS6FCkDv9fDqs4oqzqjfPSCHvYcn+CFPcM8t3uIZ3YN8cLeYW7ZsJRr1sTUGpG6UlO/7W8cGiVnobe1welSpMw8xrAiFmFFLEI8kWbzvhO8eWiUz//TywS8Hjb0t7FheRvreltYt7j5jAdITCQzbD8WJ521NPi9dDYF6WoKVfBqRM5OTQX16wdHgfxeE1I/oiE/167p5H2rY+wenGD7sTjbjsb52Y53Tp7pbW3g/EXNdDYFaW7wM5nKsm9ogt2DE+wZmuDUFe3ndEe59pxObrpwEef2NFX4ikTmp6aC+tWDIyxuaSASrKmypUQ8xrCyM8LKzgg3rOshkc5yaGSKQyemODQyxeZ9w0wksyTSWXxeQ3tjkPZIgPef00lPUwMBn4d0NsdgPMm2Y3HufmoXf/fkLnqaQ3z26n4+dtEizc+XqmTKsXnO+vXr7ebNm0v+ulf/r8dZt7iZq1aW//BcqV05azEw5w3niWSG1w+O8PL+EQ6NTOHzGK49p5NPvqeXa9d0EvDV5FY4UqOMMS9Za9ef7rGaGZoOjSc5MDzFpzcsc7oUqXKeImcENQZ9XLGigytWdHB0LMEr+07w/K4hHnnrGOGAlw+f383l/e1c2tfGsrYwHi3IEYfUTFC/fijfn76wt0Xn9knJdTeF+Mi6Hq4/r5udA3FeOTDCk9sGefDlQwBEgj7O7Yly3qJm1vY0sXZRE6u6IgR9muMt5VczQf3agRGMgXW9zQpqKRuvx7Cmu4k13U3krGUgnuTA8CSHR6Y4MprgtQP7SWVzAPi9hpWd0Zng7m1toCMSoCUcoDHgIxz00hjwaWm8nLWaCepXD4ywMhbRjUSpGI8xdDeF6D5pKl/OWobHUxwezQf3kdEpHn7zKP/68sFZXyfk99DVFGJFLMLqrihXrezg0uWtGo1L0Woi9Uan0jy7c4hbNix1uhSpcx5j6IgG6YgGuaD3ne/HE2nGEhkmkhkmUxlSGUsqkyWZzZFK5xiZSvPW4TGe2jbI3z+1i3DAy3tXtPO+NZ1c0d/O8o5GjbxlVjUR1A+/cZRUNsfHL17sdCkipxUN+YmG5t5/JpnJsnsw37p7cvsAj24dACDo89AfixAOePF6DKlMjolkhmQmh99rCPq8LG5tYGVnhHO6o1y0pIWlbWFtpVAnaiKov//qIZa1h7lQm81LjQv6vDMLbM7pjnJ8PMX+4UmOjSUYjCcZnUyTtRavxxD0eWgM+sjkLJlsjtcOjPDY1mPkCjNqW8N+VndFWdUVYUlrmFg0+M5HJEhrOKCZKi5R9UF9dDTBc7uH+OL7V2n0IK5ijJkJ1mJlc5aBeIL9w5McOlG4wXlwhEQ6967nej2GjkiA3tbwzA3Pc3uaWNMVpSGg/vhsHti0/7Tfd7L1WvVB/aPXD2MtfPyiRU6XIuI4r8fQ09xAT3MDLH/n+8lMlvFEhrFEhvFkhngizXgiQzyRYWg8xb9sPkAykw9zj4ElbWH62htZ2hamPRKgNRwgHPDSEPAS8nkJ+b2E/J7Cn/nPG2Y+99ZFP/34eJK3j8aJT6VJZnJctKSFtYuc2W6g6oP6+68e4oLeZvpjEadLEalaQZ+XYMQ76xJ4ay0nJtMcKcxWGYwn2TEQ57WDI4xMpuf9fgGvh0jIR3ODn6aQj6YGP00NfmKR/L8QOqPTf4aIRYO0NQZqJtwzuRwbtx/nyW0DZHIWn8dgDHz8rmf4zx9aw+1XLa94S6mqg/rf3jjCG4fG+PKNa50uRaSmGWNoawzQ1hjgvEU/f68nm7NMpbOkMjnS2RyZrCWVzZHJ5khnLelcjnQmR7rQK09lc6QzlmQmy1Q6/zE8kWIqnSWeyMyM3E/mMdM3XH0zf0aCPvxeg8/rwe8p/Ok1+Dwe/F4P7ZEAXU0hupqCdDeF6IyGaGrwla0FmstZ/u3No9z5+E4G40nWLW7mI+d3z2zy9eLeYf78J1vZuGOQv735YlrPsGNjqRUV1MaYDwNfB7zAvdbavyhrVcBT2wf54nde4ZKlLXzqsiXlfjuRuuX1mPz6hBLtR5XK5GbaL/FEhngyw3gizVQ6SyKdI5HOMhhPcnB4kqy1ZHP5+enZnCWXs2StJZOzpE4T+IFCgLdHAnQUbphOt2saAu+0ZxoCXhr83p9r15z8vaDfQzKdYzKdYd/QJFsOjvLIW8fYdixOLBLkM5cv+7ldFRuDPu7+zHv47osH+NMfvMmNdz7N3Z95D+cvrswEhzmD2hjjBe4CPggcBF40xjxkrX2r1MWkszneOjzGi3uH+at/38aqzijf+I3LtEm8SA0J+Dy0+QJn3CO8GKlMbmZ++vSf44Ue/HgyzfZjcSZT2fyoP5sf9S90izmvx7C2p4mv33wR8UTmtPvFGGP41GVLWdvTxOf+8SU+ftczXLMmxi9d3MsFvc20RwJly6piXvUyYKe1dneh2O8CHwNKGtTJTJb3/NmjjCczAKxb3Mw3f+NSmnU2okhdCvg8tEeCRW89a21+NJ7OFIJ7uk2TfefrdNYW2jg5fB4PAZ+H5pCP7ub8NrgTyeycm3pduKSFH37xKu7ZuJvvv3poZi48QFdTkE1/dN1ZXffpFBPUi4EDJ319EKZ5gJQAAAQfSURBVNhw6pOMMXcAdxS+HDfGbDubwvYBHb8z68MdwPFZH3UHXaN71MN1uv4aP13ENe4DzB8v+C1m3Rq0ZON0a+09wD2ler0zMcZsnm3fVrfQNbpHPVynrrG8itkZ/RBw8t283sL3RESkAooJ6heBVcaY5caYAHAz8FB5yxIRkWlztj6stRljzG8DD5Ofnne/tfbNsld2ZhVpsThM1+ge9XCdusYyKsuZiSIiUjo6vVNEpMopqEVEqlxVB7Ux5sPGmG3GmJ3GmD84zeNBY8w/Fx7fZIzpq3yVZ6eIa/xPxpi3jDGvG2MeM8bU3DHsc13jSc/7hDHGGmNqbppXMddojPnVws/yTWPMA5Wu8WwV8bu61BjzhDHmlcLv6w1O1Hk2jDH3G2MGjDFvzPK4Mcb8beG/wevGmEsqUpi1tio/yN+43AX0AwHgNWDtKc/5PPD3hc9vBv7Z6brLcI3XAuHC559z4zUWnhcFNgLPA+udrrsMP8dVwCtAa+HrTqfrLsM13gN8rvD5WmCv03Uv4Dp/AbgEeGOWx28AfgoY4HJgUyXqquYR9czSdWttCpheun6yjwHfKnz+/4APmNo6XWDOa7TWPmGtnSx8+Tz5eey1pJifI8CfAV8BEpUsrkSKucbPAndZa08AWGsHqC3FXKMFpncyagYOV7C+krDWbgSGz/CUjwHftnnPAy3GmJ5y11XNQX26peunHpo48xxrbQYYBdorUl1pFHONJ7ud/P/Na8mc11j45+MSa+2PK1lYCRXzc1wNrDbGPGOMeb6wI2UtKeYa/xtwqzHmIPAT4IuVKa2i5vt3tiS0LV2NMMbcCqwH3ud0LaVkjPEAXwVuc7iUcvORb39cQ/5fRRuNMeustSOOVlVanwK+aa39a2PMFcA/GGPOt9a+e79SmZdqHlEXs3R95jnGGB/5f24NVaS60ihqeb4x5jrgj4GbrLXJCtVWKnNdYxQ4H3jSGLOXfN/voRq7oVjMz/Eg8JC1Nm2t3QNsJx/ctaKYa7wd+BcAa+1zQIj8RkZu4siWGtUc1MUsXX8I+PXC558EHreFjn+NmPMajTEXA3eTD+la62vCHNdorR211nZYa/ustX3k+/A3WWs3O1PughTzu/p98qNpjDEd5FshuytZ5Fkq5hr3Ax8AMMacSz6oBytaZfk9BPxaYfbH5cCotfZI2d/V6busc9yBvYH8yGMX8MeF7/0P8n+RIf+L8H+BncALQL/TNZfhGh8FjgGvFj4ecrrmUl/jKc99khqb9VHkz9GQb/G8BWwBbna65jJc41rgGfIzQl4Frne65gVc43eAI0Ca/L+Cbgd+C/itk36OdxX+G2yp1O+qlpCLiFS5am59iIgICmoRkaqnoBYRqXIKahGRKqegFhGpcgpqEZEqp6AWEaly/x8yp2JDc9YaDgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TISYtM71A1Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
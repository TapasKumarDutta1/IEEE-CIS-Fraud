{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_0.9",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_0_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "adb72b0b-b686-49e9-e110-695ee6ece238"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b698c20d-1788-47a3-f543-89026935b464"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 84% 44.0M/52.2M [00:00<00:00, 111MB/s] \n",
            "100% 52.2M/52.2M [00:00<00:00, 150MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 107MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 94% 55.0M/58.3M [00:00<00:00, 129MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 168MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 108MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 76.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv80v8W_Ko2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c1f8e18c-e546-4b02-c00a-1101911fb500"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jt2pcxPije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d6ea369d-10b8-4cb4-e854-0798973c651e"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvEaxp9jhbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "bfb93745-290d-4113-9913-e6ce5a560533"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5b82519-56d8-41e9-ced2-bb24109af3ad"
      },
      "source": [
        "\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkLEBcl6LHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 0.9\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfCX4721TByP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8eb13307-b7f9-40ef-916e-5a570aa29de3"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "# trn=trn.drop(['isFraud_isna'],1)\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Adam(0.001,decay=1e-3),loss=rac,metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.00001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 24536.2656 - accuracy: 0.6770 - val_loss: 19953.7617 - val_accuracy: 0.5608\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 19412.8633 - accuracy: 0.7468 - val_loss: 17292.1719 - val_accuracy: 0.5985\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 17189.6250 - accuracy: 0.7779 - val_loss: 17323.6738 - val_accuracy: 0.6522\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 15844.2754 - accuracy: 0.7982 - val_loss: 16427.9180 - val_accuracy: 0.6843\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14911.1250 - accuracy: 0.8084 - val_loss: 16636.7129 - val_accuracy: 0.6761\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14043.2646 - accuracy: 0.8183 - val_loss: 16057.5615 - val_accuracy: 0.7029\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13511.3281 - accuracy: 0.8234 - val_loss: 15312.9092 - val_accuracy: 0.7276\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12906.7402 - accuracy: 0.8288 - val_loss: 15202.6416 - val_accuracy: 0.7227\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12497.2725 - accuracy: 0.8354 - val_loss: 14724.9648 - val_accuracy: 0.7182\n",
            "Epoch 10/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 12104.4238 - accuracy: 0.8376roc-auc_val: 0.8\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 12079.0430 - accuracy: 0.8377 - val_loss: 15138.9248 - val_accuracy: 0.7342\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11711.4150 - accuracy: 0.8427 - val_loss: 15011.1309 - val_accuracy: 0.7088\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11436.8926 - accuracy: 0.8430 - val_loss: 15074.1074 - val_accuracy: 0.7231\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11241.6484 - accuracy: 0.8451 - val_loss: 14325.1445 - val_accuracy: 0.7643\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10898.4580 - accuracy: 0.8490 - val_loss: 14500.4785 - val_accuracy: 0.7647\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10759.4150 - accuracy: 0.8493 - val_loss: 14277.3672 - val_accuracy: 0.7482\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10477.0527 - accuracy: 0.8521 - val_loss: 14147.0000 - val_accuracy: 0.7635\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10382.4287 - accuracy: 0.8518 - val_loss: 14520.2100 - val_accuracy: 0.7435\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10142.8076 - accuracy: 0.8560 - val_loss: 14417.0576 - val_accuracy: 0.7732\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10026.2305 - accuracy: 0.8563 - val_loss: 13984.7012 - val_accuracy: 0.7893\n",
            "Epoch 20/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 9899.5654 - accuracy: 0.8592roc-auc_val: 0.8142\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 9872.0449 - accuracy: 0.8592 - val_loss: 14129.6416 - val_accuracy: 0.7444\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9759.1064 - accuracy: 0.8578 - val_loss: 13897.7090 - val_accuracy: 0.7745\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9579.3125 - accuracy: 0.8595 - val_loss: 13803.8115 - val_accuracy: 0.7823\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9393.3457 - accuracy: 0.8605 - val_loss: 13989.1797 - val_accuracy: 0.7594\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9338.7607 - accuracy: 0.8616 - val_loss: 13992.2490 - val_accuracy: 0.7649\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9273.6445 - accuracy: 0.8628 - val_loss: 14089.8281 - val_accuracy: 0.7969\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9038.5010 - accuracy: 0.8648 - val_loss: 14102.9648 - val_accuracy: 0.7733\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9110.0830 - accuracy: 0.8650 - val_loss: 13905.4121 - val_accuracy: 0.7821\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8956.3271 - accuracy: 0.8647 - val_loss: 13830.6572 - val_accuracy: 0.7865\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8843.4775 - accuracy: 0.8647 - val_loss: 13734.0312 - val_accuracy: 0.7905\n",
            "Epoch 30/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 8751.0352 - accuracy: 0.8666roc-auc_val: 0.8237\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 8740.5039 - accuracy: 0.8664 - val_loss: 13586.2490 - val_accuracy: 0.8017\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8678.4209 - accuracy: 0.8668 - val_loss: 13907.5605 - val_accuracy: 0.7756\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8621.2549 - accuracy: 0.8695 - val_loss: 13675.4170 - val_accuracy: 0.7864\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8614.2666 - accuracy: 0.8698 - val_loss: 13710.6104 - val_accuracy: 0.7875\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8408.0742 - accuracy: 0.8710 - val_loss: 13662.1797 - val_accuracy: 0.7883\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8444.9219 - accuracy: 0.8697 - val_loss: 13615.8809 - val_accuracy: 0.7873\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8375.9189 - accuracy: 0.8711 - val_loss: 13719.7021 - val_accuracy: 0.7962\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8234.6797 - accuracy: 0.8733 - val_loss: 13528.9082 - val_accuracy: 0.7918\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8294.9717 - accuracy: 0.8726 - val_loss: 13520.0674 - val_accuracy: 0.7965\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8246.8135 - accuracy: 0.8709 - val_loss: 13477.5732 - val_accuracy: 0.7929\n",
            "Epoch 40/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 8140.7725 - accuracy: 0.8715roc-auc_val: 0.8222\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 8134.0112 - accuracy: 0.8716 - val_loss: 13679.4883 - val_accuracy: 0.7829\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8097.0449 - accuracy: 0.8741 - val_loss: 13716.9033 - val_accuracy: 0.7775\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7892.8628 - accuracy: 0.8746 - val_loss: 13656.7354 - val_accuracy: 0.8020\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8024.9937 - accuracy: 0.8733 - val_loss: 13613.2900 - val_accuracy: 0.7980\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7985.9263 - accuracy: 0.8737 - val_loss: 13416.8906 - val_accuracy: 0.8002\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7917.9541 - accuracy: 0.8738 - val_loss: 13545.7168 - val_accuracy: 0.8038\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7848.0615 - accuracy: 0.8774 - val_loss: 13466.7539 - val_accuracy: 0.8089\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7798.5024 - accuracy: 0.8755 - val_loss: 13628.3555 - val_accuracy: 0.7872\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7771.4634 - accuracy: 0.8749 - val_loss: 13561.8975 - val_accuracy: 0.8109\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7670.3921 - accuracy: 0.8789 - val_loss: 13390.5879 - val_accuracy: 0.8070\n",
            "Epoch 50/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 7710.0195 - accuracy: 0.8772roc-auc_val: 0.8229\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 7697.8745 - accuracy: 0.8771 - val_loss: 13656.3721 - val_accuracy: 0.7954\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7690.6616 - accuracy: 0.8761 - val_loss: 13572.9629 - val_accuracy: 0.8000\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7595.8740 - accuracy: 0.8762 - val_loss: 13515.6904 - val_accuracy: 0.8024\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7534.6504 - accuracy: 0.8766 - val_loss: 13459.6895 - val_accuracy: 0.8062\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7475.9575 - accuracy: 0.8780 - val_loss: 13585.3438 - val_accuracy: 0.7970\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7550.0225 - accuracy: 0.8776 - val_loss: 13404.8760 - val_accuracy: 0.8086\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7406.4546 - accuracy: 0.8764 - val_loss: 13651.7031 - val_accuracy: 0.7940\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7376.8359 - accuracy: 0.8780 - val_loss: 13575.6631 - val_accuracy: 0.8085\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7370.1968 - accuracy: 0.8777 - val_loss: 13521.0957 - val_accuracy: 0.7979\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7454.3667 - accuracy: 0.8795 - val_loss: 13418.2949 - val_accuracy: 0.8062\n",
            "Epoch 60/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 7319.0034 - accuracy: 0.8799roc-auc_val: 0.825\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 7308.2324 - accuracy: 0.8799 - val_loss: 13531.2832 - val_accuracy: 0.7986\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7331.3145 - accuracy: 0.8791 - val_loss: 13307.0654 - val_accuracy: 0.8120\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7385.6533 - accuracy: 0.8785 - val_loss: 13508.2744 - val_accuracy: 0.8025\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7251.9165 - accuracy: 0.8813 - val_loss: 13399.7998 - val_accuracy: 0.8134\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7197.9609 - accuracy: 0.8810 - val_loss: 13335.4482 - val_accuracy: 0.8036\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7200.5991 - accuracy: 0.8800 - val_loss: 13599.1660 - val_accuracy: 0.7907\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7196.7905 - accuracy: 0.8799 - val_loss: 13421.1201 - val_accuracy: 0.8090\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7099.1924 - accuracy: 0.8797 - val_loss: 13427.7334 - val_accuracy: 0.8034\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7143.8774 - accuracy: 0.8806 - val_loss: 13579.4443 - val_accuracy: 0.7952\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7158.3286 - accuracy: 0.8812 - val_loss: 13358.7061 - val_accuracy: 0.8153\n",
            "Epoch 70/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 7061.8643 - accuracy: 0.8824roc-auc_val: 0.828\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 7052.0786 - accuracy: 0.8825 - val_loss: 13377.0059 - val_accuracy: 0.8096\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6962.6992 - accuracy: 0.8832 - val_loss: 13399.8789 - val_accuracy: 0.8094\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7112.9819 - accuracy: 0.8820 - val_loss: 13439.8379 - val_accuracy: 0.7887\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7080.8403 - accuracy: 0.8810 - val_loss: 13559.4766 - val_accuracy: 0.8129\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6993.0991 - accuracy: 0.8831 - val_loss: 13461.1699 - val_accuracy: 0.8173\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6984.9292 - accuracy: 0.8822 - val_loss: 13610.8340 - val_accuracy: 0.8022\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6870.6416 - accuracy: 0.8835 - val_loss: 13565.4756 - val_accuracy: 0.8152\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6972.0688 - accuracy: 0.8828 - val_loss: 13538.1230 - val_accuracy: 0.8097\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6838.8687 - accuracy: 0.8828 - val_loss: 13420.2773 - val_accuracy: 0.8097\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6830.4795 - accuracy: 0.8826 - val_loss: 13444.2969 - val_accuracy: 0.8074\n",
            "Epoch 80/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 6935.1953 - accuracy: 0.8836roc-auc_val: 0.8252\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 6912.1938 - accuracy: 0.8835 - val_loss: 13541.4756 - val_accuracy: 0.8010\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6853.3037 - accuracy: 0.8820 - val_loss: 13274.5264 - val_accuracy: 0.8114\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6877.0977 - accuracy: 0.8839 - val_loss: 13428.3525 - val_accuracy: 0.8098\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6751.4751 - accuracy: 0.8833 - val_loss: 13454.4463 - val_accuracy: 0.8132\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6815.1899 - accuracy: 0.8844 - val_loss: 13457.9609 - val_accuracy: 0.8056\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6769.8955 - accuracy: 0.8832 - val_loss: 13375.5508 - val_accuracy: 0.8202\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6805.1548 - accuracy: 0.8860 - val_loss: 13391.1104 - val_accuracy: 0.8140\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6703.7729 - accuracy: 0.8850 - val_loss: 13385.6533 - val_accuracy: 0.8065\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6712.8394 - accuracy: 0.8854 - val_loss: 13555.3252 - val_accuracy: 0.8105\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6733.0239 - accuracy: 0.8848 - val_loss: 13439.4629 - val_accuracy: 0.8060\n",
            "Epoch 90/1000\n",
            "218/225 [============================>.] - ETA: 0s - loss: 6620.3198 - accuracy: 0.8848roc-auc_val: 0.8256\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 6629.2109 - accuracy: 0.8848 - val_loss: 13495.6631 - val_accuracy: 0.8057\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6625.7744 - accuracy: 0.8855 - val_loss: 13478.4072 - val_accuracy: 0.8108\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6574.7974 - accuracy: 0.8863 - val_loss: 13359.4834 - val_accuracy: 0.8050\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6693.0054 - accuracy: 0.8854 - val_loss: 13385.8271 - val_accuracy: 0.8079\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6589.9209 - accuracy: 0.8852 - val_loss: 13489.2197 - val_accuracy: 0.8019\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6572.8389 - accuracy: 0.8833 - val_loss: 13420.0078 - val_accuracy: 0.8103\n",
            "Epoch 96/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6554.8740 - accuracy: 0.8860 - val_loss: 13475.7793 - val_accuracy: 0.8021\n",
            "Epoch 97/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6584.1567 - accuracy: 0.8869 - val_loss: 13356.5234 - val_accuracy: 0.8173\n",
            "Epoch 98/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6665.9326 - accuracy: 0.8851 - val_loss: 13355.6914 - val_accuracy: 0.8107\n",
            "Epoch 99/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6599.0308 - accuracy: 0.8865 - val_loss: 13411.2529 - val_accuracy: 0.8037\n",
            "Epoch 100/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 6523.5898 - accuracy: 0.8860roc-auc_val: 0.8272\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 6507.1763 - accuracy: 0.8860 - val_loss: 13409.9756 - val_accuracy: 0.7998\n",
            "Epoch 101/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6530.0283 - accuracy: 0.8843 - val_loss: 13335.5039 - val_accuracy: 0.8110\n",
            "Epoch 102/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6474.6235 - accuracy: 0.8860 - val_loss: 13322.1621 - val_accuracy: 0.8114\n",
            "Epoch 103/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6533.8857 - accuracy: 0.8866 - val_loss: 13497.4844 - val_accuracy: 0.8040\n",
            "Epoch 104/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6495.8896 - accuracy: 0.8863 - val_loss: 13424.0918 - val_accuracy: 0.8044\n",
            "Epoch 105/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6490.8647 - accuracy: 0.8860 - val_loss: 13525.8965 - val_accuracy: 0.8134\n",
            "Epoch 106/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6497.9058 - accuracy: 0.8870 - val_loss: 13362.3701 - val_accuracy: 0.8055\n",
            "Epoch 107/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6384.3052 - accuracy: 0.8868 - val_loss: 13411.9629 - val_accuracy: 0.8064\n",
            "Epoch 108/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6544.7065 - accuracy: 0.8861 - val_loss: 13410.3145 - val_accuracy: 0.8101\n",
            "Epoch 109/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6332.8433 - accuracy: 0.8875 - val_loss: 13459.4639 - val_accuracy: 0.8100\n",
            "Epoch 110/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 6448.8267 - accuracy: 0.8879roc-auc_val: 0.8256\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 6463.1060 - accuracy: 0.8879 - val_loss: 13526.4492 - val_accuracy: 0.8030\n",
            "Epoch 111/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6489.1235 - accuracy: 0.8855 - val_loss: 13428.5859 - val_accuracy: 0.8020\n",
            "Epoch 112/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6416.7354 - accuracy: 0.8861 - val_loss: 13494.7461 - val_accuracy: 0.8025\n",
            "Epoch 113/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6443.2832 - accuracy: 0.8862 - val_loss: 13504.8887 - val_accuracy: 0.8085\n",
            "Epoch 114/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6379.2046 - accuracy: 0.8881 - val_loss: 13391.9023 - val_accuracy: 0.8061\n",
            "Epoch 115/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6270.0254 - accuracy: 0.8888 - val_loss: 13496.0635 - val_accuracy: 0.7958\n",
            "Epoch 116/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6275.4443 - accuracy: 0.8879 - val_loss: 13376.0059 - val_accuracy: 0.8111\n",
            "Epoch 117/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6306.4233 - accuracy: 0.8896 - val_loss: 13433.5596 - val_accuracy: 0.8018\n",
            "Epoch 118/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6243.2979 - accuracy: 0.8887 - val_loss: 13470.5234 - val_accuracy: 0.8202\n",
            "Epoch 119/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6327.3809 - accuracy: 0.8887 - val_loss: 13473.2598 - val_accuracy: 0.8141\n",
            "Epoch 120/1000\n",
            "214/225 [===========================>..] - ETA: 0s - loss: 6319.8965 - accuracy: 0.8889roc-auc_val: 0.8273\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 6296.3198 - accuracy: 0.8890 - val_loss: 13410.8115 - val_accuracy: 0.8121\n",
            "Epoch 121/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6292.6660 - accuracy: 0.8889 - val_loss: 13388.8779 - val_accuracy: 0.8117\n",
            "Epoch 122/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6295.1338 - accuracy: 0.8898 - val_loss: 13396.5762 - val_accuracy: 0.8126\n",
            "Epoch 123/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6233.1626 - accuracy: 0.8893 - val_loss: 13433.2168 - val_accuracy: 0.8081\n",
            "Epoch 124/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6288.8628 - accuracy: 0.8889 - val_loss: 13465.6338 - val_accuracy: 0.8060\n",
            "Epoch 125/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6229.2095 - accuracy: 0.8887 - val_loss: 13410.9268 - val_accuracy: 0.8125\n",
            "Epoch 126/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6130.9170 - accuracy: 0.8905 - val_loss: 13395.2207 - val_accuracy: 0.8142\n",
            "Epoch 127/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6231.9834 - accuracy: 0.8898 - val_loss: 13437.3838 - val_accuracy: 0.8082\n",
            "Epoch 128/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6249.1309 - accuracy: 0.8885 - val_loss: 13276.6289 - val_accuracy: 0.8238\n",
            "Epoch 129/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6184.3027 - accuracy: 0.8895 - val_loss: 13328.9150 - val_accuracy: 0.8172\n",
            "Epoch 130/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 6218.0723 - accuracy: 0.8893roc-auc_val: 0.8287\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 6213.0874 - accuracy: 0.8892 - val_loss: 13333.2246 - val_accuracy: 0.8125\n",
            "Epoch 131/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6107.3276 - accuracy: 0.8905 - val_loss: 13334.5479 - val_accuracy: 0.8138\n",
            "Epoch 1/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 24257.0879 - accuracy: 0.6628 - val_loss: 20613.8809 - val_accuracy: 0.6365\n",
            "Epoch 2/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 19347.8262 - accuracy: 0.7312 - val_loss: 18401.1211 - val_accuracy: 0.6554\n",
            "Epoch 3/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 17172.2363 - accuracy: 0.7691 - val_loss: 17734.7598 - val_accuracy: 0.6926\n",
            "Epoch 4/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 15711.8320 - accuracy: 0.7923 - val_loss: 16924.3555 - val_accuracy: 0.7320\n",
            "Epoch 5/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14612.2012 - accuracy: 0.8054 - val_loss: 17468.9238 - val_accuracy: 0.7137\n",
            "Epoch 6/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13799.7402 - accuracy: 0.8186 - val_loss: 17445.4590 - val_accuracy: 0.7606\n",
            "Epoch 7/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12971.0732 - accuracy: 0.8257 - val_loss: 17601.5371 - val_accuracy: 0.7579\n",
            "Epoch 8/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12658.7451 - accuracy: 0.8331 - val_loss: 17114.5859 - val_accuracy: 0.7537\n",
            "Epoch 9/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12159.9580 - accuracy: 0.8348 - val_loss: 17475.8438 - val_accuracy: 0.7329\n",
            "Epoch 10/1000\n",
            "175/181 [============================>.] - ETA: 0s - loss: 11837.9824 - accuracy: 0.8396roc-auc_val: 0.8197\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 11824.3262 - accuracy: 0.8394 - val_loss: 16601.0586 - val_accuracy: 0.7542\n",
            "Epoch 11/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11312.8779 - accuracy: 0.8436 - val_loss: 16813.6250 - val_accuracy: 0.7415\n",
            "Epoch 12/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 11000.8838 - accuracy: 0.8436 - val_loss: 17257.2129 - val_accuracy: 0.7766\n",
            "Epoch 13/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 10698.7832 - accuracy: 0.8496 - val_loss: 16826.2773 - val_accuracy: 0.7712\n",
            "Epoch 14/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 10358.5293 - accuracy: 0.8488 - val_loss: 16884.4062 - val_accuracy: 0.7933\n",
            "Epoch 15/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 10273.6816 - accuracy: 0.8502 - val_loss: 16822.4434 - val_accuracy: 0.7717\n",
            "Epoch 16/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 10138.7979 - accuracy: 0.8522 - val_loss: 17057.8418 - val_accuracy: 0.7995\n",
            "Epoch 17/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 9753.3945 - accuracy: 0.8573 - val_loss: 17111.5117 - val_accuracy: 0.7802\n",
            "Epoch 18/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 9542.6211 - accuracy: 0.8596 - val_loss: 17218.5625 - val_accuracy: 0.7951\n",
            "Epoch 19/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 9486.4756 - accuracy: 0.8574 - val_loss: 16811.3418 - val_accuracy: 0.8000\n",
            "Epoch 20/1000\n",
            "171/181 [===========================>..] - ETA: 0s - loss: 9241.5986 - accuracy: 0.8597roc-auc_val: 0.8178\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 9214.7041 - accuracy: 0.8596 - val_loss: 16841.4512 - val_accuracy: 0.8039\n",
            "Epoch 21/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9252.5713 - accuracy: 0.8608 - val_loss: 16954.7930 - val_accuracy: 0.7960\n",
            "Epoch 22/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9071.5537 - accuracy: 0.8632 - val_loss: 17236.6719 - val_accuracy: 0.7853\n",
            "Epoch 23/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8850.8867 - accuracy: 0.8619 - val_loss: 17356.5410 - val_accuracy: 0.7867\n",
            "Epoch 24/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8836.8945 - accuracy: 0.8628 - val_loss: 17099.6660 - val_accuracy: 0.8110\n",
            "Epoch 25/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8720.7666 - accuracy: 0.8658 - val_loss: 16949.8047 - val_accuracy: 0.7997\n",
            "Epoch 26/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8504.8740 - accuracy: 0.8655 - val_loss: 16857.5918 - val_accuracy: 0.7941\n",
            "Epoch 27/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8506.2559 - accuracy: 0.8661 - val_loss: 16887.9316 - val_accuracy: 0.7867\n",
            "Epoch 28/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8501.9961 - accuracy: 0.8667 - val_loss: 17183.9707 - val_accuracy: 0.7977\n",
            "Epoch 29/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8292.8809 - accuracy: 0.8686 - val_loss: 17062.1367 - val_accuracy: 0.8008\n",
            "Epoch 30/1000\n",
            "173/181 [===========================>..] - ETA: 0s - loss: 8155.1162 - accuracy: 0.8707roc-auc_val: 0.816\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 8150.6636 - accuracy: 0.8707 - val_loss: 17059.5703 - val_accuracy: 0.8058\n",
            "Epoch 31/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8104.3975 - accuracy: 0.8709 - val_loss: 16907.4180 - val_accuracy: 0.8025\n",
            "Epoch 32/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8000.6230 - accuracy: 0.8689 - val_loss: 17525.4766 - val_accuracy: 0.8171\n",
            "Epoch 33/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7906.4482 - accuracy: 0.8712 - val_loss: 17365.6250 - val_accuracy: 0.8073\n",
            "Epoch 34/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7825.8164 - accuracy: 0.8708 - val_loss: 17348.6094 - val_accuracy: 0.7996\n",
            "Epoch 35/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7809.8701 - accuracy: 0.8709 - val_loss: 17419.9180 - val_accuracy: 0.7891\n",
            "Epoch 36/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7776.4478 - accuracy: 0.8725 - val_loss: 17042.9922 - val_accuracy: 0.8040\n",
            "Epoch 37/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7668.5947 - accuracy: 0.8722 - val_loss: 17276.4551 - val_accuracy: 0.8005\n",
            "Epoch 38/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7615.5996 - accuracy: 0.8717 - val_loss: 17408.3379 - val_accuracy: 0.8016\n",
            "Epoch 39/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7573.1934 - accuracy: 0.8716 - val_loss: 17437.4824 - val_accuracy: 0.8113\n",
            "Epoch 40/1000\n",
            "172/181 [===========================>..] - ETA: 0s - loss: 7560.9067 - accuracy: 0.8725roc-auc_val: 0.8144\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 7553.5952 - accuracy: 0.8730 - val_loss: 17297.6523 - val_accuracy: 0.8282\n",
            "Epoch 41/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7423.0278 - accuracy: 0.8756 - val_loss: 17493.4785 - val_accuracy: 0.8041\n",
            "Epoch 42/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 7439.0029 - accuracy: 0.8747 - val_loss: 17399.0840 - val_accuracy: 0.8184\n",
            "Epoch 43/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7307.9062 - accuracy: 0.8766 - val_loss: 17316.8984 - val_accuracy: 0.8228\n",
            "Epoch 44/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7397.4282 - accuracy: 0.8750 - val_loss: 17201.9102 - val_accuracy: 0.8251\n",
            "Epoch 45/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7346.4048 - accuracy: 0.8764 - val_loss: 17270.0762 - val_accuracy: 0.8086\n",
            "Epoch 46/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7195.2720 - accuracy: 0.8773 - val_loss: 17464.9648 - val_accuracy: 0.8193\n",
            "Epoch 47/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7191.7539 - accuracy: 0.8791 - val_loss: 17637.9668 - val_accuracy: 0.8145\n",
            "Epoch 48/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7045.4756 - accuracy: 0.8792 - val_loss: 17368.9141 - val_accuracy: 0.8283\n",
            "Epoch 49/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6982.9067 - accuracy: 0.8788 - val_loss: 17623.7793 - val_accuracy: 0.8173\n",
            "Epoch 50/1000\n",
            "179/181 [============================>.] - ETA: 0s - loss: 7022.7373 - accuracy: 0.8792roc-auc_val: 0.8145\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 7013.8672 - accuracy: 0.8790 - val_loss: 17356.1270 - val_accuracy: 0.8321\n",
            "Epoch 51/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7003.5396 - accuracy: 0.8784 - val_loss: 17683.0391 - val_accuracy: 0.8159\n",
            "Epoch 52/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6892.0518 - accuracy: 0.8812 - val_loss: 17523.3066 - val_accuracy: 0.8215\n",
            "Epoch 53/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6892.7153 - accuracy: 0.8786 - val_loss: 17805.2266 - val_accuracy: 0.8176\n",
            "Epoch 54/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6837.9238 - accuracy: 0.8794 - val_loss: 17674.2402 - val_accuracy: 0.8348\n",
            "Epoch 55/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6782.9312 - accuracy: 0.8805 - val_loss: 17519.2188 - val_accuracy: 0.8301\n",
            "Epoch 56/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6761.9219 - accuracy: 0.8819 - val_loss: 17603.2930 - val_accuracy: 0.8295\n",
            "Epoch 57/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6768.7817 - accuracy: 0.8805 - val_loss: 17580.9688 - val_accuracy: 0.8250\n",
            "Epoch 58/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6716.6997 - accuracy: 0.8806 - val_loss: 17527.5059 - val_accuracy: 0.8181\n",
            "Epoch 59/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6591.0215 - accuracy: 0.8805 - val_loss: 17521.8633 - val_accuracy: 0.8234\n",
            "Epoch 60/1000\n",
            "175/181 [============================>.] - ETA: 0s - loss: 6659.6138 - accuracy: 0.8811roc-auc_val: 0.8197\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 6657.2856 - accuracy: 0.8812 - val_loss: 17413.9004 - val_accuracy: 0.8298\n",
            "Epoch 1/1000\n",
            "136/136 [==============================] - 1s 11ms/step - loss: 24370.0703 - accuracy: 0.6570 - val_loss: 21393.3379 - val_accuracy: 0.7295\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 19055.8496 - accuracy: 0.7280 - val_loss: 20433.9609 - val_accuracy: 0.7480\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 17228.6855 - accuracy: 0.7461 - val_loss: 19944.6426 - val_accuracy: 0.7088\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 15770.4619 - accuracy: 0.7651 - val_loss: 19121.7500 - val_accuracy: 0.7544\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14600.8057 - accuracy: 0.7818 - val_loss: 19662.2773 - val_accuracy: 0.8179\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13834.1533 - accuracy: 0.7947 - val_loss: 18888.9062 - val_accuracy: 0.7889\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12982.3965 - accuracy: 0.8069 - val_loss: 18911.3477 - val_accuracy: 0.8095\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12379.8184 - accuracy: 0.8140 - val_loss: 18638.3652 - val_accuracy: 0.8260\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11687.9277 - accuracy: 0.8239 - val_loss: 19297.5645 - val_accuracy: 0.8267\n",
            "Epoch 10/1000\n",
            "129/136 [===========================>..] - ETA: 0s - loss: 11350.2578 - accuracy: 0.8276roc-auc_val: 0.8132\n",
            "136/136 [==============================] - 9s 66ms/step - loss: 11329.0566 - accuracy: 0.8277 - val_loss: 19276.8164 - val_accuracy: 0.8517\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11084.9375 - accuracy: 0.8300 - val_loss: 18909.7246 - val_accuracy: 0.8431\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10739.8682 - accuracy: 0.8324 - val_loss: 18826.6895 - val_accuracy: 0.8320\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10295.2979 - accuracy: 0.8363 - val_loss: 18957.6777 - val_accuracy: 0.8194\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10110.6387 - accuracy: 0.8384 - val_loss: 19062.9922 - val_accuracy: 0.8412\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9641.8633 - accuracy: 0.8465 - val_loss: 19337.5938 - val_accuracy: 0.8542\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9446.8711 - accuracy: 0.8441 - val_loss: 19305.2070 - val_accuracy: 0.8559\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9241.2207 - accuracy: 0.8511 - val_loss: 19108.1543 - val_accuracy: 0.8592\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9103.3955 - accuracy: 0.8507 - val_loss: 19338.6699 - val_accuracy: 0.8518\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8811.1172 - accuracy: 0.8554 - val_loss: 19282.4004 - val_accuracy: 0.8456\n",
            "Epoch 20/1000\n",
            "128/136 [===========================>..] - ETA: 0s - loss: 8891.7617 - accuracy: 0.8535roc-auc_val: 0.8092\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 8818.3984 - accuracy: 0.8536 - val_loss: 19426.4707 - val_accuracy: 0.8533\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8517.7432 - accuracy: 0.8565 - val_loss: 19487.3828 - val_accuracy: 0.8611\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8422.9932 - accuracy: 0.8577 - val_loss: 19973.7754 - val_accuracy: 0.8626\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8152.1890 - accuracy: 0.8571 - val_loss: 19436.7871 - val_accuracy: 0.8597\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8164.3955 - accuracy: 0.8584 - val_loss: 19579.9805 - val_accuracy: 0.8402\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7988.2734 - accuracy: 0.8560 - val_loss: 19661.7109 - val_accuracy: 0.8453\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7831.1660 - accuracy: 0.8615 - val_loss: 20053.5703 - val_accuracy: 0.8650\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7712.9214 - accuracy: 0.8623 - val_loss: 19863.7207 - val_accuracy: 0.8610\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7715.8823 - accuracy: 0.8641 - val_loss: 19777.7227 - val_accuracy: 0.8662\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7600.0400 - accuracy: 0.8646 - val_loss: 19503.1562 - val_accuracy: 0.8734\n",
            "Epoch 30/1000\n",
            "130/136 [===========================>..] - ETA: 0s - loss: 7344.5879 - accuracy: 0.8665roc-auc_val: 0.8047\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 7386.7349 - accuracy: 0.8667 - val_loss: 19939.8438 - val_accuracy: 0.8721\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7346.2163 - accuracy: 0.8683 - val_loss: 19784.8398 - val_accuracy: 0.8597\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7317.8384 - accuracy: 0.8702 - val_loss: 19787.6016 - val_accuracy: 0.8641\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7143.3413 - accuracy: 0.8688 - val_loss: 19836.4355 - val_accuracy: 0.8451\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7046.8413 - accuracy: 0.8707 - val_loss: 19653.2422 - val_accuracy: 0.8624\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7039.2358 - accuracy: 0.8704 - val_loss: 20050.2461 - val_accuracy: 0.8580\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6935.8379 - accuracy: 0.8729 - val_loss: 20053.4570 - val_accuracy: 0.8635\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6784.7930 - accuracy: 0.8742 - val_loss: 19926.5977 - val_accuracy: 0.8546\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6831.1221 - accuracy: 0.8729 - val_loss: 20095.3848 - val_accuracy: 0.8650\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6791.7500 - accuracy: 0.8731 - val_loss: 20021.3320 - val_accuracy: 0.8697\n",
            "Epoch 40/1000\n",
            "129/136 [===========================>..] - ETA: 0s - loss: 6617.2134 - accuracy: 0.8764roc-auc_val: 0.802\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 6617.6255 - accuracy: 0.8762 - val_loss: 20062.8496 - val_accuracy: 0.8726\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6689.4097 - accuracy: 0.8726 - val_loss: 19811.8086 - val_accuracy: 0.8620\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6518.0386 - accuracy: 0.8734 - val_loss: 20281.0332 - val_accuracy: 0.8807\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6431.6768 - accuracy: 0.8771 - val_loss: 20108.5938 - val_accuracy: 0.8749\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6417.3188 - accuracy: 0.8782 - val_loss: 20110.5293 - val_accuracy: 0.8684\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6415.6089 - accuracy: 0.8768 - val_loss: 20321.8398 - val_accuracy: 0.8700\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6361.7822 - accuracy: 0.8802 - val_loss: 20140.0430 - val_accuracy: 0.8556\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6242.1123 - accuracy: 0.8773 - val_loss: 19916.0312 - val_accuracy: 0.8727\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6257.9370 - accuracy: 0.8799 - val_loss: 20197.3203 - val_accuracy: 0.8582\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6204.8486 - accuracy: 0.8797 - val_loss: 20217.5762 - val_accuracy: 0.8654\n",
            "Epoch 50/1000\n",
            "136/136 [==============================] - ETA: 0s - loss: 6096.4673 - accuracy: 0.8803roc-auc_val: 0.795\n",
            "136/136 [==============================] - 9s 66ms/step - loss: 6096.4673 - accuracy: 0.8803 - val_loss: 20644.4473 - val_accuracy: 0.8742\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6055.2031 - accuracy: 0.8819 - val_loss: 20468.8105 - val_accuracy: 0.8642\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6130.8569 - accuracy: 0.8789 - val_loss: 20514.2969 - val_accuracy: 0.8677\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 5983.8545 - accuracy: 0.8817 - val_loss: 20446.3906 - val_accuracy: 0.8841\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 5916.8936 - accuracy: 0.8836 - val_loss: 20621.3301 - val_accuracy: 0.8801\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 5901.7788 - accuracy: 0.8846 - val_loss: 20576.6289 - val_accuracy: 0.8745\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 5797.8149 - accuracy: 0.8850 - val_loss: 20571.9844 - val_accuracy: 0.8782\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 5766.3687 - accuracy: 0.8839 - val_loss: 20554.5312 - val_accuracy: 0.8762\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 5836.8589 - accuracy: 0.8835 - val_loss: 20494.1680 - val_accuracy: 0.8756\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 1s 17ms/step - loss: 24031.8730 - accuracy: 0.6453 - val_loss: 23074.3594 - val_accuracy: 0.8019\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 18530.9512 - accuracy: 0.7334 - val_loss: 21408.3008 - val_accuracy: 0.7749\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 16508.4688 - accuracy: 0.7621 - val_loss: 21161.7285 - val_accuracy: 0.8193\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 15012.9512 - accuracy: 0.7779 - val_loss: 20090.0273 - val_accuracy: 0.8064\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13786.7549 - accuracy: 0.8005 - val_loss: 20057.9258 - val_accuracy: 0.8128\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13163.1514 - accuracy: 0.8035 - val_loss: 20261.5156 - val_accuracy: 0.8415\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12109.1973 - accuracy: 0.8193 - val_loss: 19754.5273 - val_accuracy: 0.8425\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11536.4053 - accuracy: 0.8187 - val_loss: 19754.8066 - val_accuracy: 0.8569\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11163.2842 - accuracy: 0.8287 - val_loss: 20045.5996 - val_accuracy: 0.8720\n",
            "Epoch 10/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 10407.8311 - accuracy: 0.8347roc-auc_val: 0.8131\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 10477.9863 - accuracy: 0.8342 - val_loss: 20733.6172 - val_accuracy: 0.8769\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10138.7998 - accuracy: 0.8376 - val_loss: 19989.2539 - val_accuracy: 0.8675\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9842.9863 - accuracy: 0.8396 - val_loss: 20446.8867 - val_accuracy: 0.8788\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9293.7617 - accuracy: 0.8437 - val_loss: 20576.7715 - val_accuracy: 0.8943\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8999.6074 - accuracy: 0.8528 - val_loss: 20332.6934 - val_accuracy: 0.8967\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8891.6650 - accuracy: 0.8513 - val_loss: 20873.6680 - val_accuracy: 0.9085\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8514.0264 - accuracy: 0.8575 - val_loss: 20560.8242 - val_accuracy: 0.8991\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8192.8721 - accuracy: 0.8567 - val_loss: 20788.7246 - val_accuracy: 0.9068\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7926.8442 - accuracy: 0.8628 - val_loss: 20761.6562 - val_accuracy: 0.8994\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7915.6353 - accuracy: 0.8634 - val_loss: 20597.9238 - val_accuracy: 0.9032\n",
            "Epoch 20/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 7675.8052 - accuracy: 0.8654roc-auc_val: 0.8142\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 7699.7827 - accuracy: 0.8652 - val_loss: 20627.7520 - val_accuracy: 0.9016\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7379.3330 - accuracy: 0.8658 - val_loss: 20662.7207 - val_accuracy: 0.8918\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7219.7212 - accuracy: 0.8729 - val_loss: 21110.2422 - val_accuracy: 0.9095\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7298.1689 - accuracy: 0.8684 - val_loss: 20998.6426 - val_accuracy: 0.9020\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7015.1689 - accuracy: 0.8718 - val_loss: 20858.4023 - val_accuracy: 0.9049\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6903.1929 - accuracy: 0.8730 - val_loss: 20807.0391 - val_accuracy: 0.9004\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6714.7690 - accuracy: 0.8737 - val_loss: 21181.8789 - val_accuracy: 0.8993\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6627.9429 - accuracy: 0.8786 - val_loss: 21187.6914 - val_accuracy: 0.9062\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6546.7632 - accuracy: 0.8758 - val_loss: 21038.3770 - val_accuracy: 0.9065\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6205.8477 - accuracy: 0.8773 - val_loss: 21403.6484 - val_accuracy: 0.9123\n",
            "Epoch 30/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 6185.7290 - accuracy: 0.8798roc-auc_val: 0.8067\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 6152.2280 - accuracy: 0.8796 - val_loss: 21344.6270 - val_accuracy: 0.9088\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6204.1831 - accuracy: 0.8817 - val_loss: 21393.4160 - val_accuracy: 0.9073\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5938.1099 - accuracy: 0.8808 - val_loss: 21781.9336 - val_accuracy: 0.8974\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5919.0107 - accuracy: 0.8809 - val_loss: 22028.2695 - val_accuracy: 0.9098\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5851.5381 - accuracy: 0.8851 - val_loss: 21513.2344 - val_accuracy: 0.9048\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5860.8477 - accuracy: 0.8869 - val_loss: 21765.1973 - val_accuracy: 0.9130\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5651.4678 - accuracy: 0.8871 - val_loss: 21443.7754 - val_accuracy: 0.9013\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5646.0869 - accuracy: 0.8888 - val_loss: 21399.0664 - val_accuracy: 0.9053\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5546.9570 - accuracy: 0.8863 - val_loss: 21807.3242 - val_accuracy: 0.9080\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5484.3198 - accuracy: 0.8869 - val_loss: 21669.9492 - val_accuracy: 0.9075\n",
            "Epoch 40/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 5444.3994 - accuracy: 0.8849roc-auc_val: 0.8011\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 5441.2412 - accuracy: 0.8852 - val_loss: 21720.6855 - val_accuracy: 0.9040\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5256.5879 - accuracy: 0.8900 - val_loss: 21803.2988 - val_accuracy: 0.8953\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5271.2852 - accuracy: 0.8881 - val_loss: 21608.1230 - val_accuracy: 0.9034\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5271.9600 - accuracy: 0.8897 - val_loss: 21765.4688 - val_accuracy: 0.9106\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5096.0981 - accuracy: 0.8944 - val_loss: 21440.1660 - val_accuracy: 0.8981\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4980.7837 - accuracy: 0.8931 - val_loss: 21726.3438 - val_accuracy: 0.9005\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4872.1392 - accuracy: 0.8915 - val_loss: 22062.9023 - val_accuracy: 0.9098\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4906.8047 - accuracy: 0.8969 - val_loss: 21983.9453 - val_accuracy: 0.9130\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4911.5776 - accuracy: 0.8959 - val_loss: 22064.1895 - val_accuracy: 0.9160\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4842.3413 - accuracy: 0.8982 - val_loss: 22118.2969 - val_accuracy: 0.9100\n",
            "Epoch 50/1000\n",
            "84/88 [===========================>..] - ETA: 0s - loss: 4840.0034 - accuracy: 0.8979roc-auc_val: 0.7996\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 4800.9424 - accuracy: 0.8980 - val_loss: 21938.5664 - val_accuracy: 0.9051\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4630.3604 - accuracy: 0.8983 - val_loss: 22343.4160 - val_accuracy: 0.9082\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4637.2734 - accuracy: 0.9006 - val_loss: 22045.5410 - val_accuracy: 0.9013\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4590.2202 - accuracy: 0.8973 - val_loss: 21971.8438 - val_accuracy: 0.9065\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4528.9985 - accuracy: 0.8994 - val_loss: 22169.2188 - val_accuracy: 0.9011\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 4423.5298 - accuracy: 0.9012 - val_loss: 22088.5781 - val_accuracy: 0.9171\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 4524.7783 - accuracy: 0.9022 - val_loss: 22022.9121 - val_accuracy: 0.9048\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 4446.1401 - accuracy: 0.9010 - val_loss: 21952.9707 - val_accuracy: 0.9028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5PePWLxWIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9895f36-d9b2-446a-98ba-014c3ddc8a98"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdCElEQVR4nO3de3TcZ33n8fd3bhqNRveLZUu+X+M4zgU3OCQQSoAA5QBtOW0oKdfdFHZpu9Clhz3tOd1d9kJ7Fihw6LZZmnIpBgJlwQ1bCoRALiQODo5zsR3f4li2ZWls6zK6zfXZP2akyLZsSTOjGf1Gn9c5Opr56aeZ73Mkf/zo+T3P8zPnHCIi4j2+ShcgIiKFUYCLiHiUAlxExKMU4CIiHqUAFxHxqEA536ytrc2tWbOmnG8pIuJ5Tz311DnnXPulx8sa4GvWrGHv3r3lfEsREc8zs5dmOq4hFBERj1KAi4h4lAJcRMSjFOAiIh6lABcR8SgFuIiIRynARUQ8SgEuIuJRCnAREY8q60rMStq15+Rlx37vlasqUImISGmoBy4i4lEKcBERj1KAi4h41KwBbmb3mVm/mT037ViLmf3YzI7kPzcvbJkiInKpufTAvwy86ZJjnwAedM5tBB7MPxcRkTKaNcCdcw8DFy45/HbgK/nHXwHeUeK6RERkFoWOgS9zzvXmH58Fll3pRDO7x8z2mtneWCxW4NuJiMilir6I6ZxzgLvK1+91zu1wzu1ob7/sjkAiIlKgQgO8z8yWA+Q/95euJBERmYtCA3w38N784/cC3y9NOSIiMldzmUb4DeBxYLOZnTKzDwKfAt5gZkeA1+efi4hIGc26F4pz7l1X+NIdJa5FRETmQSsxRUQ8SgEuIuJRCnAREY9SgIuIeJQCXETEoxTgIiIepQAXEfEoBbiIiEcpwEVEPEoBLiLiUQpwERGPUoCLiHiUAlxExKMU4CIiHqUAFxHxKAW4iIhHKcBFRDxKAS4i4lEKcBERj1KAi4h4lAJcRMSjFOAiIh6lABcR8SgFuIiIRynARUQ8SgEuIuJRCnAREY9SgIuIeJQCXETEoxTgIiIepQAXEfGoogLczD5qZs+b2XNm9g0zC5eqMBERubqCA9zMuoA/AnY457YBfuCuUhUmIiJXV+wQSgCoNbMAEAHOFF+SiIjMRcEB7pw7Dfwv4CTQCww553506Xlmdo+Z7TWzvbFYrPBKRUTkIsUMoTQDbwfWAiuAOjO7+9LznHP3Oud2OOd2tLe3F16piIhcpJghlNcDLzrnYs65FPBd4FWlKUtERGZTTICfBHaaWcTMDLgDOFiaskREZDbFjIHvAb4D/Ap4Nv9a95aoLhERmUWgmG92zv0F8BclqkVEROZBKzFFRDxKAS4i4lEKcBERj1KAi4h4lAJcRMSjFOAiIh6lABcR8SgFuIiIRynARUQ8SgEuIuJRCnAREY9SgIuIeJQCXETEoxTgIiIepQAXEfEoBbiIiEcpwEVEPEoBLiLiUQpwERGPUoCLiHiUAlxExKMU4CIiHqUAFxHxKAW4iIhHKcBFRDxKAS4i4lEKcBERj1KAi4h4lAJcRMSjFOAiIh6lABcR8aiiAtzMmszsO2Z2yMwOmtktpSpMRESuLlDk938O+KFz7p1mFgIiJahJRETmoOAAN7NG4DXA+wCcc0kgWZqyRERkNsUMoawFYsA/mNk+M/uSmdVdepKZ3WNme81sbywWK+LtRERkumICPADcBPxv59yNwCjwiUtPcs7d65zb4Zzb0d7eXsTbiYjIdMUE+CnglHNuT/75d8gFuoiIlEHBAe6cOwv0mNnm/KE7gAMlqUpERGZV7CyUPwS+np+Bchx4f/EliYjIXBQV4M65p4EdJapFRETmQSsxRUQ8SgEuIuJRCnAREY9SgIuIeJQCXETEoxTgIiIepQAXEfEoBbiIiEcpwEVEPEoBLiLiUUsuwJ89PcTXHj9BJusqXYqISFGWXID/4ug5Dp6N88Tx85UuRUSkKEsqwOMTKU5eGMPvMx481MeFUd0BTkS8a0kF+KHeOA54503dJNNZPvPjFypdkohIwZZUgB/oHaY5EmR7dyM3r21l156THOmLV7osEZGCLJkAT6QyHI2NsHV5A2bG67Z0kHXwowN9lS5NRKQgSybAD/ePkMk6tq5oBCBaE2B1a4RnTg1WuDIRkcIsmQA/cGaISMjP6tbI1LHt3U08c2qoglWJiBRuyQT4kf4RtnTW4zObOnZ9dyO9QxP0xycqWJmISGGWRIAPjacYS2ZY1hC+6Pj27iYAnulRL1xEvGdJBPipgTEAmiOhi45v62rAZ2gcXEQ8aUkEeM+FceDyAI+EAmzsqGe/xsFFxIOWRIC/3AMPXva17d2NPHt6COe0N4qIeMsSCfBxagI+akP+y762fWUTF0aTnBoYr0BlIiKFWyIBPkZzJIRNm4Ey6fru3LxwTScUEa9ZIgE+TtMMwycAWzobCPl9upApIp5T9QHunOPUwDjNdaEZvx4K+LhmeT37FeAi4jFVH+CDYylGEunLZqBMt3VFA4fOxnUhU0Q8peoDfPLi5EwzUCZt6WxgcCxF33CiXGWJiBSt6gO85wqLeKbb0lkPwKGzw2WpSUSkFKo+wK+0CnO6LZ0NABw6q73BRcQ7ig5wM/Ob2T4ze6AUBZXaqYFxGsKBGeeAT2qMBFnRGOZQr3rgIuIdpeiB/zFwsASvsyB6LozR3RyZ9bzNnfXqgYuIpxQV4GbWDfwG8KXSlFN6pwbG6W6unfW8LcsbOBYbIZnOlqEqEZHiFdsD/2vgT4Erpp6Z3WNme81sbywWK/Lt5mdyDvjKltl74Fs660llHMfPjZShMhGR4hUc4Gb2VqDfOffU1c5zzt3rnNvhnNvR3t5e6NsV5PxokvFUZk498GuW5y9k9moYRUS8oZge+K3A28zsBPBN4HVm9o8lqapEJueAz2UMfG1bHUG/cVBTCUXEIwoOcOfcf3LOdTvn1gB3AT91zt1dsspK4PRUgM/eAw/6fWzoqFcPXEQ8I1DpAhZS33DuXpeX3kpt0q49Jy96Hg742HdyYMHrEhEphZIs5HHO/cw599ZSvFYp9ccTBP121WX003U2hhmeSDMwmlzgykREilfVKzH7hyfoqA/PuA/4TJY35oZanjujvcFFZPGr7gCPJ2ivr5nz+V1NuQDXzR1ExAuqPMAn6JhHgNeG/LTWhdjfo73BRWTxq/IAT9DRMPcAh9yMlWdPqwcuIotf1QZ4Ip1hcCxFR/3MM1CupLs5Qu/QBP3xiQWqTESkNKo2wPvzN2dYVkAPHOCZHvXCRWRxq94Aj+cCfL498OWNtfh9ppsci8iiV7UBHssPgcxnFgrkbnK8sSPKfs1EEZFFrmoDfKoHPs8hFIDru5t45tSgbnIsIota9Qb4cAKfQWvd/AN8+8pGBsZSU5thiYgsRlUb4H3DE7TX1+D3zW0V5nTbu5oA2K9xcBFZxKo2wPvjiXlfwJy0ubOeUMDHvpMKcBFZvKo8wOc/fAK5C5m/tqaZx46eK3FVIiKlU7UBHotPFHQBc9KrN7Zz6Gyc/mEt6BGRxakqAzydyXJ+NEl7gUMoAK/e2AbAw0fUCxeRxakqA/zcSBLn5r8Kc7prOhtoi9bwyJHy3ohZRGSuqjLAJ+/EU+hFTACfz3j1xjYePXKObFbzwUVk8anKAH95GX3hPXDIDaOcH01yoFc3OhaRxadKAzzfAy9iCAXgtg2T4+AaRhGRxac6A3w4gRm0RYsL8I6GMFs663nksC5kisjiU50BHk/QWhci6C++ebdvbueXJy5wfiRRgspEREonUOkCFkL/8ERRUwh37Tk59bgm4CeddXzv6TN88La1pShPRKQkqrIH3hefoLPI8e9JnQ1huptr+fbeHu1OKCKLSlUG+NmhCTobC++BX+oVq5s5dDaue2WKyKJSdQGeSGc4N5Kks6G2ZK+5vauJmoCP+/f2lOw1RUSKVXUBPnkvzOUl7IHXhvy8eVsn33/6DBOpTMleV0SkGFUX4GfzqzCXlTDAAX5nx0riE2m+//Tpkr6uiEihqi7Ae4dyAV7KHjjALetbub67kc8/eJREWr1wEam8qgvwvnyAL2sobYCbGR+/cwunB8f5+hMnZ/8GEZEFVnUB3js0QSTkpyFc+inut21s41XrW/niQ0cZSaRL/voiIvNRdQHeN5ybQmg2/3thzsXH79zM+dEkX3rk+IK8vojIXBUc4Ga20sweMrMDZva8mf1xKQsrVO/QOJ0lHj6Z7sZVzfzGdcv5m4eOcVC7FIpIBRXTA08Df+Kc2wrsBP69mW0tTVmFK/Uinkm79pyc+rh+ZROhgI/3/cOTmlYoIhVTcIA753qdc7/KP44DB4GuUhVWiEzW0R9PlHwGyqWiNQF++6Zu+oYT/NUPX1jQ9xIRuZKSjIGb2RrgRmDPDF+7x8z2mtneWGxh99U+P5IgnXULOoQyaXNnPTvXtXLfYy/yvX2aGy4i5Vd0gJtZFPgn4D845y4bFHbO3euc2+Gc29He3l7s213V5BzwzsbSLaO/mjdv62TnuhY+/p39unemiJRdUQFuZkFy4f1159x3S1NS4SZXYS70EMqkoN/Hve/Zwfr2KB/62lM83TNYlvcVEYHiZqEY8PfAQefcZ0pXUuHOLtAinqtpCAf5ygdupiUa4t3/5wkeO6q794hIeRTTA78V+H3gdWb2dP7jLSWqqyBnhycI+o3WulDZ3nPXnpM8eLCfd9+8mmg4wHvue5IHnjlTtvcXkaWr4OWKzrlHgYVZLVOgs0MTdNSH8fnKX1ZDbZB7Xr2erz5xgo/s2seBM8P8yRs3469ALSKyNFTVLdV6h8bLNv49k9qQnw/cupZDZ4f5m58dY/+pQT77uzfQUcTt3URErqSqltL3DScWZBHPfAT9Pv7nb23nr965nb0nBnjjZx9m9/4zuh2biJRc1QS4c27Bl9HP1a49J0lnHB9+7XqiNQH+6Bv7eMvnH6XnwlilSxORKlI1AT40nmIila14D3y6jvowf/Ca9dx5bSdH++Pc8Zmf8+kfvcDwRKrSpYlIFaiaMfCT+d5td3OkwpVczO8zbt/Uzg0rmzjYO8wXfnqUrz7+Ev/mtrW899Y1NISDlS5RRDyqagL8eGwUgPXtdRWuZGaNtUF2rmtlZXOEBw/18ekfH+YLPz3K7/7aSn7/ltVsWlZf6RJFxGOqJsCPxUbw+4xVrYurB36pruZa3nPLGs4MjvOLY+f51t4evvbES1zX1cg7buzijVuXsbJlcbdBRBaHqgnw47FRVrVEqAn4K13KnKxoquWdr+jmTds62d8zyL6eAT75wAE++cAB2qIh3rp9Bbdvbmfn2lZqQ95ok4iUV9UE+LHYCOvaFufwydVEawLcuqGNWze0cS6e4HB/nMN9cb7x5Em+/IsTBP3GNcsbuL67ie3djdywsol17VEtEBKR6gjwTNbx4rlRXrNpYXc7XGht9TW01dfwqvVtpDJZXjw3yvHYCD0D49yfH2oBqA362bQsyubOejYtq2dLZwNr2+tYVl9DwF81E4tEZBZVEeBnBsdJpLOe7IFfSdDvY9Oy+qmLm1nnOBdPcGpgnDND4/QNT/CDZ89y/95TU9/js9xGXiuaalneGKYtWkNTJEhTbZDmuhCNtUGaIiGaI0GaakPUhwMV2XZAREqjKgL8WGwEgPUd0QpXsnB8ZnQ0hOloCHMTzVPHRxJp+oYnuDCaZHAsxdB4koGxJCfOjTKaTDORyl7xNY3c8v/OhjCNkSAtkRBNkRAtdbnAb6oNURPwEQr4CPp91OQ/B/xGwGcE/D4CPiMS8tMaraEhHFiwm0mLyOWqJMBzUwirqQc+V9GaANH2KOuvMHqUyTomUhnGkhnGk2nGkhnGLn2ezBAfT9M3NMFoMsNYMk0qM/+l/wGfUVcTIFoToCkS5LWb21nbFmVdex3r2upoipRvl0iRpaAqAvx4bISmSJCWMm4j6xX+fKjW1QSAmjl/XzKdZTyVIZ3Jksk6Ms6Rzripx9msI+scmSwk0hlGEmlGEmlGE2niE7m/Cv7u58dJZ1/+j6ClLsTatlyYr22vY10+3Fe3emf2kMhiUhUBPjkDRX++l04oP3RSjEzWMTCa5NxIgthIgnMjuceH++LEJ9JT5xnQ3VLLurYoa9vqWN9eN9Vz72yozPbAIl5QFQF+PDbK7R6fgVKN/D6bmlmz5ZKvTaQynB9J5oM993GkP87jx86TzLw8bh8O+ljfHmXzsno2ddZPfV7RGNZ/2LLkeT7A4xMp+uOJqr6AWY3CQT9dzbV0NV98A2rnHPGJ9MvBHs/13h881M93952eOq+uJsDa1ghr2upY01rHR9+wSXPjZcnxfIAfX8IXMKuRmdFQG6ShNsj69ov/Ux5PZuiPT9A7NEHPhTFePD/Kc2eGAfjK4yfYua6V2za0cdvGNg2pyZLg+QBfClMIJac25Gd1ax2rW+vYua4VgMGxJCfOj+Iz47Fj5/jxgT4AVjSGuWV9GzesbGRbVyPXLG8gHNSFUqkung/wF87GCfiMVdoAaklqioS4IT89cXt3ExdGkxzpj3Osf4SHXujnn36VW+jk9xkbO6JsXdHA6pY6VrbUsqolwsqWCO3RGl0olSm79pyc8fjvvXJVmSuZnecD/JEj53jF6maCWkIu5KYqvnJtK69c24pzjsHxFKcHxjkzOM7pwXF+cqCP4WkzYCA346arqZYVTWG6mmrpaorkxuebalnZUsuKxloFvCxKng7w/vgEB3qH+fidmytdiixCZkZzJERzJMS2rsap46lMlsGxFBdGk1wYSzI4llvFevL8GM/0DBFPXBzwtUE/69rrWN8eZUNHbg+abV2NmgkjFefpAH/k8DkATSGUeQn6fbTX19BeP/PCplQmy/B4ioF8yMfiE8RGEjx8JMbu/WemzmuOBNnW1ci1KxrZ1tXAthWNrGqJqLcuZePpAP/54Rht0Rq2Lm+odClSRYJ+H63RGlqjlwd8Mp2lb3iC04O5YZljsRF+cfQ8GZdbcVpfE2Drigau68pdPN3W1cDaNm3/KwvDswGeyToeORLj1zd3qMcjZRMK+FiZv/g5KZ3N0j+cmBpnPzM4zlMvDUxtI1Ab9LN1RQNblzewujVCd3OElS21rGyJ6J6oUhTPBvhzp4cYGEtx+2YNn0hlBXw+VjTVsqKplh35Y5msIzaSC/XJj28/1XPZ7pCNtcFcmDdH6MgP60x9RMO019fQGg3pIr3MyLMB/vDhGGZw24a2Spcichm/z+hsCNPZEOamVbntf51zjKcyDIymuDCWZGA0t/XvwFiSvScGSKQzl82QmdQQDkzt5d6Y/9wcye3x3hzJ7fPelD/WlH9eX6P93gsRn0hxLDZCLJ7A5zOCPh8bly3OdSaeDfCfHY5xXVfjjOOUIouRmREJBYiEApdtITAplcnmdnacyO3uGJ9IE0+kGEtkGE/ltvo9HxvJbwN89f3e/T6jIRwgGg4QrQlSHw5QXxOg/tJj4QDNkRCt0RDt+bH/ptrgkgr/WDzB958+zXd/dZoDvbnVvQZM7qX5w+fhoRf6+cCta/nNG7sWzewjTwb4Qy/089RLA3zizZdukSTibUG/b2rq41xknWM8OfN+72PJNOPJDIl0lkQqQyye4NSFMSbSWSZSGRKp7NTF10v5fbkpmG3REG3R3DBOa13u88vHamisDRIO+ggH/NTkP3sh+J1znDg/xs9f6Odfn+/jyRMXyGQd169s4s5rO9nQEWV5YxiAsWSGZ08NcqR/hI/dv5/v/uo0/+M3r2NVa+UXD3ouwMeSaf78/z7H+vY63n/rmkqXI1JRPitsv/dJqUx26oYfk/u5T/88ksj9R9DTM8b5kSQjiZmHeKYL+X3UBH3UBPz5uzgZQX/ubk7BgI8av49g4OVjIf/kXZ+mHQvkjgfz54amHc+dc/mxUMAAI53Jkso4Utks6YxjLJlmeDzF+dEkvYMT9AyM8fyZYYbGUwBs7Ijy4dvX844bu9jQEb1sJWa0JsAt69v43F03suvJk3zqXw5x518/zH+8czPve9Wais4w8lyAf/bHhzk9OM79f3CLbgIgUqTJwKwPB1k2h/OT6SyjyZcDfjyZIT0tLFMXhWeWTBYy2SzpbO5mIGOJNPGsm/EmIZmsy5+Xv4lI1pGd/42hrqo+HKCpNsimZVG6myOsaa2bWg/w5IsXePLFC1f8Xp/PuHvnal63pYM//95zfPKBA/zz/jP8t3dsu2ihWDkVFeBm9ibgc4Af+JJz7lMlqWoG6UyWb/6yh/seO8G7bl7FzWtbFuqtROQKcjf6mPsQT7Gy+WB/Odwv/khPC/tM1uHIDf/4LHffVp/PqPH7CIf8hIM+Ar7iZ/OsaKrl79+7g937z/Bf/vkAb/3Co7zp2k4+8roNXLuioazj4wUHuJn5gS8CbwBOAb80s93OuQOlKm7Szw/H+O8/OMDhvhFuXtuisW+RJcJnhs9vLLaNJM2Mt9/QxWs3d3Dfoy9y36Mv8sPnz7KqJcId13Tk5/zX0RoNTd0AvKO+puTTQYvpgd8MHHXOHQcws28CbwdKHuBfe/wlEuksf3v3Tdx5beeiuQIsIktbY22Qj75hE++/dQ0PPNPLgwf7+PqekyTTl88O+snHXsOGjvqSvn8xAd4F9Ex7fgp45aUnmdk9wD35pyNm9kKhb/jwnxb6nQC0AeeKeoXFqVrbBdXbtmptF1Rx295dZNs2/mVRb796poMLfhHTOXcvcO9Cv89szGyvc27H7Gd6S7W2C6q3bdXaLlDbyq2YAZnTwMppz7vzx0REpAyKCfBfAhvNbK2ZhYC7gN2lKUtERGZT8BCKcy5tZh8B/pXcNML7nHPPl6yy0qv4MM4CqdZ2QfW2rVrbBWpbWZm7wlJaERFZ3LRHpYiIRynARUQ8quoC3MzeZGYvmNlRM/vEDF+vMbNv5b++x8zWlL/K+ZtDuz5mZgfM7Bkze9DMZpw3uhjN1rZp5/22mTkzW1RTua5kLu0ys9/J/9yeN7Nd5a6xUHP4fVxlZg+Z2b787+RbKlHnfJnZfWbWb2bPXeHrZmafz7f7GTO7qdw1XsQ5VzUf5C6mHgPWASFgP7D1knP+HfC3+cd3Ad+qdN0latevA5H84w97oV1zbVv+vHrgYeAJYEel6y7Rz2wjsA9ozj/vqHTdJWzbvcCH84+3AicqXfcc2/Ya4CbguSt8/S3Av5DbLnwnsKeS9VZbD3xqeb9zLglMLu+f7u3AV/KPvwPcYYt/bf6s7XLOPeScG8s/fYLcvHwvmMvPDOCTwF8CE+Usrghzade/Bb7onBsAcM71l7nGQs2lbQ6YvNt4I3CmjPUVzDn3MHDlLQlz7fyqy3kCaDKz5eWp7nLVFuAzLe/vutI5zrk0MAS0lqW6ws2lXdN9kFwvwQtmbVv+z9SVzrkflLOwIs3lZ7YJ2GRmj5nZE/ndPb1gLm37z8DdZnYK+H/AH5antAU333+LC8pz+4HL1ZnZ3cAO4PZK11IKZuYDPgO8r8KlLIQAuWGU15L7i+lhM7vOOTdY0apK413Al51znzazW4Cvmdk259yV7wEn81ZtPfC5LO+fOsfMAuT+vDtfluoKN6dtC8zs9cCfAW9zziXKVFuxZmtbPbAN+JmZnSA37rjbAxcy5/IzOwXsds6lnHMvAofJBfpiN5e2fRC4H8A59zgQJrcZlNctqi1Eqi3A57K8fzfw3vzjdwI/dfmrE4vYrO0ysxuBvyMX3l4ZS4VZ2uacG3LOtTnn1jjn1pAb33+bc25vZcqds7n8Ln6PXO8bM2sjN6RyvJxFFmgubTsJ3AFgZteQC/BYWatcGLuB9+Rno+wEhpxzvRWrptJXfRfgKvJbyPVkjgF/lj/2X8n9o4fcL9K3gaPAk8C6Stdconb9BOgDns5/7K50zaVq2yXn/gwPzEKZ48/MyA0PHQCeBe6qdM0lbNtW4DFyM1SeBt5Y6Zrn2K5vAL1AitxfSB8EPgR8aNrP7Iv5dj9b6d9FLaUXEfGoahtCERFZMhTgIiIepQAXEfEoBbiIiEcpwEVEPEoBLiLiUQpwERGP+v/IMouSOaaHngAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNDmjro-2ndD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_1",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "adb72b0b-b686-49e9-e110-695ee6ece238"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b698c20d-1788-47a3-f543-89026935b464"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 84% 44.0M/52.2M [00:00<00:00, 111MB/s] \n",
            "100% 52.2M/52.2M [00:00<00:00, 150MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 107MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 94% 55.0M/58.3M [00:00<00:00, 129MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 168MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 108MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 76.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv80v8W_Ko2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c1f8e18c-e546-4b02-c00a-1101911fb500"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jt2pcxPije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d6ea369d-10b8-4cb4-e854-0798973c651e"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvEaxp9jhbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "bfb93745-290d-4113-9913-e6ce5a560533"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5b82519-56d8-41e9-ced2-bb24109af3ad"
      },
      "source": [
        "\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkLEBcl6LHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 0.8\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfCX4721TByP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8778f9bb-5117-4ecf-a54a-dc118b098c1b"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "# trn=trn.drop(['isFraud_isna'],1)\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Adam(0.001,decay=1e-3),loss=rac,metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.00001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 26260.7871 - accuracy: 0.6871 - val_loss: 21935.1914 - val_accuracy: 0.6202\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 21003.3477 - accuracy: 0.7509 - val_loss: 18755.4746 - val_accuracy: 0.6459\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 18608.0488 - accuracy: 0.7805 - val_loss: 18098.9277 - val_accuracy: 0.6679\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 17223.6172 - accuracy: 0.7959 - val_loss: 17577.3340 - val_accuracy: 0.6958\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 16096.9971 - accuracy: 0.8111 - val_loss: 17427.0410 - val_accuracy: 0.6908\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 15116.2422 - accuracy: 0.8188 - val_loss: 17389.6660 - val_accuracy: 0.7154\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14571.9844 - accuracy: 0.8249 - val_loss: 16573.4238 - val_accuracy: 0.7678\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14156.0664 - accuracy: 0.8325 - val_loss: 17002.9316 - val_accuracy: 0.7396\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13544.7432 - accuracy: 0.8369 - val_loss: 16212.3320 - val_accuracy: 0.7792\n",
            "Epoch 10/1000\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 13297.1025 - accuracy: 0.8380roc-auc_val: 0.7989\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 13260.2607 - accuracy: 0.8380 - val_loss: 16593.9414 - val_accuracy: 0.7565\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12870.5791 - accuracy: 0.8410 - val_loss: 16101.6152 - val_accuracy: 0.7900\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12566.9766 - accuracy: 0.8427 - val_loss: 16008.7930 - val_accuracy: 0.7804\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12120.9561 - accuracy: 0.8489 - val_loss: 16004.9404 - val_accuracy: 0.7596\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12020.4307 - accuracy: 0.8489 - val_loss: 16018.6572 - val_accuracy: 0.7991\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11731.8242 - accuracy: 0.8508 - val_loss: 15853.8613 - val_accuracy: 0.7828\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11442.2197 - accuracy: 0.8534 - val_loss: 16307.3066 - val_accuracy: 0.7479\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11277.6426 - accuracy: 0.8534 - val_loss: 15857.2441 - val_accuracy: 0.7889\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11120.7373 - accuracy: 0.8563 - val_loss: 15972.2617 - val_accuracy: 0.7710\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10872.3193 - accuracy: 0.8569 - val_loss: 15777.5820 - val_accuracy: 0.7893\n",
            "Epoch 20/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 10758.1895 - accuracy: 0.8582roc-auc_val: 0.8147\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 10746.0977 - accuracy: 0.8581 - val_loss: 15524.5430 - val_accuracy: 0.7927\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10627.8633 - accuracy: 0.8597 - val_loss: 15749.8174 - val_accuracy: 0.7807\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10441.9121 - accuracy: 0.8595 - val_loss: 15867.8662 - val_accuracy: 0.7793\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10340.4033 - accuracy: 0.8620 - val_loss: 15794.7979 - val_accuracy: 0.7848\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10305.7031 - accuracy: 0.8619 - val_loss: 15599.4844 - val_accuracy: 0.8067\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10061.5928 - accuracy: 0.8636 - val_loss: 15731.8164 - val_accuracy: 0.7808\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10005.3008 - accuracy: 0.8637 - val_loss: 15391.7666 - val_accuracy: 0.8101\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9857.3594 - accuracy: 0.8654 - val_loss: 15435.6357 - val_accuracy: 0.8054\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9784.8271 - accuracy: 0.8669 - val_loss: 15442.0723 - val_accuracy: 0.8043\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9718.8271 - accuracy: 0.8658 - val_loss: 15959.2432 - val_accuracy: 0.7912\n",
            "Epoch 30/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 9661.1025 - accuracy: 0.8666roc-auc_val: 0.8144\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 9657.4219 - accuracy: 0.8665 - val_loss: 15593.0068 - val_accuracy: 0.8180\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9516.0957 - accuracy: 0.8681 - val_loss: 15546.5791 - val_accuracy: 0.8132\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9463.4902 - accuracy: 0.8691 - val_loss: 15450.2158 - val_accuracy: 0.8267\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9469.7734 - accuracy: 0.8688 - val_loss: 15669.3369 - val_accuracy: 0.7926\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9229.7236 - accuracy: 0.8679 - val_loss: 15496.2559 - val_accuracy: 0.8188\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9314.5312 - accuracy: 0.8675 - val_loss: 15435.6572 - val_accuracy: 0.8110\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9088.0039 - accuracy: 0.8688 - val_loss: 15392.9092 - val_accuracy: 0.8297\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9121.4736 - accuracy: 0.8697 - val_loss: 15445.3203 - val_accuracy: 0.8071\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9040.2910 - accuracy: 0.8711 - val_loss: 15599.6914 - val_accuracy: 0.8120\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8963.0850 - accuracy: 0.8703 - val_loss: 15533.2568 - val_accuracy: 0.8217\n",
            "Epoch 40/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 8936.7324 - accuracy: 0.8702roc-auc_val: 0.8146\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 8898.1152 - accuracy: 0.8702 - val_loss: 15632.9492 - val_accuracy: 0.8199\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8804.1455 - accuracy: 0.8714 - val_loss: 15701.8906 - val_accuracy: 0.8156\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8787.1455 - accuracy: 0.8720 - val_loss: 15514.6289 - val_accuracy: 0.8067\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8793.8301 - accuracy: 0.8725 - val_loss: 15430.7402 - val_accuracy: 0.8367\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8587.3281 - accuracy: 0.8752 - val_loss: 15652.9160 - val_accuracy: 0.8295\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8721.6436 - accuracy: 0.8734 - val_loss: 15462.7783 - val_accuracy: 0.8286\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8584.2100 - accuracy: 0.8737 - val_loss: 15495.3369 - val_accuracy: 0.8279\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8611.1426 - accuracy: 0.8730 - val_loss: 15353.9375 - val_accuracy: 0.8226\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8425.1025 - accuracy: 0.8747 - val_loss: 15430.7383 - val_accuracy: 0.8231\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8385.2139 - accuracy: 0.8738 - val_loss: 15365.5830 - val_accuracy: 0.8216\n",
            "Epoch 50/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 8272.4404 - accuracy: 0.8754roc-auc_val: 0.8171\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 8262.9482 - accuracy: 0.8754 - val_loss: 15439.5957 - val_accuracy: 0.8239\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8372.4873 - accuracy: 0.8755 - val_loss: 15345.3604 - val_accuracy: 0.8216\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8248.3047 - accuracy: 0.8766 - val_loss: 15331.8213 - val_accuracy: 0.8236\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8285.2861 - accuracy: 0.8778 - val_loss: 15654.4336 - val_accuracy: 0.8160\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8172.8452 - accuracy: 0.8764 - val_loss: 15579.5820 - val_accuracy: 0.8151\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8205.7402 - accuracy: 0.8750 - val_loss: 15489.0830 - val_accuracy: 0.8139\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 8219.6133 - accuracy: 0.8765 - val_loss: 15288.8457 - val_accuracy: 0.8351\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 8195.8965 - accuracy: 0.8763 - val_loss: 15356.3359 - val_accuracy: 0.8227\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 8191.9194 - accuracy: 0.8763 - val_loss: 15413.7920 - val_accuracy: 0.8193\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 8058.0151 - accuracy: 0.8791 - val_loss: 15293.8691 - val_accuracy: 0.8330\n",
            "Epoch 60/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 8075.8838 - accuracy: 0.8759roc-auc_val: 0.8163\n",
            "225/225 [==============================] - 5s 22ms/step - loss: 8075.8262 - accuracy: 0.8760 - val_loss: 15464.3564 - val_accuracy: 0.8141\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 7952.6729 - accuracy: 0.8775 - val_loss: 15376.7695 - val_accuracy: 0.8269\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7951.3633 - accuracy: 0.8782 - val_loss: 15420.0674 - val_accuracy: 0.8209\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7963.1299 - accuracy: 0.8784 - val_loss: 15298.9990 - val_accuracy: 0.8331\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7886.5713 - accuracy: 0.8789 - val_loss: 15288.4287 - val_accuracy: 0.8223\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7841.7041 - accuracy: 0.8781 - val_loss: 15445.8887 - val_accuracy: 0.8315\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7757.9438 - accuracy: 0.8807 - val_loss: 15408.5156 - val_accuracy: 0.8264\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7755.2886 - accuracy: 0.8804 - val_loss: 15356.1338 - val_accuracy: 0.8294\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7782.8916 - accuracy: 0.8805 - val_loss: 15454.4561 - val_accuracy: 0.8249\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7812.9854 - accuracy: 0.8792 - val_loss: 15335.8555 - val_accuracy: 0.8286\n",
            "Epoch 70/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 7775.5620 - accuracy: 0.8806roc-auc_val: 0.8178\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 7756.3462 - accuracy: 0.8807 - val_loss: 15400.2744 - val_accuracy: 0.8321\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7599.3232 - accuracy: 0.8817 - val_loss: 15444.9238 - val_accuracy: 0.8411\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7680.6211 - accuracy: 0.8826 - val_loss: 15318.4805 - val_accuracy: 0.8306\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7630.3027 - accuracy: 0.8811 - val_loss: 15416.9033 - val_accuracy: 0.8305\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7597.9023 - accuracy: 0.8812 - val_loss: 15393.6797 - val_accuracy: 0.8282\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7622.8867 - accuracy: 0.8807 - val_loss: 15242.6094 - val_accuracy: 0.8438\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7553.5986 - accuracy: 0.8839 - val_loss: 15363.0586 - val_accuracy: 0.8308\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7612.2783 - accuracy: 0.8830 - val_loss: 15368.7471 - val_accuracy: 0.8283\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7584.2568 - accuracy: 0.8817 - val_loss: 15236.7207 - val_accuracy: 0.8385\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7419.9224 - accuracy: 0.8822 - val_loss: 15259.5264 - val_accuracy: 0.8354\n",
            "Epoch 80/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 7481.4023 - accuracy: 0.8825roc-auc_val: 0.8193\n",
            "225/225 [==============================] - 4s 20ms/step - loss: 7471.1436 - accuracy: 0.8824 - val_loss: 15299.9199 - val_accuracy: 0.8396\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7485.0459 - accuracy: 0.8829 - val_loss: 15271.2822 - val_accuracy: 0.8366\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7395.9624 - accuracy: 0.8835 - val_loss: 15335.4756 - val_accuracy: 0.8365\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7469.2749 - accuracy: 0.8828 - val_loss: 15350.4170 - val_accuracy: 0.8300\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7514.4922 - accuracy: 0.8812 - val_loss: 15361.9912 - val_accuracy: 0.8298\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7356.1982 - accuracy: 0.8847 - val_loss: 15376.1729 - val_accuracy: 0.8311\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7343.6821 - accuracy: 0.8826 - val_loss: 15479.5898 - val_accuracy: 0.8303\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7312.4390 - accuracy: 0.8838 - val_loss: 15471.8008 - val_accuracy: 0.8302\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7421.2554 - accuracy: 0.8836 - val_loss: 15453.9111 - val_accuracy: 0.8269\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7423.8535 - accuracy: 0.8828 - val_loss: 15354.2061 - val_accuracy: 0.8343\n",
            "Epoch 90/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 7338.5059 - accuracy: 0.8829roc-auc_val: 0.8175\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 7333.7734 - accuracy: 0.8832 - val_loss: 15416.8330 - val_accuracy: 0.8349\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7251.3057 - accuracy: 0.8839 - val_loss: 15492.2959 - val_accuracy: 0.8390\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7301.1304 - accuracy: 0.8837 - val_loss: 15522.9131 - val_accuracy: 0.8391\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7251.5815 - accuracy: 0.8857 - val_loss: 15453.9082 - val_accuracy: 0.8418\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7209.7686 - accuracy: 0.8857 - val_loss: 15346.4873 - val_accuracy: 0.8353\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7197.7139 - accuracy: 0.8862 - val_loss: 15340.2266 - val_accuracy: 0.8369\n",
            "Epoch 96/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7202.0981 - accuracy: 0.8844 - val_loss: 15480.5176 - val_accuracy: 0.8399\n",
            "Epoch 97/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7187.8076 - accuracy: 0.8840 - val_loss: 15432.2236 - val_accuracy: 0.8350\n",
            "Epoch 98/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7173.1226 - accuracy: 0.8861 - val_loss: 15347.7422 - val_accuracy: 0.8502\n",
            "Epoch 99/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7177.6050 - accuracy: 0.8860 - val_loss: 15333.2061 - val_accuracy: 0.8416\n",
            "Epoch 100/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 7210.6025 - accuracy: 0.8859roc-auc_val: 0.8199\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 7203.2490 - accuracy: 0.8859 - val_loss: 15268.6572 - val_accuracy: 0.8403\n",
            "Epoch 101/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7111.5083 - accuracy: 0.8869 - val_loss: 15334.6270 - val_accuracy: 0.8368\n",
            "Epoch 102/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7099.0229 - accuracy: 0.8864 - val_loss: 15320.5391 - val_accuracy: 0.8420\n",
            "Epoch 103/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 7046.0674 - accuracy: 0.8865 - val_loss: 15404.9736 - val_accuracy: 0.8449\n",
            "Epoch 104/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 7009.5791 - accuracy: 0.8853 - val_loss: 15381.5391 - val_accuracy: 0.8401\n",
            "Epoch 105/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7195.7539 - accuracy: 0.8866 - val_loss: 15370.0049 - val_accuracy: 0.8333\n",
            "Epoch 106/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 7096.9751 - accuracy: 0.8861 - val_loss: 15517.5488 - val_accuracy: 0.8393\n",
            "Epoch 107/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7068.8569 - accuracy: 0.8873 - val_loss: 15481.2461 - val_accuracy: 0.8413\n",
            "Epoch 108/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7140.2065 - accuracy: 0.8871 - val_loss: 15472.9551 - val_accuracy: 0.8433\n",
            "Epoch 109/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7024.6729 - accuracy: 0.8870 - val_loss: 15450.1230 - val_accuracy: 0.8400\n",
            "Epoch 110/1000\n",
            "214/225 [===========================>..] - ETA: 0s - loss: 7005.2915 - accuracy: 0.8872roc-auc_val: 0.8171\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 7015.5479 - accuracy: 0.8875 - val_loss: 15484.5557 - val_accuracy: 0.8363\n",
            "Epoch 111/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7017.9155 - accuracy: 0.8888 - val_loss: 15382.8232 - val_accuracy: 0.8348\n",
            "Epoch 112/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6972.3926 - accuracy: 0.8863 - val_loss: 15363.1943 - val_accuracy: 0.8466\n",
            "Epoch 113/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6972.6548 - accuracy: 0.8889 - val_loss: 15448.8545 - val_accuracy: 0.8397\n",
            "Epoch 114/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6973.9951 - accuracy: 0.8874 - val_loss: 15514.1123 - val_accuracy: 0.8465\n",
            "Epoch 115/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6894.2886 - accuracy: 0.8878 - val_loss: 15268.4189 - val_accuracy: 0.8437\n",
            "Epoch 116/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6853.1670 - accuracy: 0.8890 - val_loss: 15347.8857 - val_accuracy: 0.8431\n",
            "Epoch 117/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 7014.6064 - accuracy: 0.8880 - val_loss: 15218.0010 - val_accuracy: 0.8462\n",
            "Epoch 118/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6867.5127 - accuracy: 0.8879 - val_loss: 15324.0146 - val_accuracy: 0.8445\n",
            "Epoch 119/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6947.1465 - accuracy: 0.8863 - val_loss: 15331.2402 - val_accuracy: 0.8379\n",
            "Epoch 120/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 6889.3228 - accuracy: 0.8879roc-auc_val: 0.8192\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 6880.9077 - accuracy: 0.8879 - val_loss: 15339.4629 - val_accuracy: 0.8475\n",
            "Epoch 121/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6878.0356 - accuracy: 0.8878 - val_loss: 15381.2139 - val_accuracy: 0.8394\n",
            "Epoch 122/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6788.1694 - accuracy: 0.8880 - val_loss: 15438.7021 - val_accuracy: 0.8485\n",
            "Epoch 123/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6923.0083 - accuracy: 0.8873 - val_loss: 15339.3936 - val_accuracy: 0.8481\n",
            "Epoch 124/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6914.9995 - accuracy: 0.8897 - val_loss: 15279.5811 - val_accuracy: 0.8421\n",
            "Epoch 125/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6833.0342 - accuracy: 0.8874 - val_loss: 15375.6797 - val_accuracy: 0.8421\n",
            "Epoch 126/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6881.9341 - accuracy: 0.8878 - val_loss: 15270.6914 - val_accuracy: 0.8482\n",
            "Epoch 127/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6806.3140 - accuracy: 0.8901 - val_loss: 15292.3086 - val_accuracy: 0.8484\n",
            "Epoch 128/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6854.1748 - accuracy: 0.8884 - val_loss: 15259.8008 - val_accuracy: 0.8512\n",
            "Epoch 129/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6743.8364 - accuracy: 0.8903 - val_loss: 15317.0703 - val_accuracy: 0.8477\n",
            "Epoch 130/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 6787.3848 - accuracy: 0.8888roc-auc_val: 0.8181\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 6784.0391 - accuracy: 0.8890 - val_loss: 15418.5732 - val_accuracy: 0.8474\n",
            "Epoch 131/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6806.5928 - accuracy: 0.8912 - val_loss: 15429.3623 - val_accuracy: 0.8477\n",
            "Epoch 132/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6809.2690 - accuracy: 0.8893 - val_loss: 15341.1123 - val_accuracy: 0.8406\n",
            "Epoch 133/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6716.3120 - accuracy: 0.8884 - val_loss: 15429.4355 - val_accuracy: 0.8439\n",
            "Epoch 134/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6778.5674 - accuracy: 0.8887 - val_loss: 15493.8271 - val_accuracy: 0.8421\n",
            "Epoch 135/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6831.2344 - accuracy: 0.8894 - val_loss: 15461.5391 - val_accuracy: 0.8484\n",
            "Epoch 136/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6646.1689 - accuracy: 0.8901 - val_loss: 15487.9307 - val_accuracy: 0.8396\n",
            "Epoch 137/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6731.0903 - accuracy: 0.8892 - val_loss: 15412.6807 - val_accuracy: 0.8390\n",
            "Epoch 138/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6654.9321 - accuracy: 0.8898 - val_loss: 15390.9180 - val_accuracy: 0.8411\n",
            "Epoch 139/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6759.3472 - accuracy: 0.8884 - val_loss: 15448.3877 - val_accuracy: 0.8309\n",
            "Epoch 140/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 6648.9712 - accuracy: 0.8880roc-auc_val: 0.8187\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 6648.9712 - accuracy: 0.8880 - val_loss: 15356.5195 - val_accuracy: 0.8458\n",
            "Epoch 141/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6759.7710 - accuracy: 0.8909 - val_loss: 15369.2812 - val_accuracy: 0.8421\n",
            "Epoch 142/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6730.6499 - accuracy: 0.8902 - val_loss: 15238.2334 - val_accuracy: 0.8488\n",
            "Epoch 143/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6764.7710 - accuracy: 0.8886 - val_loss: 15246.4189 - val_accuracy: 0.8397\n",
            "Epoch 144/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6666.3457 - accuracy: 0.8897 - val_loss: 15284.0742 - val_accuracy: 0.8413\n",
            "Epoch 145/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6633.8481 - accuracy: 0.8885 - val_loss: 15238.6641 - val_accuracy: 0.8468\n",
            "Epoch 146/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6691.0190 - accuracy: 0.8904 - val_loss: 15311.5889 - val_accuracy: 0.8449\n",
            "Epoch 147/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6557.2480 - accuracy: 0.8917 - val_loss: 15318.5625 - val_accuracy: 0.8547\n",
            "Epoch 148/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6564.0391 - accuracy: 0.8925 - val_loss: 15255.0537 - val_accuracy: 0.8517\n",
            "Epoch 149/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6635.7739 - accuracy: 0.8918 - val_loss: 15279.7354 - val_accuracy: 0.8442\n",
            "Epoch 150/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 6543.9170 - accuracy: 0.8925roc-auc_val: 0.8218\n",
            "225/225 [==============================] - 4s 20ms/step - loss: 6529.1494 - accuracy: 0.8924 - val_loss: 15138.6436 - val_accuracy: 0.8525\n",
            "Epoch 151/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6622.4194 - accuracy: 0.8907 - val_loss: 15346.8828 - val_accuracy: 0.8449\n",
            "Epoch 152/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6564.2119 - accuracy: 0.8924 - val_loss: 15352.0801 - val_accuracy: 0.8475\n",
            "Epoch 153/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6525.3584 - accuracy: 0.8919 - val_loss: 15340.6797 - val_accuracy: 0.8498\n",
            "Epoch 154/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6623.7915 - accuracy: 0.8915 - val_loss: 15248.8447 - val_accuracy: 0.8511\n",
            "Epoch 155/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6587.5342 - accuracy: 0.8919 - val_loss: 15337.3232 - val_accuracy: 0.8467\n",
            "Epoch 156/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6587.0513 - accuracy: 0.8919 - val_loss: 15290.0049 - val_accuracy: 0.8447\n",
            "Epoch 157/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6562.1289 - accuracy: 0.8907 - val_loss: 15320.8818 - val_accuracy: 0.8472\n",
            "Epoch 158/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6540.9209 - accuracy: 0.8925 - val_loss: 15348.3818 - val_accuracy: 0.8499\n",
            "Epoch 159/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6624.8721 - accuracy: 0.8912 - val_loss: 15325.0859 - val_accuracy: 0.8425\n",
            "Epoch 160/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 6458.2144 - accuracy: 0.8920roc-auc_val: 0.8208\n",
            "225/225 [==============================] - 4s 20ms/step - loss: 6458.2144 - accuracy: 0.8920 - val_loss: 15188.9648 - val_accuracy: 0.8417\n",
            "Epoch 161/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6525.2217 - accuracy: 0.8910 - val_loss: 15265.3848 - val_accuracy: 0.8463\n",
            "Epoch 162/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6522.6479 - accuracy: 0.8920 - val_loss: 15353.9141 - val_accuracy: 0.8467\n",
            "Epoch 163/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6470.9922 - accuracy: 0.8915 - val_loss: 15339.1963 - val_accuracy: 0.8443\n",
            "Epoch 164/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6441.4863 - accuracy: 0.8931 - val_loss: 15309.3672 - val_accuracy: 0.8513\n",
            "Epoch 165/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6387.7305 - accuracy: 0.8929 - val_loss: 15476.1465 - val_accuracy: 0.8449\n",
            "Epoch 166/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6446.6670 - accuracy: 0.8930 - val_loss: 15218.1582 - val_accuracy: 0.8504\n",
            "Epoch 167/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6447.6514 - accuracy: 0.8937 - val_loss: 15371.6182 - val_accuracy: 0.8417\n",
            "Epoch 168/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6464.1826 - accuracy: 0.8937 - val_loss: 15328.4805 - val_accuracy: 0.8488\n",
            "Epoch 169/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6389.0674 - accuracy: 0.8935 - val_loss: 15333.3184 - val_accuracy: 0.8458\n",
            "Epoch 170/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 6482.4858 - accuracy: 0.8931roc-auc_val: 0.8191\n",
            "225/225 [==============================] - 4s 20ms/step - loss: 6482.4858 - accuracy: 0.8931 - val_loss: 15331.6182 - val_accuracy: 0.8438\n",
            "Epoch 171/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6463.9316 - accuracy: 0.8935 - val_loss: 15244.1084 - val_accuracy: 0.8491\n",
            "Epoch 172/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6364.6758 - accuracy: 0.8950 - val_loss: 15350.7705 - val_accuracy: 0.8501\n",
            "Epoch 173/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6376.9155 - accuracy: 0.8921 - val_loss: 15367.2344 - val_accuracy: 0.8470\n",
            "Epoch 174/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6409.6675 - accuracy: 0.8953 - val_loss: 15274.7646 - val_accuracy: 0.8463\n",
            "Epoch 175/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6438.3877 - accuracy: 0.8942 - val_loss: 15235.3936 - val_accuracy: 0.8450\n",
            "Epoch 176/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6411.9238 - accuracy: 0.8924 - val_loss: 15306.1836 - val_accuracy: 0.8436\n",
            "Epoch 177/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6463.3999 - accuracy: 0.8924 - val_loss: 15309.7803 - val_accuracy: 0.8457\n",
            "Epoch 178/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6438.3735 - accuracy: 0.8928 - val_loss: 15277.5537 - val_accuracy: 0.8504\n",
            "Epoch 179/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6340.8940 - accuracy: 0.8928 - val_loss: 15241.2686 - val_accuracy: 0.8503\n",
            "Epoch 180/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 6348.6084 - accuracy: 0.8939roc-auc_val: 0.8198\n",
            "225/225 [==============================] - 4s 20ms/step - loss: 6348.6084 - accuracy: 0.8939 - val_loss: 15294.9248 - val_accuracy: 0.8503\n",
            "Epoch 181/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6351.6519 - accuracy: 0.8926 - val_loss: 15273.6455 - val_accuracy: 0.8418\n",
            "Epoch 182/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6351.7070 - accuracy: 0.8923 - val_loss: 15312.0439 - val_accuracy: 0.8506\n",
            "Epoch 183/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6231.7397 - accuracy: 0.8948 - val_loss: 15297.2227 - val_accuracy: 0.8494\n",
            "Epoch 184/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6317.6455 - accuracy: 0.8937 - val_loss: 15346.4121 - val_accuracy: 0.8482\n",
            "Epoch 185/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6353.4521 - accuracy: 0.8929 - val_loss: 15322.8965 - val_accuracy: 0.8482\n",
            "Epoch 186/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6316.6406 - accuracy: 0.8940 - val_loss: 15297.8887 - val_accuracy: 0.8472\n",
            "Epoch 187/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6363.5439 - accuracy: 0.8942 - val_loss: 15466.0615 - val_accuracy: 0.8470\n",
            "Epoch 188/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6298.9878 - accuracy: 0.8943 - val_loss: 15347.1914 - val_accuracy: 0.8507\n",
            "Epoch 189/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6314.8652 - accuracy: 0.8945 - val_loss: 15295.2793 - val_accuracy: 0.8539\n",
            "Epoch 190/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 6298.9468 - accuracy: 0.8946roc-auc_val: 0.8194\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 6306.4229 - accuracy: 0.8946 - val_loss: 15321.8672 - val_accuracy: 0.8528\n",
            "Epoch 191/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6249.0571 - accuracy: 0.8964 - val_loss: 15320.7256 - val_accuracy: 0.8541\n",
            "Epoch 192/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6376.6201 - accuracy: 0.8933 - val_loss: 15287.6260 - val_accuracy: 0.8503\n",
            "Epoch 193/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6297.5693 - accuracy: 0.8941 - val_loss: 15332.8438 - val_accuracy: 0.8500\n",
            "Epoch 194/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6299.2720 - accuracy: 0.8950 - val_loss: 15298.4834 - val_accuracy: 0.8528\n",
            "Epoch 195/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6363.7363 - accuracy: 0.8932 - val_loss: 15271.1250 - val_accuracy: 0.8471\n",
            "Epoch 196/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6259.4004 - accuracy: 0.8937 - val_loss: 15345.2021 - val_accuracy: 0.8503\n",
            "Epoch 197/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6236.7651 - accuracy: 0.8951 - val_loss: 15267.5361 - val_accuracy: 0.8581\n",
            "Epoch 198/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6324.1797 - accuracy: 0.8943 - val_loss: 15258.1924 - val_accuracy: 0.8568\n",
            "Epoch 199/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 6263.5371 - accuracy: 0.8941 - val_loss: 15242.4893 - val_accuracy: 0.8530\n",
            "Epoch 200/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 6221.2295 - accuracy: 0.8939roc-auc_val: 0.8218\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 6211.4883 - accuracy: 0.8940 - val_loss: 15308.5801 - val_accuracy: 0.8503\n",
            "Epoch 1/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 26544.0312 - accuracy: 0.6758 - val_loss: 22273.2617 - val_accuracy: 0.6696\n",
            "Epoch 2/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 20793.2598 - accuracy: 0.7453 - val_loss: 20752.6348 - val_accuracy: 0.6650\n",
            "Epoch 3/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 18847.6680 - accuracy: 0.7787 - val_loss: 19474.5215 - val_accuracy: 0.6937\n",
            "Epoch 4/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 17226.0547 - accuracy: 0.7918 - val_loss: 19202.6895 - val_accuracy: 0.7518\n",
            "Epoch 5/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 16132.8535 - accuracy: 0.8041 - val_loss: 19135.5684 - val_accuracy: 0.7288\n",
            "Epoch 6/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 15300.0137 - accuracy: 0.8159 - val_loss: 19450.5098 - val_accuracy: 0.7439\n",
            "Epoch 7/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14476.0010 - accuracy: 0.8242 - val_loss: 18870.7812 - val_accuracy: 0.7388\n",
            "Epoch 8/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14085.1211 - accuracy: 0.8280 - val_loss: 19108.5645 - val_accuracy: 0.7492\n",
            "Epoch 9/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13572.5361 - accuracy: 0.8353 - val_loss: 18967.1484 - val_accuracy: 0.7728\n",
            "Epoch 10/1000\n",
            "174/181 [===========================>..] - ETA: 0s - loss: 13237.9844 - accuracy: 0.8333roc-auc_val: 0.8104\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 13205.3037 - accuracy: 0.8337 - val_loss: 19165.8672 - val_accuracy: 0.7721\n",
            "Epoch 11/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12654.7588 - accuracy: 0.8417 - val_loss: 18907.9492 - val_accuracy: 0.7922\n",
            "Epoch 12/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12285.8174 - accuracy: 0.8456 - val_loss: 18915.6699 - val_accuracy: 0.8054\n",
            "Epoch 13/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12099.6006 - accuracy: 0.8479 - val_loss: 19046.1523 - val_accuracy: 0.7998\n",
            "Epoch 14/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11738.3232 - accuracy: 0.8514 - val_loss: 19116.4199 - val_accuracy: 0.7821\n",
            "Epoch 15/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11568.4102 - accuracy: 0.8513 - val_loss: 18661.8398 - val_accuracy: 0.7934\n",
            "Epoch 16/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11253.8672 - accuracy: 0.8542 - val_loss: 19068.5215 - val_accuracy: 0.7947\n",
            "Epoch 17/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11043.4893 - accuracy: 0.8541 - val_loss: 18887.0527 - val_accuracy: 0.8080\n",
            "Epoch 18/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10783.6660 - accuracy: 0.8573 - val_loss: 18944.0840 - val_accuracy: 0.8026\n",
            "Epoch 19/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10784.4082 - accuracy: 0.8560 - val_loss: 19037.1055 - val_accuracy: 0.8145\n",
            "Epoch 20/1000\n",
            "173/181 [===========================>..] - ETA: 0s - loss: 10435.6436 - accuracy: 0.8595roc-auc_val: 0.8113\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 10400.3838 - accuracy: 0.8592 - val_loss: 19080.9316 - val_accuracy: 0.8003\n",
            "Epoch 21/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10360.0283 - accuracy: 0.8617 - val_loss: 18800.6680 - val_accuracy: 0.7795\n",
            "Epoch 22/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10145.5078 - accuracy: 0.8606 - val_loss: 18742.6426 - val_accuracy: 0.8146\n",
            "Epoch 23/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10054.2939 - accuracy: 0.8619 - val_loss: 19166.7051 - val_accuracy: 0.8010\n",
            "Epoch 24/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9837.1016 - accuracy: 0.8639 - val_loss: 18961.8496 - val_accuracy: 0.8167\n",
            "Epoch 25/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9863.4951 - accuracy: 0.8649 - val_loss: 19241.8438 - val_accuracy: 0.8391\n",
            "Epoch 26/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9886.4219 - accuracy: 0.8639 - val_loss: 19024.0762 - val_accuracy: 0.8229\n",
            "Epoch 27/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9530.3340 - accuracy: 0.8651 - val_loss: 18693.7598 - val_accuracy: 0.8273\n",
            "Epoch 28/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9504.3174 - accuracy: 0.8679 - val_loss: 18874.0742 - val_accuracy: 0.7992\n",
            "Epoch 29/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9386.5508 - accuracy: 0.8672 - val_loss: 18486.2754 - val_accuracy: 0.7879\n",
            "Epoch 30/1000\n",
            "173/181 [===========================>..] - ETA: 0s - loss: 9312.9746 - accuracy: 0.8662roc-auc_val: 0.8166\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 9303.0068 - accuracy: 0.8663 - val_loss: 18648.1582 - val_accuracy: 0.8002\n",
            "Epoch 31/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9131.3164 - accuracy: 0.8692 - val_loss: 18849.2598 - val_accuracy: 0.8150\n",
            "Epoch 32/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9018.5068 - accuracy: 0.8684 - val_loss: 18957.4746 - val_accuracy: 0.8105\n",
            "Epoch 33/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8972.2715 - accuracy: 0.8687 - val_loss: 18836.5957 - val_accuracy: 0.8242\n",
            "Epoch 34/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8914.0371 - accuracy: 0.8727 - val_loss: 18816.0508 - val_accuracy: 0.8245\n",
            "Epoch 35/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8770.5186 - accuracy: 0.8711 - val_loss: 19042.0039 - val_accuracy: 0.8247\n",
            "Epoch 36/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8690.3389 - accuracy: 0.8730 - val_loss: 18920.3770 - val_accuracy: 0.8391\n",
            "Epoch 37/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8738.5938 - accuracy: 0.8728 - val_loss: 18788.4160 - val_accuracy: 0.8268\n",
            "Epoch 38/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8557.8896 - accuracy: 0.8736 - val_loss: 18868.8594 - val_accuracy: 0.8326\n",
            "Epoch 39/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8579.6250 - accuracy: 0.8728 - val_loss: 18844.2188 - val_accuracy: 0.8273\n",
            "Epoch 40/1000\n",
            "172/181 [===========================>..] - ETA: 0s - loss: 8411.6875 - accuracy: 0.8774roc-auc_val: 0.8149\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 8402.4873 - accuracy: 0.8770 - val_loss: 18901.2363 - val_accuracy: 0.8332\n",
            "Epoch 41/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8434.7793 - accuracy: 0.8746 - val_loss: 19126.2832 - val_accuracy: 0.8275\n",
            "Epoch 42/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8325.5654 - accuracy: 0.8763 - val_loss: 19071.1738 - val_accuracy: 0.8363\n",
            "Epoch 43/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8288.3184 - accuracy: 0.8757 - val_loss: 18956.4023 - val_accuracy: 0.8256\n",
            "Epoch 44/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8175.0425 - accuracy: 0.8754 - val_loss: 19164.4766 - val_accuracy: 0.8367\n",
            "Epoch 45/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8286.5303 - accuracy: 0.8778 - val_loss: 18906.9941 - val_accuracy: 0.8287\n",
            "Epoch 46/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8167.7192 - accuracy: 0.8762 - val_loss: 18973.8203 - val_accuracy: 0.8312\n",
            "Epoch 47/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8078.2466 - accuracy: 0.8772 - val_loss: 19084.1602 - val_accuracy: 0.8364\n",
            "Epoch 48/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 8024.8691 - accuracy: 0.8779 - val_loss: 19053.8164 - val_accuracy: 0.8366\n",
            "Epoch 49/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 8000.0142 - accuracy: 0.8778 - val_loss: 18838.2012 - val_accuracy: 0.8347\n",
            "Epoch 50/1000\n",
            "172/181 [===========================>..] - ETA: 0s - loss: 7936.3931 - accuracy: 0.8800roc-auc_val: 0.8162\n",
            "181/181 [==============================] - 7s 40ms/step - loss: 7900.0850 - accuracy: 0.8799 - val_loss: 18827.6348 - val_accuracy: 0.8361\n",
            "Epoch 51/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7780.8423 - accuracy: 0.8802 - val_loss: 18972.3379 - val_accuracy: 0.8476\n",
            "Epoch 52/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7849.9639 - accuracy: 0.8805 - val_loss: 18839.5684 - val_accuracy: 0.8484\n",
            "Epoch 53/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7815.3921 - accuracy: 0.8789 - val_loss: 18927.2246 - val_accuracy: 0.8536\n",
            "Epoch 54/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7856.3452 - accuracy: 0.8799 - val_loss: 18854.1211 - val_accuracy: 0.8396\n",
            "Epoch 55/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7725.9561 - accuracy: 0.8802 - val_loss: 18851.3320 - val_accuracy: 0.8467\n",
            "Epoch 56/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7598.7446 - accuracy: 0.8817 - val_loss: 19072.4941 - val_accuracy: 0.8365\n",
            "Epoch 57/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7677.7061 - accuracy: 0.8818 - val_loss: 19024.0605 - val_accuracy: 0.8409\n",
            "Epoch 58/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7597.0835 - accuracy: 0.8825 - val_loss: 19003.7832 - val_accuracy: 0.8455\n",
            "Epoch 59/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7525.2812 - accuracy: 0.8814 - val_loss: 19056.0625 - val_accuracy: 0.8453\n",
            "Epoch 60/1000\n",
            "177/181 [============================>.] - ETA: 0s - loss: 7566.7764 - accuracy: 0.8838roc-auc_val: 0.8161\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 7571.9316 - accuracy: 0.8838 - val_loss: 18948.2441 - val_accuracy: 0.8496\n",
            "Epoch 61/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7470.0322 - accuracy: 0.8821 - val_loss: 19050.9277 - val_accuracy: 0.8449\n",
            "Epoch 62/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7455.0874 - accuracy: 0.8827 - val_loss: 18987.5703 - val_accuracy: 0.8463\n",
            "Epoch 63/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7365.1572 - accuracy: 0.8850 - val_loss: 19125.9863 - val_accuracy: 0.8527\n",
            "Epoch 64/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7463.0762 - accuracy: 0.8827 - val_loss: 18999.2324 - val_accuracy: 0.8483\n",
            "Epoch 65/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7413.0332 - accuracy: 0.8834 - val_loss: 19063.6523 - val_accuracy: 0.8455\n",
            "Epoch 66/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7310.8013 - accuracy: 0.8842 - val_loss: 18970.0762 - val_accuracy: 0.8551\n",
            "Epoch 67/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7299.4580 - accuracy: 0.8841 - val_loss: 19071.7910 - val_accuracy: 0.8484\n",
            "Epoch 68/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7214.2363 - accuracy: 0.8851 - val_loss: 19000.3574 - val_accuracy: 0.8474\n",
            "Epoch 69/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7254.7427 - accuracy: 0.8857 - val_loss: 19048.1582 - val_accuracy: 0.8470\n",
            "Epoch 70/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 7185.7832 - accuracy: 0.8842roc-auc_val: 0.8163\n",
            "181/181 [==============================] - 7s 37ms/step - loss: 7145.6968 - accuracy: 0.8840 - val_loss: 18956.9590 - val_accuracy: 0.8530\n",
            "Epoch 71/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7171.9258 - accuracy: 0.8869 - val_loss: 18822.8125 - val_accuracy: 0.8517\n",
            "Epoch 72/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7189.5723 - accuracy: 0.8861 - val_loss: 19122.2539 - val_accuracy: 0.8517\n",
            "Epoch 73/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7101.0142 - accuracy: 0.8862 - val_loss: 19009.0391 - val_accuracy: 0.8558\n",
            "Epoch 74/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7113.7539 - accuracy: 0.8856 - val_loss: 18929.6797 - val_accuracy: 0.8535\n",
            "Epoch 75/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6895.0967 - accuracy: 0.8871 - val_loss: 19048.5215 - val_accuracy: 0.8426\n",
            "Epoch 76/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7022.1660 - accuracy: 0.8846 - val_loss: 18914.9277 - val_accuracy: 0.8460\n",
            "Epoch 77/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7144.8384 - accuracy: 0.8854 - val_loss: 18949.5566 - val_accuracy: 0.8553\n",
            "Epoch 78/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 7011.2935 - accuracy: 0.8861 - val_loss: 18939.7051 - val_accuracy: 0.8532\n",
            "Epoch 79/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 6995.5625 - accuracy: 0.8880 - val_loss: 19087.0820 - val_accuracy: 0.8597\n",
            "Epoch 1/1000\n",
            "136/136 [==============================] - 2s 11ms/step - loss: 26234.2129 - accuracy: 0.6733 - val_loss: 23414.0684 - val_accuracy: 0.7132\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 20614.0352 - accuracy: 0.7458 - val_loss: 21839.6543 - val_accuracy: 0.7337\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 18433.2402 - accuracy: 0.7687 - val_loss: 21706.0391 - val_accuracy: 0.7447\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 16754.2422 - accuracy: 0.7907 - val_loss: 21040.8496 - val_accuracy: 0.7768\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 15775.5205 - accuracy: 0.8028 - val_loss: 20919.6875 - val_accuracy: 0.7917\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14744.4385 - accuracy: 0.8139 - val_loss: 20905.0684 - val_accuracy: 0.8209\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14112.8242 - accuracy: 0.8181 - val_loss: 21078.5957 - val_accuracy: 0.8048\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13568.1328 - accuracy: 0.8287 - val_loss: 21088.0488 - val_accuracy: 0.8399\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13001.9502 - accuracy: 0.8301 - val_loss: 20755.4023 - val_accuracy: 0.8219\n",
            "Epoch 10/1000\n",
            "126/136 [==========================>...] - ETA: 0s - loss: 12431.7510 - accuracy: 0.8391roc-auc_val: 0.8186\n",
            "136/136 [==============================] - 9s 64ms/step - loss: 12469.8252 - accuracy: 0.8382 - val_loss: 20448.4707 - val_accuracy: 0.8115\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11985.2871 - accuracy: 0.8401 - val_loss: 20526.2969 - val_accuracy: 0.8365\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11767.9541 - accuracy: 0.8444 - val_loss: 20971.0898 - val_accuracy: 0.8493\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11441.2422 - accuracy: 0.8499 - val_loss: 20682.1055 - val_accuracy: 0.8293\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11274.8379 - accuracy: 0.8489 - val_loss: 20666.0293 - val_accuracy: 0.8420\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10966.6260 - accuracy: 0.8523 - val_loss: 20858.4121 - val_accuracy: 0.8527\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10670.3770 - accuracy: 0.8543 - val_loss: 20664.1797 - val_accuracy: 0.8413\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10490.4414 - accuracy: 0.8593 - val_loss: 21030.2578 - val_accuracy: 0.8373\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10255.1504 - accuracy: 0.8591 - val_loss: 20785.4297 - val_accuracy: 0.8453\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9993.1182 - accuracy: 0.8596 - val_loss: 21086.2070 - val_accuracy: 0.8727\n",
            "Epoch 20/1000\n",
            "129/136 [===========================>..] - ETA: 0s - loss: 9770.9834 - accuracy: 0.8653roc-auc_val: 0.8164\n",
            "136/136 [==============================] - 9s 64ms/step - loss: 9787.4365 - accuracy: 0.8649 - val_loss: 20886.5801 - val_accuracy: 0.8523\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9596.5361 - accuracy: 0.8640 - val_loss: 21107.5527 - val_accuracy: 0.8470\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9392.1670 - accuracy: 0.8693 - val_loss: 20882.9297 - val_accuracy: 0.8640\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9381.4590 - accuracy: 0.8679 - val_loss: 21075.5039 - val_accuracy: 0.8592\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9163.9961 - accuracy: 0.8698 - val_loss: 20782.6602 - val_accuracy: 0.8529\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9256.9160 - accuracy: 0.8708 - val_loss: 21101.7207 - val_accuracy: 0.8747\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8927.2959 - accuracy: 0.8731 - val_loss: 20907.0254 - val_accuracy: 0.8607\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8775.5010 - accuracy: 0.8730 - val_loss: 21201.1289 - val_accuracy: 0.8705\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8788.0000 - accuracy: 0.8726 - val_loss: 21256.3359 - val_accuracy: 0.8685\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8609.2402 - accuracy: 0.8739 - val_loss: 21311.3809 - val_accuracy: 0.8713\n",
            "Epoch 30/1000\n",
            "130/136 [===========================>..] - ETA: 0s - loss: 8511.9385 - accuracy: 0.8768roc-auc_val: 0.81\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 8490.1504 - accuracy: 0.8770 - val_loss: 21631.9902 - val_accuracy: 0.8767\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8399.7627 - accuracy: 0.8754 - val_loss: 21420.3184 - val_accuracy: 0.8695\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8272.1865 - accuracy: 0.8776 - val_loss: 21100.8281 - val_accuracy: 0.8473\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8159.6201 - accuracy: 0.8771 - val_loss: 21263.8047 - val_accuracy: 0.8595\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8194.5820 - accuracy: 0.8767 - val_loss: 21366.0234 - val_accuracy: 0.8685\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8123.6738 - accuracy: 0.8771 - val_loss: 21569.6094 - val_accuracy: 0.8774\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7963.1650 - accuracy: 0.8835 - val_loss: 21567.1387 - val_accuracy: 0.8803\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7872.6968 - accuracy: 0.8832 - val_loss: 21783.4102 - val_accuracy: 0.8790\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7794.0850 - accuracy: 0.8825 - val_loss: 21598.2656 - val_accuracy: 0.8713\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7787.5698 - accuracy: 0.8820 - val_loss: 21871.4648 - val_accuracy: 0.8739\n",
            "Epoch 40/1000\n",
            "130/136 [===========================>..] - ETA: 0s - loss: 7704.4404 - accuracy: 0.8841roc-auc_val: 0.8076\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 7661.7319 - accuracy: 0.8843 - val_loss: 21879.1406 - val_accuracy: 0.8814\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7573.7446 - accuracy: 0.8830 - val_loss: 21803.2188 - val_accuracy: 0.8676\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7648.4692 - accuracy: 0.8853 - val_loss: 21782.5879 - val_accuracy: 0.8729\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7502.0464 - accuracy: 0.8849 - val_loss: 22143.4297 - val_accuracy: 0.8863\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7349.6064 - accuracy: 0.8873 - val_loss: 21961.7812 - val_accuracy: 0.8730\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7396.3408 - accuracy: 0.8871 - val_loss: 21878.7500 - val_accuracy: 0.8680\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7330.0459 - accuracy: 0.8858 - val_loss: 21505.1953 - val_accuracy: 0.8628\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7210.2598 - accuracy: 0.8848 - val_loss: 21814.4707 - val_accuracy: 0.8737\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7221.7720 - accuracy: 0.8889 - val_loss: 21682.2461 - val_accuracy: 0.8563\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7239.6064 - accuracy: 0.8877 - val_loss: 21919.0059 - val_accuracy: 0.8649\n",
            "Epoch 50/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 7108.2388 - accuracy: 0.8909roc-auc_val: 0.804\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 7096.3794 - accuracy: 0.8909 - val_loss: 22120.9082 - val_accuracy: 0.8766\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 7119.5083 - accuracy: 0.8878 - val_loss: 22139.8105 - val_accuracy: 0.8758\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6954.6753 - accuracy: 0.8897 - val_loss: 22036.8086 - val_accuracy: 0.8671\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6913.8989 - accuracy: 0.8882 - val_loss: 22078.7637 - val_accuracy: 0.8692\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6867.4028 - accuracy: 0.8899 - val_loss: 22364.9336 - val_accuracy: 0.8752\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6884.7295 - accuracy: 0.8892 - val_loss: 21957.7188 - val_accuracy: 0.8580\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6898.8970 - accuracy: 0.8892 - val_loss: 22028.0371 - val_accuracy: 0.8718\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6702.2788 - accuracy: 0.8911 - val_loss: 22153.4492 - val_accuracy: 0.8727\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6761.6919 - accuracy: 0.8921 - val_loss: 22288.1582 - val_accuracy: 0.8759\n",
            "Epoch 59/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 6708.3887 - accuracy: 0.8901 - val_loss: 22275.4629 - val_accuracy: 0.8701\n",
            "Epoch 60/1000\n",
            "131/136 [===========================>..] - ETA: 0s - loss: 6775.3979 - accuracy: 0.8909roc-auc_val: 0.8186\n",
            "136/136 [==============================] - 9s 65ms/step - loss: 6746.3477 - accuracy: 0.8908 - val_loss: 22424.6387 - val_accuracy: 0.8607\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 17ms/step - loss: 26646.4473 - accuracy: 0.6509 - val_loss: 24789.3633 - val_accuracy: 0.7247\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 20635.6523 - accuracy: 0.7194 - val_loss: 23798.8184 - val_accuracy: 0.7082\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 18428.6836 - accuracy: 0.7494 - val_loss: 22781.9492 - val_accuracy: 0.7649\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 16964.4199 - accuracy: 0.7687 - val_loss: 22437.4062 - val_accuracy: 0.7914\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 15901.4561 - accuracy: 0.7858 - val_loss: 22174.4297 - val_accuracy: 0.8048\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 14832.5264 - accuracy: 0.7984 - val_loss: 21560.1895 - val_accuracy: 0.7992\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13813.7021 - accuracy: 0.8094 - val_loss: 21810.2910 - val_accuracy: 0.8247\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13276.4746 - accuracy: 0.8136 - val_loss: 22382.3242 - val_accuracy: 0.8594\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12609.6123 - accuracy: 0.8212 - val_loss: 22174.3789 - val_accuracy: 0.8546\n",
            "Epoch 10/1000\n",
            "84/88 [===========================>..] - ETA: 0s - loss: 12175.6582 - accuracy: 0.8276roc-auc_val: 0.8161\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 12106.5869 - accuracy: 0.8276 - val_loss: 21880.5176 - val_accuracy: 0.8409\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11678.2129 - accuracy: 0.8292 - val_loss: 22315.0859 - val_accuracy: 0.8756\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11147.2461 - accuracy: 0.8399 - val_loss: 22650.2344 - val_accuracy: 0.8761\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10645.6201 - accuracy: 0.8418 - val_loss: 22176.5664 - val_accuracy: 0.8781\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10134.4209 - accuracy: 0.8476 - val_loss: 22616.9219 - val_accuracy: 0.8847\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10235.7188 - accuracy: 0.8471 - val_loss: 22398.1738 - val_accuracy: 0.8874\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9562.7852 - accuracy: 0.8499 - val_loss: 22492.5762 - val_accuracy: 0.8972\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9458.2158 - accuracy: 0.8581 - val_loss: 22798.9238 - val_accuracy: 0.8872\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9406.9502 - accuracy: 0.8580 - val_loss: 22475.9082 - val_accuracy: 0.8956\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9137.6172 - accuracy: 0.8569 - val_loss: 22718.6250 - val_accuracy: 0.9037\n",
            "Epoch 20/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 8814.1689 - accuracy: 0.8649roc-auc_val: 0.813\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 8741.4404 - accuracy: 0.8648 - val_loss: 22915.8223 - val_accuracy: 0.8962\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8591.7334 - accuracy: 0.8644 - val_loss: 23195.4414 - val_accuracy: 0.8995\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8366.2031 - accuracy: 0.8668 - val_loss: 22885.7891 - val_accuracy: 0.8894\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8204.3770 - accuracy: 0.8672 - val_loss: 23506.3672 - val_accuracy: 0.9038\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8144.8511 - accuracy: 0.8670 - val_loss: 23240.1230 - val_accuracy: 0.8951\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7783.8560 - accuracy: 0.8721 - val_loss: 22829.5254 - val_accuracy: 0.9029\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7691.6514 - accuracy: 0.8708 - val_loss: 23026.6113 - val_accuracy: 0.9005\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7578.6016 - accuracy: 0.8736 - val_loss: 23225.3789 - val_accuracy: 0.9018\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7483.2363 - accuracy: 0.8735 - val_loss: 23382.1367 - val_accuracy: 0.9012\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7140.8115 - accuracy: 0.8767 - val_loss: 23506.0547 - val_accuracy: 0.9060\n",
            "Epoch 30/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 7287.0898 - accuracy: 0.8780roc-auc_val: 0.8065\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 7310.6646 - accuracy: 0.8774 - val_loss: 23415.3789 - val_accuracy: 0.8951\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7124.7344 - accuracy: 0.8785 - val_loss: 23291.3164 - val_accuracy: 0.8979\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7001.8672 - accuracy: 0.8789 - val_loss: 23498.8047 - val_accuracy: 0.9062\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6865.0366 - accuracy: 0.8814 - val_loss: 23483.3496 - val_accuracy: 0.8971\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6716.9023 - accuracy: 0.8827 - val_loss: 23559.1875 - val_accuracy: 0.9039\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6658.7080 - accuracy: 0.8866 - val_loss: 23761.1348 - val_accuracy: 0.9088\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6559.7563 - accuracy: 0.8855 - val_loss: 23613.2383 - val_accuracy: 0.9008\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6517.7388 - accuracy: 0.8844 - val_loss: 23629.7305 - val_accuracy: 0.9047\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6325.2490 - accuracy: 0.8859 - val_loss: 23775.1914 - val_accuracy: 0.9040\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6401.0933 - accuracy: 0.8878 - val_loss: 23223.5605 - val_accuracy: 0.9006\n",
            "Epoch 40/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 6293.5547 - accuracy: 0.8885roc-auc_val: 0.8072\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 6287.4795 - accuracy: 0.8883 - val_loss: 23499.4082 - val_accuracy: 0.9055\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6000.3638 - accuracy: 0.8870 - val_loss: 23928.7109 - val_accuracy: 0.9060\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6005.9165 - accuracy: 0.8896 - val_loss: 23944.6973 - val_accuracy: 0.9096\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5992.9858 - accuracy: 0.8908 - val_loss: 23740.8457 - val_accuracy: 0.9138\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6036.4487 - accuracy: 0.8910 - val_loss: 24002.0430 - val_accuracy: 0.9116\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5978.9302 - accuracy: 0.8916 - val_loss: 23682.3008 - val_accuracy: 0.9027\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5884.0776 - accuracy: 0.8905 - val_loss: 23885.1934 - val_accuracy: 0.8976\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5698.8315 - accuracy: 0.8919 - val_loss: 23737.8477 - val_accuracy: 0.9042\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5681.0400 - accuracy: 0.8945 - val_loss: 23639.8301 - val_accuracy: 0.8996\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5799.2397 - accuracy: 0.8896 - val_loss: 23583.7383 - val_accuracy: 0.8960\n",
            "Epoch 50/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 5534.2388 - accuracy: 0.8931roc-auc_val: 0.8057\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 5537.7793 - accuracy: 0.8929 - val_loss: 23709.5176 - val_accuracy: 0.9084\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5611.7998 - accuracy: 0.8952 - val_loss: 24116.9922 - val_accuracy: 0.9136\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5452.4658 - accuracy: 0.8944 - val_loss: 23793.9082 - val_accuracy: 0.9131\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5286.2300 - accuracy: 0.8961 - val_loss: 23751.9844 - val_accuracy: 0.9112\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5382.6611 - accuracy: 0.8982 - val_loss: 23398.5117 - val_accuracy: 0.9025\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5348.2974 - accuracy: 0.8972 - val_loss: 23692.4746 - val_accuracy: 0.9010\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 5314.5879 - accuracy: 0.8994 - val_loss: 24115.7090 - val_accuracy: 0.9025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5PePWLxWIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2eb45005-8be8-4e61-d939-57ef3bffa059"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNDmjro-2ndD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
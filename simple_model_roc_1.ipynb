{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_1",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e394038f-9e2f-43f7-e0df-878c16d16486"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d9026f67-2917-4881-9568-1ee9ff1ba527"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 79.9MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 84% 44.0M/52.2M [00:00<00:00, 51.8MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 89.0MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 84% 49.0M/58.3M [00:00<00:00, 73.3MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 147MB/s] \n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 107MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 224MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv80v8W_Ko2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4c07e001-83af-407b-8b8f-bdc63fc0b2e5"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jt2pcxPije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8bfbe683-5746-45e2-88ab-130a079e1680"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvEaxp9jhbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "1f013f44-fb83-496a-c249-4f5862f96b1a"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4cb2f83c-a5d8-475d-ad8c-e79de396cc5e"
      },
      "source": [
        "\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkLEBcl6LHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 0.4\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDFTro_vTJr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "339638a6-4ba8-42d1-86c5-bca69b53935a"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "# trn=trn.drop(['isFraud_isna'],1)\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Adam(0.001,decay=1e-3),loss=rac,metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.00001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 9ms/step - loss: 38184.9062 - accuracy: 0.7035 - val_loss: 32468.2812 - val_accuracy: 0.5653\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 32249.5215 - accuracy: 0.7457 - val_loss: 30111.5684 - val_accuracy: 0.6057\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 29498.3555 - accuracy: 0.7690 - val_loss: 27755.6230 - val_accuracy: 0.6584\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 27915.3809 - accuracy: 0.7862 - val_loss: 27500.6543 - val_accuracy: 0.6544\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 26524.8848 - accuracy: 0.7952 - val_loss: 27354.4609 - val_accuracy: 0.6495\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 25428.8047 - accuracy: 0.8025 - val_loss: 27081.3242 - val_accuracy: 0.6753\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 24481.1289 - accuracy: 0.8097 - val_loss: 27040.2109 - val_accuracy: 0.6792\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 23578.2695 - accuracy: 0.8175 - val_loss: 25428.5645 - val_accuracy: 0.7305\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 22737.7930 - accuracy: 0.8213 - val_loss: 25180.1875 - val_accuracy: 0.7498\n",
            "Epoch 10/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 22309.3906 - accuracy: 0.8270roc-auc_val: 0.7963\n",
            "225/225 [==============================] - 6s 27ms/step - loss: 22277.2148 - accuracy: 0.8270 - val_loss: 24903.8379 - val_accuracy: 0.7405\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 21649.6621 - accuracy: 0.8331 - val_loss: 25090.3027 - val_accuracy: 0.7366\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 21108.3965 - accuracy: 0.8337 - val_loss: 24368.5996 - val_accuracy: 0.7638\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20674.4297 - accuracy: 0.8364 - val_loss: 24122.9395 - val_accuracy: 0.7745\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20367.4805 - accuracy: 0.8427 - val_loss: 24117.3926 - val_accuracy: 0.7823\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19985.1875 - accuracy: 0.8438 - val_loss: 25063.9922 - val_accuracy: 0.7930\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19722.4141 - accuracy: 0.8475 - val_loss: 25270.2578 - val_accuracy: 0.7664\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19405.6348 - accuracy: 0.8480 - val_loss: 24179.9180 - val_accuracy: 0.8003\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19212.3770 - accuracy: 0.8516 - val_loss: 23978.8438 - val_accuracy: 0.7916\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18996.0352 - accuracy: 0.8511 - val_loss: 24502.2227 - val_accuracy: 0.7919\n",
            "Epoch 20/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 18832.2227 - accuracy: 0.8510roc-auc_val: 0.807\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 18802.3945 - accuracy: 0.8511 - val_loss: 24072.1328 - val_accuracy: 0.8133\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18669.3906 - accuracy: 0.8496 - val_loss: 23862.7676 - val_accuracy: 0.8246\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18306.9219 - accuracy: 0.8528 - val_loss: 23955.1211 - val_accuracy: 0.8010\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18178.5078 - accuracy: 0.8565 - val_loss: 24005.3750 - val_accuracy: 0.8042\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17998.9004 - accuracy: 0.8588 - val_loss: 23756.6875 - val_accuracy: 0.8239\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17948.2793 - accuracy: 0.8579 - val_loss: 23774.8789 - val_accuracy: 0.8229\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17775.3809 - accuracy: 0.8580 - val_loss: 23735.5723 - val_accuracy: 0.8293\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17432.0137 - accuracy: 0.8597 - val_loss: 23781.7949 - val_accuracy: 0.8177\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17474.4023 - accuracy: 0.8593 - val_loss: 23398.5020 - val_accuracy: 0.8539\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17281.4648 - accuracy: 0.8620 - val_loss: 23526.4980 - val_accuracy: 0.8258\n",
            "Epoch 30/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 17178.5996 - accuracy: 0.8615roc-auc_val: 0.8153\n",
            "225/225 [==============================] - 6s 27ms/step - loss: 17155.8887 - accuracy: 0.8615 - val_loss: 23323.5742 - val_accuracy: 0.8244\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16962.2324 - accuracy: 0.8637 - val_loss: 23746.8125 - val_accuracy: 0.8237\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17031.8730 - accuracy: 0.8592 - val_loss: 23320.2930 - val_accuracy: 0.8268\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17025.4824 - accuracy: 0.8580 - val_loss: 23182.6875 - val_accuracy: 0.8179\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16783.6094 - accuracy: 0.8613 - val_loss: 23374.5762 - val_accuracy: 0.8315\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16696.0996 - accuracy: 0.8661 - val_loss: 23532.6113 - val_accuracy: 0.8274\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16612.6543 - accuracy: 0.8632 - val_loss: 23404.0547 - val_accuracy: 0.8253\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16653.2051 - accuracy: 0.8650 - val_loss: 23552.7324 - val_accuracy: 0.8470\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16498.9746 - accuracy: 0.8609 - val_loss: 23492.5312 - val_accuracy: 0.8395\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16298.3193 - accuracy: 0.8667 - val_loss: 23524.4746 - val_accuracy: 0.8368\n",
            "Epoch 40/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 16202.8145 - accuracy: 0.8684roc-auc_val: 0.8114\n",
            "225/225 [==============================] - 6s 27ms/step - loss: 16202.8145 - accuracy: 0.8684 - val_loss: 23525.0391 - val_accuracy: 0.8282\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16223.5879 - accuracy: 0.8694 - val_loss: 23400.8711 - val_accuracy: 0.8434\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16238.7031 - accuracy: 0.8692 - val_loss: 23478.2109 - val_accuracy: 0.8319\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16152.3301 - accuracy: 0.8682 - val_loss: 23176.8477 - val_accuracy: 0.8402\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 16121.7412 - accuracy: 0.8676 - val_loss: 23468.6406 - val_accuracy: 0.8393\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15848.7529 - accuracy: 0.8694 - val_loss: 23460.3281 - val_accuracy: 0.8415\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15952.1729 - accuracy: 0.8702 - val_loss: 23484.1875 - val_accuracy: 0.8267\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15814.7061 - accuracy: 0.8706 - val_loss: 23545.0176 - val_accuracy: 0.8513\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15803.0098 - accuracy: 0.8713 - val_loss: 23341.7148 - val_accuracy: 0.8520\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15686.8975 - accuracy: 0.8709 - val_loss: 23661.3535 - val_accuracy: 0.8450\n",
            "Epoch 50/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 15615.4648 - accuracy: 0.8717roc-auc_val: 0.8131\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 15588.4492 - accuracy: 0.8717 - val_loss: 23502.6836 - val_accuracy: 0.8435\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15601.9678 - accuracy: 0.8736 - val_loss: 23360.4590 - val_accuracy: 0.8471\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15632.6289 - accuracy: 0.8722 - val_loss: 23090.9434 - val_accuracy: 0.8401\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15475.0088 - accuracy: 0.8723 - val_loss: 23378.2402 - val_accuracy: 0.8438\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15392.8115 - accuracy: 0.8718 - val_loss: 23154.1406 - val_accuracy: 0.8534\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15532.3594 - accuracy: 0.8716 - val_loss: 23223.4785 - val_accuracy: 0.8551\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15290.8037 - accuracy: 0.8732 - val_loss: 23682.1387 - val_accuracy: 0.8511\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15347.1572 - accuracy: 0.8742 - val_loss: 23354.7949 - val_accuracy: 0.8446\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15341.6787 - accuracy: 0.8713 - val_loss: 23352.8516 - val_accuracy: 0.8395\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15254.3320 - accuracy: 0.8722 - val_loss: 23292.6074 - val_accuracy: 0.8518\n",
            "Epoch 60/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 15118.1387 - accuracy: 0.8726roc-auc_val: 0.8144\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 15090.5537 - accuracy: 0.8726 - val_loss: 23390.3027 - val_accuracy: 0.8489\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15009.4424 - accuracy: 0.8767 - val_loss: 23355.3008 - val_accuracy: 0.8639\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14912.2539 - accuracy: 0.8780 - val_loss: 23521.7090 - val_accuracy: 0.8625\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14924.0664 - accuracy: 0.8738 - val_loss: 23540.5020 - val_accuracy: 0.8557\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15187.0420 - accuracy: 0.8740 - val_loss: 23511.8125 - val_accuracy: 0.8534\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14840.1182 - accuracy: 0.8763 - val_loss: 23228.9512 - val_accuracy: 0.8620\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14903.8848 - accuracy: 0.8762 - val_loss: 24047.0352 - val_accuracy: 0.8437\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14977.1611 - accuracy: 0.8764 - val_loss: 23241.5996 - val_accuracy: 0.8624\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14883.8926 - accuracy: 0.8760 - val_loss: 23427.2539 - val_accuracy: 0.8583\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14838.0684 - accuracy: 0.8785 - val_loss: 23266.4707 - val_accuracy: 0.8615\n",
            "Epoch 70/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 14878.2158 - accuracy: 0.8760roc-auc_val: 0.8166\n",
            "225/225 [==============================] - 6s 28ms/step - loss: 14894.0205 - accuracy: 0.8760 - val_loss: 23298.4414 - val_accuracy: 0.8594\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 14887.5283 - accuracy: 0.8780 - val_loss: 23269.7480 - val_accuracy: 0.8547\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 14794.7881 - accuracy: 0.8786 - val_loss: 23411.4355 - val_accuracy: 0.8634\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 14664.2031 - accuracy: 0.8802 - val_loss: 23458.1758 - val_accuracy: 0.8602\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14587.4980 - accuracy: 0.8748 - val_loss: 23639.5371 - val_accuracy: 0.8304\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14635.0566 - accuracy: 0.8775 - val_loss: 23473.8105 - val_accuracy: 0.8546\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14739.4893 - accuracy: 0.8776 - val_loss: 23552.8301 - val_accuracy: 0.8531\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14662.7949 - accuracy: 0.8747 - val_loss: 23438.2168 - val_accuracy: 0.8632\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14506.0674 - accuracy: 0.8785 - val_loss: 23678.2949 - val_accuracy: 0.8609\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14622.9248 - accuracy: 0.8787 - val_loss: 23667.7891 - val_accuracy: 0.8620\n",
            "Epoch 80/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 14649.4678 - accuracy: 0.8777roc-auc_val: 0.8125\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 14622.3789 - accuracy: 0.8777 - val_loss: 23613.7637 - val_accuracy: 0.8457\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14508.2949 - accuracy: 0.8801 - val_loss: 23606.5820 - val_accuracy: 0.8532\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14408.8574 - accuracy: 0.8770 - val_loss: 23566.6094 - val_accuracy: 0.8508\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14555.4482 - accuracy: 0.8755 - val_loss: 23591.2578 - val_accuracy: 0.8614\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14329.4082 - accuracy: 0.8821 - val_loss: 23598.5000 - val_accuracy: 0.8611\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14392.9844 - accuracy: 0.8798 - val_loss: 23611.1133 - val_accuracy: 0.8517\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14369.1982 - accuracy: 0.8810 - val_loss: 23602.2715 - val_accuracy: 0.8586\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14183.2383 - accuracy: 0.8783 - val_loss: 23552.1504 - val_accuracy: 0.8548\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14316.2168 - accuracy: 0.8798 - val_loss: 23546.1387 - val_accuracy: 0.8588\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14213.4609 - accuracy: 0.8814 - val_loss: 23507.3516 - val_accuracy: 0.8673\n",
            "Epoch 90/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 14310.7207 - accuracy: 0.8805roc-auc_val: 0.8156\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 14290.5586 - accuracy: 0.8805 - val_loss: 23523.4375 - val_accuracy: 0.8687\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14252.2734 - accuracy: 0.8809 - val_loss: 23589.0352 - val_accuracy: 0.8695\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14313.9746 - accuracy: 0.8814 - val_loss: 23362.1719 - val_accuracy: 0.8708\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14145.6328 - accuracy: 0.8822 - val_loss: 23319.1797 - val_accuracy: 0.8627\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14323.0781 - accuracy: 0.8815 - val_loss: 23372.0508 - val_accuracy: 0.8694\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14014.3770 - accuracy: 0.8811 - val_loss: 23456.6914 - val_accuracy: 0.8782\n",
            "Epoch 96/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14127.7129 - accuracy: 0.8830 - val_loss: 23271.5332 - val_accuracy: 0.8738\n",
            "Epoch 97/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 13988.0322 - accuracy: 0.8808 - val_loss: 23580.6855 - val_accuracy: 0.8655\n",
            "Epoch 98/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 13954.2217 - accuracy: 0.8838 - val_loss: 23474.7910 - val_accuracy: 0.8697\n",
            "Epoch 99/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14036.9785 - accuracy: 0.8837 - val_loss: 23547.8047 - val_accuracy: 0.8718\n",
            "Epoch 100/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 14121.8838 - accuracy: 0.8834roc-auc_val: 0.8172\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 14098.7998 - accuracy: 0.8834 - val_loss: 23368.2617 - val_accuracy: 0.8692\n",
            "Epoch 101/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14007.1094 - accuracy: 0.8854 - val_loss: 23512.6289 - val_accuracy: 0.8675\n",
            "Epoch 102/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 13946.0381 - accuracy: 0.8822 - val_loss: 23383.2676 - val_accuracy: 0.8712\n",
            "Epoch 1/1000\n",
            "181/181 [==============================] - 2s 11ms/step - loss: 38158.5625 - accuracy: 0.7022 - val_loss: 33526.6016 - val_accuracy: 0.6273\n",
            "Epoch 2/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 32265.1152 - accuracy: 0.7439 - val_loss: 32680.3965 - val_accuracy: 0.6225\n",
            "Epoch 3/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 29719.1777 - accuracy: 0.7622 - val_loss: 31571.2734 - val_accuracy: 0.6410\n",
            "Epoch 4/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 28102.5684 - accuracy: 0.7719 - val_loss: 31036.2305 - val_accuracy: 0.6571\n",
            "Epoch 5/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 26721.6543 - accuracy: 0.7821 - val_loss: 30735.1426 - val_accuracy: 0.6963\n",
            "Epoch 6/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 25505.5996 - accuracy: 0.7899 - val_loss: 29739.0742 - val_accuracy: 0.7113\n",
            "Epoch 7/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 24710.6328 - accuracy: 0.7985 - val_loss: 30085.6211 - val_accuracy: 0.6944\n",
            "Epoch 8/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 23582.2988 - accuracy: 0.8045 - val_loss: 29211.6855 - val_accuracy: 0.6980\n",
            "Epoch 9/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 22712.4238 - accuracy: 0.8153 - val_loss: 29778.7012 - val_accuracy: 0.7085\n",
            "Epoch 10/1000\n",
            "177/181 [============================>.] - ETA: 0s - loss: 22020.1289 - accuracy: 0.8223roc-auc_val: 0.8018\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 22019.1660 - accuracy: 0.8224 - val_loss: 29805.3887 - val_accuracy: 0.7226\n",
            "Epoch 11/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 21590.8438 - accuracy: 0.8217 - val_loss: 28774.8301 - val_accuracy: 0.7304\n",
            "Epoch 12/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20810.8516 - accuracy: 0.8296 - val_loss: 28702.9258 - val_accuracy: 0.7206\n",
            "Epoch 13/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20690.2949 - accuracy: 0.8259 - val_loss: 29313.6055 - val_accuracy: 0.7161\n",
            "Epoch 14/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20177.1191 - accuracy: 0.8324 - val_loss: 29385.1172 - val_accuracy: 0.7418\n",
            "Epoch 15/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19537.1992 - accuracy: 0.8416 - val_loss: 28936.9922 - val_accuracy: 0.7699\n",
            "Epoch 16/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19490.8516 - accuracy: 0.8367 - val_loss: 29396.0781 - val_accuracy: 0.7600\n",
            "Epoch 17/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18965.8594 - accuracy: 0.8455 - val_loss: 29066.6504 - val_accuracy: 0.7383\n",
            "Epoch 18/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18923.9336 - accuracy: 0.8436 - val_loss: 28617.1699 - val_accuracy: 0.7539\n",
            "Epoch 19/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18476.7070 - accuracy: 0.8472 - val_loss: 28720.8281 - val_accuracy: 0.7352\n",
            "Epoch 20/1000\n",
            "177/181 [============================>.] - ETA: 0s - loss: 18310.0391 - accuracy: 0.8453roc-auc_val: 0.8116\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 18299.9980 - accuracy: 0.8452 - val_loss: 28596.0840 - val_accuracy: 0.7312\n",
            "Epoch 21/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18242.4141 - accuracy: 0.8459 - val_loss: 28491.1289 - val_accuracy: 0.7743\n",
            "Epoch 22/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17917.2188 - accuracy: 0.8499 - val_loss: 28570.3027 - val_accuracy: 0.7658\n",
            "Epoch 23/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17814.3184 - accuracy: 0.8521 - val_loss: 28268.3262 - val_accuracy: 0.7643\n",
            "Epoch 24/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17503.4395 - accuracy: 0.8547 - val_loss: 28369.4141 - val_accuracy: 0.7627\n",
            "Epoch 25/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17223.3613 - accuracy: 0.8530 - val_loss: 28880.8379 - val_accuracy: 0.7450\n",
            "Epoch 26/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17165.3379 - accuracy: 0.8574 - val_loss: 28887.6328 - val_accuracy: 0.7500\n",
            "Epoch 27/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 16764.8828 - accuracy: 0.8558 - val_loss: 28712.2031 - val_accuracy: 0.7334\n",
            "Epoch 28/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 16896.1172 - accuracy: 0.8585 - val_loss: 28577.6836 - val_accuracy: 0.7716\n",
            "Epoch 29/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 16754.4551 - accuracy: 0.8585 - val_loss: 28439.2129 - val_accuracy: 0.7756\n",
            "Epoch 30/1000\n",
            "181/181 [==============================] - ETA: 0s - loss: 16903.5918 - accuracy: 0.8590roc-auc_val: 0.8125\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 16903.5918 - accuracy: 0.8590 - val_loss: 28666.6543 - val_accuracy: 0.7744\n",
            "Epoch 31/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 16395.0801 - accuracy: 0.8589 - val_loss: 28295.1328 - val_accuracy: 0.7830\n",
            "Epoch 32/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 16497.8613 - accuracy: 0.8596 - val_loss: 28402.0742 - val_accuracy: 0.7658\n",
            "Epoch 33/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 16130.1465 - accuracy: 0.8598 - val_loss: 28379.2031 - val_accuracy: 0.7561\n",
            "Epoch 34/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 16185.2275 - accuracy: 0.8619 - val_loss: 28708.8242 - val_accuracy: 0.7825\n",
            "Epoch 35/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 16052.7637 - accuracy: 0.8639 - val_loss: 28472.7402 - val_accuracy: 0.7768\n",
            "Epoch 36/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15983.6670 - accuracy: 0.8606 - val_loss: 28657.2969 - val_accuracy: 0.7771\n",
            "Epoch 37/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15909.6338 - accuracy: 0.8671 - val_loss: 28503.0332 - val_accuracy: 0.7735\n",
            "Epoch 38/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15808.2227 - accuracy: 0.8643 - val_loss: 28345.1465 - val_accuracy: 0.7647\n",
            "Epoch 39/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15725.3896 - accuracy: 0.8625 - val_loss: 28705.1172 - val_accuracy: 0.7638\n",
            "Epoch 40/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 15583.3867 - accuracy: 0.8666roc-auc_val: 0.8145\n",
            "181/181 [==============================] - 9s 49ms/step - loss: 15556.1055 - accuracy: 0.8665 - val_loss: 28366.7422 - val_accuracy: 0.7742\n",
            "Epoch 41/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15536.9971 - accuracy: 0.8663 - val_loss: 28383.1992 - val_accuracy: 0.7827\n",
            "Epoch 42/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15525.5146 - accuracy: 0.8643 - val_loss: 28650.5566 - val_accuracy: 0.7734\n",
            "Epoch 43/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15447.6826 - accuracy: 0.8627 - val_loss: 28714.6562 - val_accuracy: 0.7765\n",
            "Epoch 44/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15326.4385 - accuracy: 0.8651 - val_loss: 28722.6953 - val_accuracy: 0.7723\n",
            "Epoch 45/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15285.7451 - accuracy: 0.8677 - val_loss: 28528.3594 - val_accuracy: 0.7800\n",
            "Epoch 46/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15011.6836 - accuracy: 0.8729 - val_loss: 28922.1523 - val_accuracy: 0.8010\n",
            "Epoch 47/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15158.9385 - accuracy: 0.8707 - val_loss: 28519.9180 - val_accuracy: 0.7829\n",
            "Epoch 48/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15059.5791 - accuracy: 0.8696 - val_loss: 28258.4648 - val_accuracy: 0.7987\n",
            "Epoch 49/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15144.0586 - accuracy: 0.8688 - val_loss: 28571.5859 - val_accuracy: 0.7629\n",
            "Epoch 50/1000\n",
            "178/181 [============================>.] - ETA: 0s - loss: 15047.2812 - accuracy: 0.8674roc-auc_val: 0.8161\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 15023.2754 - accuracy: 0.8674 - val_loss: 28249.7266 - val_accuracy: 0.7896\n",
            "Epoch 51/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14908.8828 - accuracy: 0.8728 - val_loss: 28466.1758 - val_accuracy: 0.7844\n",
            "Epoch 52/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14924.8066 - accuracy: 0.8708 - val_loss: 28234.1699 - val_accuracy: 0.7832\n",
            "Epoch 53/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14791.9453 - accuracy: 0.8723 - val_loss: 28595.0977 - val_accuracy: 0.7691\n",
            "Epoch 54/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14725.8887 - accuracy: 0.8695 - val_loss: 28383.6387 - val_accuracy: 0.7680\n",
            "Epoch 55/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14719.2725 - accuracy: 0.8714 - val_loss: 28409.6738 - val_accuracy: 0.7724\n",
            "Epoch 56/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14522.9893 - accuracy: 0.8715 - val_loss: 28521.8008 - val_accuracy: 0.7973\n",
            "Epoch 57/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14601.7666 - accuracy: 0.8722 - val_loss: 28670.5469 - val_accuracy: 0.7931\n",
            "Epoch 58/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14533.9541 - accuracy: 0.8718 - val_loss: 28535.2031 - val_accuracy: 0.7735\n",
            "Epoch 59/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14419.5049 - accuracy: 0.8727 - val_loss: 28585.2637 - val_accuracy: 0.7897\n",
            "Epoch 60/1000\n",
            "180/181 [============================>.] - ETA: 0s - loss: 14362.0195 - accuracy: 0.8749roc-auc_val: 0.8152\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 14329.7246 - accuracy: 0.8749 - val_loss: 28462.5000 - val_accuracy: 0.7949\n",
            "Epoch 61/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14353.6406 - accuracy: 0.8714 - val_loss: 28476.1445 - val_accuracy: 0.7864\n",
            "Epoch 62/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14226.9004 - accuracy: 0.8770 - val_loss: 28290.0176 - val_accuracy: 0.8044\n",
            "Epoch 63/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14337.6836 - accuracy: 0.8735 - val_loss: 28493.2246 - val_accuracy: 0.7864\n",
            "Epoch 64/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14178.3447 - accuracy: 0.8751 - val_loss: 28481.3848 - val_accuracy: 0.7900\n",
            "Epoch 65/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14280.8398 - accuracy: 0.8742 - val_loss: 28590.2715 - val_accuracy: 0.8009\n",
            "Epoch 66/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14214.3428 - accuracy: 0.8741 - val_loss: 28643.7227 - val_accuracy: 0.7862\n",
            "Epoch 67/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14062.6904 - accuracy: 0.8769 - val_loss: 28784.3555 - val_accuracy: 0.8009\n",
            "Epoch 68/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14091.7705 - accuracy: 0.8786 - val_loss: 28812.3066 - val_accuracy: 0.7969\n",
            "Epoch 69/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13896.0127 - accuracy: 0.8764 - val_loss: 28540.3730 - val_accuracy: 0.7971\n",
            "Epoch 70/1000\n",
            "177/181 [============================>.] - ETA: 0s - loss: 14156.6445 - accuracy: 0.8748roc-auc_val: 0.8158\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 14165.8838 - accuracy: 0.8748 - val_loss: 28545.8730 - val_accuracy: 0.7911\n",
            "Epoch 71/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13911.5254 - accuracy: 0.8807 - val_loss: 28815.6602 - val_accuracy: 0.7931\n",
            "Epoch 72/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 13965.7021 - accuracy: 0.8742 - val_loss: 28674.5254 - val_accuracy: 0.8043\n",
            "Epoch 73/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 14004.1504 - accuracy: 0.8747 - val_loss: 28531.7168 - val_accuracy: 0.7947\n",
            "Epoch 74/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 13639.1084 - accuracy: 0.8805 - val_loss: 28790.8145 - val_accuracy: 0.7979\n",
            "Epoch 75/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13819.9785 - accuracy: 0.8774 - val_loss: 29029.9961 - val_accuracy: 0.7841\n",
            "Epoch 76/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13732.1826 - accuracy: 0.8752 - val_loss: 28878.5449 - val_accuracy: 0.7841\n",
            "Epoch 77/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13835.4160 - accuracy: 0.8771 - val_loss: 28722.3438 - val_accuracy: 0.7985\n",
            "Epoch 78/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13674.5010 - accuracy: 0.8805 - val_loss: 28843.7305 - val_accuracy: 0.8015\n",
            "Epoch 79/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13542.7666 - accuracy: 0.8774 - val_loss: 28792.4922 - val_accuracy: 0.7931\n",
            "Epoch 80/1000\n",
            "179/181 [============================>.] - ETA: 0s - loss: 13693.2100 - accuracy: 0.8788roc-auc_val: 0.8136\n",
            "181/181 [==============================] - 9s 47ms/step - loss: 13662.5254 - accuracy: 0.8788 - val_loss: 28752.5957 - val_accuracy: 0.7970\n",
            "Epoch 81/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13829.9795 - accuracy: 0.8743 - val_loss: 28470.8652 - val_accuracy: 0.7934\n",
            "Epoch 82/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13567.8877 - accuracy: 0.8779 - val_loss: 28713.0234 - val_accuracy: 0.8047\n",
            "Epoch 83/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13682.7334 - accuracy: 0.8746 - val_loss: 28711.8867 - val_accuracy: 0.7912\n",
            "Epoch 84/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13566.3613 - accuracy: 0.8718 - val_loss: 28614.6836 - val_accuracy: 0.7633\n",
            "Epoch 85/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13571.8057 - accuracy: 0.8715 - val_loss: 28828.6934 - val_accuracy: 0.7780\n",
            "Epoch 86/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13379.9980 - accuracy: 0.8738 - val_loss: 28664.0527 - val_accuracy: 0.7802\n",
            "Epoch 87/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13680.8916 - accuracy: 0.8792 - val_loss: 28782.4785 - val_accuracy: 0.7817\n",
            "Epoch 88/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13309.3408 - accuracy: 0.8801 - val_loss: 28826.1387 - val_accuracy: 0.7978\n",
            "Epoch 89/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13448.6582 - accuracy: 0.8829 - val_loss: 28471.2285 - val_accuracy: 0.7784\n",
            "Epoch 90/1000\n",
            "178/181 [============================>.] - ETA: 0s - loss: 13382.4453 - accuracy: 0.8785roc-auc_val: 0.8132\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 13363.4990 - accuracy: 0.8786 - val_loss: 28884.7695 - val_accuracy: 0.7915\n",
            "Epoch 91/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13253.5566 - accuracy: 0.8780 - val_loss: 28856.0977 - val_accuracy: 0.7961\n",
            "Epoch 92/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13267.2119 - accuracy: 0.8834 - val_loss: 28743.8496 - val_accuracy: 0.8006\n",
            "Epoch 93/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13401.5342 - accuracy: 0.8808 - val_loss: 28596.7695 - val_accuracy: 0.7844\n",
            "Epoch 94/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13290.1455 - accuracy: 0.8641 - val_loss: 28720.3242 - val_accuracy: 0.7718\n",
            "Epoch 95/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13251.0586 - accuracy: 0.8779 - val_loss: 28680.8672 - val_accuracy: 0.7714\n",
            "Epoch 96/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13253.7109 - accuracy: 0.8805 - val_loss: 28611.8301 - val_accuracy: 0.7767\n",
            "Epoch 97/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13278.5352 - accuracy: 0.8789 - val_loss: 28779.4473 - val_accuracy: 0.7785\n",
            "Epoch 98/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13235.0605 - accuracy: 0.8801 - val_loss: 28779.3203 - val_accuracy: 0.7901\n",
            "Epoch 99/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13116.8584 - accuracy: 0.8785 - val_loss: 28676.1172 - val_accuracy: 0.7830\n",
            "Epoch 100/1000\n",
            "180/181 [============================>.] - ETA: 0s - loss: 13394.6572 - accuracy: 0.8794roc-auc_val: 0.8156\n",
            "181/181 [==============================] - 9s 47ms/step - loss: 13368.5000 - accuracy: 0.8794 - val_loss: 28571.3828 - val_accuracy: 0.7911\n",
            "Epoch 101/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13163.7109 - accuracy: 0.8817 - val_loss: 28652.3203 - val_accuracy: 0.7856\n",
            "Epoch 102/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13098.4395 - accuracy: 0.8836 - val_loss: 28524.6562 - val_accuracy: 0.7991\n",
            "Epoch 1/1000\n",
            "136/136 [==============================] - 2s 14ms/step - loss: 38219.2773 - accuracy: 0.6906 - val_loss: 35173.1445 - val_accuracy: 0.6707\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 31693.6367 - accuracy: 0.7432 - val_loss: 33695.6602 - val_accuracy: 0.6905\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 29302.4258 - accuracy: 0.7590 - val_loss: 33269.8711 - val_accuracy: 0.7023\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 27638.1387 - accuracy: 0.7679 - val_loss: 32261.0156 - val_accuracy: 0.7268\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 26245.8984 - accuracy: 0.7823 - val_loss: 32500.3691 - val_accuracy: 0.7026\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 25469.9473 - accuracy: 0.7881 - val_loss: 33019.8281 - val_accuracy: 0.7268\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 24586.5293 - accuracy: 0.7955 - val_loss: 33456.3203 - val_accuracy: 0.7479\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 23668.3789 - accuracy: 0.8032 - val_loss: 33826.6250 - val_accuracy: 0.8047\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 22932.1738 - accuracy: 0.8063 - val_loss: 32016.0762 - val_accuracy: 0.7671\n",
            "Epoch 10/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 22500.7461 - accuracy: 0.8112roc-auc_val: 0.808\n",
            "136/136 [==============================] - 12s 85ms/step - loss: 22365.8789 - accuracy: 0.8115 - val_loss: 32498.8574 - val_accuracy: 0.8177\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 21423.5684 - accuracy: 0.8204 - val_loss: 31352.0859 - val_accuracy: 0.8134\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 20727.0117 - accuracy: 0.8233 - val_loss: 31531.0117 - val_accuracy: 0.8110\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 20065.9766 - accuracy: 0.8310 - val_loss: 31716.4121 - val_accuracy: 0.8183\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 19733.1465 - accuracy: 0.8350 - val_loss: 31349.7168 - val_accuracy: 0.8197\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 19106.7949 - accuracy: 0.8409 - val_loss: 32784.2656 - val_accuracy: 0.8365\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 18890.2559 - accuracy: 0.8413 - val_loss: 31640.8477 - val_accuracy: 0.8138\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 18465.5273 - accuracy: 0.8443 - val_loss: 32612.4355 - val_accuracy: 0.8542\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17921.7285 - accuracy: 0.8475 - val_loss: 32359.4570 - val_accuracy: 0.8473\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17945.2656 - accuracy: 0.8497 - val_loss: 31987.3262 - val_accuracy: 0.8517\n",
            "Epoch 20/1000\n",
            "130/136 [===========================>..] - ETA: 0s - loss: 17471.1777 - accuracy: 0.8521roc-auc_val: 0.8071\n",
            "136/136 [==============================] - 12s 85ms/step - loss: 17428.7676 - accuracy: 0.8520 - val_loss: 32896.8477 - val_accuracy: 0.8441\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 17154.4121 - accuracy: 0.8517 - val_loss: 32441.4492 - val_accuracy: 0.8365\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 16955.2422 - accuracy: 0.8581 - val_loss: 33096.9453 - val_accuracy: 0.8621\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 16922.5488 - accuracy: 0.8577 - val_loss: 33101.5664 - val_accuracy: 0.8454\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 16524.6719 - accuracy: 0.8588 - val_loss: 32109.7090 - val_accuracy: 0.8530\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 16486.0781 - accuracy: 0.8627 - val_loss: 32364.4980 - val_accuracy: 0.8480\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 16278.5615 - accuracy: 0.8643 - val_loss: 32514.2285 - val_accuracy: 0.8554\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 16076.1934 - accuracy: 0.8636 - val_loss: 32245.8008 - val_accuracy: 0.8438\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 15734.2021 - accuracy: 0.8661 - val_loss: 32386.9727 - val_accuracy: 0.8592\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 15740.1582 - accuracy: 0.8691 - val_loss: 32648.3398 - val_accuracy: 0.8199\n",
            "Epoch 30/1000\n",
            "136/136 [==============================] - ETA: 0s - loss: 15550.0176 - accuracy: 0.8645roc-auc_val: 0.8096\n",
            "136/136 [==============================] - 11s 84ms/step - loss: 15550.0176 - accuracy: 0.8645 - val_loss: 32160.8242 - val_accuracy: 0.8420\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 15333.3389 - accuracy: 0.8694 - val_loss: 32389.6484 - val_accuracy: 0.8535\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 15333.5371 - accuracy: 0.8672 - val_loss: 32787.3203 - val_accuracy: 0.8716\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 15085.1152 - accuracy: 0.8703 - val_loss: 32999.4961 - val_accuracy: 0.8506\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 15112.6533 - accuracy: 0.8694 - val_loss: 32955.2461 - val_accuracy: 0.8622\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14936.2236 - accuracy: 0.8699 - val_loss: 33168.4883 - val_accuracy: 0.8547\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14684.0020 - accuracy: 0.8742 - val_loss: 33123.4141 - val_accuracy: 0.8698\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14644.2598 - accuracy: 0.8713 - val_loss: 32870.5781 - val_accuracy: 0.8313\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14672.2266 - accuracy: 0.8720 - val_loss: 33098.9180 - val_accuracy: 0.8421\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14424.4121 - accuracy: 0.8759 - val_loss: 33287.6406 - val_accuracy: 0.8427\n",
            "Epoch 40/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 14681.7773 - accuracy: 0.8718roc-auc_val: 0.8064\n",
            "136/136 [==============================] - 11s 84ms/step - loss: 14615.1436 - accuracy: 0.8720 - val_loss: 33267.2383 - val_accuracy: 0.8646\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14315.4248 - accuracy: 0.8740 - val_loss: 33310.1641 - val_accuracy: 0.8586\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14264.6123 - accuracy: 0.8719 - val_loss: 33118.0664 - val_accuracy: 0.8624\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14328.5371 - accuracy: 0.8776 - val_loss: 33537.1680 - val_accuracy: 0.8643\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13991.0205 - accuracy: 0.8779 - val_loss: 33941.7461 - val_accuracy: 0.8780\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14210.7803 - accuracy: 0.8760 - val_loss: 33536.1172 - val_accuracy: 0.8353\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13866.2822 - accuracy: 0.8704 - val_loss: 33971.2383 - val_accuracy: 0.8572\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13962.1074 - accuracy: 0.8779 - val_loss: 34151.4883 - val_accuracy: 0.8688\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13771.0400 - accuracy: 0.8791 - val_loss: 33313.7773 - val_accuracy: 0.8391\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13709.3135 - accuracy: 0.8795 - val_loss: 34101.0430 - val_accuracy: 0.8576\n",
            "Epoch 50/1000\n",
            "134/136 [============================>.] - ETA: 0s - loss: 13706.0469 - accuracy: 0.8796roc-auc_val: 0.8035\n",
            "136/136 [==============================] - 11s 84ms/step - loss: 13671.3721 - accuracy: 0.8796 - val_loss: 33248.8633 - val_accuracy: 0.8602\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13389.4492 - accuracy: 0.8825 - val_loss: 34011.1328 - val_accuracy: 0.8577\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13506.6650 - accuracy: 0.8766 - val_loss: 33763.6328 - val_accuracy: 0.8462\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13483.3389 - accuracy: 0.8834 - val_loss: 34127.5312 - val_accuracy: 0.8741\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13381.6670 - accuracy: 0.8808 - val_loss: 34098.6602 - val_accuracy: 0.8687\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13315.3252 - accuracy: 0.8814 - val_loss: 33843.5039 - val_accuracy: 0.8727\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13257.1553 - accuracy: 0.8830 - val_loss: 33928.8320 - val_accuracy: 0.8401\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13126.5068 - accuracy: 0.8842 - val_loss: 34087.3242 - val_accuracy: 0.8625\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13152.8330 - accuracy: 0.8801 - val_loss: 33377.0000 - val_accuracy: 0.8172\n",
            "Epoch 59/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13282.3291 - accuracy: 0.8804 - val_loss: 33469.7852 - val_accuracy: 0.8443\n",
            "Epoch 60/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 13282.9365 - accuracy: 0.8834roc-auc_val: 0.8003\n",
            "136/136 [==============================] - 11s 84ms/step - loss: 13271.6396 - accuracy: 0.8834 - val_loss: 33435.7344 - val_accuracy: 0.8479\n",
            "Epoch 61/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13157.1816 - accuracy: 0.8837 - val_loss: 33609.5469 - val_accuracy: 0.8267\n",
            "Epoch 62/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13063.8330 - accuracy: 0.8820 - val_loss: 33915.9023 - val_accuracy: 0.8511\n",
            "Epoch 63/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 12867.1973 - accuracy: 0.8847 - val_loss: 34287.6367 - val_accuracy: 0.8551\n",
            "Epoch 64/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 12822.8262 - accuracy: 0.8834 - val_loss: 34423.1875 - val_accuracy: 0.8454\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 22ms/step - loss: 37515.3672 - accuracy: 0.6865 - val_loss: 37241.3086 - val_accuracy: 0.6946\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 30903.0938 - accuracy: 0.7352 - val_loss: 35894.7148 - val_accuracy: 0.7243\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 28411.5879 - accuracy: 0.7504 - val_loss: 35730.5586 - val_accuracy: 0.7547\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 26781.1914 - accuracy: 0.7599 - val_loss: 35342.3633 - val_accuracy: 0.7580\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 25144.6152 - accuracy: 0.7712 - val_loss: 35263.9883 - val_accuracy: 0.7309\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 24532.3066 - accuracy: 0.7886 - val_loss: 34537.5586 - val_accuracy: 0.7632\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 23470.7559 - accuracy: 0.7917 - val_loss: 33884.7344 - val_accuracy: 0.7585\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 22738.6016 - accuracy: 0.7922 - val_loss: 33553.5312 - val_accuracy: 0.7651\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 22047.1738 - accuracy: 0.8044 - val_loss: 34721.4570 - val_accuracy: 0.8089\n",
            "Epoch 10/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 21323.9414 - accuracy: 0.8082roc-auc_val: 0.7995\n",
            "88/88 [==============================] - 14s 164ms/step - loss: 21375.2891 - accuracy: 0.8073 - val_loss: 34581.7461 - val_accuracy: 0.8009\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 20926.9941 - accuracy: 0.8153 - val_loss: 35038.3242 - val_accuracy: 0.8067\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 20092.2227 - accuracy: 0.8171 - val_loss: 35772.9297 - val_accuracy: 0.8227\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 19623.1348 - accuracy: 0.8287 - val_loss: 36247.1719 - val_accuracy: 0.8421\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 19194.4902 - accuracy: 0.8230 - val_loss: 36446.2305 - val_accuracy: 0.8482\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 18670.8848 - accuracy: 0.8277 - val_loss: 36510.3398 - val_accuracy: 0.8487\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 18144.5078 - accuracy: 0.8301 - val_loss: 35074.2227 - val_accuracy: 0.8510\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 17739.4141 - accuracy: 0.8329 - val_loss: 36004.7188 - val_accuracy: 0.8790\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 17033.1543 - accuracy: 0.8401 - val_loss: 35131.2500 - val_accuracy: 0.8562\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 16886.1016 - accuracy: 0.8404 - val_loss: 35368.1445 - val_accuracy: 0.8492\n",
            "Epoch 20/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 16537.8516 - accuracy: 0.8459roc-auc_val: 0.8046\n",
            "88/88 [==============================] - 14s 164ms/step - loss: 16456.3027 - accuracy: 0.8465 - val_loss: 36296.7227 - val_accuracy: 0.8821\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 16114.9307 - accuracy: 0.8496 - val_loss: 35674.5117 - val_accuracy: 0.8710\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 15618.4883 - accuracy: 0.8514 - val_loss: 36204.5156 - val_accuracy: 0.8914\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 15263.7070 - accuracy: 0.8508 - val_loss: 36031.1328 - val_accuracy: 0.8861\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 15415.6973 - accuracy: 0.8580 - val_loss: 35820.6367 - val_accuracy: 0.8855\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 14762.1455 - accuracy: 0.8586 - val_loss: 36652.7227 - val_accuracy: 0.9012\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 14727.5088 - accuracy: 0.8599 - val_loss: 36506.3164 - val_accuracy: 0.9056\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 14191.2969 - accuracy: 0.8634 - val_loss: 36942.0820 - val_accuracy: 0.9154\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 14299.9170 - accuracy: 0.8661 - val_loss: 35550.2852 - val_accuracy: 0.8952\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 14096.2412 - accuracy: 0.8682 - val_loss: 37034.5234 - val_accuracy: 0.9158\n",
            "Epoch 30/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 13776.3701 - accuracy: 0.8672roc-auc_val: 0.8077\n",
            "88/88 [==============================] - 15s 165ms/step - loss: 13672.1680 - accuracy: 0.8674 - val_loss: 36547.7539 - val_accuracy: 0.9073\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13742.8574 - accuracy: 0.8704 - val_loss: 36789.5625 - val_accuracy: 0.9097\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13460.1621 - accuracy: 0.8696 - val_loss: 36533.5117 - val_accuracy: 0.9026\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13137.8877 - accuracy: 0.8749 - val_loss: 37331.3867 - val_accuracy: 0.9103\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13198.7012 - accuracy: 0.8778 - val_loss: 37226.2812 - val_accuracy: 0.9124\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12980.4404 - accuracy: 0.8722 - val_loss: 36588.3438 - val_accuracy: 0.9086\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12803.5059 - accuracy: 0.8829 - val_loss: 36973.4883 - val_accuracy: 0.9132\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12546.4941 - accuracy: 0.8803 - val_loss: 37051.0977 - val_accuracy: 0.9155\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12720.4092 - accuracy: 0.8785 - val_loss: 37384.7383 - val_accuracy: 0.9143\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12290.3076 - accuracy: 0.8856 - val_loss: 37653.2969 - val_accuracy: 0.9204\n",
            "Epoch 40/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 12175.4863 - accuracy: 0.8842roc-auc_val: 0.806\n",
            "88/88 [==============================] - 14s 165ms/step - loss: 12175.5537 - accuracy: 0.8840 - val_loss: 37945.0391 - val_accuracy: 0.9232\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12138.2812 - accuracy: 0.8860 - val_loss: 37717.2383 - val_accuracy: 0.9214\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12064.0234 - accuracy: 0.8894 - val_loss: 36620.9727 - val_accuracy: 0.9069\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11868.0986 - accuracy: 0.8851 - val_loss: 37740.8867 - val_accuracy: 0.9209\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11782.8330 - accuracy: 0.8886 - val_loss: 38120.6484 - val_accuracy: 0.9224\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11779.8760 - accuracy: 0.8874 - val_loss: 37725.1367 - val_accuracy: 0.9175\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11460.7959 - accuracy: 0.8888 - val_loss: 37486.1914 - val_accuracy: 0.9169\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11550.1738 - accuracy: 0.8877 - val_loss: 37106.5508 - val_accuracy: 0.9108\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11794.2617 - accuracy: 0.8858 - val_loss: 36984.0234 - val_accuracy: 0.9175\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11380.8398 - accuracy: 0.8931 - val_loss: 37594.5312 - val_accuracy: 0.9153\n",
            "Epoch 50/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 11181.3467 - accuracy: 0.8884roc-auc_val: 0.8048\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 11186.9932 - accuracy: 0.8883 - val_loss: 37097.9766 - val_accuracy: 0.9091\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11406.3350 - accuracy: 0.8954 - val_loss: 37506.6406 - val_accuracy: 0.9207\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11078.5625 - accuracy: 0.8965 - val_loss: 37831.3320 - val_accuracy: 0.9157\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11086.6465 - accuracy: 0.8922 - val_loss: 37704.8242 - val_accuracy: 0.9164\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 10931.2256 - accuracy: 0.8933 - val_loss: 38129.2266 - val_accuracy: 0.9215\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11027.2578 - accuracy: 0.8932 - val_loss: 38820.8047 - val_accuracy: 0.9239\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 10780.3740 - accuracy: 0.8902 - val_loss: 37558.5039 - val_accuracy: 0.9113\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 10782.4404 - accuracy: 0.8928 - val_loss: 38411.3516 - val_accuracy: 0.9219\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 10843.0459 - accuracy: 0.8907 - val_loss: 38624.1523 - val_accuracy: 0.9248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5PePWLxWIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adede34b-1aa3-4e5b-9ab6-1548122fe22b"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNDmjro-2ndD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
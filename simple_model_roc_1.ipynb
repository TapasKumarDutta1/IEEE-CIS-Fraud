{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_1",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "894813cf-931d-4133-e101-21e9882ce3cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "c8d41742-a8f6-495d-fbdf-4ff521047d14"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 94% 49.0M/52.2M [00:00<00:00, 45.4MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 71.5MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 106MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 161MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 84% 49.0M/58.3M [00:00<00:00, 59.6MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 60.2MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 108MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv80v8W_Ko2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "7bba2fcf-e0a2-43e5-b726-fd03829d4e5b"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jt2pcxPije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "575d7cb3-35de-4372-dc1e-6e1690e857ed"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvEaxp9jhbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "229b64c2-fd90-4fc9-bc23-0bec0b915304"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2a311d73-a5ea-49e2-9505-51bbd7ae0210"
      },
      "source": [
        "\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkLEBcl6LHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 0.1\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRW4vyPJtU5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a47ce82-e32a-4867-ae80-c3d37c938a43"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "# trn=trn.drop(['isFraud_isna'],1)\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Adam(0.001,decay=1e-3),loss=rac,metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 58754.3750 - accuracy: 0.6827 - val_loss: 49285.6406 - val_accuracy: 0.5133\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 54070.9727 - accuracy: 0.7008 - val_loss: 44854.0938 - val_accuracy: 0.5326\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 51344.1211 - accuracy: 0.7210 - val_loss: 42723.7695 - val_accuracy: 0.5640\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 49413.9727 - accuracy: 0.7311 - val_loss: 45034.3242 - val_accuracy: 0.5273\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 48767.2227 - accuracy: 0.7334 - val_loss: 45801.1758 - val_accuracy: 0.5332\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 48048.1133 - accuracy: 0.7297 - val_loss: 46183.9219 - val_accuracy: 0.5298\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 46952.4453 - accuracy: 0.7359 - val_loss: 42045.7070 - val_accuracy: 0.5826\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 46368.3945 - accuracy: 0.7454 - val_loss: 44177.0430 - val_accuracy: 0.5426\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 45511.0508 - accuracy: 0.7500 - val_loss: 42996.3359 - val_accuracy: 0.5687\n",
            "Epoch 10/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 45734.8711 - accuracy: 0.7418roc-auc_val: 0.728\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 45656.4219 - accuracy: 0.7415 - val_loss: 44882.2969 - val_accuracy: 0.5068\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 45154.1680 - accuracy: 0.7528 - val_loss: 44402.2500 - val_accuracy: 0.5231\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 44458.0703 - accuracy: 0.7418 - val_loss: 43306.1523 - val_accuracy: 0.5692\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 44011.1055 - accuracy: 0.7471 - val_loss: 43212.5312 - val_accuracy: 0.5361\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 44410.7695 - accuracy: 0.7389 - val_loss: 42045.3320 - val_accuracy: 0.5841\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 44053.6016 - accuracy: 0.7532 - val_loss: 42829.2266 - val_accuracy: 0.5471\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 43552.8555 - accuracy: 0.7501 - val_loss: 42594.0938 - val_accuracy: 0.5547\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 43262.9453 - accuracy: 0.7544 - val_loss: 40885.2305 - val_accuracy: 0.5845\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 42891.0547 - accuracy: 0.7473 - val_loss: 42181.8320 - val_accuracy: 0.5649\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 42845.6250 - accuracy: 0.7486 - val_loss: 42826.2773 - val_accuracy: 0.5662\n",
            "Epoch 20/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 43030.5234 - accuracy: 0.7558roc-auc_val: 0.7349\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 42910.1445 - accuracy: 0.7558 - val_loss: 43309.8750 - val_accuracy: 0.5307\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 42610.9453 - accuracy: 0.7488 - val_loss: 42620.7578 - val_accuracy: 0.5513\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 42262.4219 - accuracy: 0.7640 - val_loss: 42863.4336 - val_accuracy: 0.5856\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 42168.8672 - accuracy: 0.7626 - val_loss: 42084.7891 - val_accuracy: 0.5621\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 42223.4375 - accuracy: 0.7642 - val_loss: 41512.3086 - val_accuracy: 0.5639\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 41839.3281 - accuracy: 0.7594 - val_loss: 41511.4180 - val_accuracy: 0.5672\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 41665.9336 - accuracy: 0.7500 - val_loss: 41909.1172 - val_accuracy: 0.5502\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 41367.0625 - accuracy: 0.7558 - val_loss: 41415.9883 - val_accuracy: 0.5679\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 41029.5391 - accuracy: 0.7586 - val_loss: 39082.6484 - val_accuracy: 0.6211\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 41183.6953 - accuracy: 0.7704 - val_loss: 41008.1875 - val_accuracy: 0.5714\n",
            "Epoch 30/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 40702.4219 - accuracy: 0.7553roc-auc_val: 0.7538\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 40634.7383 - accuracy: 0.7552 - val_loss: 40505.8438 - val_accuracy: 0.5646\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 40738.2930 - accuracy: 0.7689 - val_loss: 40030.3242 - val_accuracy: 0.5898\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 40675.8945 - accuracy: 0.7691 - val_loss: 39438.4727 - val_accuracy: 0.5522\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 40755.4414 - accuracy: 0.7507 - val_loss: 39874.6641 - val_accuracy: 0.5513\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 40342.1172 - accuracy: 0.7650 - val_loss: 40238.7227 - val_accuracy: 0.5746\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 40305.5664 - accuracy: 0.7673 - val_loss: 40177.1562 - val_accuracy: 0.5898\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 40299.9219 - accuracy: 0.7724 - val_loss: 40451.4766 - val_accuracy: 0.5741\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 40128.3477 - accuracy: 0.7616 - val_loss: 40032.4648 - val_accuracy: 0.5822\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39906.2109 - accuracy: 0.7639 - val_loss: 40611.6250 - val_accuracy: 0.5788\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39603.6836 - accuracy: 0.7672 - val_loss: 40047.1680 - val_accuracy: 0.5924\n",
            "Epoch 40/1000\n",
            "218/225 [============================>.] - ETA: 0s - loss: 39877.0234 - accuracy: 0.7599roc-auc_val: 0.7549\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 39806.9453 - accuracy: 0.7599 - val_loss: 40419.0547 - val_accuracy: 0.5741\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39536.7656 - accuracy: 0.7670 - val_loss: 40137.7031 - val_accuracy: 0.6060\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39459.2461 - accuracy: 0.7728 - val_loss: 40209.2539 - val_accuracy: 0.5767\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39289.6328 - accuracy: 0.7722 - val_loss: 39207.5000 - val_accuracy: 0.6021\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39153.1367 - accuracy: 0.7622 - val_loss: 40750.2891 - val_accuracy: 0.5753\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39087.8672 - accuracy: 0.7668 - val_loss: 41118.6328 - val_accuracy: 0.5884\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39161.8242 - accuracy: 0.7706 - val_loss: 40335.9375 - val_accuracy: 0.5865\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38866.4023 - accuracy: 0.7638 - val_loss: 40078.7383 - val_accuracy: 0.5925\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38905.7539 - accuracy: 0.7673 - val_loss: 39811.9414 - val_accuracy: 0.5966\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38663.7500 - accuracy: 0.7668 - val_loss: 39962.7109 - val_accuracy: 0.5967\n",
            "Epoch 50/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 38398.4805 - accuracy: 0.7721roc-auc_val: 0.7602\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 38376.5078 - accuracy: 0.7723 - val_loss: 39552.8516 - val_accuracy: 0.5980\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38405.5664 - accuracy: 0.7790 - val_loss: 39682.8828 - val_accuracy: 0.5915\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38369.1250 - accuracy: 0.7689 - val_loss: 40221.1133 - val_accuracy: 0.5805\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38443.2383 - accuracy: 0.7777 - val_loss: 40225.0312 - val_accuracy: 0.5756\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38528.1641 - accuracy: 0.7679 - val_loss: 40062.0977 - val_accuracy: 0.5659\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38336.2969 - accuracy: 0.7621 - val_loss: 39848.0859 - val_accuracy: 0.5856\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38348.9297 - accuracy: 0.7662 - val_loss: 39840.7656 - val_accuracy: 0.5581\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37941.4609 - accuracy: 0.7662 - val_loss: 39678.7891 - val_accuracy: 0.5779\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38332.4023 - accuracy: 0.7859 - val_loss: 39237.9375 - val_accuracy: 0.5851\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38077.3789 - accuracy: 0.7709 - val_loss: 39446.2500 - val_accuracy: 0.5712\n",
            "Epoch 60/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 37883.2305 - accuracy: 0.7676roc-auc_val: 0.7673\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 37883.2305 - accuracy: 0.7676 - val_loss: 38397.8984 - val_accuracy: 0.6008\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38075.5938 - accuracy: 0.7763 - val_loss: 39078.7305 - val_accuracy: 0.5878\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37854.4805 - accuracy: 0.7690 - val_loss: 39361.3281 - val_accuracy: 0.5926\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38051.0078 - accuracy: 0.7747 - val_loss: 39056.8203 - val_accuracy: 0.5994\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 37512.5391 - accuracy: 0.7747 - val_loss: 39169.1953 - val_accuracy: 0.5810\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37548.3828 - accuracy: 0.7723 - val_loss: 38860.8438 - val_accuracy: 0.6053\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37796.5547 - accuracy: 0.7727 - val_loss: 38997.0195 - val_accuracy: 0.5749\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37421.2266 - accuracy: 0.7718 - val_loss: 38837.3711 - val_accuracy: 0.6000\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37565.5156 - accuracy: 0.7744 - val_loss: 38900.1328 - val_accuracy: 0.6033\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37629.4844 - accuracy: 0.7811 - val_loss: 38993.3281 - val_accuracy: 0.5697\n",
            "Epoch 70/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 37726.9922 - accuracy: 0.7606roc-auc_val: 0.7631\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 37667.6094 - accuracy: 0.7606 - val_loss: 38941.5977 - val_accuracy: 0.5827\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37527.6914 - accuracy: 0.7878 - val_loss: 39317.9336 - val_accuracy: 0.5725\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37377.9219 - accuracy: 0.7655 - val_loss: 38984.7656 - val_accuracy: 0.5722\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37461.0312 - accuracy: 0.7709 - val_loss: 38330.4219 - val_accuracy: 0.5890\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37318.5117 - accuracy: 0.7730 - val_loss: 39163.0078 - val_accuracy: 0.5900\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37268.3555 - accuracy: 0.7827 - val_loss: 38624.1055 - val_accuracy: 0.6070\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37094.7227 - accuracy: 0.7747 - val_loss: 38595.7891 - val_accuracy: 0.5914\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37092.7969 - accuracy: 0.7755 - val_loss: 38429.5469 - val_accuracy: 0.6034\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37199.9375 - accuracy: 0.7716 - val_loss: 38157.1172 - val_accuracy: 0.5915\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37235.8984 - accuracy: 0.7779 - val_loss: 38390.9844 - val_accuracy: 0.5888\n",
            "Epoch 80/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 37140.9766 - accuracy: 0.7703roc-auc_val: 0.7662\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 37054.6367 - accuracy: 0.7702 - val_loss: 38261.3594 - val_accuracy: 0.5956\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37087.1680 - accuracy: 0.7768 - val_loss: 38528.3320 - val_accuracy: 0.5967\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37019.8359 - accuracy: 0.7782 - val_loss: 38668.2812 - val_accuracy: 0.6122\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36912.8828 - accuracy: 0.7788 - val_loss: 38705.2109 - val_accuracy: 0.5883\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36702.6172 - accuracy: 0.7755 - val_loss: 38440.8711 - val_accuracy: 0.5931\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36783.4688 - accuracy: 0.7706 - val_loss: 38656.3906 - val_accuracy: 0.5742\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36745.9102 - accuracy: 0.7789 - val_loss: 38178.8398 - val_accuracy: 0.5958\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36887.9531 - accuracy: 0.7689 - val_loss: 38169.0977 - val_accuracy: 0.5848\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36803.7734 - accuracy: 0.7749 - val_loss: 38373.0273 - val_accuracy: 0.5897\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37003.9570 - accuracy: 0.7719 - val_loss: 38793.1211 - val_accuracy: 0.5949\n",
            "Epoch 90/1000\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 36832.1445 - accuracy: 0.7798roc-auc_val: 0.7641\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 36744.7695 - accuracy: 0.7797 - val_loss: 38552.3398 - val_accuracy: 0.6008\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36604.2773 - accuracy: 0.7728 - val_loss: 38127.5547 - val_accuracy: 0.5996\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36557.7852 - accuracy: 0.7827 - val_loss: 38220.1641 - val_accuracy: 0.5904\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36636.8867 - accuracy: 0.7786 - val_loss: 38177.2812 - val_accuracy: 0.5943\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36600.5195 - accuracy: 0.7761 - val_loss: 38336.3906 - val_accuracy: 0.5840\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36685.7617 - accuracy: 0.7714 - val_loss: 38277.7734 - val_accuracy: 0.5827\n",
            "Epoch 96/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36555.3477 - accuracy: 0.7672 - val_loss: 37784.9141 - val_accuracy: 0.5908\n",
            "Epoch 97/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36635.0430 - accuracy: 0.7794 - val_loss: 38371.2461 - val_accuracy: 0.5998\n",
            "Epoch 98/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36617.6250 - accuracy: 0.7721 - val_loss: 38502.5664 - val_accuracy: 0.5833\n",
            "Epoch 99/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36412.0703 - accuracy: 0.7757 - val_loss: 37902.4023 - val_accuracy: 0.5937\n",
            "Epoch 100/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 36693.1523 - accuracy: 0.7667roc-auc_val: 0.7673\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 36633.8516 - accuracy: 0.7668 - val_loss: 37986.1211 - val_accuracy: 0.5937\n",
            "Epoch 101/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36348.2227 - accuracy: 0.7757 - val_loss: 38534.6836 - val_accuracy: 0.6012\n",
            "Epoch 102/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36289.3789 - accuracy: 0.7767 - val_loss: 38585.0898 - val_accuracy: 0.5992\n",
            "Epoch 103/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36482.7461 - accuracy: 0.7770 - val_loss: 38771.0195 - val_accuracy: 0.5816\n",
            "Epoch 104/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36287.5117 - accuracy: 0.7762 - val_loss: 38781.2148 - val_accuracy: 0.5921\n",
            "Epoch 105/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36220.2305 - accuracy: 0.7721 - val_loss: 38501.5859 - val_accuracy: 0.5932\n",
            "Epoch 106/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36294.7500 - accuracy: 0.7780 - val_loss: 38547.8828 - val_accuracy: 0.5903\n",
            "Epoch 107/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36095.6289 - accuracy: 0.7758 - val_loss: 37989.3281 - val_accuracy: 0.5972\n",
            "Epoch 108/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36316.6953 - accuracy: 0.7830 - val_loss: 38056.6719 - val_accuracy: 0.5984\n",
            "Epoch 109/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36052.8633 - accuracy: 0.7841 - val_loss: 38256.5977 - val_accuracy: 0.6050\n",
            "Epoch 110/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 36174.5000 - accuracy: 0.7849roc-auc_val: 0.7661\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 36166.4023 - accuracy: 0.7848 - val_loss: 38200.4688 - val_accuracy: 0.5943\n",
            "Epoch 111/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36195.0703 - accuracy: 0.7804 - val_loss: 38335.1406 - val_accuracy: 0.5944\n",
            "Epoch 112/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36087.9727 - accuracy: 0.7841 - val_loss: 38400.2070 - val_accuracy: 0.6027\n",
            "Epoch 113/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36153.4961 - accuracy: 0.7804 - val_loss: 37513.5625 - val_accuracy: 0.5965\n",
            "Epoch 114/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35932.6055 - accuracy: 0.7798 - val_loss: 37954.9297 - val_accuracy: 0.5886\n",
            "Epoch 115/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35880.6914 - accuracy: 0.7794 - val_loss: 38322.3633 - val_accuracy: 0.5847\n",
            "Epoch 116/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35936.2188 - accuracy: 0.7821 - val_loss: 38421.5742 - val_accuracy: 0.5826\n",
            "Epoch 117/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35983.3477 - accuracy: 0.7782 - val_loss: 38321.5156 - val_accuracy: 0.5846\n",
            "Epoch 118/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36053.3633 - accuracy: 0.7787 - val_loss: 37839.2578 - val_accuracy: 0.5949\n",
            "Epoch 119/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36072.6016 - accuracy: 0.7891 - val_loss: 38094.0898 - val_accuracy: 0.5936\n",
            "Epoch 120/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 35844.1523 - accuracy: 0.7775roc-auc_val: 0.7653\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 35798.2227 - accuracy: 0.7774 - val_loss: 38213.5078 - val_accuracy: 0.5833\n",
            "Epoch 121/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36011.6055 - accuracy: 0.7766 - val_loss: 38550.0547 - val_accuracy: 0.5856\n",
            "Epoch 122/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36047.2227 - accuracy: 0.7742 - val_loss: 38574.9258 - val_accuracy: 0.5791\n",
            "Epoch 123/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35847.9297 - accuracy: 0.7718 - val_loss: 38334.1133 - val_accuracy: 0.5745\n",
            "Epoch 124/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35905.6445 - accuracy: 0.7706 - val_loss: 38479.4375 - val_accuracy: 0.5728\n",
            "Epoch 125/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35923.6172 - accuracy: 0.7754 - val_loss: 38695.6523 - val_accuracy: 0.5694\n",
            "Epoch 126/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35727.5117 - accuracy: 0.7755 - val_loss: 38204.8281 - val_accuracy: 0.5733\n",
            "Epoch 127/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35683.2188 - accuracy: 0.7724 - val_loss: 37993.0820 - val_accuracy: 0.5722\n",
            "Epoch 128/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35929.0234 - accuracy: 0.7738 - val_loss: 37483.7812 - val_accuracy: 0.5712\n",
            "Epoch 129/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35624.3516 - accuracy: 0.7718 - val_loss: 37986.4453 - val_accuracy: 0.5491\n",
            "Epoch 130/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 35765.6992 - accuracy: 0.7697roc-auc_val: 0.7707\n",
            "225/225 [==============================] - 5s 20ms/step - loss: 35657.7578 - accuracy: 0.7698 - val_loss: 37533.4297 - val_accuracy: 0.5622\n",
            "Epoch 131/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35663.5508 - accuracy: 0.7790 - val_loss: 37647.2969 - val_accuracy: 0.5794\n",
            "Epoch 132/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35729.9766 - accuracy: 0.7859 - val_loss: 37582.9023 - val_accuracy: 0.5804\n",
            "Epoch 133/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35641.1758 - accuracy: 0.7842 - val_loss: 37681.5938 - val_accuracy: 0.5679\n",
            "Epoch 134/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35565.2422 - accuracy: 0.7781 - val_loss: 37694.2266 - val_accuracy: 0.5711\n",
            "Epoch 135/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35515.8320 - accuracy: 0.7788 - val_loss: 37811.3945 - val_accuracy: 0.5662\n",
            "Epoch 136/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35581.0039 - accuracy: 0.7771 - val_loss: 37771.1289 - val_accuracy: 0.5703\n",
            "Epoch 137/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35679.6367 - accuracy: 0.7769 - val_loss: 37698.1562 - val_accuracy: 0.5725\n",
            "Epoch 138/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35428.0234 - accuracy: 0.7755 - val_loss: 37870.3633 - val_accuracy: 0.5721\n",
            "Epoch 139/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35399.8047 - accuracy: 0.7821 - val_loss: 37943.6250 - val_accuracy: 0.5714\n",
            "Epoch 140/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 35454.1562 - accuracy: 0.7789roc-auc_val: 0.7676\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 35280.7812 - accuracy: 0.7792 - val_loss: 37929.7852 - val_accuracy: 0.5872\n",
            "Epoch 141/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35269.4922 - accuracy: 0.7849 - val_loss: 37771.0898 - val_accuracy: 0.5818\n",
            "Epoch 142/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35277.9844 - accuracy: 0.7761 - val_loss: 38114.0039 - val_accuracy: 0.5754\n",
            "Epoch 143/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35362.0156 - accuracy: 0.7749 - val_loss: 38118.1562 - val_accuracy: 0.5620\n",
            "Epoch 144/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35498.4766 - accuracy: 0.7684 - val_loss: 38246.8359 - val_accuracy: 0.5640\n",
            "Epoch 145/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35367.8203 - accuracy: 0.7723 - val_loss: 38160.6719 - val_accuracy: 0.5618\n",
            "Epoch 146/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35255.4922 - accuracy: 0.7696 - val_loss: 38207.8906 - val_accuracy: 0.5620\n",
            "Epoch 147/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35187.3945 - accuracy: 0.7687 - val_loss: 37995.1836 - val_accuracy: 0.5613\n",
            "Epoch 148/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35230.3477 - accuracy: 0.7710 - val_loss: 38291.3750 - val_accuracy: 0.5673\n",
            "Epoch 149/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35285.8555 - accuracy: 0.7804 - val_loss: 38274.2109 - val_accuracy: 0.5755\n",
            "Epoch 150/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 35257.5898 - accuracy: 0.7784roc-auc_val: 0.7658\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 35205.9258 - accuracy: 0.7785 - val_loss: 38280.0977 - val_accuracy: 0.5709\n",
            "Epoch 151/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35172.2031 - accuracy: 0.7780 - val_loss: 38632.6562 - val_accuracy: 0.5601\n",
            "Epoch 152/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35153.0820 - accuracy: 0.7729 - val_loss: 38397.6094 - val_accuracy: 0.5646\n",
            "Epoch 153/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35091.3438 - accuracy: 0.7783 - val_loss: 38301.4180 - val_accuracy: 0.5731\n",
            "Epoch 154/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35101.8125 - accuracy: 0.7767 - val_loss: 38360.8086 - val_accuracy: 0.5642\n",
            "Epoch 155/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34983.5742 - accuracy: 0.7775 - val_loss: 38246.8203 - val_accuracy: 0.5734\n",
            "Epoch 156/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35079.7539 - accuracy: 0.7798 - val_loss: 38054.8477 - val_accuracy: 0.5730\n",
            "Epoch 157/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35039.2344 - accuracy: 0.7780 - val_loss: 37803.2695 - val_accuracy: 0.5738\n",
            "Epoch 158/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34938.6211 - accuracy: 0.7805 - val_loss: 37583.8672 - val_accuracy: 0.5809\n",
            "Epoch 159/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34909.3594 - accuracy: 0.7802 - val_loss: 37608.1367 - val_accuracy: 0.5771\n",
            "Epoch 160/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 34969.2773 - accuracy: 0.7852roc-auc_val: 0.7695\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 34801.0039 - accuracy: 0.7852 - val_loss: 37614.0234 - val_accuracy: 0.5818\n",
            "Epoch 161/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34679.4219 - accuracy: 0.7806 - val_loss: 37789.1367 - val_accuracy: 0.5809\n",
            "Epoch 162/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34821.5195 - accuracy: 0.7825 - val_loss: 37130.7188 - val_accuracy: 0.5907\n",
            "Epoch 163/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34707.5898 - accuracy: 0.7867 - val_loss: 37286.9961 - val_accuracy: 0.5842\n",
            "Epoch 164/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34833.6797 - accuracy: 0.7827 - val_loss: 37416.2617 - val_accuracy: 0.5815\n",
            "Epoch 165/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 35082.0938 - accuracy: 0.7881 - val_loss: 37461.3633 - val_accuracy: 0.5850\n",
            "Epoch 166/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34918.7383 - accuracy: 0.7861 - val_loss: 37449.9688 - val_accuracy: 0.5794\n",
            "Epoch 167/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34754.8047 - accuracy: 0.7812 - val_loss: 37172.2812 - val_accuracy: 0.5738\n",
            "Epoch 168/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34661.3047 - accuracy: 0.7772 - val_loss: 37627.3047 - val_accuracy: 0.5713\n",
            "Epoch 169/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34869.8828 - accuracy: 0.7895 - val_loss: 37577.7344 - val_accuracy: 0.5815\n",
            "Epoch 170/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 35019.1484 - accuracy: 0.7860roc-auc_val: 0.7723\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 34916.9297 - accuracy: 0.7859 - val_loss: 37298.1328 - val_accuracy: 0.5790\n",
            "Epoch 171/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34603.1172 - accuracy: 0.7805 - val_loss: 37321.7578 - val_accuracy: 0.5712\n",
            "Epoch 172/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34709.1445 - accuracy: 0.7777 - val_loss: 37447.4102 - val_accuracy: 0.5641\n",
            "Epoch 173/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34468.3359 - accuracy: 0.7764 - val_loss: 37211.2422 - val_accuracy: 0.5706\n",
            "Epoch 174/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34693.0820 - accuracy: 0.7752 - val_loss: 37430.0234 - val_accuracy: 0.5655\n",
            "Epoch 175/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34626.9102 - accuracy: 0.7748 - val_loss: 37549.0352 - val_accuracy: 0.5643\n",
            "Epoch 176/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34575.4805 - accuracy: 0.7756 - val_loss: 37541.8555 - val_accuracy: 0.5669\n",
            "Epoch 177/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34605.2148 - accuracy: 0.7798 - val_loss: 37391.9492 - val_accuracy: 0.5685\n",
            "Epoch 178/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34498.5742 - accuracy: 0.7785 - val_loss: 37300.0898 - val_accuracy: 0.5715\n",
            "Epoch 179/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34569.7305 - accuracy: 0.7752 - val_loss: 37350.4531 - val_accuracy: 0.5703\n",
            "Epoch 180/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 34446.3125 - accuracy: 0.7773roc-auc_val: 0.7741\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 34391.7383 - accuracy: 0.7774 - val_loss: 37198.4688 - val_accuracy: 0.5793\n",
            "Epoch 181/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34662.1250 - accuracy: 0.7852 - val_loss: 36892.5117 - val_accuracy: 0.5803\n",
            "Epoch 182/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34445.6328 - accuracy: 0.7820 - val_loss: 36599.9727 - val_accuracy: 0.5773\n",
            "Epoch 183/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34311.1445 - accuracy: 0.7807 - val_loss: 36707.4883 - val_accuracy: 0.5730\n",
            "Epoch 184/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34600.8047 - accuracy: 0.7788 - val_loss: 36479.5039 - val_accuracy: 0.5779\n",
            "Epoch 185/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34459.9883 - accuracy: 0.7800 - val_loss: 36654.6836 - val_accuracy: 0.5663\n",
            "Epoch 186/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34714.4570 - accuracy: 0.7753 - val_loss: 36833.9375 - val_accuracy: 0.5656\n",
            "Epoch 187/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34382.4609 - accuracy: 0.7798 - val_loss: 37117.5000 - val_accuracy: 0.5614\n",
            "Epoch 188/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34549.2031 - accuracy: 0.7787 - val_loss: 37099.2461 - val_accuracy: 0.5622\n",
            "Epoch 189/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34352.3281 - accuracy: 0.7786 - val_loss: 37297.0273 - val_accuracy: 0.5616\n",
            "Epoch 190/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 34435.5469 - accuracy: 0.7831roc-auc_val: 0.7739\n",
            "225/225 [==============================] - 5s 23ms/step - loss: 34386.6602 - accuracy: 0.7832 - val_loss: 37203.2188 - val_accuracy: 0.5690\n",
            "Epoch 191/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34292.8594 - accuracy: 0.7811 - val_loss: 37043.8984 - val_accuracy: 0.5607\n",
            "Epoch 192/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34267.5508 - accuracy: 0.7783 - val_loss: 37062.5781 - val_accuracy: 0.5637\n",
            "Epoch 193/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34140.6406 - accuracy: 0.7812 - val_loss: 37181.1289 - val_accuracy: 0.5683\n",
            "Epoch 194/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34305.9492 - accuracy: 0.7791 - val_loss: 37238.3320 - val_accuracy: 0.5605\n",
            "Epoch 195/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34402.3359 - accuracy: 0.7784 - val_loss: 36995.9609 - val_accuracy: 0.5663\n",
            "Epoch 196/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34067.9180 - accuracy: 0.7788 - val_loss: 36796.5781 - val_accuracy: 0.5723\n",
            "Epoch 197/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34110.1367 - accuracy: 0.7803 - val_loss: 36729.5391 - val_accuracy: 0.5692\n",
            "Epoch 198/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34141.8750 - accuracy: 0.7808 - val_loss: 36804.4062 - val_accuracy: 0.5748\n",
            "Epoch 199/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34157.3711 - accuracy: 0.7838 - val_loss: 36322.7969 - val_accuracy: 0.5886\n",
            "Epoch 200/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 34122.6211 - accuracy: 0.7908roc-auc_val: 0.7793\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 34129.9844 - accuracy: 0.7908 - val_loss: 36207.9531 - val_accuracy: 0.5911\n",
            "Epoch 201/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34216.6719 - accuracy: 0.7869 - val_loss: 36243.7695 - val_accuracy: 0.5865\n",
            "Epoch 202/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34021.7344 - accuracy: 0.7850 - val_loss: 36135.4414 - val_accuracy: 0.5817\n",
            "Epoch 203/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34119.8203 - accuracy: 0.7868 - val_loss: 35825.6719 - val_accuracy: 0.5846\n",
            "Epoch 204/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34022.6953 - accuracy: 0.7861 - val_loss: 36290.3594 - val_accuracy: 0.5699\n",
            "Epoch 205/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34276.9180 - accuracy: 0.7794 - val_loss: 36378.3789 - val_accuracy: 0.5620\n",
            "Epoch 206/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34019.8711 - accuracy: 0.7796 - val_loss: 36640.8828 - val_accuracy: 0.5639\n",
            "Epoch 207/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33891.9570 - accuracy: 0.7791 - val_loss: 36503.7031 - val_accuracy: 0.5665\n",
            "Epoch 208/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 33935.3867 - accuracy: 0.7807 - val_loss: 36557.0977 - val_accuracy: 0.5751\n",
            "Epoch 209/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 34184.1523 - accuracy: 0.7845 - val_loss: 36578.3008 - val_accuracy: 0.5789\n",
            "Epoch 210/1000\n",
            "218/225 [============================>.] - ETA: 0s - loss: 34140.8359 - accuracy: 0.7884roc-auc_val: 0.7752\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 34065.7852 - accuracy: 0.7883 - val_loss: 36830.1133 - val_accuracy: 0.5681\n",
            "Epoch 211/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 33825.4492 - accuracy: 0.7861 - val_loss: 36554.3555 - val_accuracy: 0.5767\n",
            "Epoch 212/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 33888.1523 - accuracy: 0.7849 - val_loss: 36487.8750 - val_accuracy: 0.5672\n",
            "Epoch 213/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33907.0273 - accuracy: 0.7809 - val_loss: 36646.2734 - val_accuracy: 0.5629\n",
            "Epoch 214/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33801.6992 - accuracy: 0.7784 - val_loss: 36826.2500 - val_accuracy: 0.5609\n",
            "Epoch 215/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33733.2227 - accuracy: 0.7811 - val_loss: 36749.8594 - val_accuracy: 0.5680\n",
            "Epoch 216/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33853.9922 - accuracy: 0.7820 - val_loss: 36527.6484 - val_accuracy: 0.5751\n",
            "Epoch 217/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 33836.9180 - accuracy: 0.7807 - val_loss: 36902.1133 - val_accuracy: 0.5619\n",
            "Epoch 218/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 33835.2969 - accuracy: 0.7835 - val_loss: 36581.9609 - val_accuracy: 0.5684\n",
            "Epoch 219/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33978.2070 - accuracy: 0.7921 - val_loss: 36721.8516 - val_accuracy: 0.5760\n",
            "Epoch 220/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 33795.2578 - accuracy: 0.7908roc-auc_val: 0.7759\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 33750.4141 - accuracy: 0.7907 - val_loss: 36641.4961 - val_accuracy: 0.5766\n",
            "Epoch 221/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33856.8477 - accuracy: 0.7892 - val_loss: 36587.3594 - val_accuracy: 0.5789\n",
            "Epoch 222/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33832.8438 - accuracy: 0.7894 - val_loss: 36249.5547 - val_accuracy: 0.5834\n",
            "Epoch 223/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33734.4258 - accuracy: 0.7873 - val_loss: 36356.2891 - val_accuracy: 0.5720\n",
            "Epoch 224/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33775.7148 - accuracy: 0.7845 - val_loss: 36412.8672 - val_accuracy: 0.5684\n",
            "Epoch 225/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33681.4297 - accuracy: 0.7828 - val_loss: 36460.3789 - val_accuracy: 0.5670\n",
            "Epoch 226/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33542.0078 - accuracy: 0.7828 - val_loss: 36572.9414 - val_accuracy: 0.5654\n",
            "Epoch 227/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 33761.6250 - accuracy: 0.7860 - val_loss: 36153.0234 - val_accuracy: 0.5883\n",
            "Epoch 228/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33650.5977 - accuracy: 0.7891 - val_loss: 36197.2773 - val_accuracy: 0.5871\n",
            "Epoch 229/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33586.0508 - accuracy: 0.7891 - val_loss: 36256.5664 - val_accuracy: 0.5831\n",
            "Epoch 230/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 33836.3281 - accuracy: 0.7820roc-auc_val: 0.7779\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 33785.9961 - accuracy: 0.7819 - val_loss: 36476.4375 - val_accuracy: 0.5582\n",
            "Epoch 231/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33819.1797 - accuracy: 0.7794 - val_loss: 36513.5430 - val_accuracy: 0.5609\n",
            "Epoch 232/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33586.5547 - accuracy: 0.7818 - val_loss: 36546.5625 - val_accuracy: 0.5576\n",
            "Epoch 233/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33707.1797 - accuracy: 0.7798 - val_loss: 36502.1914 - val_accuracy: 0.5601\n",
            "Epoch 234/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33857.6367 - accuracy: 0.7790 - val_loss: 36358.3203 - val_accuracy: 0.5612\n",
            "Epoch 235/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33689.5352 - accuracy: 0.7810 - val_loss: 36221.1016 - val_accuracy: 0.5659\n",
            "Epoch 236/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33574.9961 - accuracy: 0.7831 - val_loss: 36211.5625 - val_accuracy: 0.5650\n",
            "Epoch 237/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33708.6875 - accuracy: 0.7802 - val_loss: 36058.1641 - val_accuracy: 0.5662\n",
            "Epoch 238/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33527.1836 - accuracy: 0.7832 - val_loss: 36063.2422 - val_accuracy: 0.5650\n",
            "Epoch 239/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33508.3828 - accuracy: 0.7764 - val_loss: 36140.7344 - val_accuracy: 0.5607\n",
            "Epoch 240/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 33640.3984 - accuracy: 0.7768roc-auc_val: 0.778\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 33582.0000 - accuracy: 0.7768 - val_loss: 36420.6172 - val_accuracy: 0.5650\n",
            "Epoch 241/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33665.8164 - accuracy: 0.7807 - val_loss: 36229.6523 - val_accuracy: 0.5671\n",
            "Epoch 242/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33346.5781 - accuracy: 0.7835 - val_loss: 36154.7500 - val_accuracy: 0.5741\n",
            "Epoch 243/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33650.0508 - accuracy: 0.7877 - val_loss: 35972.0078 - val_accuracy: 0.5810\n",
            "Epoch 244/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33448.3047 - accuracy: 0.7871 - val_loss: 35937.8086 - val_accuracy: 0.5771\n",
            "Epoch 245/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33503.1250 - accuracy: 0.7881 - val_loss: 36169.0742 - val_accuracy: 0.5830\n",
            "Epoch 246/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33610.2500 - accuracy: 0.7872 - val_loss: 36563.9609 - val_accuracy: 0.5742\n",
            "Epoch 247/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33463.6602 - accuracy: 0.7891 - val_loss: 36555.3438 - val_accuracy: 0.5791\n",
            "Epoch 248/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33512.9648 - accuracy: 0.7869 - val_loss: 36326.6992 - val_accuracy: 0.5770\n",
            "Epoch 249/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33361.1055 - accuracy: 0.7845 - val_loss: 36087.9453 - val_accuracy: 0.5790\n",
            "Epoch 250/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 33673.9883 - accuracy: 0.7821roc-auc_val: 0.7797\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 33606.2305 - accuracy: 0.7824 - val_loss: 36154.9062 - val_accuracy: 0.5695\n",
            "Epoch 251/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33284.1680 - accuracy: 0.7800 - val_loss: 36228.1562 - val_accuracy: 0.5694\n",
            "Epoch 252/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33349.8789 - accuracy: 0.7808 - val_loss: 36346.1797 - val_accuracy: 0.5731\n",
            "Epoch 253/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33341.7656 - accuracy: 0.7823 - val_loss: 36364.8125 - val_accuracy: 0.5717\n",
            "Epoch 1/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 59425.2695 - accuracy: 0.6638 - val_loss: 53269.2070 - val_accuracy: 0.5616\n",
            "Epoch 2/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 53356.4180 - accuracy: 0.7076 - val_loss: 50348.0078 - val_accuracy: 0.5802\n",
            "Epoch 3/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 52442.9375 - accuracy: 0.7080 - val_loss: 53262.9531 - val_accuracy: 0.5723\n",
            "Epoch 4/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 52438.7109 - accuracy: 0.7029 - val_loss: 52321.1680 - val_accuracy: 0.5808\n",
            "Epoch 5/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 50438.1719 - accuracy: 0.7166 - val_loss: 49678.3984 - val_accuracy: 0.5838\n",
            "Epoch 6/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 48730.9258 - accuracy: 0.7365 - val_loss: 48541.7734 - val_accuracy: 0.5952\n",
            "Epoch 7/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 48293.0273 - accuracy: 0.7155 - val_loss: 49581.4805 - val_accuracy: 0.5632\n",
            "Epoch 8/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 48057.4609 - accuracy: 0.7344 - val_loss: 47213.9531 - val_accuracy: 0.6035\n",
            "Epoch 9/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 46379.9023 - accuracy: 0.7373 - val_loss: 47388.5703 - val_accuracy: 0.6484\n",
            "Epoch 10/1000\n",
            "180/181 [============================>.] - ETA: 0s - loss: 46046.3984 - accuracy: 0.7468roc-auc_val: 0.7594\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 45967.6562 - accuracy: 0.7468 - val_loss: 47933.9180 - val_accuracy: 0.6178\n",
            "Epoch 11/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 45645.5391 - accuracy: 0.7453 - val_loss: 47398.6133 - val_accuracy: 0.6084\n",
            "Epoch 12/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 44954.9570 - accuracy: 0.7463 - val_loss: 46542.4883 - val_accuracy: 0.5911\n",
            "Epoch 13/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 45766.4570 - accuracy: 0.7569 - val_loss: 48744.4297 - val_accuracy: 0.5921\n",
            "Epoch 14/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 45723.5234 - accuracy: 0.7366 - val_loss: 46752.6875 - val_accuracy: 0.5954\n",
            "Epoch 15/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 44858.4023 - accuracy: 0.7411 - val_loss: 46764.9961 - val_accuracy: 0.5921\n",
            "Epoch 16/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 44413.5078 - accuracy: 0.7398 - val_loss: 46252.1055 - val_accuracy: 0.5952\n",
            "Epoch 17/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 43828.6914 - accuracy: 0.7572 - val_loss: 46010.0625 - val_accuracy: 0.6586\n",
            "Epoch 18/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 43910.9727 - accuracy: 0.7462 - val_loss: 46314.6094 - val_accuracy: 0.6190\n",
            "Epoch 19/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 43465.0898 - accuracy: 0.7479 - val_loss: 45349.8828 - val_accuracy: 0.6352\n",
            "Epoch 20/1000\n",
            "178/181 [============================>.] - ETA: 0s - loss: 43226.1562 - accuracy: 0.7480roc-auc_val: 0.7723\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 43121.7422 - accuracy: 0.7481 - val_loss: 45618.7148 - val_accuracy: 0.6150\n",
            "Epoch 21/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 42868.7070 - accuracy: 0.7551 - val_loss: 46032.3438 - val_accuracy: 0.6199\n",
            "Epoch 22/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 42724.5000 - accuracy: 0.7461 - val_loss: 46044.4180 - val_accuracy: 0.6090\n",
            "Epoch 23/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 41975.7617 - accuracy: 0.7557 - val_loss: 46150.8750 - val_accuracy: 0.6092\n",
            "Epoch 24/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 42263.2344 - accuracy: 0.7496 - val_loss: 45973.6172 - val_accuracy: 0.6091\n",
            "Epoch 25/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 42016.4219 - accuracy: 0.7563 - val_loss: 45271.1211 - val_accuracy: 0.6388\n",
            "Epoch 26/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 41570.2344 - accuracy: 0.7569 - val_loss: 45223.9766 - val_accuracy: 0.6276\n",
            "Epoch 27/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 41549.8008 - accuracy: 0.7468 - val_loss: 45828.6992 - val_accuracy: 0.6093\n",
            "Epoch 28/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 41157.1719 - accuracy: 0.7613 - val_loss: 44999.8086 - val_accuracy: 0.6433\n",
            "Epoch 29/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 41171.4375 - accuracy: 0.7637 - val_loss: 45104.7578 - val_accuracy: 0.6331\n",
            "Epoch 30/1000\n",
            "178/181 [============================>.] - ETA: 0s - loss: 40951.9648 - accuracy: 0.7586roc-auc_val: 0.7745\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 40948.6289 - accuracy: 0.7587 - val_loss: 45136.3008 - val_accuracy: 0.6179\n",
            "Epoch 31/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 40757.4141 - accuracy: 0.7590 - val_loss: 45079.5586 - val_accuracy: 0.6133\n",
            "Epoch 32/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 40550.1367 - accuracy: 0.7552 - val_loss: 45236.4297 - val_accuracy: 0.5945\n",
            "Epoch 33/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 40615.4961 - accuracy: 0.7615 - val_loss: 45542.2422 - val_accuracy: 0.6177\n",
            "Epoch 34/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 40446.4297 - accuracy: 0.7598 - val_loss: 45479.3672 - val_accuracy: 0.6006\n",
            "Epoch 35/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 40220.2031 - accuracy: 0.7603 - val_loss: 45323.0625 - val_accuracy: 0.6444\n",
            "Epoch 36/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 40312.2305 - accuracy: 0.7720 - val_loss: 44982.4102 - val_accuracy: 0.6357\n",
            "Epoch 37/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 40253.5195 - accuracy: 0.7634 - val_loss: 44345.4375 - val_accuracy: 0.6207\n",
            "Epoch 38/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 39998.5469 - accuracy: 0.7598 - val_loss: 44778.7344 - val_accuracy: 0.6132\n",
            "Epoch 39/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 39871.2578 - accuracy: 0.7759 - val_loss: 44204.4297 - val_accuracy: 0.6395\n",
            "Epoch 40/1000\n",
            "179/181 [============================>.] - ETA: 0s - loss: 39692.4883 - accuracy: 0.7769roc-auc_val: 0.7829\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 39623.0195 - accuracy: 0.7768 - val_loss: 44019.6055 - val_accuracy: 0.6136\n",
            "Epoch 41/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 39538.6797 - accuracy: 0.7691 - val_loss: 45684.2969 - val_accuracy: 0.6041\n",
            "Epoch 42/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 39679.2695 - accuracy: 0.7724 - val_loss: 44778.8398 - val_accuracy: 0.6424\n",
            "Epoch 43/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 39233.3672 - accuracy: 0.7641 - val_loss: 44736.6172 - val_accuracy: 0.6277\n",
            "Epoch 44/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 39108.0898 - accuracy: 0.7687 - val_loss: 44333.4453 - val_accuracy: 0.6280\n",
            "Epoch 45/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 39220.3789 - accuracy: 0.7806 - val_loss: 44649.7812 - val_accuracy: 0.6232\n",
            "Epoch 46/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 39014.1055 - accuracy: 0.7608 - val_loss: 44167.2852 - val_accuracy: 0.6069\n",
            "Epoch 47/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 38961.6875 - accuracy: 0.7576 - val_loss: 44649.3359 - val_accuracy: 0.5942\n",
            "Epoch 48/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 39220.1523 - accuracy: 0.7770 - val_loss: 44626.0625 - val_accuracy: 0.6387\n",
            "Epoch 49/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 38820.3320 - accuracy: 0.7693 - val_loss: 44831.6445 - val_accuracy: 0.6168\n",
            "Epoch 50/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 38757.0156 - accuracy: 0.7699roc-auc_val: 0.7814\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 38642.6914 - accuracy: 0.7701 - val_loss: 43835.1484 - val_accuracy: 0.6361\n",
            "Epoch 51/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 38668.7695 - accuracy: 0.7726 - val_loss: 44148.0547 - val_accuracy: 0.6315\n",
            "Epoch 52/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 38577.8281 - accuracy: 0.7706 - val_loss: 44574.4102 - val_accuracy: 0.6125\n",
            "Epoch 53/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 38376.9102 - accuracy: 0.7637 - val_loss: 44417.6836 - val_accuracy: 0.6009\n",
            "Epoch 54/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 38781.3086 - accuracy: 0.7972 - val_loss: 43950.6445 - val_accuracy: 0.6607\n",
            "Epoch 55/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 38260.3164 - accuracy: 0.7834 - val_loss: 43897.0039 - val_accuracy: 0.6465\n",
            "Epoch 56/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37993.3750 - accuracy: 0.7801 - val_loss: 44108.0977 - val_accuracy: 0.6345\n",
            "Epoch 57/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37822.7031 - accuracy: 0.7761 - val_loss: 44003.0586 - val_accuracy: 0.6268\n",
            "Epoch 58/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37803.4258 - accuracy: 0.7754 - val_loss: 43627.1836 - val_accuracy: 0.6268\n",
            "Epoch 59/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37752.6836 - accuracy: 0.7752 - val_loss: 43811.1836 - val_accuracy: 0.6430\n",
            "Epoch 60/1000\n",
            "171/181 [===========================>..] - ETA: 0s - loss: 38040.9727 - accuracy: 0.7754roc-auc_val: 0.7804\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 37938.8906 - accuracy: 0.7753 - val_loss: 44350.3867 - val_accuracy: 0.6313\n",
            "Epoch 61/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37684.2227 - accuracy: 0.7748 - val_loss: 43861.9180 - val_accuracy: 0.6375\n",
            "Epoch 62/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37524.3750 - accuracy: 0.7803 - val_loss: 43719.4258 - val_accuracy: 0.6235\n",
            "Epoch 63/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37330.8320 - accuracy: 0.7849 - val_loss: 43575.1367 - val_accuracy: 0.6391\n",
            "Epoch 64/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37131.7188 - accuracy: 0.7848 - val_loss: 43491.4648 - val_accuracy: 0.6464\n",
            "Epoch 65/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37548.4570 - accuracy: 0.7968 - val_loss: 43089.1836 - val_accuracy: 0.6617\n",
            "Epoch 66/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37159.5273 - accuracy: 0.7915 - val_loss: 43238.5469 - val_accuracy: 0.6520\n",
            "Epoch 67/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37061.8750 - accuracy: 0.7880 - val_loss: 43022.3945 - val_accuracy: 0.6555\n",
            "Epoch 68/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36804.4297 - accuracy: 0.7892 - val_loss: 42899.8281 - val_accuracy: 0.6623\n",
            "Epoch 69/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36780.9141 - accuracy: 0.7899 - val_loss: 43725.9336 - val_accuracy: 0.6234\n",
            "Epoch 70/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 37159.4766 - accuracy: 0.7832roc-auc_val: 0.7872\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 37089.3320 - accuracy: 0.7830 - val_loss: 43318.4688 - val_accuracy: 0.6393\n",
            "Epoch 71/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36943.7734 - accuracy: 0.8048 - val_loss: 42823.9219 - val_accuracy: 0.6636\n",
            "Epoch 72/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36993.2422 - accuracy: 0.7950 - val_loss: 43377.8945 - val_accuracy: 0.6617\n",
            "Epoch 73/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 37187.1836 - accuracy: 0.7929 - val_loss: 43089.1719 - val_accuracy: 0.6466\n",
            "Epoch 74/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36702.0430 - accuracy: 0.7935 - val_loss: 43092.2422 - val_accuracy: 0.6544\n",
            "Epoch 75/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36973.8281 - accuracy: 0.7907 - val_loss: 43125.6641 - val_accuracy: 0.6407\n",
            "Epoch 76/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36897.0430 - accuracy: 0.7830 - val_loss: 43267.3086 - val_accuracy: 0.6336\n",
            "Epoch 77/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36744.3867 - accuracy: 0.7821 - val_loss: 43177.5938 - val_accuracy: 0.6353\n",
            "Epoch 78/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36747.1758 - accuracy: 0.7885 - val_loss: 43062.0234 - val_accuracy: 0.6444\n",
            "Epoch 79/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36809.5312 - accuracy: 0.7989 - val_loss: 42686.6211 - val_accuracy: 0.6516\n",
            "Epoch 80/1000\n",
            "175/181 [============================>.] - ETA: 0s - loss: 37040.7461 - accuracy: 0.7845roc-auc_val: 0.7893\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 36788.3125 - accuracy: 0.7843 - val_loss: 42487.8789 - val_accuracy: 0.6457\n",
            "Epoch 81/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36734.2227 - accuracy: 0.7781 - val_loss: 41833.4102 - val_accuracy: 0.6670\n",
            "Epoch 82/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36997.5156 - accuracy: 0.7800 - val_loss: 42099.8750 - val_accuracy: 0.6575\n",
            "Epoch 83/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36843.9883 - accuracy: 0.7842 - val_loss: 42587.0156 - val_accuracy: 0.6502\n",
            "Epoch 84/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36508.4570 - accuracy: 0.7872 - val_loss: 42538.5469 - val_accuracy: 0.6521\n",
            "Epoch 85/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36658.5586 - accuracy: 0.7863 - val_loss: 42584.3008 - val_accuracy: 0.6616\n",
            "Epoch 86/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36568.9531 - accuracy: 0.7928 - val_loss: 42846.4883 - val_accuracy: 0.6586\n",
            "Epoch 87/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36359.5625 - accuracy: 0.7913 - val_loss: 42781.4102 - val_accuracy: 0.6499\n",
            "Epoch 88/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36348.3867 - accuracy: 0.7876 - val_loss: 43699.9102 - val_accuracy: 0.6238\n",
            "Epoch 89/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36607.8945 - accuracy: 0.7780 - val_loss: 43798.5234 - val_accuracy: 0.6119\n",
            "Epoch 90/1000\n",
            "172/181 [===========================>..] - ETA: 0s - loss: 36369.1367 - accuracy: 0.7815roc-auc_val: 0.7863\n",
            "181/181 [==============================] - 7s 40ms/step - loss: 36298.9922 - accuracy: 0.7818 - val_loss: 43095.2500 - val_accuracy: 0.6430\n",
            "Epoch 91/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36338.8789 - accuracy: 0.7891 - val_loss: 43161.4844 - val_accuracy: 0.6225\n",
            "Epoch 92/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36341.3828 - accuracy: 0.7867 - val_loss: 43081.1680 - val_accuracy: 0.6335\n",
            "Epoch 93/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36017.6953 - accuracy: 0.7848 - val_loss: 42994.0781 - val_accuracy: 0.6398\n",
            "Epoch 94/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36227.2383 - accuracy: 0.7950 - val_loss: 43152.2109 - val_accuracy: 0.6476\n",
            "Epoch 95/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35907.5938 - accuracy: 0.7949 - val_loss: 42727.3438 - val_accuracy: 0.6536\n",
            "Epoch 96/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36091.7031 - accuracy: 0.7952 - val_loss: 42885.5469 - val_accuracy: 0.6528\n",
            "Epoch 97/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36159.4570 - accuracy: 0.7974 - val_loss: 43005.1836 - val_accuracy: 0.6374\n",
            "Epoch 98/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35999.7578 - accuracy: 0.7871 - val_loss: 43135.2656 - val_accuracy: 0.6280\n",
            "Epoch 99/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 36082.3438 - accuracy: 0.7870 - val_loss: 42916.5586 - val_accuracy: 0.6423\n",
            "Epoch 100/1000\n",
            "177/181 [============================>.] - ETA: 0s - loss: 36305.2148 - accuracy: 0.7908roc-auc_val: 0.7868\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 36147.9102 - accuracy: 0.7908 - val_loss: 42959.7422 - val_accuracy: 0.6511\n",
            "Epoch 101/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35834.4258 - accuracy: 0.7955 - val_loss: 43376.5586 - val_accuracy: 0.6471\n",
            "Epoch 102/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35892.5781 - accuracy: 0.7946 - val_loss: 43025.8672 - val_accuracy: 0.6470\n",
            "Epoch 103/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35804.4609 - accuracy: 0.7893 - val_loss: 43143.0742 - val_accuracy: 0.6408\n",
            "Epoch 104/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35778.0078 - accuracy: 0.7903 - val_loss: 42983.9141 - val_accuracy: 0.6433\n",
            "Epoch 105/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35910.1289 - accuracy: 0.7956 - val_loss: 43029.5469 - val_accuracy: 0.6466\n",
            "Epoch 106/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35803.2188 - accuracy: 0.7934 - val_loss: 43054.4531 - val_accuracy: 0.6370\n",
            "Epoch 107/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35843.1914 - accuracy: 0.7872 - val_loss: 43059.4414 - val_accuracy: 0.6384\n",
            "Epoch 108/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35709.4648 - accuracy: 0.7871 - val_loss: 42769.3320 - val_accuracy: 0.6468\n",
            "Epoch 109/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35657.1445 - accuracy: 0.7900 - val_loss: 42535.4688 - val_accuracy: 0.6527\n",
            "Epoch 110/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 35714.0625 - accuracy: 0.7999roc-auc_val: 0.7875\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 35652.1602 - accuracy: 0.8001 - val_loss: 42741.9297 - val_accuracy: 0.6651\n",
            "Epoch 111/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35694.8008 - accuracy: 0.7989 - val_loss: 43268.3242 - val_accuracy: 0.6433\n",
            "Epoch 112/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 35673.7500 - accuracy: 0.7851 - val_loss: 43343.7969 - val_accuracy: 0.6280\n",
            "Epoch 113/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 35616.6836 - accuracy: 0.7873 - val_loss: 43159.0078 - val_accuracy: 0.6446\n",
            "Epoch 114/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 35649.7227 - accuracy: 0.8108 - val_loss: 43232.2305 - val_accuracy: 0.6732\n",
            "Epoch 115/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 35490.2773 - accuracy: 0.8042 - val_loss: 42951.7617 - val_accuracy: 0.6721\n",
            "Epoch 116/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 35399.9492 - accuracy: 0.8023 - val_loss: 43045.5508 - val_accuracy: 0.6720\n",
            "Epoch 117/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 35555.0898 - accuracy: 0.7997 - val_loss: 42702.5117 - val_accuracy: 0.6665\n",
            "Epoch 118/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 35472.6992 - accuracy: 0.7928 - val_loss: 42808.7305 - val_accuracy: 0.6615\n",
            "Epoch 119/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 35410.0977 - accuracy: 0.7906 - val_loss: 42806.2109 - val_accuracy: 0.6528\n",
            "Epoch 120/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 35309.1758 - accuracy: 0.7888roc-auc_val: 0.7887\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 35208.5859 - accuracy: 0.7888 - val_loss: 42596.0586 - val_accuracy: 0.6534\n",
            "Epoch 121/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35338.8438 - accuracy: 0.7888 - val_loss: 42657.1094 - val_accuracy: 0.6485\n",
            "Epoch 122/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35300.5898 - accuracy: 0.7950 - val_loss: 42471.5508 - val_accuracy: 0.6561\n",
            "Epoch 123/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35159.3711 - accuracy: 0.7948 - val_loss: 42358.1758 - val_accuracy: 0.6613\n",
            "Epoch 124/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35214.1562 - accuracy: 0.7874 - val_loss: 42840.0156 - val_accuracy: 0.6457\n",
            "Epoch 125/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35197.2266 - accuracy: 0.7884 - val_loss: 42820.4102 - val_accuracy: 0.6561\n",
            "Epoch 126/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35122.9258 - accuracy: 0.7924 - val_loss: 42679.3008 - val_accuracy: 0.6481\n",
            "Epoch 127/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35226.8711 - accuracy: 0.7868 - val_loss: 42663.9297 - val_accuracy: 0.6525\n",
            "Epoch 128/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35431.1406 - accuracy: 0.7920 - val_loss: 42885.6875 - val_accuracy: 0.6680\n",
            "Epoch 129/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35220.1328 - accuracy: 0.8066 - val_loss: 42745.0312 - val_accuracy: 0.6676\n",
            "Epoch 130/1000\n",
            "175/181 [============================>.] - ETA: 0s - loss: 35127.1328 - accuracy: 0.8015roc-auc_val: 0.7871\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 35153.3711 - accuracy: 0.8016 - val_loss: 42623.0234 - val_accuracy: 0.6713\n",
            "Epoch 131/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 35065.1992 - accuracy: 0.7998 - val_loss: 42496.5352 - val_accuracy: 0.6691\n",
            "Epoch 1/1000\n",
            "136/136 [==============================] - 2s 12ms/step - loss: 57153.3594 - accuracy: 0.6837 - val_loss: 51773.1758 - val_accuracy: 0.6189\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 52063.4531 - accuracy: 0.7070 - val_loss: 55000.1875 - val_accuracy: 0.5719\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 51541.9258 - accuracy: 0.7124 - val_loss: 53282.0977 - val_accuracy: 0.6011\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 48899.6836 - accuracy: 0.7314 - val_loss: 49049.7422 - val_accuracy: 0.6636\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 48856.1133 - accuracy: 0.7083 - val_loss: 52342.2500 - val_accuracy: 0.5978\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 47448.7305 - accuracy: 0.7327 - val_loss: 49670.8672 - val_accuracy: 0.6684\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 45813.3281 - accuracy: 0.7357 - val_loss: 48957.9961 - val_accuracy: 0.6571\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 44336.7070 - accuracy: 0.7439 - val_loss: 48591.3984 - val_accuracy: 0.6434\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 43581.3711 - accuracy: 0.7442 - val_loss: 51948.0078 - val_accuracy: 0.5817\n",
            "Epoch 10/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 44556.7773 - accuracy: 0.7358roc-auc_val: 0.7874\n",
            "136/136 [==============================] - 9s 68ms/step - loss: 44433.6055 - accuracy: 0.7353 - val_loss: 49373.8242 - val_accuracy: 0.6089\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 43716.0078 - accuracy: 0.7493 - val_loss: 47203.9492 - val_accuracy: 0.6841\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 42798.2344 - accuracy: 0.7570 - val_loss: 47674.9219 - val_accuracy: 0.6485\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 42092.7305 - accuracy: 0.7704 - val_loss: 46560.0820 - val_accuracy: 0.6885\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 41490.7031 - accuracy: 0.7595 - val_loss: 48709.8359 - val_accuracy: 0.6227\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 41826.6523 - accuracy: 0.7640 - val_loss: 46549.7500 - val_accuracy: 0.7114\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 41192.7656 - accuracy: 0.7813 - val_loss: 46094.2461 - val_accuracy: 0.7090\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 40983.8555 - accuracy: 0.7660 - val_loss: 46367.7500 - val_accuracy: 0.6985\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 40318.8164 - accuracy: 0.7678 - val_loss: 45411.5430 - val_accuracy: 0.7043\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 41339.9219 - accuracy: 0.7843 - val_loss: 46693.2539 - val_accuracy: 0.6695\n",
            "Epoch 20/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 40890.9570 - accuracy: 0.7770roc-auc_val: 0.7894\n",
            "136/136 [==============================] - 9s 68ms/step - loss: 40872.8125 - accuracy: 0.7766 - val_loss: 46182.2617 - val_accuracy: 0.6706\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 40811.1562 - accuracy: 0.7848 - val_loss: 45649.5781 - val_accuracy: 0.6797\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 39730.4648 - accuracy: 0.7747 - val_loss: 46852.0273 - val_accuracy: 0.6373\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 39480.4336 - accuracy: 0.7674 - val_loss: 46371.4453 - val_accuracy: 0.6640\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 39288.7500 - accuracy: 0.7648 - val_loss: 46642.6055 - val_accuracy: 0.6495\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 38873.2344 - accuracy: 0.7667 - val_loss: 46086.8750 - val_accuracy: 0.6653\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 38543.9414 - accuracy: 0.7832 - val_loss: 46094.4961 - val_accuracy: 0.6462\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 38346.5078 - accuracy: 0.7803 - val_loss: 46239.2305 - val_accuracy: 0.7353\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 38875.1484 - accuracy: 0.7924 - val_loss: 46334.7812 - val_accuracy: 0.6417\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 38895.3281 - accuracy: 0.7809 - val_loss: 46187.9648 - val_accuracy: 0.6342\n",
            "Epoch 30/1000\n",
            "131/136 [===========================>..] - ETA: 0s - loss: 38341.4141 - accuracy: 0.7620roc-auc_val: 0.7908\n",
            "136/136 [==============================] - 9s 68ms/step - loss: 38241.9570 - accuracy: 0.7619 - val_loss: 46661.9922 - val_accuracy: 0.6187\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 38570.9883 - accuracy: 0.7759 - val_loss: 46860.5000 - val_accuracy: 0.6342\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 38031.0977 - accuracy: 0.7761 - val_loss: 45723.0977 - val_accuracy: 0.6814\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 37687.6133 - accuracy: 0.7835 - val_loss: 45825.4609 - val_accuracy: 0.6562\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 37352.2070 - accuracy: 0.7837 - val_loss: 45298.3828 - val_accuracy: 0.6761\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 37602.7031 - accuracy: 0.7810 - val_loss: 45475.9453 - val_accuracy: 0.6856\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 37717.2852 - accuracy: 0.7705 - val_loss: 45934.7148 - val_accuracy: 0.6519\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 37651.3477 - accuracy: 0.7573 - val_loss: 44972.3281 - val_accuracy: 0.7011\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 37110.3945 - accuracy: 0.7850 - val_loss: 45250.3594 - val_accuracy: 0.6982\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 37043.6367 - accuracy: 0.7781 - val_loss: 45752.3516 - val_accuracy: 0.6912\n",
            "Epoch 40/1000\n",
            "134/136 [============================>.] - ETA: 0s - loss: 37161.3594 - accuracy: 0.7786roc-auc_val: 0.7897\n",
            "136/136 [==============================] - 9s 70ms/step - loss: 37104.3633 - accuracy: 0.7785 - val_loss: 45921.4570 - val_accuracy: 0.6889\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 36911.4258 - accuracy: 0.7730 - val_loss: 45289.8086 - val_accuracy: 0.7051\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 36560.6719 - accuracy: 0.7801 - val_loss: 45161.2383 - val_accuracy: 0.7061\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 36472.6719 - accuracy: 0.7817 - val_loss: 45520.0781 - val_accuracy: 0.7166\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 36446.6484 - accuracy: 0.7797 - val_loss: 45231.4492 - val_accuracy: 0.7137\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 36372.2461 - accuracy: 0.7832 - val_loss: 44985.5391 - val_accuracy: 0.7075\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 36156.1680 - accuracy: 0.7782 - val_loss: 45292.3828 - val_accuracy: 0.6986\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 36367.8750 - accuracy: 0.8012 - val_loss: 45196.3008 - val_accuracy: 0.6923\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35979.8867 - accuracy: 0.7858 - val_loss: 45043.3125 - val_accuracy: 0.6841\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35873.9844 - accuracy: 0.7853 - val_loss: 45007.0938 - val_accuracy: 0.6800\n",
            "Epoch 50/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 35768.6211 - accuracy: 0.7890roc-auc_val: 0.7911\n",
            "136/136 [==============================] - 9s 68ms/step - loss: 35724.0391 - accuracy: 0.7887 - val_loss: 45302.8867 - val_accuracy: 0.6670\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35701.9688 - accuracy: 0.7852 - val_loss: 45093.7812 - val_accuracy: 0.6876\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35546.6445 - accuracy: 0.7903 - val_loss: 44739.6406 - val_accuracy: 0.6891\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35892.6797 - accuracy: 0.7915 - val_loss: 44837.2266 - val_accuracy: 0.6866\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35438.4062 - accuracy: 0.7835 - val_loss: 44748.7891 - val_accuracy: 0.6897\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35530.5664 - accuracy: 0.7857 - val_loss: 44753.9062 - val_accuracy: 0.7398\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 36227.0273 - accuracy: 0.8200 - val_loss: 44490.8711 - val_accuracy: 0.7225\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35412.1289 - accuracy: 0.7941 - val_loss: 44583.0078 - val_accuracy: 0.6920\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35109.8906 - accuracy: 0.7933 - val_loss: 44718.5000 - val_accuracy: 0.7001\n",
            "Epoch 59/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 35237.9922 - accuracy: 0.7873 - val_loss: 44527.7422 - val_accuracy: 0.6972\n",
            "Epoch 60/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 35039.0039 - accuracy: 0.7860roc-auc_val: 0.7957\n",
            "136/136 [==============================] - 9s 69ms/step - loss: 34930.7891 - accuracy: 0.7860 - val_loss: 44438.3359 - val_accuracy: 0.6941\n",
            "Epoch 61/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34946.9609 - accuracy: 0.7875 - val_loss: 44417.8516 - val_accuracy: 0.7013\n",
            "Epoch 62/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34869.7422 - accuracy: 0.7947 - val_loss: 44434.5078 - val_accuracy: 0.7084\n",
            "Epoch 63/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34861.4141 - accuracy: 0.7861 - val_loss: 44386.5820 - val_accuracy: 0.6940\n",
            "Epoch 64/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34893.3438 - accuracy: 0.7889 - val_loss: 44488.1250 - val_accuracy: 0.6850\n",
            "Epoch 65/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 35026.3516 - accuracy: 0.7873 - val_loss: 44568.9414 - val_accuracy: 0.6792\n",
            "Epoch 66/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34552.6133 - accuracy: 0.7955 - val_loss: 44409.3945 - val_accuracy: 0.6968\n",
            "Epoch 67/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34822.0273 - accuracy: 0.7926 - val_loss: 44608.9766 - val_accuracy: 0.6689\n",
            "Epoch 68/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34501.3711 - accuracy: 0.7902 - val_loss: 44448.0000 - val_accuracy: 0.6923\n",
            "Epoch 69/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34284.7773 - accuracy: 0.7976 - val_loss: 44583.4492 - val_accuracy: 0.6891\n",
            "Epoch 70/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 34760.7070 - accuracy: 0.7942roc-auc_val: 0.7957\n",
            "136/136 [==============================] - 9s 69ms/step - loss: 34638.2852 - accuracy: 0.7942 - val_loss: 44540.6523 - val_accuracy: 0.7047\n",
            "Epoch 71/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34535.3789 - accuracy: 0.8014 - val_loss: 44666.0586 - val_accuracy: 0.6956\n",
            "Epoch 72/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 34337.0156 - accuracy: 0.7959 - val_loss: 44449.0234 - val_accuracy: 0.6998\n",
            "Epoch 73/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34813.6719 - accuracy: 0.8059 - val_loss: 44821.6172 - val_accuracy: 0.7615\n",
            "Epoch 74/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34816.0977 - accuracy: 0.8154 - val_loss: 44279.4492 - val_accuracy: 0.7179\n",
            "Epoch 75/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34447.8516 - accuracy: 0.8025 - val_loss: 44291.1523 - val_accuracy: 0.7257\n",
            "Epoch 76/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34450.7031 - accuracy: 0.7980 - val_loss: 44475.9922 - val_accuracy: 0.7104\n",
            "Epoch 77/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34312.4062 - accuracy: 0.7969 - val_loss: 44426.1211 - val_accuracy: 0.7031\n",
            "Epoch 78/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34193.9727 - accuracy: 0.7915 - val_loss: 44473.7070 - val_accuracy: 0.6879\n",
            "Epoch 79/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 34164.3398 - accuracy: 0.7916 - val_loss: 44541.1914 - val_accuracy: 0.6870\n",
            "Epoch 80/1000\n",
            "131/136 [===========================>..] - ETA: 0s - loss: 34352.2422 - accuracy: 0.7925roc-auc_val: 0.7933\n",
            "136/136 [==============================] - 10s 70ms/step - loss: 34284.2617 - accuracy: 0.7921 - val_loss: 44630.9961 - val_accuracy: 0.6766\n",
            "Epoch 81/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 34017.9102 - accuracy: 0.7895 - val_loss: 44620.0625 - val_accuracy: 0.6873\n",
            "Epoch 82/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 34061.6094 - accuracy: 0.7945 - val_loss: 44274.9023 - val_accuracy: 0.7100\n",
            "Epoch 83/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 34101.0625 - accuracy: 0.8011 - val_loss: 44384.2109 - val_accuracy: 0.6988\n",
            "Epoch 84/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33874.8789 - accuracy: 0.8040 - val_loss: 44392.0195 - val_accuracy: 0.7185\n",
            "Epoch 85/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33865.6211 - accuracy: 0.7960 - val_loss: 44385.3242 - val_accuracy: 0.6930\n",
            "Epoch 86/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33803.1602 - accuracy: 0.7963 - val_loss: 44246.9414 - val_accuracy: 0.7001\n",
            "Epoch 87/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33669.3281 - accuracy: 0.7952 - val_loss: 44257.0195 - val_accuracy: 0.7040\n",
            "Epoch 88/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 33679.5664 - accuracy: 0.8040 - val_loss: 44378.8242 - val_accuracy: 0.7009\n",
            "Epoch 89/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33750.1914 - accuracy: 0.7963 - val_loss: 44439.1133 - val_accuracy: 0.6927\n",
            "Epoch 90/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 33671.0859 - accuracy: 0.8004roc-auc_val: 0.7949\n",
            "136/136 [==============================] - 9s 70ms/step - loss: 33565.0938 - accuracy: 0.8005 - val_loss: 44561.2734 - val_accuracy: 0.7064\n",
            "Epoch 91/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33902.2422 - accuracy: 0.8027 - val_loss: 44532.6758 - val_accuracy: 0.7217\n",
            "Epoch 92/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33553.0586 - accuracy: 0.8039 - val_loss: 44544.3164 - val_accuracy: 0.7040\n",
            "Epoch 93/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 33464.4062 - accuracy: 0.7975 - val_loss: 44394.6914 - val_accuracy: 0.7026\n",
            "Epoch 94/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33486.2539 - accuracy: 0.7936 - val_loss: 44561.3086 - val_accuracy: 0.6719\n",
            "Epoch 95/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 33391.8438 - accuracy: 0.7926 - val_loss: 44562.4453 - val_accuracy: 0.7038\n",
            "Epoch 96/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 33634.5547 - accuracy: 0.7944 - val_loss: 44571.2227 - val_accuracy: 0.6885\n",
            "Epoch 97/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 33424.2148 - accuracy: 0.7903 - val_loss: 44707.2656 - val_accuracy: 0.6940\n",
            "Epoch 98/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 33339.8086 - accuracy: 0.7912 - val_loss: 44549.3828 - val_accuracy: 0.6952\n",
            "Epoch 99/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 33340.8359 - accuracy: 0.7926 - val_loss: 44431.0195 - val_accuracy: 0.7030\n",
            "Epoch 100/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 33349.1172 - accuracy: 0.7925roc-auc_val: 0.7933\n",
            "136/136 [==============================] - 9s 69ms/step - loss: 33346.0664 - accuracy: 0.7927 - val_loss: 44515.7188 - val_accuracy: 0.7077\n",
            "Epoch 101/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 33008.6836 - accuracy: 0.8046 - val_loss: 44571.4219 - val_accuracy: 0.6923\n",
            "Epoch 102/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33269.4531 - accuracy: 0.8008 - val_loss: 44455.4375 - val_accuracy: 0.7102\n",
            "Epoch 103/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 32998.9766 - accuracy: 0.8054 - val_loss: 44564.7656 - val_accuracy: 0.7162\n",
            "Epoch 104/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 32913.3359 - accuracy: 0.8119 - val_loss: 44638.7812 - val_accuracy: 0.7132\n",
            "Epoch 105/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33107.2930 - accuracy: 0.8034 - val_loss: 44577.5234 - val_accuracy: 0.7040\n",
            "Epoch 106/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32913.7344 - accuracy: 0.7979 - val_loss: 44632.7188 - val_accuracy: 0.7075\n",
            "Epoch 107/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32997.5977 - accuracy: 0.8049 - val_loss: 44481.2734 - val_accuracy: 0.7111\n",
            "Epoch 108/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 33007.0195 - accuracy: 0.8006 - val_loss: 44504.3789 - val_accuracy: 0.7131\n",
            "Epoch 109/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32968.5977 - accuracy: 0.7977 - val_loss: 44521.2148 - val_accuracy: 0.6977\n",
            "Epoch 110/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 32741.1543 - accuracy: 0.8046roc-auc_val: 0.7953\n",
            "136/136 [==============================] - 9s 69ms/step - loss: 32698.0352 - accuracy: 0.8047 - val_loss: 44341.1875 - val_accuracy: 0.7040\n",
            "Epoch 111/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 33144.4297 - accuracy: 0.7864 - val_loss: 44357.9336 - val_accuracy: 0.6687\n",
            "Epoch 112/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32761.3438 - accuracy: 0.7918 - val_loss: 44205.4180 - val_accuracy: 0.6831\n",
            "Epoch 113/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 32987.5625 - accuracy: 0.7917 - val_loss: 44197.7773 - val_accuracy: 0.7088\n",
            "Epoch 114/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32843.3789 - accuracy: 0.8015 - val_loss: 44270.1719 - val_accuracy: 0.6982\n",
            "Epoch 115/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32765.4980 - accuracy: 0.8023 - val_loss: 44679.5156 - val_accuracy: 0.6861\n",
            "Epoch 116/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32832.1836 - accuracy: 0.7936 - val_loss: 44437.7305 - val_accuracy: 0.6932\n",
            "Epoch 117/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32590.3477 - accuracy: 0.8012 - val_loss: 44467.4375 - val_accuracy: 0.7201\n",
            "Epoch 118/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32701.2910 - accuracy: 0.8084 - val_loss: 44421.8711 - val_accuracy: 0.7060\n",
            "Epoch 119/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32640.8574 - accuracy: 0.8074 - val_loss: 44512.6602 - val_accuracy: 0.7174\n",
            "Epoch 120/1000\n",
            "134/136 [============================>.] - ETA: 0s - loss: 32528.2383 - accuracy: 0.8090roc-auc_val: 0.7977\n",
            "136/136 [==============================] - 9s 70ms/step - loss: 32430.9785 - accuracy: 0.8089 - val_loss: 44397.1016 - val_accuracy: 0.7212\n",
            "Epoch 121/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32455.0996 - accuracy: 0.8106 - val_loss: 44393.0391 - val_accuracy: 0.7201\n",
            "Epoch 122/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32496.0762 - accuracy: 0.8049 - val_loss: 44301.8672 - val_accuracy: 0.6983\n",
            "Epoch 123/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32287.1582 - accuracy: 0.8047 - val_loss: 44486.9805 - val_accuracy: 0.6990\n",
            "Epoch 124/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32376.1035 - accuracy: 0.8089 - val_loss: 44401.0312 - val_accuracy: 0.7248\n",
            "Epoch 125/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32222.0117 - accuracy: 0.8135 - val_loss: 44308.1484 - val_accuracy: 0.6907\n",
            "Epoch 126/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32479.6836 - accuracy: 0.7957 - val_loss: 44298.8555 - val_accuracy: 0.6925\n",
            "Epoch 127/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32482.0078 - accuracy: 0.8116 - val_loss: 44550.2578 - val_accuracy: 0.7308\n",
            "Epoch 128/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32316.7363 - accuracy: 0.8150 - val_loss: 44392.9492 - val_accuracy: 0.7047\n",
            "Epoch 129/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32349.3164 - accuracy: 0.7944 - val_loss: 44648.2617 - val_accuracy: 0.6682\n",
            "Epoch 130/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 32155.0684 - accuracy: 0.7921roc-auc_val: 0.7962\n",
            "136/136 [==============================] - 9s 70ms/step - loss: 32075.6094 - accuracy: 0.7921 - val_loss: 44553.4492 - val_accuracy: 0.6630\n",
            "Epoch 131/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32317.3047 - accuracy: 0.7942 - val_loss: 44488.6992 - val_accuracy: 0.6831\n",
            "Epoch 132/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32045.5527 - accuracy: 0.7990 - val_loss: 44405.0430 - val_accuracy: 0.6851\n",
            "Epoch 133/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32029.1680 - accuracy: 0.8153 - val_loss: 44416.4727 - val_accuracy: 0.7141\n",
            "Epoch 134/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31933.4785 - accuracy: 0.8173 - val_loss: 44340.9062 - val_accuracy: 0.7263\n",
            "Epoch 135/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 32171.9375 - accuracy: 0.8114 - val_loss: 44271.6953 - val_accuracy: 0.7102\n",
            "Epoch 136/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31963.9297 - accuracy: 0.8110 - val_loss: 44286.4766 - val_accuracy: 0.7075\n",
            "Epoch 137/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 32368.3965 - accuracy: 0.8294 - val_loss: 44678.1211 - val_accuracy: 0.7357\n",
            "Epoch 138/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31940.3711 - accuracy: 0.8287 - val_loss: 44503.8008 - val_accuracy: 0.7433\n",
            "Epoch 139/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31988.7656 - accuracy: 0.8266 - val_loss: 44416.9727 - val_accuracy: 0.7444\n",
            "Epoch 140/1000\n",
            "131/136 [===========================>..] - ETA: 0s - loss: 32044.4395 - accuracy: 0.8207roc-auc_val: 0.7973\n",
            "136/136 [==============================] - 10s 72ms/step - loss: 32093.3047 - accuracy: 0.8206 - val_loss: 44289.3398 - val_accuracy: 0.7351\n",
            "Epoch 141/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 31729.0137 - accuracy: 0.8162 - val_loss: 44255.5703 - val_accuracy: 0.7267\n",
            "Epoch 142/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 31847.6973 - accuracy: 0.8113 - val_loss: 44181.3711 - val_accuracy: 0.7158\n",
            "Epoch 143/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 31753.1543 - accuracy: 0.8093 - val_loss: 44090.6523 - val_accuracy: 0.7114\n",
            "Epoch 144/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31944.8867 - accuracy: 0.8069 - val_loss: 44151.3633 - val_accuracy: 0.7013\n",
            "Epoch 145/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31960.5098 - accuracy: 0.8088 - val_loss: 44256.3320 - val_accuracy: 0.7158\n",
            "Epoch 146/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 31717.4609 - accuracy: 0.8133 - val_loss: 44221.8672 - val_accuracy: 0.7264\n",
            "Epoch 147/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31873.4043 - accuracy: 0.8154 - val_loss: 44057.6992 - val_accuracy: 0.7218\n",
            "Epoch 148/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31840.2305 - accuracy: 0.8100 - val_loss: 44113.1445 - val_accuracy: 0.7127\n",
            "Epoch 149/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31764.4746 - accuracy: 0.8100 - val_loss: 44067.6914 - val_accuracy: 0.7029\n",
            "Epoch 150/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 31861.0918 - accuracy: 0.8080roc-auc_val: 0.7976\n",
            "136/136 [==============================] - 9s 69ms/step - loss: 31822.5664 - accuracy: 0.8082 - val_loss: 44055.9570 - val_accuracy: 0.7003\n",
            "Epoch 151/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 32086.4043 - accuracy: 0.8080 - val_loss: 44031.2070 - val_accuracy: 0.7055\n",
            "Epoch 152/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31921.0117 - accuracy: 0.8074 - val_loss: 44039.8125 - val_accuracy: 0.7029\n",
            "Epoch 153/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31940.1602 - accuracy: 0.8112 - val_loss: 44026.9102 - val_accuracy: 0.7156\n",
            "Epoch 154/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31739.0410 - accuracy: 0.8131 - val_loss: 43984.5781 - val_accuracy: 0.7095\n",
            "Epoch 155/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31519.8750 - accuracy: 0.8112 - val_loss: 44030.5703 - val_accuracy: 0.7030\n",
            "Epoch 156/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31627.2793 - accuracy: 0.8111 - val_loss: 44056.9453 - val_accuracy: 0.7195\n",
            "Epoch 157/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31381.7734 - accuracy: 0.8150 - val_loss: 44046.0430 - val_accuracy: 0.7169\n",
            "Epoch 158/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31614.7031 - accuracy: 0.8115 - val_loss: 44068.7500 - val_accuracy: 0.7106\n",
            "Epoch 159/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31372.0137 - accuracy: 0.8166 - val_loss: 44070.1289 - val_accuracy: 0.7152\n",
            "Epoch 160/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 31646.8809 - accuracy: 0.8137roc-auc_val: 0.7974\n",
            "136/136 [==============================] - 10s 70ms/step - loss: 31595.3691 - accuracy: 0.8138 - val_loss: 44106.6289 - val_accuracy: 0.7136\n",
            "Epoch 161/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31465.7246 - accuracy: 0.8129 - val_loss: 44206.9023 - val_accuracy: 0.7249\n",
            "Epoch 162/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31690.4062 - accuracy: 0.8234 - val_loss: 44214.3086 - val_accuracy: 0.7200\n",
            "Epoch 163/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31543.1211 - accuracy: 0.8181 - val_loss: 44104.9609 - val_accuracy: 0.7145\n",
            "Epoch 164/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31647.3535 - accuracy: 0.8131 - val_loss: 44035.9453 - val_accuracy: 0.7013\n",
            "Epoch 165/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31501.7363 - accuracy: 0.8125 - val_loss: 44073.9219 - val_accuracy: 0.7151\n",
            "Epoch 166/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31630.5977 - accuracy: 0.8175 - val_loss: 44003.7930 - val_accuracy: 0.7147\n",
            "Epoch 167/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31544.4473 - accuracy: 0.8001 - val_loss: 43962.6055 - val_accuracy: 0.6871\n",
            "Epoch 168/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31272.8477 - accuracy: 0.8037 - val_loss: 43950.6250 - val_accuracy: 0.7001\n",
            "Epoch 169/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31445.1094 - accuracy: 0.8072 - val_loss: 43906.2422 - val_accuracy: 0.6899\n",
            "Epoch 170/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 31479.9512 - accuracy: 0.8050roc-auc_val: 0.7984\n",
            "136/136 [==============================] - 10s 71ms/step - loss: 31416.3086 - accuracy: 0.8048 - val_loss: 44010.0195 - val_accuracy: 0.6927\n",
            "Epoch 171/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31396.2285 - accuracy: 0.8114 - val_loss: 44072.5000 - val_accuracy: 0.7053\n",
            "Epoch 172/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31391.4688 - accuracy: 0.8132 - val_loss: 44155.1406 - val_accuracy: 0.7112\n",
            "Epoch 173/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31427.6660 - accuracy: 0.8137 - val_loss: 44127.6016 - val_accuracy: 0.7059\n",
            "Epoch 174/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31277.1875 - accuracy: 0.8145 - val_loss: 44253.0586 - val_accuracy: 0.7100\n",
            "Epoch 175/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31273.6387 - accuracy: 0.8103 - val_loss: 44190.9492 - val_accuracy: 0.7084\n",
            "Epoch 176/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31298.1895 - accuracy: 0.8103 - val_loss: 44152.4805 - val_accuracy: 0.7076\n",
            "Epoch 177/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31432.9727 - accuracy: 0.8088 - val_loss: 44216.5234 - val_accuracy: 0.7141\n",
            "Epoch 178/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 30947.2793 - accuracy: 0.8133 - val_loss: 44110.5625 - val_accuracy: 0.7195\n",
            "Epoch 179/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31286.9121 - accuracy: 0.8133 - val_loss: 43994.2422 - val_accuracy: 0.7124\n",
            "Epoch 180/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 31139.4551 - accuracy: 0.8107roc-auc_val: 0.7983\n",
            "136/136 [==============================] - 9s 69ms/step - loss: 31129.2559 - accuracy: 0.8106 - val_loss: 44103.3828 - val_accuracy: 0.7147\n",
            "Epoch 181/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31300.8301 - accuracy: 0.8240 - val_loss: 44192.0586 - val_accuracy: 0.7421\n",
            "Epoch 182/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31171.2637 - accuracy: 0.8280 - val_loss: 44262.9727 - val_accuracy: 0.7405\n",
            "Epoch 183/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31125.1445 - accuracy: 0.8234 - val_loss: 44205.8047 - val_accuracy: 0.7324\n",
            "Epoch 184/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31064.6543 - accuracy: 0.8204 - val_loss: 44180.5391 - val_accuracy: 0.7331\n",
            "Epoch 185/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30859.6504 - accuracy: 0.8185 - val_loss: 44201.5039 - val_accuracy: 0.7283\n",
            "Epoch 186/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31290.3301 - accuracy: 0.8192 - val_loss: 44228.2773 - val_accuracy: 0.7311\n",
            "Epoch 187/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31112.7402 - accuracy: 0.8186 - val_loss: 44131.6953 - val_accuracy: 0.7263\n",
            "Epoch 188/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31108.9844 - accuracy: 0.8187 - val_loss: 44183.7852 - val_accuracy: 0.7318\n",
            "Epoch 189/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 31217.4531 - accuracy: 0.8188 - val_loss: 44205.5312 - val_accuracy: 0.7388\n",
            "Epoch 190/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 31528.2207 - accuracy: 0.8198roc-auc_val: 0.7992\n",
            "136/136 [==============================] - 9s 69ms/step - loss: 31412.5703 - accuracy: 0.8199 - val_loss: 44172.7500 - val_accuracy: 0.7347\n",
            "Epoch 191/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30992.6816 - accuracy: 0.8197 - val_loss: 44214.6211 - val_accuracy: 0.7380\n",
            "Epoch 192/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30873.6953 - accuracy: 0.8199 - val_loss: 44091.8164 - val_accuracy: 0.7255\n",
            "Epoch 193/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 30988.9922 - accuracy: 0.8154 - val_loss: 44221.8438 - val_accuracy: 0.7278\n",
            "Epoch 194/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30968.7344 - accuracy: 0.8153 - val_loss: 44043.9570 - val_accuracy: 0.7147\n",
            "Epoch 195/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31105.0645 - accuracy: 0.8125 - val_loss: 44007.0312 - val_accuracy: 0.7127\n",
            "Epoch 196/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 30805.0977 - accuracy: 0.8124 - val_loss: 43975.6680 - val_accuracy: 0.7142\n",
            "Epoch 197/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31227.3516 - accuracy: 0.8099 - val_loss: 43925.5938 - val_accuracy: 0.7038\n",
            "Epoch 198/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30958.1270 - accuracy: 0.8099 - val_loss: 43980.7422 - val_accuracy: 0.7097\n",
            "Epoch 199/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30960.3594 - accuracy: 0.8130 - val_loss: 43985.7422 - val_accuracy: 0.7119\n",
            "Epoch 200/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 31037.1289 - accuracy: 0.8140roc-auc_val: 0.7982\n",
            "136/136 [==============================] - 9s 69ms/step - loss: 31018.0312 - accuracy: 0.8142 - val_loss: 44106.2617 - val_accuracy: 0.7199\n",
            "Epoch 201/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 30955.9141 - accuracy: 0.8185 - val_loss: 44186.1602 - val_accuracy: 0.7226\n",
            "Epoch 202/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 30848.9297 - accuracy: 0.8164 - val_loss: 44186.6797 - val_accuracy: 0.7092\n",
            "Epoch 203/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 30931.0254 - accuracy: 0.8121 - val_loss: 44125.2539 - val_accuracy: 0.7065\n",
            "Epoch 204/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30980.3516 - accuracy: 0.8095 - val_loss: 44099.8711 - val_accuracy: 0.7111\n",
            "Epoch 205/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30727.4277 - accuracy: 0.8110 - val_loss: 44048.7148 - val_accuracy: 0.7173\n",
            "Epoch 206/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 30809.5586 - accuracy: 0.8086 - val_loss: 44060.9062 - val_accuracy: 0.6963\n",
            "Epoch 207/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30942.5703 - accuracy: 0.8013 - val_loss: 44060.3203 - val_accuracy: 0.6918\n",
            "Epoch 208/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30979.2871 - accuracy: 0.8065 - val_loss: 44070.0039 - val_accuracy: 0.7004\n",
            "Epoch 209/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30649.7012 - accuracy: 0.8087 - val_loss: 44057.6562 - val_accuracy: 0.7047\n",
            "Epoch 210/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 30779.7090 - accuracy: 0.8073roc-auc_val: 0.799\n",
            "136/136 [==============================] - 10s 70ms/step - loss: 30671.0898 - accuracy: 0.8068 - val_loss: 44010.0469 - val_accuracy: 0.7005\n",
            "Epoch 211/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30816.3438 - accuracy: 0.8065 - val_loss: 44076.5625 - val_accuracy: 0.6941\n",
            "Epoch 212/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30840.8574 - accuracy: 0.8020 - val_loss: 44225.9258 - val_accuracy: 0.7033\n",
            "Epoch 213/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 31042.7832 - accuracy: 0.8089 - val_loss: 44393.6875 - val_accuracy: 0.7191\n",
            "Epoch 214/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30591.8984 - accuracy: 0.8126 - val_loss: 44354.4336 - val_accuracy: 0.7159\n",
            "Epoch 215/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30794.7520 - accuracy: 0.8128 - val_loss: 44382.1211 - val_accuracy: 0.7138\n",
            "Epoch 216/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30791.2930 - accuracy: 0.8105 - val_loss: 44327.5156 - val_accuracy: 0.7074\n",
            "Epoch 217/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30581.2441 - accuracy: 0.8085 - val_loss: 44251.2461 - val_accuracy: 0.7048\n",
            "Epoch 218/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30785.4707 - accuracy: 0.8127 - val_loss: 44322.0117 - val_accuracy: 0.7204\n",
            "Epoch 219/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 30878.7461 - accuracy: 0.8155 - val_loss: 44315.9258 - val_accuracy: 0.7132\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 18ms/step - loss: 56289.9023 - accuracy: 0.6491 - val_loss: 54659.8945 - val_accuracy: 0.6369\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 49076.9609 - accuracy: 0.7119 - val_loss: 53089.9023 - val_accuracy: 0.6391\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 47974.3086 - accuracy: 0.7292 - val_loss: 54396.5898 - val_accuracy: 0.6265\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 45992.8555 - accuracy: 0.7433 - val_loss: 53328.1094 - val_accuracy: 0.6216\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 44252.1250 - accuracy: 0.7411 - val_loss: 51754.5195 - val_accuracy: 0.6428\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 43414.1328 - accuracy: 0.7322 - val_loss: 51785.9141 - val_accuracy: 0.6112\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 41790.0156 - accuracy: 0.7526 - val_loss: 51389.4023 - val_accuracy: 0.6161\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 41031.3242 - accuracy: 0.7539 - val_loss: 50342.0859 - val_accuracy: 0.7097\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 42426.3398 - accuracy: 0.7560 - val_loss: 50199.7656 - val_accuracy: 0.6651\n",
            "Epoch 10/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 42498.0664 - accuracy: 0.7602roc-auc_val: 0.7848\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 42449.0234 - accuracy: 0.7601 - val_loss: 50354.5469 - val_accuracy: 0.6575\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 41672.5312 - accuracy: 0.7427 - val_loss: 51034.5234 - val_accuracy: 0.6688\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 40572.3945 - accuracy: 0.7655 - val_loss: 51590.4141 - val_accuracy: 0.6655\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 40443.5391 - accuracy: 0.7781 - val_loss: 49141.4492 - val_accuracy: 0.6970\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 39075.4844 - accuracy: 0.7651 - val_loss: 49931.6133 - val_accuracy: 0.7192\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 39708.1016 - accuracy: 0.7798 - val_loss: 50476.7617 - val_accuracy: 0.6460\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 38583.3281 - accuracy: 0.7690 - val_loss: 49638.0664 - val_accuracy: 0.7186\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 38767.7344 - accuracy: 0.7804 - val_loss: 52249.1094 - val_accuracy: 0.7095\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 39589.2773 - accuracy: 0.7636 - val_loss: 50076.6016 - val_accuracy: 0.6709\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 39015.2617 - accuracy: 0.7769 - val_loss: 49627.1172 - val_accuracy: 0.6525\n",
            "Epoch 20/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 38310.0234 - accuracy: 0.7631roc-auc_val: 0.7867\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 37830.4844 - accuracy: 0.7642 - val_loss: 48972.4023 - val_accuracy: 0.6670\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 37036.9688 - accuracy: 0.7733 - val_loss: 49198.3477 - val_accuracy: 0.6736\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 36498.6445 - accuracy: 0.7805 - val_loss: 49321.2031 - val_accuracy: 0.6445\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 36244.8242 - accuracy: 0.7749 - val_loss: 48951.6055 - val_accuracy: 0.6749\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 36470.4336 - accuracy: 0.7723 - val_loss: 48787.8047 - val_accuracy: 0.6812\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 37349.1680 - accuracy: 0.7851 - val_loss: 49247.7461 - val_accuracy: 0.7371\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 36405.8164 - accuracy: 0.7767 - val_loss: 48596.8125 - val_accuracy: 0.6783\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 35807.1914 - accuracy: 0.7794 - val_loss: 49064.5742 - val_accuracy: 0.6824\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 35811.3203 - accuracy: 0.7807 - val_loss: 49198.5664 - val_accuracy: 0.6643\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 36255.0977 - accuracy: 0.7795 - val_loss: 49632.0703 - val_accuracy: 0.6756\n",
            "Epoch 30/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 35250.5977 - accuracy: 0.7751roc-auc_val: 0.7888\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 35582.3711 - accuracy: 0.7757 - val_loss: 48686.6758 - val_accuracy: 0.6835\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 36058.6523 - accuracy: 0.8103 - val_loss: 48666.7461 - val_accuracy: 0.6922\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 35513.9453 - accuracy: 0.7873 - val_loss: 49055.6758 - val_accuracy: 0.6769\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 35292.2969 - accuracy: 0.7885 - val_loss: 49346.8750 - val_accuracy: 0.6632\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 35671.4922 - accuracy: 0.7761 - val_loss: 48293.8008 - val_accuracy: 0.6780\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 35298.6133 - accuracy: 0.7822 - val_loss: 47946.9883 - val_accuracy: 0.7113\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 35295.5977 - accuracy: 0.7779 - val_loss: 48782.4102 - val_accuracy: 0.6656\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 35275.5781 - accuracy: 0.7763 - val_loss: 48629.1875 - val_accuracy: 0.6931\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 34997.4297 - accuracy: 0.7974 - val_loss: 49133.2852 - val_accuracy: 0.7461\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 34714.4258 - accuracy: 0.8022 - val_loss: 48053.2070 - val_accuracy: 0.6969\n",
            "Epoch 40/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 34271.0977 - accuracy: 0.7889roc-auc_val: 0.7904\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 34115.7461 - accuracy: 0.7861 - val_loss: 48623.1719 - val_accuracy: 0.6644\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 34747.3320 - accuracy: 0.7673 - val_loss: 48249.4922 - val_accuracy: 0.7113\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 34274.1289 - accuracy: 0.7790 - val_loss: 48849.9141 - val_accuracy: 0.7097\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 34185.9141 - accuracy: 0.7776 - val_loss: 48943.6484 - val_accuracy: 0.7095\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 34025.2383 - accuracy: 0.7952 - val_loss: 49286.8086 - val_accuracy: 0.7184\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 34369.8008 - accuracy: 0.7846 - val_loss: 49634.5742 - val_accuracy: 0.6776\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 33943.4414 - accuracy: 0.7847 - val_loss: 48759.3398 - val_accuracy: 0.7091\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 33601.2031 - accuracy: 0.7889 - val_loss: 49168.3555 - val_accuracy: 0.7067\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 33801.9805 - accuracy: 0.7941 - val_loss: 48684.8203 - val_accuracy: 0.6988\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 33516.7500 - accuracy: 0.7909 - val_loss: 48656.8867 - val_accuracy: 0.6991\n",
            "Epoch 50/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 33054.9961 - accuracy: 0.7988roc-auc_val: 0.7885\n",
            "88/88 [==============================] - 12s 142ms/step - loss: 33039.7266 - accuracy: 0.8003 - val_loss: 48433.1875 - val_accuracy: 0.7130\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32939.0742 - accuracy: 0.8037 - val_loss: 48230.4766 - val_accuracy: 0.7322\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 33441.1172 - accuracy: 0.8164 - val_loss: 47939.6719 - val_accuracy: 0.7167\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 33280.4883 - accuracy: 0.7969 - val_loss: 48223.7500 - val_accuracy: 0.7030\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32504.6152 - accuracy: 0.7988 - val_loss: 48107.9102 - val_accuracy: 0.7178\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32390.4883 - accuracy: 0.7963 - val_loss: 48051.2695 - val_accuracy: 0.7075\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32413.1543 - accuracy: 0.8038 - val_loss: 48223.3633 - val_accuracy: 0.7374\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32358.2383 - accuracy: 0.8037 - val_loss: 48378.1992 - val_accuracy: 0.7161\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32721.7539 - accuracy: 0.7903 - val_loss: 48279.5078 - val_accuracy: 0.7188\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32740.5215 - accuracy: 0.7898 - val_loss: 48581.1133 - val_accuracy: 0.7334\n",
            "Epoch 60/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 32958.7070 - accuracy: 0.8080roc-auc_val: 0.7891\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 32780.4141 - accuracy: 0.8090 - val_loss: 48655.9492 - val_accuracy: 0.7424\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32649.8496 - accuracy: 0.8000 - val_loss: 48533.6328 - val_accuracy: 0.6857\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32747.3418 - accuracy: 0.7827 - val_loss: 48014.8281 - val_accuracy: 0.7046\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32654.5430 - accuracy: 0.8000 - val_loss: 47864.2734 - val_accuracy: 0.7173\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32308.9297 - accuracy: 0.7988 - val_loss: 47584.5859 - val_accuracy: 0.7219\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32073.2812 - accuracy: 0.7939 - val_loss: 47692.1211 - val_accuracy: 0.7148\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32156.7715 - accuracy: 0.8009 - val_loss: 47789.2305 - val_accuracy: 0.7320\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31744.0371 - accuracy: 0.8053 - val_loss: 47652.7734 - val_accuracy: 0.7275\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 32068.8750 - accuracy: 0.8003 - val_loss: 47675.8438 - val_accuracy: 0.7351\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31962.2207 - accuracy: 0.8032 - val_loss: 47756.9141 - val_accuracy: 0.7277\n",
            "Epoch 70/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 31706.2188 - accuracy: 0.8068roc-auc_val: 0.7937\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 31616.8281 - accuracy: 0.8068 - val_loss: 47485.0078 - val_accuracy: 0.7289\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31838.7344 - accuracy: 0.8139 - val_loss: 47591.4023 - val_accuracy: 0.7360\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31374.7539 - accuracy: 0.8067 - val_loss: 47468.8203 - val_accuracy: 0.7256\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31835.9473 - accuracy: 0.8096 - val_loss: 47589.1875 - val_accuracy: 0.7457\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31241.9414 - accuracy: 0.7960 - val_loss: 47133.3242 - val_accuracy: 0.7204\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31459.9277 - accuracy: 0.7980 - val_loss: 47278.2461 - val_accuracy: 0.7136\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31456.3438 - accuracy: 0.8034 - val_loss: 47508.2422 - val_accuracy: 0.7267\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31402.7734 - accuracy: 0.8052 - val_loss: 47765.9180 - val_accuracy: 0.7124\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 31566.7871 - accuracy: 0.7917 - val_loss: 48297.2148 - val_accuracy: 0.7288\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 12ms/step - loss: 31373.8496 - accuracy: 0.8058 - val_loss: 47826.2461 - val_accuracy: 0.7337\n",
            "Epoch 80/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 31147.2188 - accuracy: 0.8047roc-auc_val: 0.7936\n",
            "88/88 [==============================] - 13s 147ms/step - loss: 31147.2188 - accuracy: 0.8047 - val_loss: 47804.4297 - val_accuracy: 0.7391\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 31353.0898 - accuracy: 0.8069 - val_loss: 47734.0195 - val_accuracy: 0.7328\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31151.8711 - accuracy: 0.8049 - val_loss: 47645.8164 - val_accuracy: 0.7413\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31016.5977 - accuracy: 0.8032 - val_loss: 47943.1680 - val_accuracy: 0.7520\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31204.6934 - accuracy: 0.8127 - val_loss: 47796.9180 - val_accuracy: 0.7653\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31108.6523 - accuracy: 0.8041 - val_loss: 47326.2422 - val_accuracy: 0.7480\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30678.1250 - accuracy: 0.8076 - val_loss: 47323.5117 - val_accuracy: 0.7476\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31009.9961 - accuracy: 0.8050 - val_loss: 47259.6484 - val_accuracy: 0.7409\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 31007.3379 - accuracy: 0.8104 - val_loss: 47675.4023 - val_accuracy: 0.7368\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30663.6602 - accuracy: 0.8071 - val_loss: 47378.6797 - val_accuracy: 0.7391\n",
            "Epoch 90/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 30580.5352 - accuracy: 0.8100roc-auc_val: 0.7948\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 30470.6035 - accuracy: 0.8094 - val_loss: 47549.0742 - val_accuracy: 0.7400\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30560.2637 - accuracy: 0.8032 - val_loss: 47759.4375 - val_accuracy: 0.7412\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30031.5996 - accuracy: 0.8100 - val_loss: 48021.9609 - val_accuracy: 0.7484\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30261.1211 - accuracy: 0.8156 - val_loss: 48924.5391 - val_accuracy: 0.7867\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30650.3965 - accuracy: 0.8213 - val_loss: 48131.1094 - val_accuracy: 0.7597\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30167.8965 - accuracy: 0.8094 - val_loss: 48102.3359 - val_accuracy: 0.7528\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30336.6582 - accuracy: 0.8110 - val_loss: 48182.4961 - val_accuracy: 0.7626\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30279.6074 - accuracy: 0.8139 - val_loss: 48269.7031 - val_accuracy: 0.7643\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30004.1211 - accuracy: 0.8143 - val_loss: 48120.6172 - val_accuracy: 0.7671\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30329.1426 - accuracy: 0.8168 - val_loss: 48321.7148 - val_accuracy: 0.7673\n",
            "Epoch 100/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 30078.3301 - accuracy: 0.8175roc-auc_val: 0.7933\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 30088.0410 - accuracy: 0.8182 - val_loss: 48281.2812 - val_accuracy: 0.7713\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 30067.2969 - accuracy: 0.8174 - val_loss: 48321.6484 - val_accuracy: 0.7734\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29863.1367 - accuracy: 0.8167 - val_loss: 48134.5820 - val_accuracy: 0.7597\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29689.2168 - accuracy: 0.8112 - val_loss: 47978.9922 - val_accuracy: 0.7590\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29844.7305 - accuracy: 0.8104 - val_loss: 48104.3203 - val_accuracy: 0.7569\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29969.3906 - accuracy: 0.8174 - val_loss: 48219.8320 - val_accuracy: 0.7544\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29902.2012 - accuracy: 0.8241 - val_loss: 48761.3828 - val_accuracy: 0.7860\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29788.4395 - accuracy: 0.8246 - val_loss: 48062.8672 - val_accuracy: 0.7633\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29503.1230 - accuracy: 0.8133 - val_loss: 47815.2656 - val_accuracy: 0.7524\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29595.1094 - accuracy: 0.8205 - val_loss: 47729.3633 - val_accuracy: 0.7578\n",
            "Epoch 110/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 29784.9805 - accuracy: 0.8096roc-auc_val: 0.7951\n",
            "88/88 [==============================] - 12s 139ms/step - loss: 29707.0938 - accuracy: 0.8098 - val_loss: 47556.6836 - val_accuracy: 0.7458\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29674.6934 - accuracy: 0.8172 - val_loss: 47962.3164 - val_accuracy: 0.7693\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29838.1699 - accuracy: 0.8225 - val_loss: 47775.0469 - val_accuracy: 0.7473\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29445.0723 - accuracy: 0.8129 - val_loss: 47833.5312 - val_accuracy: 0.7535\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29608.2441 - accuracy: 0.8141 - val_loss: 47803.6953 - val_accuracy: 0.7513\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29306.9902 - accuracy: 0.8168 - val_loss: 47840.0898 - val_accuracy: 0.7633\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29277.2246 - accuracy: 0.8210 - val_loss: 47822.8828 - val_accuracy: 0.7625\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29345.0137 - accuracy: 0.8236 - val_loss: 47661.3594 - val_accuracy: 0.7641\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29391.6094 - accuracy: 0.8145 - val_loss: 47694.2266 - val_accuracy: 0.7600\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29151.9531 - accuracy: 0.8039 - val_loss: 47465.1523 - val_accuracy: 0.7473\n",
            "Epoch 120/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 29497.0078 - accuracy: 0.8088roc-auc_val: 0.7958\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 29446.2754 - accuracy: 0.8087 - val_loss: 47576.0508 - val_accuracy: 0.7528\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 29250.6191 - accuracy: 0.8178 - val_loss: 48559.3281 - val_accuracy: 0.7858\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29156.7656 - accuracy: 0.8292 - val_loss: 48478.9453 - val_accuracy: 0.7759\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 28990.0371 - accuracy: 0.8175 - val_loss: 48224.6953 - val_accuracy: 0.7577\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 29010.7637 - accuracy: 0.8119 - val_loss: 48242.1172 - val_accuracy: 0.7489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L3_TlYw5nac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f4e4614-fd04-432e-f8f1-686d572d3f3b"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xcdZ3/8ddnJremSdo0SUtvaaEthXJpgdhyVRCo2HWpq6jFRUHxV2+st3V/st5w8ffbxetvVVywKgqusIgKVEUuooJoKaRQ2tJyaUtpk97SW9JL0mRmPr8/5qSdhkkzmZkkw+n7+XjMY87lO+d8v6S8z5nv95wz5u6IiEh4RYa6AiIiMrAU9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnJFfRUws4nAHcAYwIFF7v4dMxsF3A1MBjYA73b33Wk+fzXwxWD2/7j77X3ts7a21idPnpxhE0REZNmyZTvcvS7dOuvrOnozGwuMdfdnzKwSWAa8HbgG2OXuN5nZ9UC1u3+ux2dHAY1AA8mDxDLgrHQHhFQNDQ3e2NiYUeNERATMbJm7N6Rb12fXjbtvcfdngum9wBpgPDAf6D47v51k+Pf0FuARd98VhPsjwGX9b4KIiGSrX330ZjYZOANYCoxx9y3Bqq0ku3Z6Gg9sSplvCpaJiMggyTjozawC+BXwKXdvS13nyf6fnJ6lYGYLzazRzBpbWlpy2ZSIiKTIKOjNrJhkyP/c3X8dLN4W9N939+NvT/PRZmBiyvyEYNlruPsid29w94a6urTjCSIikoU+g97MDPgxsMbdv52yajFwdTB9NXB/mo8/BMw1s2ozqwbmBstERGSQZHJGfx7wPuDNZrY8eM0DbgIuNbOXgUuCecyswcx+BODuu4CvAk8HrxuDZSIiMkj6vLxyKOjyShGR/snp8koREXl9U9CLiIRcn49AELhz6cbXLHvvnPohqImISP/pjF5EJOQU9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iEnIJeRCTkFPQiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJyCnoRkZDr83n0ZnYb8DZgu7ufGiy7G5geFBkJ7HH3WWk+uwHYC8SBWG8/cyUiIgMnkx8e+SlwM3BH9wJ3f0/3tJl9C2g9yucvcvcd2VZQRERy02fQu/vjZjY53TozM+DdwJvzWy0REcmXXPvoLwC2ufvLvax34GEzW2ZmC3Pcl4iIZCHX34y9ErjrKOvPd/dmMxsNPGJmL7j74+kKBgeChQD19fo9VhGRfMn6jN7MioB3AHf3Vsbdm4P37cC9wOyjlF3k7g3u3lBXV5dttUREpIdcum4uAV5w96Z0K81suJlVdk8Dc4FVOexPRESy0GfQm9ldwBJgupk1mdm1waoF9Oi2MbNxZvZAMDsGeMLMngOeAn7n7g/mr+oiIpKJTK66ubKX5dekWbYZmBdMrwdm5lg/ERHJke6MFREJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iEnIJeRCTkFPQiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJyCnoRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMhl8puxt5nZdjNblbLsK2bWbGbLg9e8Xj57mZm9aGZrzez6fFZcREQyk8kZ/U+By9Is/3/uPit4PdBzpZlFge8DbwVmAFea2YxcKisiIv3XZ9C7++PAriy2PRtY6+7r3b0T+B9gfhbbERGRHOTSR3+dma0Iunaq06wfD2xKmW8KlqVlZgvNrNHMGltaWnKoloiIpMo26G8BpgCzgC3At3KtiLsvcvcGd2+oq6vLdXMiIhLIKujdfZu7x909AfyQZDdNT83AxJT5CcEyEREZRFkFvZmNTZn9B2BVmmJPA9PM7HgzKwEWAIuz2Z+IiGSvqK8CZnYXcCFQa2ZNwA3AhWY2C3BgA/DhoOw44EfuPs/dY2Z2HfAQEAVuc/fnB6QVIiLSqz6D3t2vTLP4x72U3QzMS5l/AHjNpZciIjJ4dGesiEjIKehFREJOQS8iEnIKehGRkFPQi4iEnIJeRCTkFPQiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJyCnoRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMj1GfRmdpuZbTezVSnLvmFmL5jZCjO718xG9vLZDWa20syWm1ljPisuIiKZyeSM/qfAZT2WPQKc6u6nAy8B/3qUz1/k7rPcvSG7KoqISC76DHp3fxzY1WPZw+4eC2afBCYMQN1ERCQP8tFH/0Hg972sc+BhM1tmZgvzsC8REemnolw+bGZfAGLAz3spcr67N5vZaOARM3sh+IaQblsLgYUA9fX1uVRLRERSZH1Gb2bXAG8D/tHdPV0Zd28O3rcD9wKze9ueuy9y9wZ3b6irq8u2WiIi0kNWQW9mlwH/G7jc3Q/0Uma4mVV2TwNzgVXpyoqIyMDJ5PLKu4AlwHQzazKza4GbgUqS3THLzezWoOw4M3sg+OgY4Akzew54Cviduz84IK0QEZFe9dlH7+5Xpln8417KbgbmBdPrgZk51U5ERHKmO2NFREJOQS8iEnIKehGRkFPQi4iEnIJeRCTkFPQiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJyCnoRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyISchkFvZndZmbbzWxVyrJRZvaImb0cvFf38tmrgzIvm9nV+aq4iIhkJtMz+p8Cl/VYdj3wqLtPAx4N5o9gZqOAG4A5wGzght4OCCIiMjAyCnp3fxzY1WPxfOD2YPp24O1pPvoW4BF33+Xuu4FHeO0BQ0REBlAuffRj3H1LML0VGJOmzHhgU8p8U7BMREQGSV4GY93dAc9lG2a20MwazayxpaUlH9USERFyC/ptZjYWIHjfnqZMMzAxZX5CsOw13H2Ruze4e0NdXV0O1RIRkVS5BP1ioPsqmquB+9OUeQiYa2bVwSDs3GCZiIgMkkwvr7wLWAJMN7MmM7sWuAm41MxeBi4J5jGzBjP7EYC77wK+CjwdvG4MlomIyCApyqSQu1/Zy6qL05RtBD6UMn8bcFtWtRMRkZzpzlgRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iEXNZBb2bTzWx5yqvNzD7Vo8yFZtaaUubLuVdZRET6I6PfjE3H3V8EZgGYWRRoBu5NU/Qv7v62bPcjIiK5yVfXzcXAOnd/NU/bExGRPMlX0C8A7upl3Tlm9pyZ/d7MTsnT/kREJEM5B72ZlQCXA/ekWf0MMMndZwLfA+47ynYWmlmjmTW2tLTkWi0REQnk44z+rcAz7r6t5wp3b3P3fcH0A0CxmdWm24i7L3L3BndvqKury0O1REQE8hP0V9JLt42ZHWdmFkzPDva3Mw/7FBGRDGV91Q2AmQ0HLgU+nLLsIwDufitwBfBRM4sB7cACd/dc9ikiIv2TU9C7+36gpseyW1OmbwZuzmUfIiKSG90ZKyIScgp6EZGQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjIKegzFE84r+zYT0dXfKirIiLSLzndMHUsaOvoYvFzzaxoauVAZ5wzJo7kXQ0Th7paIiIZ0xl9HxY9tp6l63cxpa6CGWOreK5pD7v2dw51tUREMqYz+qNwd+5/rpmpoyu4cnY9be1dvLRtL4+9tJ3r3jx1qKsnIoPkzqUbX7PsvXPqh6Am2dEZ/VE8s3EPm3a1M3PiSACqhhVz1qRqnnl1D5v3tA9x7UREMqOgP4rFy5spLYowY2zVoWVvPLEOx/nBY+uGsGYiIplT0PciFk/w2xVbuOTkMZQVRw8try4vYdbEkfxyWROdscQQ1lBEJDMK+l78dd1Odu7vZP6sca9Zd/LYKvZ3xnl24+4hqJmISP8o6Htx//JmqsqKeNP01/6s4Qm1FUQMnli7YwhqJiLSPwr6NNydJ17ewUUnjaa0KPqa9cNKosycOJK/vKygF5HCp6BPY0trB9v3HuSM4GqbdC6YWsuKpj20HugaxJqJiPRfzkFvZhvMbKWZLTezxjTrzcy+a2ZrzWyFmZ2Z6z4H2vJNewCYVV/da5nzp9WRcFiyXr91LiKFLV9n9Be5+yx3b0iz7q3AtOC1ELglT/scMMs37aEkGuHksZW9ljmjfiTDS6I8sbZlEGsmItJ/g9F1Mx+4w5OeBEaa2dhB2G/Wlm/cw4xxVWn757sVRyPMOaGGJ9RPLyIFLh9B78DDZrbMzBamWT8e2JQy3xQsO4KZLTSzRjNrbGkZurPkWDzByuZWZh2lf77b+VNr2bDzAJt2HRiEmomIZCcfQX++u59Jsovm42b2xmw24u6L3L3B3Rvq6l57SeNgeWnbPtq74pkF/bRaQP30IlLYcg56d28O3rcD9wKzexRpBlKf6zshWFaQDg3EZhD0U+sqGDGsmGUbdOOUiBSunILezIabWWX3NDAXWNWj2GLg/cHVN2cDre6+JZf9DqTlm3ZTXV7MpJryPstGIsZZk6ppfHXXINRMRCQ7uT6meAxwr5l1b+tOd3/QzD4C4O63Ag8A84C1wAHgAznuc0At37SHmRNHErSpT2dNquaPL2xn1/5ORg0vGeDaiYj0X05B7+7rgZlplt+aMu3Ax3PZz2DZ29HFy9v3Me+0zC8KapiUvNZ+2au7uXTGmIGqmohI1nRnbIrnN7fhDjMn9N0/323mxJEUR03dNyJSsBT0KdZsaQPglHFVfZQ8rKw4yinjRmhAVkQKloI+xerNbdQML6GusrRfn2uYVM2K5lYOxuIDVDMRkewp6FOs2drGjHFVGQ/EdmuYPIrOWIJVza0DVDMRkewp6ANd8QQvbd13xM8GZuqsYEC2Ud03IlKAFPSBdS376IwnODmLoK+rLGVyTTlPK+hFpAAp6APdA7Ez+jEQm2r28aN4esMuEgnPZ7VERHKmoA+s3txGSVGEE2qHZ/X5OcfX0NrexQtb9+a5ZiIiuVHQB9Zs2cv0MZUURbP7TzLnhFEALH1FDzgTkcKioCf5G7Grt7RlNRDbbUJ1OROqh7F0vW6cEpHCoqAHtrUdZNf+zqz757vNOb6Gpa/sVD+9iBQUBT2HB2KzueIm1ZwTRrH7QPJ5OSIihUJBD6wOgv6ko/xGbCbOOaEGUD+9iBQWBT2wqrmV+lHlVJUV57SdCdXDGDeiTP30IlJQFPTAyuZWTpswIuftmBlzTkj20yefziwiMvSO+aDfvb+Tpt3tnDY+96CHZPfNjn2drNmi6+lFpDAc80G/MngQWb6C/qKTRmMGf1izLS/bExHJVdZBb2YTzexPZrbazJ43s0+mKXOhmbWa2fLg9eXcqpt/3UF/6rj8BH1dZSlnTBzJI6sV9CJSGHI5o48B/+zuM4CzgY+b2Yw05f7i7rOC14057G9ArGxqZVJNOSPKcxuITXXpjONY2dzKltb2vG1TRCRbWQe9u29x92eC6b3AGmB8vio2WFY2t+at26bbpTNGA/AHndWLSAHI6cfBu5nZZOAMYGma1eeY2XPAZuCz7v58PvaZD7v2d9K8p533nzOp35+9c+nGtMvfO6eeKXUVHF87nIdXb+N950zOsZYiIrnJeTDWzCqAXwGfcve2HqufASa5+0zge8B9R9nOQjNrNLPGlpaWXKuVkXwPxHYzMy6dMYYn1++kraMrr9sWEemvnILezIpJhvzP3f3XPde7e5u77wumHwCKzaw23bbcfZG7N7h7Q11dXS7Vylj3T/+dkuegB7h0xhi64s5jLw7OQUtEpDe5XHVjwI+BNe7+7V7KHBeUw8xmB/srmOcDrGjaw+SackYMy99AbLcz66s5rqqMXzRuyvu2RUT6I5c++vOA9wErzWx5sOzzQD2Au98KXAF81MxiQDuwwAvoltFVzW2cUT9yQLYdjRhXnV3PNx9+ibXb9zF1dMWA7EdEpC9ZB727PwFYH2VuBm7Odh8DqXlPO8172vng+ccP2D4WzK7nu4+u5Y4lG7hx/qkDth8RkaM5Zu+MXbIu2YPU/cTJgVBbUcrfzxzHL5c1aVBWRIbMMR301eXFnHRcbo8m7ss1507mQGecexqbBnQ/IjI4trV18Ne1O3hp2+vneVZ5uY7+9cbdWbJuB+dMqSESOWrvU85OmzCCsyZVc9sTr/De2fUMK4kO6P5EZGCsa9nHg6u20rwnecf771Zu4S2njOGzc6czbczAnjDm6pg8o9+46wCbWzsGtNsm1WfnTqd5TzvfefTlQdmfiOTXqzv3c8eSDXR0xfm708by6UtO5BMXT2PJup28+wdL2LjzwFBX8aiOyaD/W3f//JS0l/Tn3TlTanh3wwR++Jf1PL+5dVD2KSL5sWZLG7cv2cCIYcV8+E1TOG9qLXWVpXzm0hNZfN35JByuvf3pgh6HOyaDfsm6ndRVljKlbnjet33n0o2veQF8ft7JVJcXc/2vVhKLJ/K+XxHJv937O7nmJ09RWhTlA+cdT0Xpkb3dk2uHc8tVZ/LKjv38053PEk8UzNXjRzjmgt7d+du6nZw7pYbgXq5BMbK8hK9cfgorm1v5/L0rSRToPwgROexL969i1/5O3nf2JKrLS9KWOXdKLTdcfgqPvdTCz5e+Osg1zMwxF/TrWvaxY99Bzp0yOP3zqd52+jg+cfE0ftHYxBfuW6WwFylgv3luM79dsYVPXjyNcSOHHbXsVXPquWBaLV9/8EW2tnYMUg0zd8wF/aNrtgPJo/BQ+PQl0/jYhVO466mNfOYXywu6X0/kWLW9rYMv3b+KmRNH8pE3TemzvJnxf99+GrFEghsWrxqEGvbPMXV5pbtzz7ImzqwfycRR5YO2356PNB4/chifufRE/vMPL7H0lV38+ztO46LpowetPiLSO3fnX3+9kvbOON9610yKopmdD9fXlPPJi0/kaw++wIOrtnLZqccNcE0zd0yd0S/ftIe12/fxroaJQ1oPM+MTF0/j1x87j+GlRXzgJ0/zjv/6Kw89v7VgB3NEjhX3NDbx6Avb+dxlJ/X7GVUfuuB4Tjqukq8sfp69BfRt/ZgK+nuWNVFWHOFtp48d6qpw59KNrN7cxvvOnsTfnz6WV3bs58M/W8acf3+UL923iide3kFHV3yoqylyTGnafYAbf7uas08YxTXnTu7354ujEW565+ls29vBNx96Mf8VzNIx03XT3hnnN8s3M+/UsVSW5f+xxNkqjkY4Z0ots4+voa6yhN88t4V7lm3iZ0++Skk0wqz6kZw+fgQnj61ixrgqpo6uoDjDr5IikrnOWIJP3518EO83rpiZ9V3zsyaO5OpzJnP7kg3MP2M8Z9ZX57GW2Tlmgv6h57ey92CMKxomDHVV0opGjF37uzhvai1vmDyKV3bsZ33LPl7ZuZ/nNu3hYCx57X1JNMIJdcOZXDOc+ppyJo4qp35UOeNHDqOuspSqsqJBvWxUJAzcnRsWP8/TG3bznQWzch7D++xbpvPQ81u5/lcrWHzd+ZQVD+2jT46JoE8knNuXbGBC9TDOPn7wL6vsr5KiCNOPq2R68MC1eMLZue8gW1o72NLazra2g6xt2cefXtx+6ADQrbQoQl1lafJVUcroqlJqhpcyYlgxVcOKqSororKsmKphRVSVFVNVVkxZSYSSaOSYPUAkEk5nPMHBWILOWIKuePK9M56cjpgRjRgRg4jZ4fmIUVYUYVhJlLKi6IA/N0kGzs+efJW7ntrIRy+cwvxZ43PeXkVpETe983Su+clTfPn+VXz9ipl5qGX2jomgv/OpjTy7cQ9ff+fpr8v/GaMRY3RVGaOrypg58fAPpSTc2dcRY9f+Tva0d7Gvo4u9B2Ps7Yixtz3G5j3t7O2IcaCz775+s+RBoqw4GVplxRFKu9+Lo5QWRYhGjKKIURSJEI0mpw8ti0aOmI+YkXAn4cl6evDevczdSSQ4PE+yjLvjkJwmmD9i/eHpvjjJr+MdXXEOxpJBfvDQdJyOruR7Vzw/A+BlxRGGFUeTr5LgVRylorSI6uEljCovSb4PL6G6PPk+aniJvokNsV88vYl/+81qLjl5NP8yd3retvumE+u47qKpfO+Pa2mYPIp3D+FFIKEP+i2t7dz0+xc4b2oN7yrQbptsRcySZ+l9/BRiPOGHgq2jK057V5yOQ6/kWWtX3InFE3QlktNd8QSxuNPeFaetI0YsnjgU2okjQtqJJzztuogZZsmDiBFMk7zq6IhpgJT5YDb53mN9cllye5koiiYPTMXR5MGooqyIkZEIRVGjuPsAFZQ5fCBLHrC6X90Hne4DlqccuLoSTlfK2X/y24Afmt7Z2cnmPR0c6Iyx/2Cczl4ef9H9TWx0ZSmjK8sOTddVJr+VdS+rGV6S8eV+cnSJhPPNh1/kv/68jgum1fKfC87I+4ngpy45kWWv7uZL961iSl0FZ00amv76UAe9u/Ol+1YRSyT4j384/Zg9Y4pGjPKSInq5g1sGUVc8wYHOOPsPJr9p7TsYS34T64ix92CM1vYumnYnv4m1p7nqygxqhpceeSA4NF0WHBRKGVleQkVpEdHX4TfYwbDs1d187cEXeOqVXVw5eyI3zj91QC5yiEaM7155Blfc8jeu+tFSbrnqTC4cgntmcgp6M7sM+A4QBX7k7jf1WF8K3AGcRfJHwd/j7hty2Wem2jvjXP/rFfxhzXa++HcnU18zeDdIifSmOBphxLBIRj9IH4sn2Hswxr6OGHtTu+WCA8Pa7ft4duNuDnTGifVy/8XwkigVwbhMZVkRFaXJsZmK0iIqyoooT+liKi+JUlYcpbyk6HD3U7A8tSvq9XjVl7uzubWDP7+4nQdXbeUvL++gtqKU/3jHaSx4w8QBPQmsrSjlno+cyzU/eYoP3d7Iv80/hQVvqB/Ug3DWQW9mUeD7wKVAE/C0mS1299Upxa4Fdrv7VDNbAHwNeE8uFe6Lu7OyuZXrf7WSNVvb+OzcE/ngeQP3u7AiA6UoGqG6vKTXh2l1S7hzoDPO3o6u4KAQ40DQNXewK05HMDaxtyNGy96DHOxK0BGLczDotuvvCEVx1CgtilJSlBzELymKHJouLopQ2mPZoemi7kF/iAYD2mZGNNJjkNsgEozzRIxD4zHdYzfJ6SPHaTyYiSX8iG9M+w/G2NbWwSs79tPWEQNgQvWwZC6cfzzlJUdGYM+72POlrrKUuxaezUf/exlfuHcVP39yI59760mcP7V2UAI/lzP62cBad18PYGb/A8wHUoN+PvCVYPqXwM1mZu6ZDKX1T3tnnFseW8dvn9vM+h37qSor4rZr3qBHC0joRcySZ+ilRTCif591d2IJP+Jqo664HxpzOLQ8njg0FtEZc+KJBLFE8rPxRHJ8J55wDhyM0da9LJEI1nXPJ9+dwwPynjJYn69QKI4aJUXJCwhKohEqSos4eWxV8GjyCkZXlmJm3Pfs5jztMTNVZcX897Vz+M2KLXzt9y9w9W1PMWJYMRdMq2Xa6EpqKkoYXVnK3FPy/+iEXIJ+PLApZb4JmNNbGXePmVkrUAPsyGG/aZUWRfjVsiYm1ZSz8I0ncNmpxzFSndIiR2VmFEetILpjuq+4OjzoHZyp9xh8T+1lsZSJ7gH/SAGPxZkZl88cx9wZY/jDmm089mILf3l5B79dsQVInvkXWtDnlZktBBYGs/vMLKv7h/8G3JW3Wh1SywAcnIZQ2NoD4WtT2NoDIWvTPw5Ae14F7ItZf3xSbytyCfpmIPXC0AnBsnRlmsysiOQXy53pNubui4BFOdRnwJhZo7s3DHU98iVs7YHwtSls7YHwten11J5cvq89DUwzs+PNrARYACzuUWYxcHUwfQXwx4HonxcRkd5lfUYf9LlfBzxE8vLK29z9eTO7EWh098XAj4GfmdlaYBfJg4GIiAyinPro3f0B4IEey76cMt0BvCuXfRSIguxSykHY2gPha1PY2gPha9Prpj2mnhQRkXAb+muqRERkQCnoU5jZZWb2opmtNbPr06wvNbO7g/VLzWzy4Ncycxm05zNmttrMVpjZo2bW6+VZhaKvNqWUe6eZuZkV9FURmbTHzN4d/J2eN7M7B7uO/ZHBv7l6M/uTmT0b/LubNxT1zJSZ3WZm280s7S9+W9J3g/auMLMzB7uOGUnenaYXyQHldcAJQAnwHDCjR5mPAbcG0wuAu4e63jm25yKgPJj+aCG3J9M2BeUqgceBJ4GGoa53jn+jacCzQHUwP3qo651jexYBHw2mZwAbhrrefbTpjcCZwKpe1s8Dfk/y3q2zgaVDXed0L53RH3bokQ7u3gl0P9Ih1Xzg9mD6l8DFVriPxOyzPe7+J3c/EMw+SfJeiEKWyd8I4Kskn6vUMZiVy0Im7flfwPfdfTeAu28f5Dr2RybtcaAqmB4BDO5zCPrJ3R8necVgb+YDd3jSk8BIMxv6H6XuQUF/WLpHOvT8qZkjHukAdD/SoRBl0p5U15I8MylkfbYp+Oo80d1/N5gVy1Imf6MTgRPN7K9m9mTwxNhClUl7vgJcZWZNJK/Y+6fBqdqA6e//Z0OiYB6BIEPHzK4CGoA3DXVdcmFmEeDbwDVDXJV8KiLZfXMhyW9cj5vZae6+Z0hrlb0rgZ+6+7fM7ByS99mc6u7pf5FF8kJn9If155EO9PVIhwKQSXsws0uALwCXu/vBQapbtvpqUyVwKvBnM9tAss90cQEPyGbyN2oCFrt7l7u/ArxEMvgLUSbtuRb4BYC7LwHKSD4z5vUqo//PhpqC/rCwPdKhz/aY2RnAD0iGfCH3/XY7apvcvdXda919srtPJjnucLm7Nw5NdfuUyb+5+0iezWNmtSS7ctYPZiX7IZP2bAQuBjCzk0kGfcug1jK/FgPvD66+ORtodfctQ12pntR1E/CQPdIhw/Z8A6gA7gnGlDe6++VDVuk+ZNim140M2/MQMNfMVgNx4F/cvSC/RWbYnn8GfmhmnyY5MHtNAZ8sYWZ3kTzQ1gbjCjcAxQDufivJcYZ5wFrgAPCBoanp0enOWBGRkLzOKccAAAAySURBVFPXjYhIyCnoRURCTkEvIhJyCnoRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQm5/w9Q/zPtVLOfPQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5PePWLxWIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
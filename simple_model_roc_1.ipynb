{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_1",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "894813cf-931d-4133-e101-21e9882ce3cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "c8d41742-a8f6-495d-fbdf-4ff521047d14"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 94% 49.0M/52.2M [00:00<00:00, 45.4MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 71.5MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 106MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 161MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 84% 49.0M/58.3M [00:00<00:00, 59.6MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 60.2MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 108MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv80v8W_Ko2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "7bba2fcf-e0a2-43e5-b726-fd03829d4e5b"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jt2pcxPije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "575d7cb3-35de-4372-dc1e-6e1690e857ed"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvEaxp9jhbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "229b64c2-fd90-4fc9-bc23-0bec0b915304"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2a311d73-a5ea-49e2-9505-51bbd7ae0210"
      },
      "source": [
        "\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkLEBcl6LHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 0.5\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRW4vyPJtU5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51413dea-a748-45b4-9940-6ece18d3257e"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "# trn=trn.drop(['isFraud_isna'],1)\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Adam(0.001,decay=1e-3),loss=rac,metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 34161.8086 - accuracy: 0.6995 - val_loss: 29264.3887 - val_accuracy: 0.5810\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28528.4473 - accuracy: 0.7467 - val_loss: 26094.0996 - val_accuracy: 0.6413\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 26166.5684 - accuracy: 0.7674 - val_loss: 24954.8008 - val_accuracy: 0.6587\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 24763.5332 - accuracy: 0.7800 - val_loss: 24026.0547 - val_accuracy: 0.6807\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 23142.5820 - accuracy: 0.7946 - val_loss: 23806.3945 - val_accuracy: 0.6858\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 21913.3438 - accuracy: 0.8086 - val_loss: 22553.4141 - val_accuracy: 0.7552\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 21035.1289 - accuracy: 0.8138 - val_loss: 22650.6973 - val_accuracy: 0.7630\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 20130.7832 - accuracy: 0.8227 - val_loss: 22005.5410 - val_accuracy: 0.7584\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 19507.3984 - accuracy: 0.8288 - val_loss: 21375.7402 - val_accuracy: 0.7780\n",
            "Epoch 10/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 19095.3535 - accuracy: 0.8332roc-auc_val: 0.8065\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 19041.7051 - accuracy: 0.8333 - val_loss: 21356.9219 - val_accuracy: 0.7771\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 18627.4688 - accuracy: 0.8349 - val_loss: 21058.8809 - val_accuracy: 0.7811\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 18196.1367 - accuracy: 0.8396 - val_loss: 21163.1855 - val_accuracy: 0.7852\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 17826.5352 - accuracy: 0.8430 - val_loss: 21414.1953 - val_accuracy: 0.7748\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 17553.1523 - accuracy: 0.8452 - val_loss: 21313.2266 - val_accuracy: 0.7805\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 17338.7617 - accuracy: 0.8478 - val_loss: 21008.5957 - val_accuracy: 0.7768\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 16948.2637 - accuracy: 0.8501 - val_loss: 20943.6523 - val_accuracy: 0.8037\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 16717.5918 - accuracy: 0.8515 - val_loss: 20980.9609 - val_accuracy: 0.7979\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 16450.7129 - accuracy: 0.8510 - val_loss: 20881.4180 - val_accuracy: 0.7766\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 16374.3369 - accuracy: 0.8561 - val_loss: 20737.5137 - val_accuracy: 0.8034\n",
            "Epoch 20/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 16062.6162 - accuracy: 0.8569roc-auc_val: 0.8128\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 16067.0078 - accuracy: 0.8567 - val_loss: 20903.4824 - val_accuracy: 0.8046\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 15816.6846 - accuracy: 0.8564 - val_loss: 20741.4180 - val_accuracy: 0.8156\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 15747.7529 - accuracy: 0.8560 - val_loss: 20445.9375 - val_accuracy: 0.8171\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 15508.0293 - accuracy: 0.8613 - val_loss: 20682.2402 - val_accuracy: 0.8148\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 15389.9482 - accuracy: 0.8629 - val_loss: 20444.1094 - val_accuracy: 0.8366\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 15258.6523 - accuracy: 0.8604 - val_loss: 20438.0137 - val_accuracy: 0.8264\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 15220.7412 - accuracy: 0.8611 - val_loss: 20243.2578 - val_accuracy: 0.8370\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14844.8906 - accuracy: 0.8641 - val_loss: 20272.7891 - val_accuracy: 0.8523\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14900.8223 - accuracy: 0.8650 - val_loss: 20203.9707 - val_accuracy: 0.8267\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14747.2520 - accuracy: 0.8620 - val_loss: 20774.8145 - val_accuracy: 0.8195\n",
            "Epoch 30/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 14744.0508 - accuracy: 0.8655roc-auc_val: 0.8202\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 14758.5625 - accuracy: 0.8654 - val_loss: 20594.3691 - val_accuracy: 0.8439\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14569.8975 - accuracy: 0.8633 - val_loss: 20409.6191 - val_accuracy: 0.8447\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 14564.8506 - accuracy: 0.8639 - val_loss: 20210.3926 - val_accuracy: 0.8350\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14312.1240 - accuracy: 0.8687 - val_loss: 20438.2812 - val_accuracy: 0.8266\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14167.1875 - accuracy: 0.8679 - val_loss: 20182.7871 - val_accuracy: 0.8385\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14098.3389 - accuracy: 0.8679 - val_loss: 20398.5781 - val_accuracy: 0.8537\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 14049.4453 - accuracy: 0.8717 - val_loss: 20353.7500 - val_accuracy: 0.8437\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13939.6846 - accuracy: 0.8684 - val_loss: 20180.7656 - val_accuracy: 0.8552\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13921.7471 - accuracy: 0.8704 - val_loss: 20160.8867 - val_accuracy: 0.8532\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13891.2451 - accuracy: 0.8717 - val_loss: 20055.2812 - val_accuracy: 0.8462\n",
            "Epoch 40/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 13761.0293 - accuracy: 0.8709roc-auc_val: 0.8205\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 13761.1797 - accuracy: 0.8708 - val_loss: 20318.5156 - val_accuracy: 0.8299\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13740.8730 - accuracy: 0.8729 - val_loss: 20122.8555 - val_accuracy: 0.8610\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13690.9609 - accuracy: 0.8740 - val_loss: 20203.5117 - val_accuracy: 0.8506\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13569.5088 - accuracy: 0.8738 - val_loss: 20205.7832 - val_accuracy: 0.8298\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13568.9619 - accuracy: 0.8723 - val_loss: 20238.0820 - val_accuracy: 0.8508\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13521.0674 - accuracy: 0.8759 - val_loss: 20189.2695 - val_accuracy: 0.8572\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13376.5508 - accuracy: 0.8751 - val_loss: 20415.7188 - val_accuracy: 0.8525\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13260.2842 - accuracy: 0.8773 - val_loss: 20484.2402 - val_accuracy: 0.8456\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13109.3467 - accuracy: 0.8766 - val_loss: 20198.0918 - val_accuracy: 0.8544\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13264.2471 - accuracy: 0.8761 - val_loss: 20170.8184 - val_accuracy: 0.8548\n",
            "Epoch 50/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 13281.9062 - accuracy: 0.8781roc-auc_val: 0.8238\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 13255.5576 - accuracy: 0.8782 - val_loss: 20273.9375 - val_accuracy: 0.8610\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13198.2715 - accuracy: 0.8768 - val_loss: 20122.5742 - val_accuracy: 0.8456\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12978.7930 - accuracy: 0.8770 - val_loss: 20226.9004 - val_accuracy: 0.8574\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13113.1143 - accuracy: 0.8767 - val_loss: 20305.0391 - val_accuracy: 0.8490\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 13015.3477 - accuracy: 0.8772 - val_loss: 20355.7207 - val_accuracy: 0.8524\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13009.4180 - accuracy: 0.8758 - val_loss: 20025.9277 - val_accuracy: 0.8648\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 13023.8096 - accuracy: 0.8771 - val_loss: 20077.7188 - val_accuracy: 0.8578\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12789.0762 - accuracy: 0.8801 - val_loss: 20125.0820 - val_accuracy: 0.8663\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12819.8721 - accuracy: 0.8782 - val_loss: 20300.8711 - val_accuracy: 0.8613\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12852.7988 - accuracy: 0.8784 - val_loss: 20222.6562 - val_accuracy: 0.8612\n",
            "Epoch 60/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 12862.2148 - accuracy: 0.8767roc-auc_val: 0.8249\n",
            "225/225 [==============================] - 5s 22ms/step - loss: 12882.2422 - accuracy: 0.8768 - val_loss: 20138.5566 - val_accuracy: 0.8614\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12708.8984 - accuracy: 0.8798 - val_loss: 20032.9355 - val_accuracy: 0.8609\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12632.9707 - accuracy: 0.8785 - val_loss: 20052.7832 - val_accuracy: 0.8675\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12731.8799 - accuracy: 0.8773 - val_loss: 20105.5527 - val_accuracy: 0.8622\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12636.9609 - accuracy: 0.8796 - val_loss: 20144.5645 - val_accuracy: 0.8660\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12474.1182 - accuracy: 0.8783 - val_loss: 20142.4414 - val_accuracy: 0.8607\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12465.2734 - accuracy: 0.8800 - val_loss: 20183.2754 - val_accuracy: 0.8586\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12568.0811 - accuracy: 0.8788 - val_loss: 20069.3574 - val_accuracy: 0.8561\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12410.9707 - accuracy: 0.8799 - val_loss: 20243.1543 - val_accuracy: 0.8640\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12317.4473 - accuracy: 0.8822 - val_loss: 20250.4492 - val_accuracy: 0.8677\n",
            "Epoch 70/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 12391.9082 - accuracy: 0.8804roc-auc_val: 0.825\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 12410.2861 - accuracy: 0.8804 - val_loss: 20212.2090 - val_accuracy: 0.8651\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12394.6006 - accuracy: 0.8810 - val_loss: 20084.7637 - val_accuracy: 0.8607\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12384.3135 - accuracy: 0.8801 - val_loss: 20299.6914 - val_accuracy: 0.8649\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12321.7725 - accuracy: 0.8794 - val_loss: 20144.0449 - val_accuracy: 0.8670\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12290.5186 - accuracy: 0.8802 - val_loss: 20415.5605 - val_accuracy: 0.8546\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12230.9180 - accuracy: 0.8822 - val_loss: 20274.4160 - val_accuracy: 0.8617\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12167.6670 - accuracy: 0.8830 - val_loss: 20178.0703 - val_accuracy: 0.8639\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12135.1816 - accuracy: 0.8818 - val_loss: 20127.7754 - val_accuracy: 0.8586\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11999.6748 - accuracy: 0.8813 - val_loss: 20289.9512 - val_accuracy: 0.8678\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12043.4082 - accuracy: 0.8829 - val_loss: 20396.5137 - val_accuracy: 0.8715\n",
            "Epoch 80/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 12200.1436 - accuracy: 0.8831roc-auc_val: 0.8247\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 12146.7686 - accuracy: 0.8832 - val_loss: 20219.6680 - val_accuracy: 0.8654\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12100.5225 - accuracy: 0.8808 - val_loss: 20345.6289 - val_accuracy: 0.8592\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11879.2744 - accuracy: 0.8846 - val_loss: 20090.7520 - val_accuracy: 0.8698\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11915.3535 - accuracy: 0.8838 - val_loss: 20232.3066 - val_accuracy: 0.8700\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12062.1641 - accuracy: 0.8833 - val_loss: 20175.1035 - val_accuracy: 0.8707\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12056.3896 - accuracy: 0.8828 - val_loss: 20123.1934 - val_accuracy: 0.8762\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11816.0342 - accuracy: 0.8856 - val_loss: 20225.4941 - val_accuracy: 0.8694\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11888.3379 - accuracy: 0.8845 - val_loss: 20217.5664 - val_accuracy: 0.8699\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11872.8369 - accuracy: 0.8821 - val_loss: 20068.7930 - val_accuracy: 0.8668\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11724.2773 - accuracy: 0.8822 - val_loss: 20076.9824 - val_accuracy: 0.8690\n",
            "Epoch 90/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 11781.3398 - accuracy: 0.8830roc-auc_val: 0.8249\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 11764.2568 - accuracy: 0.8830 - val_loss: 20245.0469 - val_accuracy: 0.8691\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11769.2246 - accuracy: 0.8852 - val_loss: 20248.7520 - val_accuracy: 0.8657\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11859.1611 - accuracy: 0.8856 - val_loss: 20168.0645 - val_accuracy: 0.8750\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11842.4434 - accuracy: 0.8848 - val_loss: 20131.4688 - val_accuracy: 0.8751\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11751.9902 - accuracy: 0.8850 - val_loss: 20368.9590 - val_accuracy: 0.8725\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11712.4619 - accuracy: 0.8855 - val_loss: 20170.7871 - val_accuracy: 0.8650\n",
            "Epoch 96/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11737.4697 - accuracy: 0.8838 - val_loss: 20136.8164 - val_accuracy: 0.8643\n",
            "Epoch 97/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11684.7412 - accuracy: 0.8843 - val_loss: 20121.2012 - val_accuracy: 0.8639\n",
            "Epoch 98/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11705.9551 - accuracy: 0.8836 - val_loss: 20177.6367 - val_accuracy: 0.8748\n",
            "Epoch 99/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11641.3643 - accuracy: 0.8879 - val_loss: 20043.9668 - val_accuracy: 0.8741\n",
            "Epoch 100/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 11633.2754 - accuracy: 0.8855roc-auc_val: 0.8256\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 11640.4082 - accuracy: 0.8855 - val_loss: 20097.9883 - val_accuracy: 0.8627\n",
            "Epoch 101/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11660.7559 - accuracy: 0.8836 - val_loss: 19992.4883 - val_accuracy: 0.8695\n",
            "Epoch 102/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11533.6104 - accuracy: 0.8864 - val_loss: 20134.1562 - val_accuracy: 0.8686\n",
            "Epoch 103/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11516.0195 - accuracy: 0.8860 - val_loss: 20071.7129 - val_accuracy: 0.8683\n",
            "Epoch 104/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11580.3750 - accuracy: 0.8870 - val_loss: 20037.5273 - val_accuracy: 0.8734\n",
            "Epoch 105/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11624.3740 - accuracy: 0.8885 - val_loss: 20180.1641 - val_accuracy: 0.8733\n",
            "Epoch 106/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11491.7100 - accuracy: 0.8854 - val_loss: 20108.1172 - val_accuracy: 0.8744\n",
            "Epoch 107/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11461.8672 - accuracy: 0.8875 - val_loss: 20123.0820 - val_accuracy: 0.8721\n",
            "Epoch 108/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11591.0479 - accuracy: 0.8859 - val_loss: 20096.7910 - val_accuracy: 0.8700\n",
            "Epoch 109/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11433.3486 - accuracy: 0.8882 - val_loss: 20183.2520 - val_accuracy: 0.8742\n",
            "Epoch 110/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 11492.5137 - accuracy: 0.8896roc-auc_val: 0.8286\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 11453.6992 - accuracy: 0.8895 - val_loss: 19935.3164 - val_accuracy: 0.8723\n",
            "Epoch 111/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11425.5742 - accuracy: 0.8874 - val_loss: 20054.0195 - val_accuracy: 0.8697\n",
            "Epoch 112/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11374.7236 - accuracy: 0.8824 - val_loss: 20001.4863 - val_accuracy: 0.8688\n",
            "Epoch 113/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11456.7559 - accuracy: 0.8861 - val_loss: 20054.7207 - val_accuracy: 0.8695\n",
            "Epoch 114/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11299.7363 - accuracy: 0.8860 - val_loss: 20021.6934 - val_accuracy: 0.8717\n",
            "Epoch 115/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11472.4629 - accuracy: 0.8897 - val_loss: 20205.3633 - val_accuracy: 0.8754\n",
            "Epoch 116/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11383.5322 - accuracy: 0.8888 - val_loss: 19984.8184 - val_accuracy: 0.8727\n",
            "Epoch 117/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11346.7705 - accuracy: 0.8862 - val_loss: 20062.5859 - val_accuracy: 0.8765\n",
            "Epoch 118/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11211.4248 - accuracy: 0.8875 - val_loss: 20151.2988 - val_accuracy: 0.8714\n",
            "Epoch 119/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11250.1895 - accuracy: 0.8901 - val_loss: 20258.2637 - val_accuracy: 0.8687\n",
            "Epoch 120/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 11282.5410 - accuracy: 0.8879roc-auc_val: 0.8261\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 11241.0713 - accuracy: 0.8879 - val_loss: 20116.3750 - val_accuracy: 0.8659\n",
            "Epoch 121/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11174.3740 - accuracy: 0.8886 - val_loss: 20063.5000 - val_accuracy: 0.8725\n",
            "Epoch 122/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11284.7930 - accuracy: 0.8867 - val_loss: 20003.1211 - val_accuracy: 0.8703\n",
            "Epoch 123/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11140.8604 - accuracy: 0.8885 - val_loss: 20153.4355 - val_accuracy: 0.8703\n",
            "Epoch 124/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11249.6777 - accuracy: 0.8860 - val_loss: 19899.4043 - val_accuracy: 0.8697\n",
            "Epoch 125/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11127.7354 - accuracy: 0.8876 - val_loss: 19965.3867 - val_accuracy: 0.8717\n",
            "Epoch 126/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11159.5156 - accuracy: 0.8886 - val_loss: 20077.2871 - val_accuracy: 0.8741\n",
            "Epoch 127/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11299.9688 - accuracy: 0.8875 - val_loss: 20030.4629 - val_accuracy: 0.8731\n",
            "Epoch 128/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10990.4121 - accuracy: 0.8900 - val_loss: 20086.4473 - val_accuracy: 0.8702\n",
            "Epoch 129/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11035.0723 - accuracy: 0.8888 - val_loss: 20175.1289 - val_accuracy: 0.8706\n",
            "Epoch 130/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 11109.8027 - accuracy: 0.8889roc-auc_val: 0.8264\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 11109.8027 - accuracy: 0.8889 - val_loss: 20105.5176 - val_accuracy: 0.8745\n",
            "Epoch 131/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11193.0420 - accuracy: 0.8906 - val_loss: 20054.2344 - val_accuracy: 0.8733\n",
            "Epoch 132/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11107.2559 - accuracy: 0.8888 - val_loss: 19989.7656 - val_accuracy: 0.8736\n",
            "Epoch 133/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11198.0635 - accuracy: 0.8882 - val_loss: 19962.4258 - val_accuracy: 0.8769\n",
            "Epoch 134/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11072.6826 - accuracy: 0.8885 - val_loss: 19975.9238 - val_accuracy: 0.8772\n",
            "Epoch 135/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11159.8623 - accuracy: 0.8870 - val_loss: 20062.5430 - val_accuracy: 0.8732\n",
            "Epoch 136/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11038.6807 - accuracy: 0.8875 - val_loss: 20082.4531 - val_accuracy: 0.8735\n",
            "Epoch 137/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11046.6533 - accuracy: 0.8878 - val_loss: 19976.9258 - val_accuracy: 0.8764\n",
            "Epoch 138/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11045.7188 - accuracy: 0.8876 - val_loss: 19979.1699 - val_accuracy: 0.8671\n",
            "Epoch 139/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11071.6582 - accuracy: 0.8876 - val_loss: 20041.9668 - val_accuracy: 0.8751\n",
            "Epoch 140/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 10904.6865 - accuracy: 0.8888roc-auc_val: 0.8278\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 10889.9365 - accuracy: 0.8888 - val_loss: 19995.1289 - val_accuracy: 0.8734\n",
            "Epoch 141/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10994.2090 - accuracy: 0.8886 - val_loss: 19927.5254 - val_accuracy: 0.8732\n",
            "Epoch 142/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11008.3877 - accuracy: 0.8879 - val_loss: 20056.7363 - val_accuracy: 0.8780\n",
            "Epoch 143/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 11017.5098 - accuracy: 0.8879 - val_loss: 20070.6602 - val_accuracy: 0.8738\n",
            "Epoch 144/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10930.7178 - accuracy: 0.8891 - val_loss: 20004.4980 - val_accuracy: 0.8761\n",
            "Epoch 145/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10945.0107 - accuracy: 0.8861 - val_loss: 20015.3008 - val_accuracy: 0.8758\n",
            "Epoch 146/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10921.8594 - accuracy: 0.8912 - val_loss: 20073.8770 - val_accuracy: 0.8800\n",
            "Epoch 147/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10851.5635 - accuracy: 0.8899 - val_loss: 19945.5156 - val_accuracy: 0.8743\n",
            "Epoch 148/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10877.2578 - accuracy: 0.8883 - val_loss: 20029.6758 - val_accuracy: 0.8754\n",
            "Epoch 149/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10800.8828 - accuracy: 0.8907 - val_loss: 20078.2246 - val_accuracy: 0.8740\n",
            "Epoch 150/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 10887.0693 - accuracy: 0.8893roc-auc_val: 0.8292\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 10869.6143 - accuracy: 0.8893 - val_loss: 19944.3223 - val_accuracy: 0.8767\n",
            "Epoch 151/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10722.1152 - accuracy: 0.8907 - val_loss: 20045.4844 - val_accuracy: 0.8792\n",
            "Epoch 152/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10871.3447 - accuracy: 0.8910 - val_loss: 20040.5938 - val_accuracy: 0.8801\n",
            "Epoch 153/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10772.4307 - accuracy: 0.8905 - val_loss: 20099.0859 - val_accuracy: 0.8777\n",
            "Epoch 154/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10675.8389 - accuracy: 0.8907 - val_loss: 20038.2188 - val_accuracy: 0.8763\n",
            "Epoch 155/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10689.0938 - accuracy: 0.8911 - val_loss: 19968.9395 - val_accuracy: 0.8781\n",
            "Epoch 156/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10869.3955 - accuracy: 0.8891 - val_loss: 19997.0957 - val_accuracy: 0.8758\n",
            "Epoch 157/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10862.8262 - accuracy: 0.8900 - val_loss: 19991.5879 - val_accuracy: 0.8757\n",
            "Epoch 158/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10855.4795 - accuracy: 0.8908 - val_loss: 20021.9199 - val_accuracy: 0.8765\n",
            "Epoch 159/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10715.4072 - accuracy: 0.8896 - val_loss: 19981.3691 - val_accuracy: 0.8747\n",
            "Epoch 160/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 10795.1221 - accuracy: 0.8900roc-auc_val: 0.8282\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 10816.2939 - accuracy: 0.8899 - val_loss: 19953.4785 - val_accuracy: 0.8731\n",
            "Epoch 161/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10782.9268 - accuracy: 0.8897 - val_loss: 19824.2910 - val_accuracy: 0.8712\n",
            "Epoch 162/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10673.8848 - accuracy: 0.8915 - val_loss: 20038.1348 - val_accuracy: 0.8801\n",
            "Epoch 163/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10763.9990 - accuracy: 0.8901 - val_loss: 19922.7793 - val_accuracy: 0.8744\n",
            "Epoch 164/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10728.5088 - accuracy: 0.8899 - val_loss: 19955.4805 - val_accuracy: 0.8813\n",
            "Epoch 165/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10635.7490 - accuracy: 0.8910 - val_loss: 19945.9727 - val_accuracy: 0.8830\n",
            "Epoch 166/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10615.8330 - accuracy: 0.8920 - val_loss: 19931.4883 - val_accuracy: 0.8811\n",
            "Epoch 167/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10629.9521 - accuracy: 0.8910 - val_loss: 20018.5820 - val_accuracy: 0.8752\n",
            "Epoch 168/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10636.1328 - accuracy: 0.8906 - val_loss: 19932.6641 - val_accuracy: 0.8773\n",
            "Epoch 169/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10706.7676 - accuracy: 0.8892 - val_loss: 19992.3418 - val_accuracy: 0.8753\n",
            "Epoch 170/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 10728.1064 - accuracy: 0.8900roc-auc_val: 0.828\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 10710.5840 - accuracy: 0.8899 - val_loss: 20069.7852 - val_accuracy: 0.8779\n",
            "Epoch 171/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10653.5977 - accuracy: 0.8920 - val_loss: 20113.5156 - val_accuracy: 0.8813\n",
            "Epoch 172/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10563.6592 - accuracy: 0.8929 - val_loss: 20110.3906 - val_accuracy: 0.8797\n",
            "Epoch 173/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10760.7480 - accuracy: 0.8918 - val_loss: 20065.3223 - val_accuracy: 0.8725\n",
            "Epoch 174/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10488.0000 - accuracy: 0.8930 - val_loss: 20063.1172 - val_accuracy: 0.8785\n",
            "Epoch 175/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10535.8584 - accuracy: 0.8928 - val_loss: 20142.4219 - val_accuracy: 0.8763\n",
            "Epoch 176/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10521.6729 - accuracy: 0.8919 - val_loss: 20026.5254 - val_accuracy: 0.8737\n",
            "Epoch 177/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10566.1592 - accuracy: 0.8905 - val_loss: 20010.0312 - val_accuracy: 0.8748\n",
            "Epoch 178/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10499.5352 - accuracy: 0.8923 - val_loss: 20103.7266 - val_accuracy: 0.8759\n",
            "Epoch 179/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10542.6641 - accuracy: 0.8929 - val_loss: 20026.0957 - val_accuracy: 0.8761\n",
            "Epoch 180/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 10583.4473 - accuracy: 0.8930roc-auc_val: 0.8295\n",
            "225/225 [==============================] - 5s 22ms/step - loss: 10564.7969 - accuracy: 0.8929 - val_loss: 19955.6875 - val_accuracy: 0.8804\n",
            "Epoch 181/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10558.5674 - accuracy: 0.8906 - val_loss: 20002.4668 - val_accuracy: 0.8736\n",
            "Epoch 182/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10598.2578 - accuracy: 0.8928 - val_loss: 19919.5156 - val_accuracy: 0.8807\n",
            "Epoch 183/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10551.0488 - accuracy: 0.8910 - val_loss: 19944.6621 - val_accuracy: 0.8763\n",
            "Epoch 184/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10612.3164 - accuracy: 0.8895 - val_loss: 20005.3184 - val_accuracy: 0.8759\n",
            "Epoch 185/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10532.2988 - accuracy: 0.8901 - val_loss: 20080.7383 - val_accuracy: 0.8751\n",
            "Epoch 186/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10539.3926 - accuracy: 0.8929 - val_loss: 19989.0449 - val_accuracy: 0.8785\n",
            "Epoch 187/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10456.9971 - accuracy: 0.8919 - val_loss: 20006.4746 - val_accuracy: 0.8790\n",
            "Epoch 188/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10501.8164 - accuracy: 0.8912 - val_loss: 20047.5039 - val_accuracy: 0.8755\n",
            "Epoch 189/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10428.8008 - accuracy: 0.8933 - val_loss: 20135.1270 - val_accuracy: 0.8764\n",
            "Epoch 190/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 10479.8496 - accuracy: 0.8923roc-auc_val: 0.8274\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 10445.1689 - accuracy: 0.8923 - val_loss: 20066.5547 - val_accuracy: 0.8747\n",
            "Epoch 191/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10474.8223 - accuracy: 0.8912 - val_loss: 20023.6562 - val_accuracy: 0.8748\n",
            "Epoch 192/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10476.9912 - accuracy: 0.8921 - val_loss: 20023.4668 - val_accuracy: 0.8782\n",
            "Epoch 193/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10561.9658 - accuracy: 0.8915 - val_loss: 19918.6875 - val_accuracy: 0.8749\n",
            "Epoch 194/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10427.4697 - accuracy: 0.8923 - val_loss: 20001.8184 - val_accuracy: 0.8789\n",
            "Epoch 195/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10467.0547 - accuracy: 0.8920 - val_loss: 19997.5840 - val_accuracy: 0.8829\n",
            "Epoch 196/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10454.8779 - accuracy: 0.8917 - val_loss: 19979.0938 - val_accuracy: 0.8775\n",
            "Epoch 197/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10405.2051 - accuracy: 0.8928 - val_loss: 20027.2441 - val_accuracy: 0.8776\n",
            "Epoch 198/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10517.0615 - accuracy: 0.8926 - val_loss: 19954.6055 - val_accuracy: 0.8810\n",
            "Epoch 199/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10440.3613 - accuracy: 0.8919 - val_loss: 19984.0215 - val_accuracy: 0.8785\n",
            "Epoch 200/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 10389.2998 - accuracy: 0.8934roc-auc_val: 0.8278\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 10357.3057 - accuracy: 0.8934 - val_loss: 20054.6328 - val_accuracy: 0.8763\n",
            "Epoch 201/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10383.3906 - accuracy: 0.8924 - val_loss: 20055.6367 - val_accuracy: 0.8814\n",
            "Epoch 202/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10388.1094 - accuracy: 0.8942 - val_loss: 20099.1094 - val_accuracy: 0.8820\n",
            "Epoch 203/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10410.9141 - accuracy: 0.8942 - val_loss: 20095.9863 - val_accuracy: 0.8821\n",
            "Epoch 204/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10315.0234 - accuracy: 0.8945 - val_loss: 19980.0039 - val_accuracy: 0.8801\n",
            "Epoch 205/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10350.1260 - accuracy: 0.8947 - val_loss: 20056.7559 - val_accuracy: 0.8821\n",
            "Epoch 206/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10270.5986 - accuracy: 0.8954 - val_loss: 20043.0879 - val_accuracy: 0.8791\n",
            "Epoch 207/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10372.7598 - accuracy: 0.8934 - val_loss: 20041.8867 - val_accuracy: 0.8798\n",
            "Epoch 208/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10346.6611 - accuracy: 0.8925 - val_loss: 19993.6328 - val_accuracy: 0.8793\n",
            "Epoch 209/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10278.9707 - accuracy: 0.8936 - val_loss: 20011.6367 - val_accuracy: 0.8805\n",
            "Epoch 210/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 10295.5352 - accuracy: 0.8948roc-auc_val: 0.8289\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 10285.6680 - accuracy: 0.8947 - val_loss: 20016.5723 - val_accuracy: 0.8815\n",
            "Epoch 211/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10346.5576 - accuracy: 0.8917 - val_loss: 20003.5312 - val_accuracy: 0.8800\n",
            "Epoch 1/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 34873.9961 - accuracy: 0.6950 - val_loss: 29848.4707 - val_accuracy: 0.6310\n",
            "Epoch 2/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 28506.8320 - accuracy: 0.7492 - val_loss: 28027.5215 - val_accuracy: 0.6534\n",
            "Epoch 3/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 26278.2617 - accuracy: 0.7679 - val_loss: 27276.6973 - val_accuracy: 0.7035\n",
            "Epoch 4/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 24461.8047 - accuracy: 0.7788 - val_loss: 27151.1582 - val_accuracy: 0.6839\n",
            "Epoch 5/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 22956.3516 - accuracy: 0.7949 - val_loss: 27208.4316 - val_accuracy: 0.7003\n",
            "Epoch 6/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 21632.6133 - accuracy: 0.8072 - val_loss: 26437.9180 - val_accuracy: 0.7194\n",
            "Epoch 7/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 20540.3477 - accuracy: 0.8184 - val_loss: 26102.9941 - val_accuracy: 0.7088\n",
            "Epoch 8/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 19776.2871 - accuracy: 0.8223 - val_loss: 26507.3965 - val_accuracy: 0.7371\n",
            "Epoch 9/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 19115.3203 - accuracy: 0.8274 - val_loss: 26423.9961 - val_accuracy: 0.7187\n",
            "Epoch 10/1000\n",
            "178/181 [============================>.] - ETA: 0s - loss: 18703.3789 - accuracy: 0.8321roc-auc_val: 0.8083\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 18663.8848 - accuracy: 0.8319 - val_loss: 25985.3789 - val_accuracy: 0.7497\n",
            "Epoch 11/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 18095.5938 - accuracy: 0.8354 - val_loss: 26279.3809 - val_accuracy: 0.7568\n",
            "Epoch 12/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 17896.9277 - accuracy: 0.8409 - val_loss: 26358.8672 - val_accuracy: 0.8103\n",
            "Epoch 13/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 17529.2656 - accuracy: 0.8390 - val_loss: 26086.8926 - val_accuracy: 0.7646\n",
            "Epoch 14/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 17083.2188 - accuracy: 0.8440 - val_loss: 25861.3457 - val_accuracy: 0.7677\n",
            "Epoch 15/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 16793.6387 - accuracy: 0.8441 - val_loss: 25568.7168 - val_accuracy: 0.7699\n",
            "Epoch 16/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 16470.7090 - accuracy: 0.8504 - val_loss: 25824.8145 - val_accuracy: 0.7463\n",
            "Epoch 17/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 16282.7715 - accuracy: 0.8486 - val_loss: 25420.0527 - val_accuracy: 0.7623\n",
            "Epoch 18/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 15939.5771 - accuracy: 0.8496 - val_loss: 25531.1113 - val_accuracy: 0.7837\n",
            "Epoch 19/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 15615.2373 - accuracy: 0.8544 - val_loss: 24809.9785 - val_accuracy: 0.7863\n",
            "Epoch 20/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 15537.2666 - accuracy: 0.8516roc-auc_val: 0.8147\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 15483.9043 - accuracy: 0.8517 - val_loss: 25323.6855 - val_accuracy: 0.7804\n",
            "Epoch 21/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 15140.7871 - accuracy: 0.8573 - val_loss: 26052.5527 - val_accuracy: 0.8102\n",
            "Epoch 22/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14947.7871 - accuracy: 0.8586 - val_loss: 25313.2559 - val_accuracy: 0.8101\n",
            "Epoch 23/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14713.6719 - accuracy: 0.8627 - val_loss: 25242.8848 - val_accuracy: 0.8199\n",
            "Epoch 24/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14648.3145 - accuracy: 0.8601 - val_loss: 25059.8555 - val_accuracy: 0.8112\n",
            "Epoch 25/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14425.9111 - accuracy: 0.8638 - val_loss: 25925.1367 - val_accuracy: 0.8091\n",
            "Epoch 26/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14408.8398 - accuracy: 0.8628 - val_loss: 25115.9414 - val_accuracy: 0.8044\n",
            "Epoch 27/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14324.9590 - accuracy: 0.8639 - val_loss: 25442.6523 - val_accuracy: 0.7936\n",
            "Epoch 28/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14032.4521 - accuracy: 0.8646 - val_loss: 25594.2207 - val_accuracy: 0.7907\n",
            "Epoch 29/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14036.3818 - accuracy: 0.8664 - val_loss: 25534.9141 - val_accuracy: 0.8176\n",
            "Epoch 30/1000\n",
            "178/181 [============================>.] - ETA: 0s - loss: 13820.0537 - accuracy: 0.8657roc-auc_val: 0.8173\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 13784.3340 - accuracy: 0.8657 - val_loss: 25463.3730 - val_accuracy: 0.8161\n",
            "Epoch 31/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13675.8467 - accuracy: 0.8681 - val_loss: 25946.5938 - val_accuracy: 0.8136\n",
            "Epoch 32/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13682.4521 - accuracy: 0.8696 - val_loss: 25782.7305 - val_accuracy: 0.8128\n",
            "Epoch 33/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13525.6309 - accuracy: 0.8677 - val_loss: 25795.5195 - val_accuracy: 0.8052\n",
            "Epoch 34/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13373.1768 - accuracy: 0.8682 - val_loss: 25280.1562 - val_accuracy: 0.8078\n",
            "Epoch 35/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13305.7031 - accuracy: 0.8711 - val_loss: 25975.6348 - val_accuracy: 0.8132\n",
            "Epoch 36/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13366.5488 - accuracy: 0.8699 - val_loss: 25408.8008 - val_accuracy: 0.8091\n",
            "Epoch 37/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13191.9531 - accuracy: 0.8705 - val_loss: 25703.2266 - val_accuracy: 0.8212\n",
            "Epoch 38/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13026.9229 - accuracy: 0.8734 - val_loss: 25745.6973 - val_accuracy: 0.8088\n",
            "Epoch 39/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12926.7197 - accuracy: 0.8729 - val_loss: 25697.1992 - val_accuracy: 0.8068\n",
            "Epoch 40/1000\n",
            "181/181 [==============================] - ETA: 0s - loss: 12785.5430 - accuracy: 0.8728roc-auc_val: 0.815\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 12785.5430 - accuracy: 0.8728 - val_loss: 25536.2363 - val_accuracy: 0.8030\n",
            "Epoch 41/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12851.8242 - accuracy: 0.8733 - val_loss: 25656.4766 - val_accuracy: 0.8085\n",
            "Epoch 42/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12944.7148 - accuracy: 0.8736 - val_loss: 25494.2793 - val_accuracy: 0.8162\n",
            "Epoch 43/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12605.4141 - accuracy: 0.8746 - val_loss: 25814.5703 - val_accuracy: 0.8192\n",
            "Epoch 44/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12488.2139 - accuracy: 0.8764 - val_loss: 25667.3418 - val_accuracy: 0.8080\n",
            "Epoch 45/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12610.7100 - accuracy: 0.8756 - val_loss: 25743.2793 - val_accuracy: 0.8178\n",
            "Epoch 46/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12392.4541 - accuracy: 0.8779 - val_loss: 25499.2695 - val_accuracy: 0.8031\n",
            "Epoch 47/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12282.0703 - accuracy: 0.8764 - val_loss: 25525.8242 - val_accuracy: 0.8207\n",
            "Epoch 48/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12266.9150 - accuracy: 0.8788 - val_loss: 25752.8750 - val_accuracy: 0.8141\n",
            "Epoch 49/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12335.7471 - accuracy: 0.8749 - val_loss: 26046.7617 - val_accuracy: 0.8309\n",
            "Epoch 50/1000\n",
            "178/181 [============================>.] - ETA: 0s - loss: 12178.0615 - accuracy: 0.8774roc-auc_val: 0.8162\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 12124.2100 - accuracy: 0.8772 - val_loss: 25628.0566 - val_accuracy: 0.8104\n",
            "Epoch 51/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12163.9502 - accuracy: 0.8767 - val_loss: 25519.8516 - val_accuracy: 0.8142\n",
            "Epoch 52/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12058.9951 - accuracy: 0.8783 - val_loss: 25667.9961 - val_accuracy: 0.8184\n",
            "Epoch 53/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11976.9463 - accuracy: 0.8752 - val_loss: 25800.4160 - val_accuracy: 0.8204\n",
            "Epoch 54/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12002.8047 - accuracy: 0.8811 - val_loss: 25843.9121 - val_accuracy: 0.8230\n",
            "Epoch 55/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12007.8096 - accuracy: 0.8772 - val_loss: 25871.2969 - val_accuracy: 0.8142\n",
            "Epoch 56/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11874.7920 - accuracy: 0.8832 - val_loss: 25796.4160 - val_accuracy: 0.8395\n",
            "Epoch 57/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11897.9688 - accuracy: 0.8803 - val_loss: 25702.9648 - val_accuracy: 0.8300\n",
            "Epoch 58/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11714.9131 - accuracy: 0.8782 - val_loss: 25989.7402 - val_accuracy: 0.8266\n",
            "Epoch 59/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11747.6631 - accuracy: 0.8792 - val_loss: 25819.4609 - val_accuracy: 0.8229\n",
            "Epoch 60/1000\n",
            "181/181 [==============================] - ETA: 0s - loss: 11873.0205 - accuracy: 0.8826roc-auc_val: 0.8148\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 11873.0205 - accuracy: 0.8826 - val_loss: 25721.0586 - val_accuracy: 0.8177\n",
            "Epoch 61/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11590.6348 - accuracy: 0.8825 - val_loss: 25999.7266 - val_accuracy: 0.8354\n",
            "Epoch 62/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11595.7617 - accuracy: 0.8809 - val_loss: 25953.1699 - val_accuracy: 0.8313\n",
            "Epoch 63/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11661.1572 - accuracy: 0.8804 - val_loss: 26199.8672 - val_accuracy: 0.8207\n",
            "Epoch 64/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11652.1221 - accuracy: 0.8815 - val_loss: 26144.3027 - val_accuracy: 0.8291\n",
            "Epoch 65/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11530.6123 - accuracy: 0.8817 - val_loss: 26220.9395 - val_accuracy: 0.8341\n",
            "Epoch 66/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11482.3379 - accuracy: 0.8869 - val_loss: 26204.4980 - val_accuracy: 0.8406\n",
            "Epoch 67/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11319.1680 - accuracy: 0.8846 - val_loss: 25901.1465 - val_accuracy: 0.8331\n",
            "Epoch 68/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11541.5303 - accuracy: 0.8820 - val_loss: 25861.6504 - val_accuracy: 0.8186\n",
            "Epoch 69/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11465.9580 - accuracy: 0.8848 - val_loss: 26056.3145 - val_accuracy: 0.8265\n",
            "Epoch 1/1000\n",
            "136/136 [==============================] - 2s 11ms/step - loss: 34447.4453 - accuracy: 0.6969 - val_loss: 32458.7402 - val_accuracy: 0.7245\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 28316.2266 - accuracy: 0.7454 - val_loss: 31215.9863 - val_accuracy: 0.7209\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 26271.6758 - accuracy: 0.7575 - val_loss: 30092.9141 - val_accuracy: 0.7007\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 24692.9199 - accuracy: 0.7645 - val_loss: 29774.0352 - val_accuracy: 0.7310\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 23200.6172 - accuracy: 0.7797 - val_loss: 30149.8379 - val_accuracy: 0.7449\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 22193.3535 - accuracy: 0.7880 - val_loss: 29681.8281 - val_accuracy: 0.7626\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 21225.7949 - accuracy: 0.7942 - val_loss: 29650.5781 - val_accuracy: 0.7813\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 19974.3730 - accuracy: 0.8064 - val_loss: 29618.7969 - val_accuracy: 0.8030\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 19339.1680 - accuracy: 0.8128 - val_loss: 29641.8770 - val_accuracy: 0.8219\n",
            "Epoch 10/1000\n",
            "127/136 [===========================>..] - ETA: 0s - loss: 18678.2793 - accuracy: 0.8191roc-auc_val: 0.8076\n",
            "136/136 [==============================] - 9s 68ms/step - loss: 18558.6719 - accuracy: 0.8188 - val_loss: 29255.2402 - val_accuracy: 0.8067\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 17978.6719 - accuracy: 0.8258 - val_loss: 29380.7188 - val_accuracy: 0.8208\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 17425.0156 - accuracy: 0.8266 - val_loss: 29268.9336 - val_accuracy: 0.8326\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 16990.3750 - accuracy: 0.8409 - val_loss: 29469.0430 - val_accuracy: 0.8472\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 16411.3379 - accuracy: 0.8392 - val_loss: 28916.6309 - val_accuracy: 0.8474\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 16161.3447 - accuracy: 0.8440 - val_loss: 28756.2168 - val_accuracy: 0.8299\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 15497.9385 - accuracy: 0.8477 - val_loss: 29659.9277 - val_accuracy: 0.8547\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 15523.0752 - accuracy: 0.8464 - val_loss: 29846.0215 - val_accuracy: 0.8629\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 15118.4395 - accuracy: 0.8514 - val_loss: 29694.0156 - val_accuracy: 0.8652\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14853.8438 - accuracy: 0.8554 - val_loss: 29366.7637 - val_accuracy: 0.8595\n",
            "Epoch 20/1000\n",
            "136/136 [==============================] - ETA: 0s - loss: 14631.5977 - accuracy: 0.8587roc-auc_val: 0.8107\n",
            "136/136 [==============================] - 9s 66ms/step - loss: 14631.5977 - accuracy: 0.8587 - val_loss: 29941.7773 - val_accuracy: 0.8832\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14339.9502 - accuracy: 0.8588 - val_loss: 30174.3906 - val_accuracy: 0.8857\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14198.1016 - accuracy: 0.8601 - val_loss: 29348.1543 - val_accuracy: 0.8537\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13959.2764 - accuracy: 0.8649 - val_loss: 29458.6484 - val_accuracy: 0.8761\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13802.8096 - accuracy: 0.8637 - val_loss: 29767.9883 - val_accuracy: 0.8641\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13634.8945 - accuracy: 0.8680 - val_loss: 29752.0098 - val_accuracy: 0.8644\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13532.7119 - accuracy: 0.8634 - val_loss: 29180.0352 - val_accuracy: 0.8488\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13255.5801 - accuracy: 0.8681 - val_loss: 29633.5918 - val_accuracy: 0.8793\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13020.6865 - accuracy: 0.8686 - val_loss: 30048.6348 - val_accuracy: 0.8804\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13133.4150 - accuracy: 0.8696 - val_loss: 30270.4844 - val_accuracy: 0.8838\n",
            "Epoch 30/1000\n",
            "136/136 [==============================] - ETA: 0s - loss: 13039.3418 - accuracy: 0.8722roc-auc_val: 0.8077\n",
            "136/136 [==============================] - 9s 67ms/step - loss: 13039.3418 - accuracy: 0.8722 - val_loss: 30062.8398 - val_accuracy: 0.8692\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12823.5947 - accuracy: 0.8713 - val_loss: 29838.5801 - val_accuracy: 0.8559\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12688.5703 - accuracy: 0.8715 - val_loss: 30137.1895 - val_accuracy: 0.8614\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12615.3887 - accuracy: 0.8723 - val_loss: 30121.5332 - val_accuracy: 0.8680\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12316.6602 - accuracy: 0.8751 - val_loss: 29495.2031 - val_accuracy: 0.8613\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12512.1982 - accuracy: 0.8728 - val_loss: 29553.9453 - val_accuracy: 0.8514\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12197.5254 - accuracy: 0.8772 - val_loss: 30075.5703 - val_accuracy: 0.8742\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12008.1914 - accuracy: 0.8778 - val_loss: 30298.7559 - val_accuracy: 0.8677\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12017.8242 - accuracy: 0.8769 - val_loss: 29906.9199 - val_accuracy: 0.8542\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12076.7002 - accuracy: 0.8727 - val_loss: 30110.6719 - val_accuracy: 0.8695\n",
            "Epoch 40/1000\n",
            "134/136 [============================>.] - ETA: 0s - loss: 11991.0908 - accuracy: 0.8784roc-auc_val: 0.8054\n",
            "136/136 [==============================] - 9s 66ms/step - loss: 11991.1699 - accuracy: 0.8782 - val_loss: 30061.9980 - val_accuracy: 0.8504\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11676.9111 - accuracy: 0.8806 - val_loss: 30870.9883 - val_accuracy: 0.8838\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11796.2988 - accuracy: 0.8779 - val_loss: 30396.4141 - val_accuracy: 0.8686\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11592.9619 - accuracy: 0.8812 - val_loss: 30350.6777 - val_accuracy: 0.8744\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11450.6387 - accuracy: 0.8816 - val_loss: 30326.9941 - val_accuracy: 0.8681\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11402.0811 - accuracy: 0.8821 - val_loss: 30745.7031 - val_accuracy: 0.8878\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11427.0010 - accuracy: 0.8796 - val_loss: 30323.9902 - val_accuracy: 0.8658\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11272.6650 - accuracy: 0.8847 - val_loss: 30281.9297 - val_accuracy: 0.8686\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11150.7900 - accuracy: 0.8867 - val_loss: 30575.1777 - val_accuracy: 0.8724\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11137.3477 - accuracy: 0.8814 - val_loss: 31098.2207 - val_accuracy: 0.8710\n",
            "Epoch 50/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 11224.3740 - accuracy: 0.8837roc-auc_val: 0.8019\n",
            "136/136 [==============================] - 9s 67ms/step - loss: 11212.7529 - accuracy: 0.8839 - val_loss: 31117.6660 - val_accuracy: 0.8832\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11049.1045 - accuracy: 0.8838 - val_loss: 30577.5781 - val_accuracy: 0.8689\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11120.0234 - accuracy: 0.8825 - val_loss: 30324.9121 - val_accuracy: 0.8665\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10944.9980 - accuracy: 0.8840 - val_loss: 31162.2656 - val_accuracy: 0.8803\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10925.8516 - accuracy: 0.8843 - val_loss: 30900.1426 - val_accuracy: 0.8774\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10779.8242 - accuracy: 0.8868 - val_loss: 31068.0801 - val_accuracy: 0.8881\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10650.9473 - accuracy: 0.8899 - val_loss: 31053.9570 - val_accuracy: 0.8757\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10626.7998 - accuracy: 0.8900 - val_loss: 30956.0605 - val_accuracy: 0.8860\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 10643.1641 - accuracy: 0.8903 - val_loss: 30829.5371 - val_accuracy: 0.8759\n",
            "Epoch 59/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10635.1152 - accuracy: 0.8857 - val_loss: 31052.7891 - val_accuracy: 0.8821\n",
            "Epoch 60/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 10659.0068 - accuracy: 0.8856roc-auc_val: 0.8029\n",
            "136/136 [==============================] - 9s 69ms/step - loss: 10670.9766 - accuracy: 0.8855 - val_loss: 31111.9199 - val_accuracy: 0.8856\n",
            "Epoch 61/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 10549.6963 - accuracy: 0.8895 - val_loss: 31029.1973 - val_accuracy: 0.8891\n",
            "Epoch 62/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10558.9238 - accuracy: 0.8892 - val_loss: 30932.1816 - val_accuracy: 0.8843\n",
            "Epoch 63/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10440.0254 - accuracy: 0.8836 - val_loss: 30945.3770 - val_accuracy: 0.8800\n",
            "Epoch 64/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10321.9531 - accuracy: 0.8895 - val_loss: 31063.4199 - val_accuracy: 0.8745\n",
            "Epoch 65/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10220.4854 - accuracy: 0.8920 - val_loss: 31320.4160 - val_accuracy: 0.8961\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 17ms/step - loss: 34092.8008 - accuracy: 0.6804 - val_loss: 33825.9531 - val_accuracy: 0.7227\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 27479.9961 - accuracy: 0.7352 - val_loss: 32602.6543 - val_accuracy: 0.7734\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 25171.0098 - accuracy: 0.7608 - val_loss: 31259.3945 - val_accuracy: 0.7731\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 23179.9961 - accuracy: 0.7751 - val_loss: 31698.9277 - val_accuracy: 0.8208\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 21789.6797 - accuracy: 0.7819 - val_loss: 30938.3145 - val_accuracy: 0.8081\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 20697.9297 - accuracy: 0.7932 - val_loss: 30814.8301 - val_accuracy: 0.8208\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 19809.6328 - accuracy: 0.8007 - val_loss: 30811.6582 - val_accuracy: 0.8028\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 18884.7617 - accuracy: 0.8064 - val_loss: 31279.1055 - val_accuracy: 0.8362\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 18139.7656 - accuracy: 0.8157 - val_loss: 30975.9336 - val_accuracy: 0.8358\n",
            "Epoch 10/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 17403.2383 - accuracy: 0.8213roc-auc_val: 0.8109\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 17356.3887 - accuracy: 0.8223 - val_loss: 31121.7188 - val_accuracy: 0.8506\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 16630.8008 - accuracy: 0.8333 - val_loss: 30474.8984 - val_accuracy: 0.8526\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 16133.4316 - accuracy: 0.8357 - val_loss: 30918.0918 - val_accuracy: 0.8581\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 15757.2549 - accuracy: 0.8424 - val_loss: 31820.8066 - val_accuracy: 0.8720\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 15166.3955 - accuracy: 0.8406 - val_loss: 31153.3340 - val_accuracy: 0.8755\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 14724.5850 - accuracy: 0.8398 - val_loss: 32086.4863 - val_accuracy: 0.8926\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 14171.4512 - accuracy: 0.8503 - val_loss: 32371.2578 - val_accuracy: 0.8757\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13988.8799 - accuracy: 0.8526 - val_loss: 32721.1562 - val_accuracy: 0.8939\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13700.9033 - accuracy: 0.8576 - val_loss: 32537.1348 - val_accuracy: 0.8954\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13011.0068 - accuracy: 0.8604 - val_loss: 32883.8477 - val_accuracy: 0.9015\n",
            "Epoch 20/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 13053.4277 - accuracy: 0.8609roc-auc_val: 0.8079\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 13053.4277 - accuracy: 0.8609 - val_loss: 32831.3867 - val_accuracy: 0.8967\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12746.1123 - accuracy: 0.8656 - val_loss: 32470.4004 - val_accuracy: 0.8970\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12317.2646 - accuracy: 0.8671 - val_loss: 32618.2188 - val_accuracy: 0.9050\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12313.7783 - accuracy: 0.8702 - val_loss: 32971.8125 - val_accuracy: 0.9048\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12138.1797 - accuracy: 0.8708 - val_loss: 33254.6016 - val_accuracy: 0.9117\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11829.9580 - accuracy: 0.8695 - val_loss: 32031.8340 - val_accuracy: 0.8934\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11805.6738 - accuracy: 0.8705 - val_loss: 32209.5176 - val_accuracy: 0.8999\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11321.3936 - accuracy: 0.8772 - val_loss: 33052.4609 - val_accuracy: 0.9128\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11227.5479 - accuracy: 0.8772 - val_loss: 33159.0938 - val_accuracy: 0.9110\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10977.6016 - accuracy: 0.8830 - val_loss: 33132.5273 - val_accuracy: 0.9135\n",
            "Epoch 30/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 11063.2402 - accuracy: 0.8816roc-auc_val: 0.808\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 10943.3037 - accuracy: 0.8814 - val_loss: 32634.2070 - val_accuracy: 0.9034\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10959.5879 - accuracy: 0.8803 - val_loss: 32536.7324 - val_accuracy: 0.9068\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10580.5518 - accuracy: 0.8845 - val_loss: 33617.9531 - val_accuracy: 0.9129\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10708.2617 - accuracy: 0.8811 - val_loss: 33221.1016 - val_accuracy: 0.9057\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10381.2139 - accuracy: 0.8832 - val_loss: 33081.3633 - val_accuracy: 0.9089\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10265.6709 - accuracy: 0.8873 - val_loss: 33553.8672 - val_accuracy: 0.9114\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10281.2451 - accuracy: 0.8880 - val_loss: 32829.7031 - val_accuracy: 0.9062\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10049.2012 - accuracy: 0.8870 - val_loss: 33486.3125 - val_accuracy: 0.9165\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10028.5469 - accuracy: 0.8897 - val_loss: 33612.9727 - val_accuracy: 0.9140\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9864.9941 - accuracy: 0.8906 - val_loss: 33144.0117 - val_accuracy: 0.9099\n",
            "Epoch 40/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 9724.4844 - accuracy: 0.8911roc-auc_val: 0.8047\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 9745.4082 - accuracy: 0.8909 - val_loss: 32903.3125 - val_accuracy: 0.9051\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9859.0479 - accuracy: 0.8902 - val_loss: 33346.4453 - val_accuracy: 0.9047\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9712.8848 - accuracy: 0.8910 - val_loss: 33609.8906 - val_accuracy: 0.9079\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9502.4092 - accuracy: 0.8933 - val_loss: 33618.9414 - val_accuracy: 0.9084\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9392.8633 - accuracy: 0.8926 - val_loss: 33914.3906 - val_accuracy: 0.9142\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9272.9932 - accuracy: 0.8951 - val_loss: 33622.5352 - val_accuracy: 0.9128\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9257.4609 - accuracy: 0.8936 - val_loss: 33447.8516 - val_accuracy: 0.9106\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9215.7998 - accuracy: 0.8945 - val_loss: 33401.0781 - val_accuracy: 0.9069\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9029.7988 - accuracy: 0.8924 - val_loss: 33680.2461 - val_accuracy: 0.9112\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9114.6592 - accuracy: 0.8924 - val_loss: 33233.2070 - val_accuracy: 0.9031\n",
            "Epoch 50/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 8839.7207 - accuracy: 0.8963roc-auc_val: 0.8016\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 8867.1533 - accuracy: 0.8957 - val_loss: 33491.3359 - val_accuracy: 0.9079\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8808.5498 - accuracy: 0.8974 - val_loss: 33703.0039 - val_accuracy: 0.9098\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8791.4609 - accuracy: 0.8983 - val_loss: 33912.3750 - val_accuracy: 0.9134\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8742.0518 - accuracy: 0.8996 - val_loss: 33742.5117 - val_accuracy: 0.9139\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 11ms/step - loss: 8773.6260 - accuracy: 0.9010 - val_loss: 33736.3555 - val_accuracy: 0.9107\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8420.2012 - accuracy: 0.9023 - val_loss: 33898.2266 - val_accuracy: 0.9094\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8543.3535 - accuracy: 0.9001 - val_loss: 34261.8555 - val_accuracy: 0.9165\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8548.0352 - accuracy: 0.9026 - val_loss: 34613.8320 - val_accuracy: 0.9196\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8463.0762 - accuracy: 0.9015 - val_loss: 34589.3125 - val_accuracy: 0.9143\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8246.8467 - accuracy: 0.9033 - val_loss: 34302.9805 - val_accuracy: 0.9103\n",
            "Epoch 60/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 8335.5771 - accuracy: 0.9061roc-auc_val: 0.7968\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 8166.9507 - accuracy: 0.9061 - val_loss: 34484.5508 - val_accuracy: 0.9121\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8312.4951 - accuracy: 0.9022 - val_loss: 33885.5195 - val_accuracy: 0.9037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L3_TlYw5nac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5905fc5-5a5e-41c6-a660-1eaf9f15a04f"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5PePWLxWIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
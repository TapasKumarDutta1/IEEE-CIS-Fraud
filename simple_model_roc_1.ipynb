{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_1",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e394038f-9e2f-43f7-e0df-878c16d16486"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d9026f67-2917-4881-9568-1ee9ff1ba527"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 79.9MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 84% 44.0M/52.2M [00:00<00:00, 51.8MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 89.0MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 84% 49.0M/58.3M [00:00<00:00, 73.3MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 147MB/s] \n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 107MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 224MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv80v8W_Ko2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4c07e001-83af-407b-8b8f-bdc63fc0b2e5"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jt2pcxPije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8bfbe683-5746-45e2-88ab-130a079e1680"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvEaxp9jhbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "1f013f44-fb83-496a-c249-4f5862f96b1a"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4cb2f83c-a5d8-475d-ad8c-e79de396cc5e"
      },
      "source": [
        "\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkLEBcl6LHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 1\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDFTro_vTJr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adcc51cd-9eca-4433-8934-bc590b58f65c"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "# trn=trn.drop(['isFraud_isna'],1)\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Adam(0.001,decay=1e-3),loss=rac,metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.00001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 22413.1289 - accuracy: 0.6711 - val_loss: 17404.5371 - val_accuracy: 0.5812\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 17190.7305 - accuracy: 0.7559 - val_loss: 15919.7598 - val_accuracy: 0.6158\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 15310.0762 - accuracy: 0.7922 - val_loss: 15267.5264 - val_accuracy: 0.6296\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 14087.7041 - accuracy: 0.8095 - val_loss: 15624.2822 - val_accuracy: 0.6620\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 13180.8408 - accuracy: 0.8174 - val_loss: 14971.5557 - val_accuracy: 0.6640\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 12452.8330 - accuracy: 0.8198 - val_loss: 14111.2588 - val_accuracy: 0.6969\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 11946.0801 - accuracy: 0.8287 - val_loss: 14092.8135 - val_accuracy: 0.7102\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 11450.1650 - accuracy: 0.8330 - val_loss: 13518.9932 - val_accuracy: 0.7232\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 11060.4668 - accuracy: 0.8349 - val_loss: 13958.5977 - val_accuracy: 0.7153\n",
            "Epoch 10/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 10651.9834 - accuracy: 0.8407roc-auc_val: 0.7984\n",
            "225/225 [==============================] - 6s 28ms/step - loss: 10651.9834 - accuracy: 0.8407 - val_loss: 13838.3281 - val_accuracy: 0.7176\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 10469.1064 - accuracy: 0.8429 - val_loss: 13610.0547 - val_accuracy: 0.7073\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 10151.2822 - accuracy: 0.8451 - val_loss: 13305.5479 - val_accuracy: 0.7420\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 9965.7773 - accuracy: 0.8454 - val_loss: 13744.3701 - val_accuracy: 0.7215\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 9633.1396 - accuracy: 0.8482 - val_loss: 13369.0957 - val_accuracy: 0.7234\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 9392.6738 - accuracy: 0.8478 - val_loss: 13444.7227 - val_accuracy: 0.7336\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 9241.3906 - accuracy: 0.8515 - val_loss: 13310.6025 - val_accuracy: 0.7374\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 9057.0166 - accuracy: 0.8558 - val_loss: 13214.2207 - val_accuracy: 0.7352\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 8839.2754 - accuracy: 0.8533 - val_loss: 13092.8701 - val_accuracy: 0.7646\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 8778.2988 - accuracy: 0.8555 - val_loss: 12973.5908 - val_accuracy: 0.7396\n",
            "Epoch 20/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 8655.3955 - accuracy: 0.8564roc-auc_val: 0.8106\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 8657.2256 - accuracy: 0.8564 - val_loss: 13100.0068 - val_accuracy: 0.7508\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 8524.1094 - accuracy: 0.8584 - val_loss: 12900.4912 - val_accuracy: 0.7616\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 8374.6396 - accuracy: 0.8586 - val_loss: 12863.2842 - val_accuracy: 0.7749\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 8294.6543 - accuracy: 0.8605 - val_loss: 13120.6250 - val_accuracy: 0.7742\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 8143.5225 - accuracy: 0.8627 - val_loss: 12837.4424 - val_accuracy: 0.7632\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 8037.5781 - accuracy: 0.8610 - val_loss: 12781.1885 - val_accuracy: 0.7710\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 7904.0625 - accuracy: 0.8653 - val_loss: 12756.9141 - val_accuracy: 0.7696\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7929.3086 - accuracy: 0.8629 - val_loss: 12697.4053 - val_accuracy: 0.7801\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7753.2212 - accuracy: 0.8667 - val_loss: 12752.9014 - val_accuracy: 0.7737\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7792.1265 - accuracy: 0.8656 - val_loss: 12384.3779 - val_accuracy: 0.7962\n",
            "Epoch 30/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 7651.3174 - accuracy: 0.8673roc-auc_val: 0.8212\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 7622.5996 - accuracy: 0.8671 - val_loss: 12521.1006 - val_accuracy: 0.7873\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7546.5674 - accuracy: 0.8672 - val_loss: 12707.3076 - val_accuracy: 0.7840\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7577.6724 - accuracy: 0.8689 - val_loss: 12697.1416 - val_accuracy: 0.7908\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7420.0269 - accuracy: 0.8696 - val_loss: 12717.9277 - val_accuracy: 0.7662\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7354.8994 - accuracy: 0.8684 - val_loss: 12580.4326 - val_accuracy: 0.7824\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7325.4473 - accuracy: 0.8716 - val_loss: 12878.9375 - val_accuracy: 0.7749\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7261.1689 - accuracy: 0.8703 - val_loss: 12466.4043 - val_accuracy: 0.8051\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7187.2490 - accuracy: 0.8717 - val_loss: 12493.7607 - val_accuracy: 0.7928\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7118.2334 - accuracy: 0.8728 - val_loss: 12592.9199 - val_accuracy: 0.7900\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7133.7832 - accuracy: 0.8724 - val_loss: 12474.2666 - val_accuracy: 0.8065\n",
            "Epoch 40/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 7007.2705 - accuracy: 0.8754roc-auc_val: 0.8198\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 6994.9365 - accuracy: 0.8754 - val_loss: 12598.2471 - val_accuracy: 0.7863\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 7040.3672 - accuracy: 0.8733 - val_loss: 12614.6279 - val_accuracy: 0.7884\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6927.6968 - accuracy: 0.8740 - val_loss: 12535.8516 - val_accuracy: 0.7960\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6921.2412 - accuracy: 0.8734 - val_loss: 12520.8828 - val_accuracy: 0.8009\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6995.5210 - accuracy: 0.8743 - val_loss: 12752.0146 - val_accuracy: 0.7911\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6833.4663 - accuracy: 0.8747 - val_loss: 12445.4824 - val_accuracy: 0.8041\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6777.4639 - accuracy: 0.8763 - val_loss: 12576.2207 - val_accuracy: 0.8026\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6774.7471 - accuracy: 0.8752 - val_loss: 12652.2275 - val_accuracy: 0.7991\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6672.4844 - accuracy: 0.8778 - val_loss: 12537.5566 - val_accuracy: 0.8015\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6727.2949 - accuracy: 0.8760 - val_loss: 12497.1582 - val_accuracy: 0.8156\n",
            "Epoch 50/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 6653.1860 - accuracy: 0.8775roc-auc_val: 0.8197\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 6620.9170 - accuracy: 0.8774 - val_loss: 12614.6523 - val_accuracy: 0.7984\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6624.7617 - accuracy: 0.8778 - val_loss: 12467.8447 - val_accuracy: 0.8088\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6603.0854 - accuracy: 0.8786 - val_loss: 12458.7393 - val_accuracy: 0.8037\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6562.5352 - accuracy: 0.8778 - val_loss: 12313.8994 - val_accuracy: 0.8017\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6570.0708 - accuracy: 0.8788 - val_loss: 12631.8643 - val_accuracy: 0.7891\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6466.5176 - accuracy: 0.8792 - val_loss: 12467.3047 - val_accuracy: 0.8067\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6462.6084 - accuracy: 0.8803 - val_loss: 12513.8320 - val_accuracy: 0.8152\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6370.5527 - accuracy: 0.8805 - val_loss: 12531.3750 - val_accuracy: 0.8074\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6434.5322 - accuracy: 0.8790 - val_loss: 12515.4102 - val_accuracy: 0.7990\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6425.9980 - accuracy: 0.8802 - val_loss: 12351.6250 - val_accuracy: 0.8121\n",
            "Epoch 60/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 6433.1621 - accuracy: 0.8800roc-auc_val: 0.822\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 6403.6265 - accuracy: 0.8799 - val_loss: 12473.0918 - val_accuracy: 0.8013\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6360.0991 - accuracy: 0.8780 - val_loss: 12480.1816 - val_accuracy: 0.8097\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6390.1558 - accuracy: 0.8812 - val_loss: 12515.6035 - val_accuracy: 0.8062\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6305.6768 - accuracy: 0.8800 - val_loss: 12367.2402 - val_accuracy: 0.8122\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6269.4185 - accuracy: 0.8808 - val_loss: 12515.0088 - val_accuracy: 0.8170\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6242.3066 - accuracy: 0.8835 - val_loss: 12434.1768 - val_accuracy: 0.8055\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6244.3218 - accuracy: 0.8811 - val_loss: 12417.4883 - val_accuracy: 0.8174\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6082.3027 - accuracy: 0.8830 - val_loss: 12403.2334 - val_accuracy: 0.8166\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6191.5415 - accuracy: 0.8817 - val_loss: 12387.5449 - val_accuracy: 0.8210\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6162.8008 - accuracy: 0.8819 - val_loss: 12379.0078 - val_accuracy: 0.8163\n",
            "Epoch 70/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 6039.1460 - accuracy: 0.8828roc-auc_val: 0.8206\n",
            "225/225 [==============================] - 6s 27ms/step - loss: 6035.5518 - accuracy: 0.8828 - val_loss: 12550.2627 - val_accuracy: 0.8092\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6191.8989 - accuracy: 0.8833 - val_loss: 12458.9395 - val_accuracy: 0.8166\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6143.8882 - accuracy: 0.8854 - val_loss: 12582.5127 - val_accuracy: 0.8102\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6084.6240 - accuracy: 0.8820 - val_loss: 12642.5020 - val_accuracy: 0.8022\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6000.6187 - accuracy: 0.8822 - val_loss: 12541.0703 - val_accuracy: 0.8111\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5977.9175 - accuracy: 0.8838 - val_loss: 12545.5586 - val_accuracy: 0.8038\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5943.9189 - accuracy: 0.8840 - val_loss: 12617.0957 - val_accuracy: 0.8061\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 6037.4336 - accuracy: 0.8839 - val_loss: 12485.8662 - val_accuracy: 0.8097\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5875.1177 - accuracy: 0.8843 - val_loss: 12410.7842 - val_accuracy: 0.8130\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5970.2397 - accuracy: 0.8848 - val_loss: 12441.0137 - val_accuracy: 0.8062\n",
            "Epoch 80/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 5935.9966 - accuracy: 0.8831roc-auc_val: 0.8225\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 5918.7827 - accuracy: 0.8831 - val_loss: 12416.4346 - val_accuracy: 0.8156\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5910.5156 - accuracy: 0.8847 - val_loss: 12402.6807 - val_accuracy: 0.8172\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5874.9331 - accuracy: 0.8842 - val_loss: 12325.2939 - val_accuracy: 0.8121\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5829.3809 - accuracy: 0.8852 - val_loss: 12405.5801 - val_accuracy: 0.8095\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5875.9863 - accuracy: 0.8864 - val_loss: 12364.1074 - val_accuracy: 0.8080\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5834.3218 - accuracy: 0.8859 - val_loss: 12427.2832 - val_accuracy: 0.8123\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5783.2021 - accuracy: 0.8853 - val_loss: 12370.3525 - val_accuracy: 0.8138\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5859.1880 - accuracy: 0.8854 - val_loss: 12299.8740 - val_accuracy: 0.8119\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5755.7588 - accuracy: 0.8864 - val_loss: 12501.9053 - val_accuracy: 0.8181\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5743.4507 - accuracy: 0.8875 - val_loss: 12537.0498 - val_accuracy: 0.8058\n",
            "Epoch 90/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 5748.0259 - accuracy: 0.8848roc-auc_val: 0.8241\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 5748.0259 - accuracy: 0.8848 - val_loss: 12323.2656 - val_accuracy: 0.8163\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5755.8354 - accuracy: 0.8839 - val_loss: 12251.7188 - val_accuracy: 0.8228\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5775.4092 - accuracy: 0.8868 - val_loss: 12322.5410 - val_accuracy: 0.8185\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5800.2334 - accuracy: 0.8860 - val_loss: 12371.3223 - val_accuracy: 0.8147\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5625.9990 - accuracy: 0.8869 - val_loss: 12430.3955 - val_accuracy: 0.8174\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5711.9766 - accuracy: 0.8869 - val_loss: 12369.7354 - val_accuracy: 0.8180\n",
            "Epoch 96/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5755.0508 - accuracy: 0.8861 - val_loss: 12385.3887 - val_accuracy: 0.8178\n",
            "Epoch 97/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5718.9526 - accuracy: 0.8868 - val_loss: 12333.2412 - val_accuracy: 0.8148\n",
            "Epoch 98/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5701.9585 - accuracy: 0.8864 - val_loss: 12331.9824 - val_accuracy: 0.8202\n",
            "Epoch 99/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5601.1333 - accuracy: 0.8869 - val_loss: 12359.2988 - val_accuracy: 0.8239\n",
            "Epoch 100/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 5618.9185 - accuracy: 0.8876roc-auc_val: 0.8222\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 5618.9185 - accuracy: 0.8876 - val_loss: 12419.4287 - val_accuracy: 0.8117\n",
            "Epoch 101/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5592.3682 - accuracy: 0.8862 - val_loss: 12420.4414 - val_accuracy: 0.8188\n",
            "Epoch 102/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5538.9756 - accuracy: 0.8880 - val_loss: 12356.1006 - val_accuracy: 0.8190\n",
            "Epoch 103/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5585.4448 - accuracy: 0.8869 - val_loss: 12409.5625 - val_accuracy: 0.8270\n",
            "Epoch 104/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5607.7905 - accuracy: 0.8890 - val_loss: 12382.5947 - val_accuracy: 0.8277\n",
            "Epoch 105/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5680.8628 - accuracy: 0.8873 - val_loss: 12333.8770 - val_accuracy: 0.8178\n",
            "Epoch 106/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5586.8882 - accuracy: 0.8869 - val_loss: 12336.3350 - val_accuracy: 0.8228\n",
            "Epoch 107/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5593.0918 - accuracy: 0.8881 - val_loss: 12382.0674 - val_accuracy: 0.8199\n",
            "Epoch 108/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5531.8892 - accuracy: 0.8878 - val_loss: 12326.6709 - val_accuracy: 0.8171\n",
            "Epoch 109/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5504.3989 - accuracy: 0.8894 - val_loss: 12335.3799 - val_accuracy: 0.8254\n",
            "Epoch 110/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 5467.1338 - accuracy: 0.8885roc-auc_val: 0.822\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 5462.6553 - accuracy: 0.8885 - val_loss: 12439.4707 - val_accuracy: 0.8205\n",
            "Epoch 111/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5520.4351 - accuracy: 0.8894 - val_loss: 12425.7480 - val_accuracy: 0.8171\n",
            "Epoch 112/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5524.3906 - accuracy: 0.8893 - val_loss: 12348.5166 - val_accuracy: 0.8251\n",
            "Epoch 113/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5507.7603 - accuracy: 0.8885 - val_loss: 12362.9336 - val_accuracy: 0.8191\n",
            "Epoch 114/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5395.5259 - accuracy: 0.8886 - val_loss: 12409.2734 - val_accuracy: 0.8243\n",
            "Epoch 115/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5437.0176 - accuracy: 0.8900 - val_loss: 12423.5596 - val_accuracy: 0.8187\n",
            "Epoch 116/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5445.7227 - accuracy: 0.8895 - val_loss: 12303.6660 - val_accuracy: 0.8310\n",
            "Epoch 117/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5498.2251 - accuracy: 0.8908 - val_loss: 12380.3135 - val_accuracy: 0.8313\n",
            "Epoch 118/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5519.5435 - accuracy: 0.8892 - val_loss: 12418.4570 - val_accuracy: 0.8178\n",
            "Epoch 119/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5415.7114 - accuracy: 0.8902 - val_loss: 12329.5576 - val_accuracy: 0.8228\n",
            "Epoch 120/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 5451.1528 - accuracy: 0.8902roc-auc_val: 0.8222\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 5439.7476 - accuracy: 0.8901 - val_loss: 12411.7178 - val_accuracy: 0.8183\n",
            "Epoch 121/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5424.0586 - accuracy: 0.8894 - val_loss: 12368.3770 - val_accuracy: 0.8260\n",
            "Epoch 122/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5467.1313 - accuracy: 0.8883 - val_loss: 12294.9990 - val_accuracy: 0.8257\n",
            "Epoch 123/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5429.4941 - accuracy: 0.8900 - val_loss: 12351.3389 - val_accuracy: 0.8221\n",
            "Epoch 124/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5402.3267 - accuracy: 0.8895 - val_loss: 12445.1475 - val_accuracy: 0.8174\n",
            "Epoch 125/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5421.1855 - accuracy: 0.8891 - val_loss: 12356.7402 - val_accuracy: 0.8295\n",
            "Epoch 126/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5346.9971 - accuracy: 0.8920 - val_loss: 12456.5439 - val_accuracy: 0.8282\n",
            "Epoch 127/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5294.5420 - accuracy: 0.8917 - val_loss: 12468.6035 - val_accuracy: 0.8299\n",
            "Epoch 128/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5306.7280 - accuracy: 0.8905 - val_loss: 12487.7275 - val_accuracy: 0.8282\n",
            "Epoch 129/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5350.7964 - accuracy: 0.8907 - val_loss: 12390.5430 - val_accuracy: 0.8249\n",
            "Epoch 130/1000\n",
            "221/225 [============================>.] - ETA: 0s - loss: 5356.7456 - accuracy: 0.8898roc-auc_val: 0.8205\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 5345.7300 - accuracy: 0.8898 - val_loss: 12508.1758 - val_accuracy: 0.8196\n",
            "Epoch 131/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5333.8452 - accuracy: 0.8911 - val_loss: 12420.8818 - val_accuracy: 0.8210\n",
            "Epoch 132/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5403.4844 - accuracy: 0.8883 - val_loss: 12323.0361 - val_accuracy: 0.8307\n",
            "Epoch 133/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5334.1279 - accuracy: 0.8903 - val_loss: 12364.1592 - val_accuracy: 0.8299\n",
            "Epoch 134/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5343.4233 - accuracy: 0.8903 - val_loss: 12319.9844 - val_accuracy: 0.8331\n",
            "Epoch 135/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5307.3101 - accuracy: 0.8911 - val_loss: 12397.0830 - val_accuracy: 0.8298\n",
            "Epoch 136/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5283.3379 - accuracy: 0.8924 - val_loss: 12365.2197 - val_accuracy: 0.8283\n",
            "Epoch 137/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5283.2700 - accuracy: 0.8912 - val_loss: 12383.6631 - val_accuracy: 0.8321\n",
            "Epoch 138/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5230.9839 - accuracy: 0.8906 - val_loss: 12459.2861 - val_accuracy: 0.8221\n",
            "Epoch 139/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5241.8755 - accuracy: 0.8923 - val_loss: 12402.4697 - val_accuracy: 0.8282\n",
            "Epoch 140/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 5184.6802 - accuracy: 0.8919roc-auc_val: 0.8228\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 5171.1274 - accuracy: 0.8918 - val_loss: 12376.7549 - val_accuracy: 0.8305\n",
            "Epoch 141/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 5264.3120 - accuracy: 0.8930 - val_loss: 12496.9277 - val_accuracy: 0.8249\n",
            "Epoch 1/1000\n",
            "181/181 [==============================] - 2s 12ms/step - loss: 22354.1465 - accuracy: 0.6421 - val_loss: 18536.5332 - val_accuracy: 0.6387\n",
            "Epoch 2/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17431.6211 - accuracy: 0.7326 - val_loss: 16736.8926 - val_accuracy: 0.6933\n",
            "Epoch 3/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 15392.2773 - accuracy: 0.7712 - val_loss: 16286.9902 - val_accuracy: 0.7335\n",
            "Epoch 4/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 14143.1533 - accuracy: 0.7910 - val_loss: 16112.0156 - val_accuracy: 0.7130\n",
            "Epoch 5/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 13321.7588 - accuracy: 0.8046 - val_loss: 15906.3955 - val_accuracy: 0.7294\n",
            "Epoch 6/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 12534.8271 - accuracy: 0.8109 - val_loss: 15918.7949 - val_accuracy: 0.7503\n",
            "Epoch 7/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 11920.6572 - accuracy: 0.8205 - val_loss: 15735.6504 - val_accuracy: 0.7695\n",
            "Epoch 8/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 11497.1309 - accuracy: 0.8275 - val_loss: 15729.5830 - val_accuracy: 0.7450\n",
            "Epoch 9/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 10942.7529 - accuracy: 0.8305 - val_loss: 15837.1729 - val_accuracy: 0.7749\n",
            "Epoch 10/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 10660.6553 - accuracy: 0.8346roc-auc_val: 0.8146\n",
            "181/181 [==============================] - 9s 49ms/step - loss: 10637.0381 - accuracy: 0.8349 - val_loss: 15471.7314 - val_accuracy: 0.8018\n",
            "Epoch 11/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 10322.0078 - accuracy: 0.8388 - val_loss: 15176.6621 - val_accuracy: 0.7873\n",
            "Epoch 12/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 9978.4678 - accuracy: 0.8380 - val_loss: 15186.6973 - val_accuracy: 0.7944\n",
            "Epoch 13/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 9766.0654 - accuracy: 0.8407 - val_loss: 15281.7959 - val_accuracy: 0.8061\n",
            "Epoch 14/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 9469.2393 - accuracy: 0.8472 - val_loss: 15128.2578 - val_accuracy: 0.7784\n",
            "Epoch 15/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 9190.7373 - accuracy: 0.8465 - val_loss: 15229.4697 - val_accuracy: 0.7792\n",
            "Epoch 16/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 9067.8506 - accuracy: 0.8458 - val_loss: 15389.6436 - val_accuracy: 0.8248\n",
            "Epoch 17/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 8882.4102 - accuracy: 0.8477 - val_loss: 15323.2285 - val_accuracy: 0.7936\n",
            "Epoch 18/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 8649.7637 - accuracy: 0.8495 - val_loss: 15121.6143 - val_accuracy: 0.7912\n",
            "Epoch 19/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 8566.9307 - accuracy: 0.8502 - val_loss: 15328.2490 - val_accuracy: 0.7981\n",
            "Epoch 20/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 8388.4385 - accuracy: 0.8543roc-auc_val: 0.8186\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 8383.1836 - accuracy: 0.8542 - val_loss: 15201.8135 - val_accuracy: 0.8020\n",
            "Epoch 21/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 8157.0039 - accuracy: 0.8537 - val_loss: 15194.1406 - val_accuracy: 0.8033\n",
            "Epoch 22/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 8050.2290 - accuracy: 0.8536 - val_loss: 15426.4307 - val_accuracy: 0.8102\n",
            "Epoch 23/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 8011.4219 - accuracy: 0.8564 - val_loss: 15443.4492 - val_accuracy: 0.8148\n",
            "Epoch 24/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 7814.7881 - accuracy: 0.8575 - val_loss: 15398.8613 - val_accuracy: 0.8165\n",
            "Epoch 25/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 7762.0854 - accuracy: 0.8574 - val_loss: 15438.0225 - val_accuracy: 0.8205\n",
            "Epoch 26/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 7582.9146 - accuracy: 0.8625 - val_loss: 15258.4316 - val_accuracy: 0.8190\n",
            "Epoch 27/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 7478.0845 - accuracy: 0.8598 - val_loss: 15415.9443 - val_accuracy: 0.8129\n",
            "Epoch 28/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 7477.5562 - accuracy: 0.8618 - val_loss: 15296.2021 - val_accuracy: 0.8055\n",
            "Epoch 29/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 7361.3486 - accuracy: 0.8601 - val_loss: 15148.0596 - val_accuracy: 0.8186\n",
            "Epoch 30/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 7226.2534 - accuracy: 0.8631roc-auc_val: 0.8198\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 7267.6343 - accuracy: 0.8633 - val_loss: 15269.5547 - val_accuracy: 0.8316\n",
            "Epoch 31/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 7151.6514 - accuracy: 0.8632 - val_loss: 15212.2754 - val_accuracy: 0.8206\n",
            "Epoch 32/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 7097.2725 - accuracy: 0.8642 - val_loss: 15514.5967 - val_accuracy: 0.8300\n",
            "Epoch 33/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 7132.1094 - accuracy: 0.8649 - val_loss: 15467.2998 - val_accuracy: 0.8326\n",
            "Epoch 34/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6892.0640 - accuracy: 0.8649 - val_loss: 15283.8379 - val_accuracy: 0.8276\n",
            "Epoch 35/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6925.6357 - accuracy: 0.8656 - val_loss: 15292.2559 - val_accuracy: 0.8284\n",
            "Epoch 36/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6917.1475 - accuracy: 0.8670 - val_loss: 15493.5068 - val_accuracy: 0.8266\n",
            "Epoch 37/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6777.5239 - accuracy: 0.8681 - val_loss: 15420.8506 - val_accuracy: 0.8335\n",
            "Epoch 38/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6736.4536 - accuracy: 0.8674 - val_loss: 15590.5967 - val_accuracy: 0.8348\n",
            "Epoch 39/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6704.4868 - accuracy: 0.8663 - val_loss: 15338.4355 - val_accuracy: 0.8413\n",
            "Epoch 40/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 6529.9238 - accuracy: 0.8685roc-auc_val: 0.8209\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 6530.7095 - accuracy: 0.8685 - val_loss: 15345.3467 - val_accuracy: 0.8584\n",
            "Epoch 41/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6509.3823 - accuracy: 0.8704 - val_loss: 15322.1787 - val_accuracy: 0.8510\n",
            "Epoch 42/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6481.1987 - accuracy: 0.8707 - val_loss: 15436.7676 - val_accuracy: 0.8429\n",
            "Epoch 43/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6495.2051 - accuracy: 0.8682 - val_loss: 15363.2949 - val_accuracy: 0.8369\n",
            "Epoch 44/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6370.3735 - accuracy: 0.8711 - val_loss: 15455.0986 - val_accuracy: 0.8512\n",
            "Epoch 45/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6336.3979 - accuracy: 0.8718 - val_loss: 15464.9736 - val_accuracy: 0.8508\n",
            "Epoch 46/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6281.6401 - accuracy: 0.8699 - val_loss: 15683.1465 - val_accuracy: 0.8515\n",
            "Epoch 47/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6242.2798 - accuracy: 0.8739 - val_loss: 15608.8447 - val_accuracy: 0.8497\n",
            "Epoch 48/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6103.3604 - accuracy: 0.8730 - val_loss: 15878.7930 - val_accuracy: 0.8403\n",
            "Epoch 49/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6147.9229 - accuracy: 0.8722 - val_loss: 15537.1836 - val_accuracy: 0.8573\n",
            "Epoch 50/1000\n",
            "180/181 [============================>.] - ETA: 0s - loss: 6137.1577 - accuracy: 0.8740roc-auc_val: 0.8152\n",
            "181/181 [==============================] - 9s 49ms/step - loss: 6123.9434 - accuracy: 0.8740 - val_loss: 15796.3047 - val_accuracy: 0.8400\n",
            "Epoch 51/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 6110.6006 - accuracy: 0.8715 - val_loss: 15798.0654 - val_accuracy: 0.8430\n",
            "Epoch 52/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5999.1270 - accuracy: 0.8737 - val_loss: 15731.4531 - val_accuracy: 0.8514\n",
            "Epoch 53/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5948.6562 - accuracy: 0.8765 - val_loss: 15656.4492 - val_accuracy: 0.8444\n",
            "Epoch 54/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5960.8960 - accuracy: 0.8762 - val_loss: 15700.7129 - val_accuracy: 0.8450\n",
            "Epoch 55/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5905.1953 - accuracy: 0.8738 - val_loss: 15785.9854 - val_accuracy: 0.8549\n",
            "Epoch 56/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5892.3613 - accuracy: 0.8767 - val_loss: 15817.0684 - val_accuracy: 0.8428\n",
            "Epoch 57/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5903.7715 - accuracy: 0.8746 - val_loss: 15944.5996 - val_accuracy: 0.8387\n",
            "Epoch 58/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5755.1880 - accuracy: 0.8758 - val_loss: 15870.1846 - val_accuracy: 0.8412\n",
            "Epoch 59/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5788.9839 - accuracy: 0.8765 - val_loss: 15804.5635 - val_accuracy: 0.8401\n",
            "Epoch 60/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 5718.1436 - accuracy: 0.8764roc-auc_val: 0.8155\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 5716.2920 - accuracy: 0.8765 - val_loss: 15753.8076 - val_accuracy: 0.8434\n",
            "Epoch 61/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5805.8540 - accuracy: 0.8763 - val_loss: 15839.1914 - val_accuracy: 0.8419\n",
            "Epoch 62/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5764.3296 - accuracy: 0.8769 - val_loss: 15833.3193 - val_accuracy: 0.8432\n",
            "Epoch 63/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5598.0547 - accuracy: 0.8773 - val_loss: 15843.9727 - val_accuracy: 0.8491\n",
            "Epoch 64/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5707.2485 - accuracy: 0.8819 - val_loss: 15875.8994 - val_accuracy: 0.8486\n",
            "Epoch 65/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5627.4180 - accuracy: 0.8773 - val_loss: 15878.8574 - val_accuracy: 0.8508\n",
            "Epoch 66/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5573.9253 - accuracy: 0.8784 - val_loss: 15826.3438 - val_accuracy: 0.8478\n",
            "Epoch 67/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5576.1743 - accuracy: 0.8797 - val_loss: 15835.9561 - val_accuracy: 0.8524\n",
            "Epoch 68/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 5464.9224 - accuracy: 0.8790 - val_loss: 15839.4219 - val_accuracy: 0.8532\n",
            "Epoch 1/1000\n",
            "136/136 [==============================] - 2s 15ms/step - loss: 22587.4141 - accuracy: 0.6332 - val_loss: 19755.1602 - val_accuracy: 0.6041\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17368.3965 - accuracy: 0.7222 - val_loss: 18769.9160 - val_accuracy: 0.6875\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 15251.7617 - accuracy: 0.7665 - val_loss: 17971.3320 - val_accuracy: 0.6992\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 14076.9980 - accuracy: 0.7922 - val_loss: 17695.3262 - val_accuracy: 0.7335\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 13211.9414 - accuracy: 0.8009 - val_loss: 17607.7207 - val_accuracy: 0.6936\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 12578.7100 - accuracy: 0.8077 - val_loss: 17211.8320 - val_accuracy: 0.7625\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 11779.3887 - accuracy: 0.8176 - val_loss: 17295.6953 - val_accuracy: 0.7611\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 11353.2158 - accuracy: 0.8211 - val_loss: 17199.6328 - val_accuracy: 0.7910\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 10827.6992 - accuracy: 0.8289 - val_loss: 17220.4668 - val_accuracy: 0.7784\n",
            "Epoch 10/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 10346.7812 - accuracy: 0.8353roc-auc_val: 0.8104\n",
            "136/136 [==============================] - 12s 86ms/step - loss: 10305.3047 - accuracy: 0.8351 - val_loss: 17560.8340 - val_accuracy: 0.7500\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 10000.4785 - accuracy: 0.8370 - val_loss: 17357.6875 - val_accuracy: 0.7616\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 9652.8604 - accuracy: 0.8425 - val_loss: 17185.6348 - val_accuracy: 0.8017\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 9400.2080 - accuracy: 0.8442 - val_loss: 17061.0195 - val_accuracy: 0.7870\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 9030.1338 - accuracy: 0.8452 - val_loss: 17316.1914 - val_accuracy: 0.7696\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 8847.8086 - accuracy: 0.8476 - val_loss: 17737.7793 - val_accuracy: 0.7647\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 8662.1758 - accuracy: 0.8474 - val_loss: 17459.5508 - val_accuracy: 0.8067\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 8328.0586 - accuracy: 0.8525 - val_loss: 17495.9336 - val_accuracy: 0.8130\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 8247.2578 - accuracy: 0.8540 - val_loss: 17374.2188 - val_accuracy: 0.8078\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 8135.0000 - accuracy: 0.8562 - val_loss: 17382.9531 - val_accuracy: 0.7975\n",
            "Epoch 20/1000\n",
            "134/136 [============================>.] - ETA: 0s - loss: 7895.8013 - accuracy: 0.8597roc-auc_val: 0.8073\n",
            "136/136 [==============================] - 12s 86ms/step - loss: 7881.7793 - accuracy: 0.8598 - val_loss: 17877.4629 - val_accuracy: 0.8213\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 7814.3887 - accuracy: 0.8607 - val_loss: 17899.5312 - val_accuracy: 0.8045\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 7651.2886 - accuracy: 0.8617 - val_loss: 17632.7949 - val_accuracy: 0.8197\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 7456.5532 - accuracy: 0.8605 - val_loss: 18070.2949 - val_accuracy: 0.8072\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 7377.6714 - accuracy: 0.8644 - val_loss: 18148.9102 - val_accuracy: 0.7857\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 7207.2476 - accuracy: 0.8633 - val_loss: 18314.1816 - val_accuracy: 0.7991\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 7047.2896 - accuracy: 0.8657 - val_loss: 17803.5977 - val_accuracy: 0.7598\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 7057.9072 - accuracy: 0.8652 - val_loss: 17930.4961 - val_accuracy: 0.7964\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 6897.5298 - accuracy: 0.8698 - val_loss: 18026.1895 - val_accuracy: 0.8062\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 6820.5288 - accuracy: 0.8696 - val_loss: 18024.6211 - val_accuracy: 0.7834\n",
            "Epoch 30/1000\n",
            "135/136 [============================>.] - ETA: 0s - loss: 6823.4370 - accuracy: 0.8691roc-auc_val: 0.802\n",
            "136/136 [==============================] - 12s 85ms/step - loss: 6803.0703 - accuracy: 0.8691 - val_loss: 18196.3652 - val_accuracy: 0.7923\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 6591.1143 - accuracy: 0.8698 - val_loss: 18133.1602 - val_accuracy: 0.7813\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 6459.0347 - accuracy: 0.8704 - val_loss: 18126.2910 - val_accuracy: 0.7937\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 6532.7051 - accuracy: 0.8733 - val_loss: 18037.9141 - val_accuracy: 0.7931\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 6475.1401 - accuracy: 0.8721 - val_loss: 18317.5039 - val_accuracy: 0.7725\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 6321.1597 - accuracy: 0.8727 - val_loss: 17906.6309 - val_accuracy: 0.8104\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 6231.9214 - accuracy: 0.8778 - val_loss: 18425.9941 - val_accuracy: 0.7767\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 6230.9839 - accuracy: 0.8753 - val_loss: 18375.8164 - val_accuracy: 0.8038\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 6127.1582 - accuracy: 0.8781 - val_loss: 18419.2129 - val_accuracy: 0.8059\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5961.2661 - accuracy: 0.8793 - val_loss: 18431.8965 - val_accuracy: 0.7777\n",
            "Epoch 40/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 6026.2095 - accuracy: 0.8756roc-auc_val: 0.8012\n",
            "136/136 [==============================] - 12s 86ms/step - loss: 6021.2012 - accuracy: 0.8756 - val_loss: 18187.8066 - val_accuracy: 0.7889\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5938.7822 - accuracy: 0.8785 - val_loss: 18496.5664 - val_accuracy: 0.7829\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 5879.0020 - accuracy: 0.8788 - val_loss: 18508.7480 - val_accuracy: 0.7870\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5842.5815 - accuracy: 0.8786 - val_loss: 18525.9805 - val_accuracy: 0.7756\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 5726.8550 - accuracy: 0.8777 - val_loss: 18557.9082 - val_accuracy: 0.7866\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 5796.7085 - accuracy: 0.8806 - val_loss: 18609.1211 - val_accuracy: 0.7955\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5624.9502 - accuracy: 0.8820 - val_loss: 18716.3770 - val_accuracy: 0.7865\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 5635.7959 - accuracy: 0.8814 - val_loss: 18775.1641 - val_accuracy: 0.8120\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5564.0308 - accuracy: 0.8822 - val_loss: 18840.4121 - val_accuracy: 0.7963\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 5491.9214 - accuracy: 0.8859 - val_loss: 18866.8379 - val_accuracy: 0.8051\n",
            "Epoch 50/1000\n",
            "131/136 [===========================>..] - ETA: 0s - loss: 5489.8037 - accuracy: 0.8835roc-auc_val: 0.7945\n",
            "136/136 [==============================] - 12s 85ms/step - loss: 5462.0742 - accuracy: 0.8833 - val_loss: 18848.1680 - val_accuracy: 0.7852\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5414.1870 - accuracy: 0.8830 - val_loss: 18905.2246 - val_accuracy: 0.7921\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5380.1553 - accuracy: 0.8863 - val_loss: 19065.1660 - val_accuracy: 0.7989\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5424.4150 - accuracy: 0.8858 - val_loss: 19059.3535 - val_accuracy: 0.7751\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5426.5557 - accuracy: 0.8839 - val_loss: 19249.6387 - val_accuracy: 0.7744\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5300.7544 - accuracy: 0.8853 - val_loss: 19022.9590 - val_accuracy: 0.7875\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 5250.4126 - accuracy: 0.8877 - val_loss: 19220.0020 - val_accuracy: 0.8006\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 5246.0024 - accuracy: 0.8877 - val_loss: 19292.9414 - val_accuracy: 0.7825\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5309.1050 - accuracy: 0.8872 - val_loss: 19201.1426 - val_accuracy: 0.7806\n",
            "Epoch 59/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 5096.3755 - accuracy: 0.8866 - val_loss: 19304.0938 - val_accuracy: 0.7799\n",
            "Epoch 60/1000\n",
            "136/136 [==============================] - ETA: 0s - loss: 5123.9521 - accuracy: 0.8894roc-auc_val: 0.7898\n",
            "136/136 [==============================] - 12s 86ms/step - loss: 5123.9521 - accuracy: 0.8894 - val_loss: 19189.0898 - val_accuracy: 0.7913\n",
            "Epoch 61/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 5083.7041 - accuracy: 0.8902 - val_loss: 19075.9004 - val_accuracy: 0.7803\n",
            "Epoch 62/1000\n",
            "136/136 [==============================] - 1s 11ms/step - loss: 5078.8994 - accuracy: 0.8919 - val_loss: 19306.5332 - val_accuracy: 0.7913\n",
            "Epoch 63/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 5021.2334 - accuracy: 0.8907 - val_loss: 19426.1562 - val_accuracy: 0.7796\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 23ms/step - loss: 22750.8516 - accuracy: 0.6240 - val_loss: 21614.9531 - val_accuracy: 0.6043\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 17337.0312 - accuracy: 0.7049 - val_loss: 19518.1289 - val_accuracy: 0.7143\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 15540.3809 - accuracy: 0.7413 - val_loss: 19364.2480 - val_accuracy: 0.7487\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 14077.8877 - accuracy: 0.7660 - val_loss: 18729.5586 - val_accuracy: 0.7773\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12854.4980 - accuracy: 0.7829 - val_loss: 18392.9062 - val_accuracy: 0.7470\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11874.0889 - accuracy: 0.7981 - val_loss: 18131.6309 - val_accuracy: 0.7539\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 11083.2109 - accuracy: 0.8114 - val_loss: 18256.8223 - val_accuracy: 0.7731\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 10501.6816 - accuracy: 0.8162 - val_loss: 18064.7383 - val_accuracy: 0.7877\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 10058.3242 - accuracy: 0.8265 - val_loss: 18368.2324 - val_accuracy: 0.7653\n",
            "Epoch 10/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 9665.7588 - accuracy: 0.8268roc-auc_val: 0.814\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 9657.5605 - accuracy: 0.8267 - val_loss: 18052.6094 - val_accuracy: 0.7922\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 9284.0488 - accuracy: 0.8297 - val_loss: 18469.6816 - val_accuracy: 0.8000\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 8920.2705 - accuracy: 0.8367 - val_loss: 18429.8730 - val_accuracy: 0.8204\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 8439.8076 - accuracy: 0.8432 - val_loss: 18536.2598 - val_accuracy: 0.8364\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 8213.0635 - accuracy: 0.8471 - val_loss: 18859.0723 - val_accuracy: 0.8407\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 7867.3872 - accuracy: 0.8483 - val_loss: 18369.4961 - val_accuracy: 0.8134\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 7727.2578 - accuracy: 0.8507 - val_loss: 18698.3633 - val_accuracy: 0.8237\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 7496.9551 - accuracy: 0.8506 - val_loss: 18611.8008 - val_accuracy: 0.8213\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 7239.7017 - accuracy: 0.8586 - val_loss: 18773.0078 - val_accuracy: 0.8215\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 7034.9194 - accuracy: 0.8584 - val_loss: 19219.2500 - val_accuracy: 0.8266\n",
            "Epoch 20/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 6868.4731 - accuracy: 0.8612roc-auc_val: 0.8024\n",
            "88/88 [==============================] - 15s 170ms/step - loss: 6868.4731 - accuracy: 0.8612 - val_loss: 19080.4746 - val_accuracy: 0.8216\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 6815.2495 - accuracy: 0.8634 - val_loss: 19430.8711 - val_accuracy: 0.8321\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 6625.4302 - accuracy: 0.8677 - val_loss: 19026.2246 - val_accuracy: 0.8182\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 6469.8818 - accuracy: 0.8686 - val_loss: 19196.6934 - val_accuracy: 0.8203\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 6463.6470 - accuracy: 0.8692 - val_loss: 19255.9668 - val_accuracy: 0.8064\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 6239.0137 - accuracy: 0.8641 - val_loss: 19328.1250 - val_accuracy: 0.8127\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 6034.2632 - accuracy: 0.8723 - val_loss: 19224.5352 - val_accuracy: 0.8271\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5940.1084 - accuracy: 0.8761 - val_loss: 19210.7637 - val_accuracy: 0.8301\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5767.7500 - accuracy: 0.8754 - val_loss: 19560.1758 - val_accuracy: 0.8235\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5696.0513 - accuracy: 0.8779 - val_loss: 19311.5156 - val_accuracy: 0.8221\n",
            "Epoch 30/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 5585.3428 - accuracy: 0.8811roc-auc_val: 0.7948\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 5640.6265 - accuracy: 0.8812 - val_loss: 19694.4297 - val_accuracy: 0.8083\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5517.0718 - accuracy: 0.8778 - val_loss: 19659.6289 - val_accuracy: 0.8292\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5433.6577 - accuracy: 0.8844 - val_loss: 19758.9062 - val_accuracy: 0.8378\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5239.6948 - accuracy: 0.8825 - val_loss: 19817.7793 - val_accuracy: 0.8258\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5326.7280 - accuracy: 0.8837 - val_loss: 19752.3438 - val_accuracy: 0.8279\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5233.8994 - accuracy: 0.8867 - val_loss: 19734.2754 - val_accuracy: 0.8263\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5145.2690 - accuracy: 0.8828 - val_loss: 19927.4277 - val_accuracy: 0.8223\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4957.0674 - accuracy: 0.8881 - val_loss: 20119.6328 - val_accuracy: 0.8344\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 5033.2578 - accuracy: 0.8860 - val_loss: 20024.2383 - val_accuracy: 0.8265\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4842.1006 - accuracy: 0.8884 - val_loss: 20103.4297 - val_accuracy: 0.8263\n",
            "Epoch 40/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 4747.0645 - accuracy: 0.8931roc-auc_val: 0.7938\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 4747.0645 - accuracy: 0.8931 - val_loss: 19936.3750 - val_accuracy: 0.8259\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4744.8452 - accuracy: 0.8898 - val_loss: 20143.1055 - val_accuracy: 0.8374\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4668.4971 - accuracy: 0.8910 - val_loss: 20218.4336 - val_accuracy: 0.8342\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4602.4375 - accuracy: 0.8949 - val_loss: 20298.5566 - val_accuracy: 0.8317\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4517.3721 - accuracy: 0.8943 - val_loss: 20207.4316 - val_accuracy: 0.8249\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4549.7422 - accuracy: 0.8952 - val_loss: 20141.6562 - val_accuracy: 0.8335\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4545.3335 - accuracy: 0.8945 - val_loss: 20314.5137 - val_accuracy: 0.8306\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4473.6094 - accuracy: 0.8975 - val_loss: 20539.7148 - val_accuracy: 0.8264\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4402.7808 - accuracy: 0.8955 - val_loss: 20397.9141 - val_accuracy: 0.8223\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4416.2852 - accuracy: 0.8956 - val_loss: 20475.4570 - val_accuracy: 0.8271\n",
            "Epoch 50/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 4218.3999 - accuracy: 0.8955roc-auc_val: 0.7858\n",
            "88/88 [==============================] - 15s 169ms/step - loss: 4247.0903 - accuracy: 0.8961 - val_loss: 20620.7129 - val_accuracy: 0.8268\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4349.9878 - accuracy: 0.8966 - val_loss: 20170.8477 - val_accuracy: 0.8261\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4244.2534 - accuracy: 0.8989 - val_loss: 20251.8535 - val_accuracy: 0.8283\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4252.6665 - accuracy: 0.8998 - val_loss: 20153.0039 - val_accuracy: 0.8209\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4144.9487 - accuracy: 0.8994 - val_loss: 20294.3770 - val_accuracy: 0.8291\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4199.3813 - accuracy: 0.9002 - val_loss: 20477.1426 - val_accuracy: 0.8314\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4055.8896 - accuracy: 0.9016 - val_loss: 20213.3066 - val_accuracy: 0.8224\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4039.5198 - accuracy: 0.9013 - val_loss: 20358.4590 - val_accuracy: 0.8028\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4058.4844 - accuracy: 0.8986 - val_loss: 20411.4355 - val_accuracy: 0.8193\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 4002.6426 - accuracy: 0.9032 - val_loss: 20805.5430 - val_accuracy: 0.7970\n",
            "Epoch 60/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 3779.1848 - accuracy: 0.9010roc-auc_val: 0.814\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 3805.3284 - accuracy: 0.9013 - val_loss: 20702.7285 - val_accuracy: 0.8375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5PePWLxWIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8c0c45e-fafb-4ca1-a3dc-6d2b2f275609"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNDmjro-2ndD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_4.0",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "a38cf157-c64d-4755-845d-22b679cdddea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "5364d243-24b0-4f31-f4ab-24a6d109b6b6"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 79.4MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 98% 51.0M/52.2M [00:01<00:00, 31.8MB/s]\n",
            "100% 52.2M/52.2M [00:01<00:00, 40.0MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 79% 46.0M/58.3M [00:01<00:00, 43.6MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 52.4MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 218MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 222MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDrCIAqHzl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "d072d785-b1b4-4c76-920f-7babb7c3277e"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWJ-hzcznam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5OQjb1zo6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "07632f0b-1078-49b0-9ad9-f4efb0bcf52f"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "63194746-929e-40e8-876e-11663640b4c4"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0r3SuH1K97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQ20JqWATak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQIVOLKBFIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1fab9f9f-c5db-4ada-e469-50dfdfaeac38"
      },
      "source": [
        "1-0.036"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.964"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d5e3caa-f799-41ed-afff-777eb017c39c"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=4\n",
        "        alpha=1-0.036\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss\n",
        "dk={}\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "for en,month in enumerate([(4,5),(3,4),(3,5)]):\n",
        "  train=trn.loc[trn['month']>=month[1]]\n",
        "  test=trn.loc[trn['month']<=month[0]]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.0001,decay=1e-3),loss=fl())\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 25ms/step - loss: 31.7331 - val_loss: 6.2509\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.6586 - val_loss: 6.2958\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 20.1853 - val_loss: 6.4141\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 18.9401 - val_loss: 6.5723\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.8762 - val_loss: 6.7329\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 16.5821 - val_loss: 7.0201\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 15.9039 - val_loss: 7.2441\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 15.4636 - val_loss: 7.2850\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 14.2317 - val_loss: 7.1340\n",
            "Epoch 10/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 13.4953roc-auc_val: 0.7459\n",
            "47/47 [==============================] - 13s 284ms/step - loss: 13.5923 - val_loss: 7.0808\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 13.0168 - val_loss: 7.1855\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 12.9971 - val_loss: 7.2510\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 12.5588 - val_loss: 7.1376\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 12.2201 - val_loss: 7.0572\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 11.5947 - val_loss: 7.0548\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 11.6225 - val_loss: 7.0226\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 10.7131 - val_loss: 7.0214\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 10.7518 - val_loss: 7.0866\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 10.3826 - val_loss: 6.9003\n",
            "Epoch 20/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 10.6464roc-auc_val: 0.7564\n",
            "47/47 [==============================] - 14s 288ms/step - loss: 10.5978 - val_loss: 6.8948\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 9.8064 - val_loss: 6.8610\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 10.2756 - val_loss: 6.9154\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 10.2873 - val_loss: 6.9796\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 9.6313 - val_loss: 6.8793\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 9.1332 - val_loss: 6.8472\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 9.0128 - val_loss: 6.7771\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 9.3449 - val_loss: 6.8988\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 8.7263 - val_loss: 6.8779\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 9.2798 - val_loss: 6.8638\n",
            "Epoch 30/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 8.5377roc-auc_val: 0.7646\n",
            "47/47 [==============================] - 13s 281ms/step - loss: 8.5206 - val_loss: 6.8262\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 8.4952 - val_loss: 6.8471\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 8.5552 - val_loss: 6.8082\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 8.3144 - val_loss: 6.7278\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 8.0843 - val_loss: 6.7640\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 8.3985 - val_loss: 6.7424\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 8.2815 - val_loss: 6.7860\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.9474 - val_loss: 6.7742\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.8884 - val_loss: 6.6990\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.9861 - val_loss: 6.7318\n",
            "Epoch 40/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 8.0242roc-auc_val: 0.7691\n",
            "47/47 [==============================] - 13s 279ms/step - loss: 7.9312 - val_loss: 6.7486\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.7399 - val_loss: 6.7563\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.8593 - val_loss: 6.7432\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.6347 - val_loss: 6.7453\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.4336 - val_loss: 6.7038\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.6997 - val_loss: 6.7211\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.7149 - val_loss: 6.7747\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.3738 - val_loss: 6.7456\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.2704 - val_loss: 6.7060\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.4999 - val_loss: 6.6748\n",
            "Epoch 50/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 7.0195roc-auc_val: 0.7728\n",
            "47/47 [==============================] - 13s 280ms/step - loss: 6.9835 - val_loss: 6.6690\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 7.3053 - val_loss: 6.6642\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 29.6298 - val_loss: 7.2784\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.5746 - val_loss: 7.4615\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.5524 - val_loss: 7.6889\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.9021 - val_loss: 7.9128\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9389 - val_loss: 7.6540\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.0985 - val_loss: 7.3621\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 14.3021 - val_loss: 7.5245\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 13.3546 - val_loss: 7.3467\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 13.0496 - val_loss: 7.2844\n",
            "Epoch 10/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 12.6074roc-auc_val: 0.7433\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 12.4600 - val_loss: 7.0402\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 11.6412 - val_loss: 6.9086\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 11.2438 - val_loss: 6.8013\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 10.8082 - val_loss: 6.7161\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 10.7060 - val_loss: 6.7572\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 10.5647 - val_loss: 6.6412\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 10.0426 - val_loss: 6.5276\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 9.7466 - val_loss: 6.5369\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 9.6138 - val_loss: 6.5360\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 9.4311 - val_loss: 6.4860\n",
            "Epoch 20/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 9.0378roc-auc_val: 0.7585\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 8.9883 - val_loss: 6.3652\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 8.8596 - val_loss: 6.3716\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 8.8124 - val_loss: 6.3618\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 8.3266 - val_loss: 6.2686\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 8.4079 - val_loss: 6.2772\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 8.2688 - val_loss: 6.2752\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 8.2360 - val_loss: 6.3479\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 8.0600 - val_loss: 6.3336\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.9487 - val_loss: 6.2430\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.8449 - val_loss: 6.2070\n",
            "Epoch 30/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 8.0131roc-auc_val: 0.7676\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 7.9827 - val_loss: 6.1930\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.6098 - val_loss: 6.2216\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.4350 - val_loss: 6.2276\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.3156 - val_loss: 6.1971\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.5181 - val_loss: 6.2019\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.1167 - val_loss: 6.1379\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.3090 - val_loss: 6.1144\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.1878 - val_loss: 6.0923\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.0989 - val_loss: 6.0579\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 7.0284 - val_loss: 6.0584\n",
            "Epoch 40/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 6.8035roc-auc_val: 0.7744\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 6.7998 - val_loss: 6.0626\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.8053 - val_loss: 6.0383\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.7542 - val_loss: 5.9799\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.7288 - val_loss: 5.9797\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.7303 - val_loss: 5.9634\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.7262 - val_loss: 5.9846\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.6858 - val_loss: 5.9851\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.5663 - val_loss: 5.9620\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.4474 - val_loss: 5.9136\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.4950 - val_loss: 5.8875\n",
            "Epoch 50/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 6.4311roc-auc_val: 0.7798\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 6.3833 - val_loss: 5.8921\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.4097 - val_loss: 5.8898\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.2338 - val_loss: 5.8550\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.2252 - val_loss: 5.8326\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.2556 - val_loss: 5.8106\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.2696 - val_loss: 5.8236\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.3458 - val_loss: 5.8106\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.1471 - val_loss: 5.8026\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.3369 - val_loss: 5.8329\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.0276 - val_loss: 5.8014\n",
            "Epoch 60/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 5.9660roc-auc_val: 0.784\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 5.9633 - val_loss: 5.7787\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.0554 - val_loss: 5.7763\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.0747 - val_loss: 5.7804\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.0283 - val_loss: 5.7899\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.0810 - val_loss: 5.7754\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 6.0100 - val_loss: 5.7614\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.9388 - val_loss: 5.7523\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.9353 - val_loss: 5.7853\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.8288 - val_loss: 5.7868\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.7820 - val_loss: 5.7491\n",
            "Epoch 70/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.9704roc-auc_val: 0.7884\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 5.9704 - val_loss: 5.7402\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.8551 - val_loss: 5.7133\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.7729 - val_loss: 5.7231\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.8553 - val_loss: 5.7122\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.9472 - val_loss: 5.7199\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.8186 - val_loss: 5.7293\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.6965 - val_loss: 5.7231\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.8475 - val_loss: 5.7516\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.7639 - val_loss: 5.7525\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.5681 - val_loss: 5.7361\n",
            "Epoch 80/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 5.6350roc-auc_val: 0.7918\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 5.6169 - val_loss: 5.7166\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 5.5590 - val_loss: 5.7140\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 5.6265 - val_loss: 5.7001\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 5.5022 - val_loss: 5.6783\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 5.5187 - val_loss: 5.6881\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 5.6351 - val_loss: 5.7039\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 5.6155 - val_loss: 5.6877\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.6539 - val_loss: 5.6974\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.3858 - val_loss: 5.6727\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.5904 - val_loss: 5.6592\n",
            "Epoch 90/1000\n",
            "75/88 [========================>.....] - ETA: 0s - loss: 5.5592roc-auc_val: 0.7941\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 5.5217 - val_loss: 5.6705\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.3808 - val_loss: 5.6662\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.4194 - val_loss: 5.6406\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.5650 - val_loss: 5.6363\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.3769 - val_loss: 5.6363\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.3572 - val_loss: 5.6376\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.4858 - val_loss: 5.6457\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.4299 - val_loss: 5.6323\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.3788 - val_loss: 5.6301\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.3726 - val_loss: 5.6258\n",
            "Epoch 100/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 5.3724roc-auc_val: 0.7967\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 5.4072 - val_loss: 5.6258\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.3802 - val_loss: 5.6257\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2612 - val_loss: 5.6230\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.3100 - val_loss: 5.6135\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2554 - val_loss: 5.6090\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2864 - val_loss: 5.5957\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2565 - val_loss: 5.6003\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2851 - val_loss: 5.6073\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2283 - val_loss: 5.6138\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1720 - val_loss: 5.6038\n",
            "Epoch 110/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 5.2006roc-auc_val: 0.7984\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 5.1983 - val_loss: 5.6154\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2811 - val_loss: 5.6180\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2487 - val_loss: 5.6262\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2209 - val_loss: 5.6308\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0817 - val_loss: 5.6201\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2050 - val_loss: 5.6084\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.2710 - val_loss: 5.5962\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1659 - val_loss: 5.6004\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1827 - val_loss: 5.5969\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1462 - val_loss: 5.6040\n",
            "Epoch 120/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 5.2547roc-auc_val: 0.8\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 5.2205 - val_loss: 5.5977\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0885 - val_loss: 5.5876\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1516 - val_loss: 5.6198\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1544 - val_loss: 5.6171\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1071 - val_loss: 5.6038\n",
            "Epoch 125/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0626 - val_loss: 5.5904\n",
            "Epoch 126/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0892 - val_loss: 5.5814\n",
            "Epoch 127/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1495 - val_loss: 5.5748\n",
            "Epoch 128/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0613 - val_loss: 5.5814\n",
            "Epoch 129/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0301 - val_loss: 5.5737\n",
            "Epoch 130/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 4.9543roc-auc_val: 0.8017\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 4.9683 - val_loss: 5.5735\n",
            "Epoch 131/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1024 - val_loss: 5.5683\n",
            "Epoch 132/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9537 - val_loss: 5.5779\n",
            "Epoch 133/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9751 - val_loss: 5.5884\n",
            "Epoch 134/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0653 - val_loss: 5.5932\n",
            "Epoch 135/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0553 - val_loss: 5.5919\n",
            "Epoch 136/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.1198 - val_loss: 5.5988\n",
            "Epoch 137/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9868 - val_loss: 5.6025\n",
            "Epoch 138/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9776 - val_loss: 5.5943\n",
            "Epoch 139/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0824 - val_loss: 5.5974\n",
            "Epoch 140/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 4.8960roc-auc_val: 0.8029\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 4.9247 - val_loss: 5.5994\n",
            "Epoch 141/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9355 - val_loss: 5.6019\n",
            "Epoch 142/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0081 - val_loss: 5.5967\n",
            "Epoch 143/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 5.0315 - val_loss: 5.5903\n",
            "Epoch 144/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9677 - val_loss: 5.5904\n",
            "Epoch 145/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8898 - val_loss: 5.5979\n",
            "Epoch 146/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9666 - val_loss: 5.5983\n",
            "Epoch 147/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9633 - val_loss: 5.5885\n",
            "Epoch 148/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9423 - val_loss: 5.6044\n",
            "Epoch 149/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8541 - val_loss: 5.6083\n",
            "Epoch 150/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 4.9362roc-auc_val: 0.804\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 4.9355 - val_loss: 5.6151\n",
            "Epoch 151/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8860 - val_loss: 5.6157\n",
            "Epoch 152/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9970 - val_loss: 5.6137\n",
            "Epoch 153/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8859 - val_loss: 5.6141\n",
            "Epoch 154/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9489 - val_loss: 5.6150\n",
            "Epoch 155/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9469 - val_loss: 5.6104\n",
            "Epoch 156/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8927 - val_loss: 5.6026\n",
            "Epoch 157/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8677 - val_loss: 5.6009\n",
            "Epoch 158/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.9354 - val_loss: 5.6046\n",
            "Epoch 159/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8551 - val_loss: 5.6083\n",
            "Epoch 160/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 4.9503roc-auc_val: 0.8052\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 4.9130 - val_loss: 5.5998\n",
            "Epoch 161/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8363 - val_loss: 5.6058\n",
            "Epoch 162/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8840 - val_loss: 5.6061\n",
            "Epoch 163/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8451 - val_loss: 5.5956\n",
            "Epoch 164/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8652 - val_loss: 5.6047\n",
            "Epoch 165/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8684 - val_loss: 5.6126\n",
            "Epoch 166/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.7963 - val_loss: 5.6194\n",
            "Epoch 167/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.7298 - val_loss: 5.6104\n",
            "Epoch 168/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8456 - val_loss: 5.6158\n",
            "Epoch 169/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8771 - val_loss: 5.6101\n",
            "Epoch 170/1000\n",
            "84/88 [===========================>..] - ETA: 0s - loss: 4.8660roc-auc_val: 0.8063\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 4.8420 - val_loss: 5.6077\n",
            "Epoch 171/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8819 - val_loss: 5.6138\n",
            "Epoch 172/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8035 - val_loss: 5.6219\n",
            "Epoch 173/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.7970 - val_loss: 5.6177\n",
            "Epoch 174/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8535 - val_loss: 5.6198\n",
            "Epoch 175/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.7603 - val_loss: 5.6264\n",
            "Epoch 176/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8902 - val_loss: 5.6252\n",
            "Epoch 177/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8364 - val_loss: 5.6243\n",
            "Epoch 178/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.7938 - val_loss: 5.6253\n",
            "Epoch 179/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.7872 - val_loss: 5.6302\n",
            "Epoch 180/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 4.8609roc-auc_val: 0.8071\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 4.8456 - val_loss: 5.6370\n",
            "Epoch 181/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 4.8272 - val_loss: 5.6488\n",
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 24ms/step - loss: 35.0241 - val_loss: 6.6154\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 25.5333 - val_loss: 7.1596\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.4672 - val_loss: 8.1392\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 20.7799 - val_loss: 8.5013\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 18.9654 - val_loss: 8.6027\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 17.3984 - val_loss: 8.7959\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 16.8304 - val_loss: 8.5259\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 16.5081 - val_loss: 8.4957\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 15.9852 - val_loss: 8.4667\n",
            "Epoch 10/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 14.7734roc-auc_val: 0.7568\n",
            "47/47 [==============================] - 11s 234ms/step - loss: 14.8668 - val_loss: 8.4193\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 14.0972 - val_loss: 8.2339\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 14.1837 - val_loss: 8.3097\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 14.0369 - val_loss: 8.2135\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 13.1075 - val_loss: 8.2046\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 12.8654 - val_loss: 8.1999\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 12.4021 - val_loss: 7.9960\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 12.6996 - val_loss: 7.9234\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 11.8731 - val_loss: 7.8465\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 11.9855 - val_loss: 7.8889\n",
            "Epoch 20/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 11.4159roc-auc_val: 0.7643\n",
            "47/47 [==============================] - 11s 233ms/step - loss: 11.1752 - val_loss: 7.8646\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 11.3636 - val_loss: 7.7330\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 11.1499 - val_loss: 7.6146\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 10.6897 - val_loss: 7.5003\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 10.9972 - val_loss: 7.4844\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 10.3265 - val_loss: 7.4218\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 10.0381 - val_loss: 7.2844\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 9.8401 - val_loss: 7.1909\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 9.8955 - val_loss: 7.1961\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 9.8414 - val_loss: 7.2557\n",
            "Epoch 30/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 10.0105roc-auc_val: 0.7714\n",
            "47/47 [==============================] - 11s 234ms/step - loss: 9.8919 - val_loss: 7.1271\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 9.2144 - val_loss: 7.1130\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 9.3236 - val_loss: 7.0912\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 9.0637 - val_loss: 7.1108\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 9.2761 - val_loss: 7.0931\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 9.2101 - val_loss: 7.0561\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 8.8700 - val_loss: 7.1412\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 8.9462 - val_loss: 7.1417\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 8.8793 - val_loss: 7.0826\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 8.5274 - val_loss: 7.0328\n",
            "Epoch 40/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 8.9264roc-auc_val: 0.7731\n",
            "47/47 [==============================] - 11s 239ms/step - loss: 8.8381 - val_loss: 7.0112\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 8.3796 - val_loss: 6.9307\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 8.6998 - val_loss: 6.9403\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 8.5186 - val_loss: 6.9601\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 8.1456 - val_loss: 6.9595\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 8.0412 - val_loss: 6.9392\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 7.7948 - val_loss: 6.8924\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 7.9274 - val_loss: 6.9019\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 7.7199 - val_loss: 6.9263\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 7.7997 - val_loss: 6.8555\n",
            "Epoch 50/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 7.9898roc-auc_val: 0.7765\n",
            "47/47 [==============================] - 11s 234ms/step - loss: 7.9785 - val_loss: 6.8423\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 7.8980 - val_loss: 6.8857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnpeTPNLkiCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "outputId": "46ff50b9-9ac1-4f67-9a8a-867352c59810"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6S7Pv_rc-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}
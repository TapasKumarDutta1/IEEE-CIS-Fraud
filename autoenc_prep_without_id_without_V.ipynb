{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoenc_prep_without_id_without_V.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/autoenc_prep_without_id_without_V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRxz3A0w3Nb9",
        "colab_type": "text"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJr746_23M0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "import keras\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import *\n",
        "from keras import backend as K\n",
        "from keras.utils import Sequence\n",
        "from math import *\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "70deebcf-0b88-4ae3-8ddc-086f7429fa0e"
      },
      "source": [
        "\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JScP1jz82u0P",
        "colab_type": "text"
      },
      "source": [
        "Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJem4-mp8otc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "af90186d-1cba-4403-a60a-47c3c5f3c407"
      },
      "source": [
        "\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)\n",
        "trn=trn.drop(['isFraud','id'],1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (210,222) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxsP2nEt2wJf",
        "colab_type": "text"
      },
      "source": [
        "seperating data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXizGpQm2ZB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats=list(trn.select_dtypes(include=object))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk9P-M622yh7",
        "colab_type": "text"
      },
      "source": [
        "Label Encoding data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkujRQIB_CUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "52b823b5-d5a4-4871-8618-ddfb00f70bce"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "for col in tqdm(cats):\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "100%|██████████| 21/21 [01:25<00:00,  4.08s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOdYYbZQ7IIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats+=list(trn.filter(regex='dum'))\n",
        "no_dum=[i for i in list(trn) if i not in cats]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFXHiTqX21EM",
        "colab_type": "text"
      },
      "source": [
        "Reducing memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrCs8PWA_LWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "72f03e99-3354-4717-d357-8a6d2e167dd6"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 923.62 MB\n",
            "Memory usage after optimization is: 221.89 MB\n",
            "Decreased by 76.0%\n",
            "Memory usage of dataframe is 796.34 MB\n",
            "Memory usage after optimization is: 203.97 MB\n",
            "Decreased by 74.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8azxXaFE8T0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "np.random.seed(42) # NumPy\n",
        "random.seed(42) # Python"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf25ibJI8T5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "def create_model():\n",
        "    K.clear_session()\n",
        "    num_inp = Input(shape=(num_shape,))\n",
        "    cat_inp = Input(shape=(cat_shape,))\n",
        "    inps = concatenate([num_inp, cat_inp])\n",
        "    x = Dense(512, activation=custom_gelu)(inps)\n",
        "    x = Dense(256, activation=custom_gelu)(x)\n",
        "    x = Dense(512, activation = custom_gelu)(x)\n",
        "    x = Dropout(.2)(x)\n",
        "    cat_out = Dense(cat_shape, activation = \"linear\")(x)\n",
        "    num_out = Dense(num_shape, activation = \"linear\")(x)\n",
        "    model = Model(inputs=[num_inp,cat_inp], outputs=[num_out, cat_out])\n",
        "    model.compile(\n",
        "        optimizer=Adam(.05, clipnorm = 1, clipvalue = 1),\n",
        "        loss=[\"mse\", \"mse\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "class WarmUpLearningRateScheduler(keras.callbacks.Callback):\n",
        "    \"\"\"Warmup learning rate scheduler\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, warmup_batches, init_lr, verbose=0):\n",
        "        \"\"\"Constructor for warmup learning rate scheduler\n",
        "\n",
        "        Arguments:\n",
        "            warmup_batches {int} -- Number of batch for warmup.\n",
        "            init_lr {float} -- Learning rate after warmup.\n",
        "\n",
        "        Keyword Arguments:\n",
        "            verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n",
        "        \"\"\"\n",
        "\n",
        "        super(WarmUpLearningRateScheduler, self).__init__()\n",
        "        self.warmup_batches = warmup_batches\n",
        "        self.init_lr = init_lr\n",
        "        self.verbose = verbose\n",
        "        self.batch_count = 0\n",
        "        self.learning_rates = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.batch_count = self.batch_count + 1\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.learning_rates.append(lr)\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        if self.batch_count <= self.warmup_batches:\n",
        "            lr = self.batch_count*self.init_lr/self.warmup_batches\n",
        "            K.set_value(self.model.optimizer.lr, lr)\n",
        "            if self.verbose > 0:\n",
        "                print('\\nBatch %05d: WarmUpLearningRateScheduler setting learning '\n",
        "                      'rate to %s.' % (self.batch_count + 1, lr))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyjXsW3U8Txj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DAESequence(Sequence):\n",
        "    def __init__(self,df,no_dum,frac=0.15,dumm=range(911),batch_size=2048):\n",
        "        self.batch_size=batch_size\n",
        "        self.frac=0.15\n",
        "        self.dumm=dumm\n",
        "        self.df=df\n",
        "        self.cat_data=df[dumm].values\n",
        "        self.num_data=df[no_dum].values\n",
        "        self.no_dumm=no_dum\n",
        "        self.len_data=df.shape[0]\n",
        "        self.columns=df.shape[1]\n",
        "        self.data=df\n",
        "        self.idx=[]\n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(ceil(self.len_data/self.batch_size))\n",
        "    \n",
        "    \n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        self.idx.append(idx)\n",
        "        last=min((idx+1)*self.batch_size,self.len_data)\n",
        "        idx=idx*self.batch_size\n",
        "        size=last-idx\n",
        "        \n",
        "        \n",
        "        inps=[]\n",
        "        outs=[]\n",
        "        output_x=self.data.iloc[idx:last]\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        data=output_x[self.no_dumm].values\n",
        "        noise_x=data.copy()\n",
        "        for i in range(len(self.no_dumm)):\n",
        "            to=np.random.randint(0,size,int(size*self.frac))\n",
        "            frm=np.random.randint(0,size,int(size*self.frac))\n",
        "            noise_x[to,i]=noise_x[frm,i]\n",
        "            \n",
        "        inps.append(noise_x)\n",
        "        outs.append(data)\n",
        "        \n",
        "        data=output_x[self.dumm].values\n",
        "        noise_x=data.copy()\n",
        "        for i in range(len(self.dumm)):\n",
        "            to=np.random.randint(0,size,int(size*self.frac))\n",
        "            frm=np.random.randint(0,size,int(size*self.frac))\n",
        "            noise_x[to,i]=noise_x[frm,i]\n",
        "        \n",
        "        \n",
        "        \n",
        "        inps.append(noise_x)\n",
        "        outs.append(data)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        return inps,outs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b1uGJH7Iftn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "3d992a58-6b24-4a45-cf10-06a5fa487594"
      },
      "source": [
        "import gc\n",
        "X=pd.concat([trn,tst],0).reset_index(drop=True)\n",
        "del([trn,tst])\n",
        "a=X.isna().sum()\n",
        "a[a>0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "C9    1097231\n",
              "id     590540\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie7lNLkULcU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_dum.remove('C9')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrWN7PjRLN1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=X.drop(['C9','id'],1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21jEN2-QL_fm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "73682036-b459-4487-a7fc-dc9e5ebd5f14"
      },
      "source": [
        "num=X[no_dum]\n",
        "cat=X[cats]\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss=StandardScaler()\n",
        "numerical=pd.DataFrame(ss.fit_transform(X[no_dum]))\n",
        "numerical.columns=no_dum\n",
        "categorical=pd.DataFrame(ss.fit_transform(X[cats]))\n",
        "categorical.columns=cats\n",
        "X=pd.concat([categorical,numerical],1)\n",
        "X.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>M4</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>M9</th>\n",
              "      <th>card3</th>\n",
              "      <th>M5</th>\n",
              "      <th>M2</th>\n",
              "      <th>M6</th>\n",
              "      <th>card6</th>\n",
              "      <th>M7</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card4</th>\n",
              "      <th>M3</th>\n",
              "      <th>card5</th>\n",
              "      <th>card2</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>M8</th>\n",
              "      <th>M1</th>\n",
              "      <th>card1</th>\n",
              "      <th>P_emaildomain_first</th>\n",
              "      <th>P_emaildomain_second</th>\n",
              "      <th>M4M0_dum</th>\n",
              "      <th>M4M1_dum</th>\n",
              "      <th>M4M2_dum</th>\n",
              "      <th>M4nan_dum</th>\n",
              "      <th>P_emaildomainanonymous.com_dum</th>\n",
              "      <th>P_emaildomainaol.com_dum</th>\n",
              "      <th>P_emaildomainatt.net_dum</th>\n",
              "      <th>P_emaildomaincomcast.net_dum</th>\n",
              "      <th>P_emaildomaingmail.com_dum</th>\n",
              "      <th>P_emaildomainhotmail.com_dum</th>\n",
              "      <th>P_emaildomainicloud.com_dum</th>\n",
              "      <th>P_emaildomainnan_dum</th>\n",
              "      <th>P_emaildomainother_dum</th>\n",
              "      <th>P_emaildomainoutlook.com_dum</th>\n",
              "      <th>P_emaildomainyahoo.com_dum</th>\n",
              "      <th>M9F_dum</th>\n",
              "      <th>M9T_dum</th>\n",
              "      <th>M9nan_dum</th>\n",
              "      <th>card3102.0_dum</th>\n",
              "      <th>...</th>\n",
              "      <th>M6_isna</th>\n",
              "      <th>M7_isna</th>\n",
              "      <th>M8_isna</th>\n",
              "      <th>M9_isna</th>\n",
              "      <th>C1</th>\n",
              "      <th>D5</th>\n",
              "      <th>C12</th>\n",
              "      <th>C6</th>\n",
              "      <th>C13</th>\n",
              "      <th>C5</th>\n",
              "      <th>C7</th>\n",
              "      <th>D4</th>\n",
              "      <th>dist1</th>\n",
              "      <th>C8</th>\n",
              "      <th>C2</th>\n",
              "      <th>D3</th>\n",
              "      <th>C14</th>\n",
              "      <th>C3</th>\n",
              "      <th>C11</th>\n",
              "      <th>D10</th>\n",
              "      <th>D1</th>\n",
              "      <th>C4</th>\n",
              "      <th>D2</th>\n",
              "      <th>D11</th>\n",
              "      <th>D15</th>\n",
              "      <th>C10</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>numerical</th>\n",
              "      <th>categorical</th>\n",
              "      <th>day</th>\n",
              "      <th>week</th>\n",
              "      <th>month</th>\n",
              "      <th>d_1</th>\n",
              "      <th>d_2</th>\n",
              "      <th>d_3</th>\n",
              "      <th>d_4</th>\n",
              "      <th>d_5</th>\n",
              "      <th>d_10</th>\n",
              "      <th>d_11</th>\n",
              "      <th>d_15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.006874</td>\n",
              "      <td>0.659180</td>\n",
              "      <td>1.260742</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>-1.500977</td>\n",
              "      <td>-0.651855</td>\n",
              "      <td>-0.066772</td>\n",
              "      <td>-1.628906</td>\n",
              "      <td>1.200195</td>\n",
              "      <td>0.584473</td>\n",
              "      <td>-2.150391</td>\n",
              "      <td>-0.540527</td>\n",
              "      <td>1.296875</td>\n",
              "      <td>0.805176</td>\n",
              "      <td>-0.682129</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>1.196289</td>\n",
              "      <td>-0.771973</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>0.375488</td>\n",
              "      <td>2.283203</td>\n",
              "      <td>-0.695801</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>2.814453</td>\n",
              "      <td>-0.947754</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>-0.811523</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>2.388672</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>-0.095642</td>\n",
              "      <td>-0.447021</td>\n",
              "      <td>-0.269043</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>0.941895</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.653418</td>\n",
              "      <td>0.942126</td>\n",
              "      <td>0.942173</td>\n",
              "      <td>0.942173</td>\n",
              "      <td>-0.100535</td>\n",
              "      <td>-0.053435</td>\n",
              "      <td>-0.046555</td>\n",
              "      <td>-0.114874</td>\n",
              "      <td>-0.250920</td>\n",
              "      <td>-0.206339</td>\n",
              "      <td>-0.044308</td>\n",
              "      <td>-0.082070</td>\n",
              "      <td>-0.381381</td>\n",
              "      <td>-0.050383</td>\n",
              "      <td>-0.094629</td>\n",
              "      <td>-0.319885</td>\n",
              "      <td>-0.152502</td>\n",
              "      <td>-0.0824</td>\n",
              "      <td>-0.086245</td>\n",
              "      <td>-0.622125</td>\n",
              "      <td>-0.521343</td>\n",
              "      <td>-0.056967</td>\n",
              "      <td>-0.064293</td>\n",
              "      <td>-0.956898</td>\n",
              "      <td>-0.805644</td>\n",
              "      <td>-0.050806</td>\n",
              "      <td>-0.274063</td>\n",
              "      <td>0.151174</td>\n",
              "      <td>0.102999</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.497518</td>\n",
              "      <td>-1.003158</td>\n",
              "      <td>-1.269802</td>\n",
              "      <td>-0.765257</td>\n",
              "      <td>-1.302708</td>\n",
              "      <td>-0.262279</td>\n",
              "      <td>-0.111830</td>\n",
              "      <td>-0.018393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.225586</td>\n",
              "      <td>-0.274902</td>\n",
              "      <td>1.260742</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>-0.619141</td>\n",
              "      <td>1.522461</td>\n",
              "      <td>-0.066772</td>\n",
              "      <td>-1.628906</td>\n",
              "      <td>1.200195</td>\n",
              "      <td>0.584473</td>\n",
              "      <td>-1.201172</td>\n",
              "      <td>1.477539</td>\n",
              "      <td>-2.544922</td>\n",
              "      <td>0.805176</td>\n",
              "      <td>-0.416016</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>1.196289</td>\n",
              "      <td>1.583984</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>-0.413086</td>\n",
              "      <td>-0.476807</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>-0.355225</td>\n",
              "      <td>-0.947754</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>1.232422</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>-0.418701</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>-0.095642</td>\n",
              "      <td>-0.447021</td>\n",
              "      <td>-0.269043</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>0.941895</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.653418</td>\n",
              "      <td>0.942126</td>\n",
              "      <td>0.942173</td>\n",
              "      <td>0.942173</td>\n",
              "      <td>-0.100535</td>\n",
              "      <td>-0.053435</td>\n",
              "      <td>-0.046555</td>\n",
              "      <td>-0.114874</td>\n",
              "      <td>-0.250920</td>\n",
              "      <td>-0.206339</td>\n",
              "      <td>-0.044308</td>\n",
              "      <td>-0.791864</td>\n",
              "      <td>0.065146</td>\n",
              "      <td>-0.050383</td>\n",
              "      <td>-0.094629</td>\n",
              "      <td>-0.042242</td>\n",
              "      <td>-0.152502</td>\n",
              "      <td>-0.0824</td>\n",
              "      <td>-0.098622</td>\n",
              "      <td>-0.685543</td>\n",
              "      <td>-0.605482</td>\n",
              "      <td>-0.056967</td>\n",
              "      <td>-0.064293</td>\n",
              "      <td>-0.190178</td>\n",
              "      <td>-0.805644</td>\n",
              "      <td>-0.050806</td>\n",
              "      <td>-0.437126</td>\n",
              "      <td>-0.037020</td>\n",
              "      <td>0.368876</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.428902</td>\n",
              "      <td>-1.003158</td>\n",
              "      <td>-1.384026</td>\n",
              "      <td>-0.142623</td>\n",
              "      <td>-1.302708</td>\n",
              "      <td>-0.206136</td>\n",
              "      <td>-0.807352</td>\n",
              "      <td>-0.018393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.225586</td>\n",
              "      <td>1.281250</td>\n",
              "      <td>-1.835938</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>-1.500977</td>\n",
              "      <td>-0.651855</td>\n",
              "      <td>-1.003906</td>\n",
              "      <td>0.541016</td>\n",
              "      <td>-1.108398</td>\n",
              "      <td>0.584473</td>\n",
              "      <td>0.694824</td>\n",
              "      <td>-0.540527</td>\n",
              "      <td>-0.797852</td>\n",
              "      <td>-0.728516</td>\n",
              "      <td>-0.149658</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>-1.280273</td>\n",
              "      <td>-0.771973</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>0.611816</td>\n",
              "      <td>-0.476807</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>-0.355225</td>\n",
              "      <td>-0.947754</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>-0.811523</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>-0.418701</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>10.460938</td>\n",
              "      <td>-0.447021</td>\n",
              "      <td>3.716797</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>-1.061523</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.653418</td>\n",
              "      <td>-1.061429</td>\n",
              "      <td>-1.061376</td>\n",
              "      <td>-1.061376</td>\n",
              "      <td>-0.100535</td>\n",
              "      <td>-0.053435</td>\n",
              "      <td>-0.046555</td>\n",
              "      <td>-0.114874</td>\n",
              "      <td>-0.250920</td>\n",
              "      <td>-0.206339</td>\n",
              "      <td>-0.044308</td>\n",
              "      <td>-0.791864</td>\n",
              "      <td>0.821325</td>\n",
              "      <td>-0.050383</td>\n",
              "      <td>-0.094629</td>\n",
              "      <td>-0.042242</td>\n",
              "      <td>-0.152502</td>\n",
              "      <td>-0.0824</td>\n",
              "      <td>-0.098622</td>\n",
              "      <td>-0.685543</td>\n",
              "      <td>-0.605482</td>\n",
              "      <td>-0.056967</td>\n",
              "      <td>-0.064293</td>\n",
              "      <td>0.775932</td>\n",
              "      <td>0.576191</td>\n",
              "      <td>-0.050806</td>\n",
              "      <td>-0.313281</td>\n",
              "      <td>-0.526324</td>\n",
              "      <td>-1.226384</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.428902</td>\n",
              "      <td>-1.003158</td>\n",
              "      <td>-1.384026</td>\n",
              "      <td>-0.142623</td>\n",
              "      <td>-1.302708</td>\n",
              "      <td>-0.206136</td>\n",
              "      <td>-1.683748</td>\n",
              "      <td>-1.276178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.225586</td>\n",
              "      <td>1.592773</td>\n",
              "      <td>1.260742</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>-0.619141</td>\n",
              "      <td>1.522461</td>\n",
              "      <td>-1.003906</td>\n",
              "      <td>0.541016</td>\n",
              "      <td>1.200195</td>\n",
              "      <td>0.584473</td>\n",
              "      <td>-1.201172</td>\n",
              "      <td>1.477539</td>\n",
              "      <td>-2.195312</td>\n",
              "      <td>0.805176</td>\n",
              "      <td>0.915039</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>1.196289</td>\n",
              "      <td>1.583984</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>1.794922</td>\n",
              "      <td>-0.476807</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>-0.355225</td>\n",
              "      <td>-0.947754</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>-0.811523</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>-0.418701</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>-0.095642</td>\n",
              "      <td>2.236328</td>\n",
              "      <td>-0.269043</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>0.941895</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.653418</td>\n",
              "      <td>0.942126</td>\n",
              "      <td>0.942173</td>\n",
              "      <td>0.942173</td>\n",
              "      <td>-0.091595</td>\n",
              "      <td>-0.621471</td>\n",
              "      <td>-0.046555</td>\n",
              "      <td>-0.065975</td>\n",
              "      <td>-0.045798</td>\n",
              "      <td>-0.206339</td>\n",
              "      <td>-0.044308</td>\n",
              "      <td>-0.315288</td>\n",
              "      <td>0.065146</td>\n",
              "      <td>-0.050383</td>\n",
              "      <td>-0.063517</td>\n",
              "      <td>-0.555117</td>\n",
              "      <td>-0.152502</td>\n",
              "      <td>-0.0824</td>\n",
              "      <td>-0.098622</td>\n",
              "      <td>-0.275763</td>\n",
              "      <td>0.067624</td>\n",
              "      <td>-0.056967</td>\n",
              "      <td>-0.486896</td>\n",
              "      <td>-0.190178</td>\n",
              "      <td>-0.318712</td>\n",
              "      <td>-0.050806</td>\n",
              "      <td>-0.350434</td>\n",
              "      <td>-0.149936</td>\n",
              "      <td>0.368876</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.977829</td>\n",
              "      <td>-0.681109</td>\n",
              "      <td>-1.173026</td>\n",
              "      <td>-0.560677</td>\n",
              "      <td>-1.003359</td>\n",
              "      <td>-0.568908</td>\n",
              "      <td>-0.807352</td>\n",
              "      <td>-0.461612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.239258</td>\n",
              "      <td>-0.274902</td>\n",
              "      <td>1.260742</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>1.144531</td>\n",
              "      <td>1.522461</td>\n",
              "      <td>1.806641</td>\n",
              "      <td>-1.628906</td>\n",
              "      <td>1.200195</td>\n",
              "      <td>-1.544922</td>\n",
              "      <td>-1.201172</td>\n",
              "      <td>1.477539</td>\n",
              "      <td>-2.544922</td>\n",
              "      <td>-0.473145</td>\n",
              "      <td>0.915039</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>1.196289</td>\n",
              "      <td>1.583984</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>-0.413086</td>\n",
              "      <td>-0.476807</td>\n",
              "      <td>-0.695801</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>-0.355225</td>\n",
              "      <td>1.055664</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>1.232422</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>-0.418701</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>-0.095642</td>\n",
              "      <td>-0.447021</td>\n",
              "      <td>-0.269043</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>0.941895</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>1.530415</td>\n",
              "      <td>0.942126</td>\n",
              "      <td>0.942173</td>\n",
              "      <td>0.942173</td>\n",
              "      <td>-0.100535</td>\n",
              "      <td>-0.053435</td>\n",
              "      <td>-0.046555</td>\n",
              "      <td>-0.114874</td>\n",
              "      <td>-0.250920</td>\n",
              "      <td>-0.206339</td>\n",
              "      <td>-0.044308</td>\n",
              "      <td>-0.082070</td>\n",
              "      <td>0.065146</td>\n",
              "      <td>-0.036555</td>\n",
              "      <td>-0.094629</td>\n",
              "      <td>-0.042242</td>\n",
              "      <td>-0.152502</td>\n",
              "      <td>-0.0824</td>\n",
              "      <td>-0.098622</td>\n",
              "      <td>-0.080629</td>\n",
              "      <td>-0.605482</td>\n",
              "      <td>-0.056967</td>\n",
              "      <td>-0.064293</td>\n",
              "      <td>-0.190178</td>\n",
              "      <td>-0.087309</td>\n",
              "      <td>-0.036911</td>\n",
              "      <td>-0.350434</td>\n",
              "      <td>3.199918</td>\n",
              "      <td>1.166506</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.428902</td>\n",
              "      <td>-1.003158</td>\n",
              "      <td>-1.384026</td>\n",
              "      <td>-0.765257</td>\n",
              "      <td>-1.302708</td>\n",
              "      <td>-0.741656</td>\n",
              "      <td>-0.807352</td>\n",
              "      <td>-0.672241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 204 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         M4  P_emaildomain        M9  ...      d_10      d_11      d_15\n",
              "0  0.006874       0.659180  1.260742  ... -0.262279 -0.111830 -0.018393\n",
              "1 -1.225586      -0.274902  1.260742  ... -0.206136 -0.807352 -0.018393\n",
              "2 -1.225586       1.281250 -1.835938  ... -0.206136 -1.683748 -1.276178\n",
              "3 -1.225586       1.592773  1.260742  ... -0.568908 -0.807352 -0.461612\n",
              "4  1.239258      -0.274902  1.260742  ... -0.741656 -0.807352 -0.672241\n",
              "\n",
              "[5 rows x 204 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeTh6RLbLxUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXdXM2kpNXgw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec53a971-5b3e-4afa-c1b1-e55a101526e3"
      },
      "source": [
        "num_shape=len(no_dum)\n",
        "cat_shape=len(cats)\n",
        "model_mse = create_model()\n",
        "model_mse.summary()\n",
        "X=X.dropna()\n",
        "batch_size=2048\n",
        "auto_ckpt = ModelCheckpoint(\"ae.model\", monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min', period=1)\n",
        "warm_up_lr = WarmUpLearningRateScheduler(400, init_lr=0.0001)\n",
        "gc.collect()\n",
        "epochs = 100\n",
        "batch_size=2048\n",
        "train_gen=DAESequence(X,no_dum,batch_size=batch_size,dumm=cats)\n",
        "hist = model_mse.fit_generator(train_gen, steps_per_epoch=len(X)//batch_size, epochs=epochs,\n",
        "                              callbacks=[auto_ckpt, warm_up_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 63)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 141)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 204)          0           input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          104960      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          131328      dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          131584      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 63)           32319       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 141)          72333       dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 472,524\n",
            "Trainable params: 472,524\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "WARNING:tensorflow:From <ipython-input-15-924266836df7>:14: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 1.4742 - dense_4_loss: 0.6474 - dense_3_loss: 0.8268\n",
            "Epoch 00001: loss improved from inf to 1.47425, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 1.4742 - dense_4_loss: 0.6474 - dense_3_loss: 0.8268\n",
            "Epoch 2/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.6750 - dense_4_loss: 0.2316 - dense_3_loss: 0.4434\n",
            "Epoch 00002: loss improved from 1.47425 to 0.67503, saving model to ae.model\n",
            "535/535 [==============================] - 50s 93ms/step - loss: 0.6750 - dense_4_loss: 0.2316 - dense_3_loss: 0.4434\n",
            "Epoch 3/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.5018 - dense_4_loss: 0.1799 - dense_3_loss: 0.3220\n",
            "Epoch 00003: loss improved from 0.67503 to 0.50183, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.5018 - dense_4_loss: 0.1799 - dense_3_loss: 0.3220\n",
            "Epoch 4/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.4340 - dense_4_loss: 0.1586 - dense_3_loss: 0.2754\n",
            "Epoch 00004: loss improved from 0.50183 to 0.43405, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.4340 - dense_4_loss: 0.1586 - dense_3_loss: 0.2754\n",
            "Epoch 5/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3957 - dense_4_loss: 0.1451 - dense_3_loss: 0.2507\n",
            "Epoch 00005: loss improved from 0.43405 to 0.39574, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.3957 - dense_4_loss: 0.1451 - dense_3_loss: 0.2507\n",
            "Epoch 6/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3672 - dense_4_loss: 0.1339 - dense_3_loss: 0.2334\n",
            "Epoch 00006: loss improved from 0.39574 to 0.36723, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.3672 - dense_4_loss: 0.1339 - dense_3_loss: 0.2334\n",
            "Epoch 7/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3500 - dense_4_loss: 0.1260 - dense_3_loss: 0.2241\n",
            "Epoch 00007: loss improved from 0.36723 to 0.35003, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.3500 - dense_4_loss: 0.1260 - dense_3_loss: 0.2241\n",
            "Epoch 8/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3409 - dense_4_loss: 0.1223 - dense_3_loss: 0.2186\n",
            "Epoch 00008: loss improved from 0.35003 to 0.34093, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.3409 - dense_4_loss: 0.1223 - dense_3_loss: 0.2186\n",
            "Epoch 9/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3263 - dense_4_loss: 0.1176 - dense_3_loss: 0.2087\n",
            "Epoch 00009: loss improved from 0.34093 to 0.32630, saving model to ae.model\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.3263 - dense_4_loss: 0.1176 - dense_3_loss: 0.2087\n",
            "Epoch 10/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3206 - dense_4_loss: 0.1139 - dense_3_loss: 0.2067\n",
            "Epoch 00010: loss improved from 0.32630 to 0.32061, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.3206 - dense_4_loss: 0.1139 - dense_3_loss: 0.2067\n",
            "Epoch 11/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3113 - dense_4_loss: 0.1099 - dense_3_loss: 0.2013\n",
            "Epoch 00011: loss improved from 0.32061 to 0.31126, saving model to ae.model\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.3113 - dense_4_loss: 0.1099 - dense_3_loss: 0.2013\n",
            "Epoch 12/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3053 - dense_4_loss: 0.1082 - dense_3_loss: 0.1972\n",
            "Epoch 00012: loss improved from 0.31126 to 0.30534, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.3053 - dense_4_loss: 0.1082 - dense_3_loss: 0.1972\n",
            "Epoch 13/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2990 - dense_4_loss: 0.1057 - dense_3_loss: 0.1933\n",
            "Epoch 00013: loss improved from 0.30534 to 0.29898, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2990 - dense_4_loss: 0.1057 - dense_3_loss: 0.1933\n",
            "Epoch 14/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2920 - dense_4_loss: 0.1024 - dense_3_loss: 0.1896\n",
            "Epoch 00014: loss improved from 0.29898 to 0.29202, saving model to ae.model\n",
            "535/535 [==============================] - 45s 85ms/step - loss: 0.2920 - dense_4_loss: 0.1024 - dense_3_loss: 0.1896\n",
            "Epoch 15/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2887 - dense_4_loss: 0.1014 - dense_3_loss: 0.1873\n",
            "Epoch 00015: loss improved from 0.29202 to 0.28870, saving model to ae.model\n",
            "535/535 [==============================] - 51s 95ms/step - loss: 0.2887 - dense_4_loss: 0.1014 - dense_3_loss: 0.1873\n",
            "Epoch 16/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2881 - dense_4_loss: 0.1025 - dense_3_loss: 0.1856\n",
            "Epoch 00016: loss improved from 0.28870 to 0.28807, saving model to ae.model\n",
            "535/535 [==============================] - 46s 86ms/step - loss: 0.2881 - dense_4_loss: 0.1025 - dense_3_loss: 0.1856\n",
            "Epoch 17/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2831 - dense_4_loss: 0.0989 - dense_3_loss: 0.1841\n",
            "Epoch 00017: loss improved from 0.28807 to 0.28309, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2831 - dense_4_loss: 0.0989 - dense_3_loss: 0.1841\n",
            "Epoch 18/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2809 - dense_4_loss: 0.0991 - dense_3_loss: 0.1818\n",
            "Epoch 00018: loss improved from 0.28309 to 0.28089, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2809 - dense_4_loss: 0.0991 - dense_3_loss: 0.1818\n",
            "Epoch 19/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2762 - dense_4_loss: 0.0975 - dense_3_loss: 0.1787\n",
            "Epoch 00019: loss improved from 0.28089 to 0.27621, saving model to ae.model\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2762 - dense_4_loss: 0.0975 - dense_3_loss: 0.1787\n",
            "Epoch 20/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2727 - dense_4_loss: 0.0966 - dense_3_loss: 0.1761\n",
            "Epoch 00020: loss improved from 0.27621 to 0.27273, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2727 - dense_4_loss: 0.0966 - dense_3_loss: 0.1761\n",
            "Epoch 21/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2690 - dense_4_loss: 0.0945 - dense_3_loss: 0.1745\n",
            "Epoch 00021: loss improved from 0.27273 to 0.26898, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2690 - dense_4_loss: 0.0945 - dense_3_loss: 0.1745\n",
            "Epoch 22/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2681 - dense_4_loss: 0.0939 - dense_3_loss: 0.1742\n",
            "Epoch 00022: loss improved from 0.26898 to 0.26805, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2681 - dense_4_loss: 0.0939 - dense_3_loss: 0.1742\n",
            "Epoch 23/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2657 - dense_4_loss: 0.0938 - dense_3_loss: 0.1719\n",
            "Epoch 00023: loss improved from 0.26805 to 0.26567, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2657 - dense_4_loss: 0.0938 - dense_3_loss: 0.1719\n",
            "Epoch 24/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2609 - dense_4_loss: 0.0923 - dense_3_loss: 0.1686\n",
            "Epoch 00024: loss improved from 0.26567 to 0.26091, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2609 - dense_4_loss: 0.0923 - dense_3_loss: 0.1686\n",
            "Epoch 25/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2620 - dense_4_loss: 0.0923 - dense_3_loss: 0.1697\n",
            "Epoch 00025: loss did not improve from 0.26091\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2620 - dense_4_loss: 0.0923 - dense_3_loss: 0.1697\n",
            "Epoch 26/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2582 - dense_4_loss: 0.0904 - dense_3_loss: 0.1678\n",
            "Epoch 00026: loss improved from 0.26091 to 0.25823, saving model to ae.model\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2582 - dense_4_loss: 0.0904 - dense_3_loss: 0.1678\n",
            "Epoch 27/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2585 - dense_4_loss: 0.0913 - dense_3_loss: 0.1672\n",
            "Epoch 00027: loss did not improve from 0.25823\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2585 - dense_4_loss: 0.0913 - dense_3_loss: 0.1672\n",
            "Epoch 28/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2549 - dense_4_loss: 0.0902 - dense_3_loss: 0.1647\n",
            "Epoch 00028: loss improved from 0.25823 to 0.25487, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2549 - dense_4_loss: 0.0902 - dense_3_loss: 0.1647\n",
            "Epoch 29/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2526 - dense_4_loss: 0.0879 - dense_3_loss: 0.1646\n",
            "Epoch 00029: loss improved from 0.25487 to 0.25257, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2526 - dense_4_loss: 0.0879 - dense_3_loss: 0.1646\n",
            "Epoch 30/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2512 - dense_4_loss: 0.0889 - dense_3_loss: 0.1623\n",
            "Epoch 00030: loss improved from 0.25257 to 0.25122, saving model to ae.model\n",
            "535/535 [==============================] - 47s 88ms/step - loss: 0.2512 - dense_4_loss: 0.0889 - dense_3_loss: 0.1623\n",
            "Epoch 31/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2496 - dense_4_loss: 0.0874 - dense_3_loss: 0.1622\n",
            "Epoch 00031: loss improved from 0.25122 to 0.24965, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2496 - dense_4_loss: 0.0874 - dense_3_loss: 0.1622\n",
            "Epoch 32/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2487 - dense_4_loss: 0.0877 - dense_3_loss: 0.1610\n",
            "Epoch 00032: loss improved from 0.24965 to 0.24873, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2487 - dense_4_loss: 0.0877 - dense_3_loss: 0.1610\n",
            "Epoch 33/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2443 - dense_4_loss: 0.0856 - dense_3_loss: 0.1587\n",
            "Epoch 00033: loss improved from 0.24873 to 0.24433, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2443 - dense_4_loss: 0.0856 - dense_3_loss: 0.1587\n",
            "Epoch 34/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2448 - dense_4_loss: 0.0849 - dense_3_loss: 0.1598\n",
            "Epoch 00034: loss did not improve from 0.24433\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2448 - dense_4_loss: 0.0849 - dense_3_loss: 0.1598\n",
            "Epoch 35/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2440 - dense_4_loss: 0.0854 - dense_3_loss: 0.1586\n",
            "Epoch 00035: loss improved from 0.24433 to 0.24396, saving model to ae.model\n",
            "535/535 [==============================] - 47s 88ms/step - loss: 0.2440 - dense_4_loss: 0.0854 - dense_3_loss: 0.1586\n",
            "Epoch 36/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2452 - dense_4_loss: 0.0864 - dense_3_loss: 0.1587\n",
            "Epoch 00036: loss did not improve from 0.24396\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2452 - dense_4_loss: 0.0864 - dense_3_loss: 0.1587\n",
            "Epoch 37/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2387 - dense_4_loss: 0.0840 - dense_3_loss: 0.1546\n",
            "Epoch 00037: loss improved from 0.24396 to 0.23866, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2387 - dense_4_loss: 0.0840 - dense_3_loss: 0.1546\n",
            "Epoch 38/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2383 - dense_4_loss: 0.0836 - dense_3_loss: 0.1547\n",
            "Epoch 00038: loss improved from 0.23866 to 0.23827, saving model to ae.model\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2383 - dense_4_loss: 0.0836 - dense_3_loss: 0.1547\n",
            "Epoch 39/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2391 - dense_4_loss: 0.0848 - dense_3_loss: 0.1543\n",
            "Epoch 00039: loss did not improve from 0.23827\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2391 - dense_4_loss: 0.0848 - dense_3_loss: 0.1543\n",
            "Epoch 40/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2343 - dense_4_loss: 0.0819 - dense_3_loss: 0.1524\n",
            "Epoch 00040: loss improved from 0.23827 to 0.23428, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2343 - dense_4_loss: 0.0819 - dense_3_loss: 0.1524\n",
            "Epoch 41/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2354 - dense_4_loss: 0.0827 - dense_3_loss: 0.1528\n",
            "Epoch 00041: loss did not improve from 0.23428\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2354 - dense_4_loss: 0.0827 - dense_3_loss: 0.1528\n",
            "Epoch 42/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2343 - dense_4_loss: 0.0813 - dense_3_loss: 0.1530\n",
            "Epoch 00042: loss improved from 0.23428 to 0.23427, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2343 - dense_4_loss: 0.0813 - dense_3_loss: 0.1530\n",
            "Epoch 43/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2307 - dense_4_loss: 0.0811 - dense_3_loss: 0.1496\n",
            "Epoch 00043: loss improved from 0.23427 to 0.23070, saving model to ae.model\n",
            "535/535 [==============================] - 45s 85ms/step - loss: 0.2307 - dense_4_loss: 0.0811 - dense_3_loss: 0.1496\n",
            "Epoch 44/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2307 - dense_4_loss: 0.0810 - dense_3_loss: 0.1498\n",
            "Epoch 00044: loss did not improve from 0.23070\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2307 - dense_4_loss: 0.0810 - dense_3_loss: 0.1498\n",
            "Epoch 45/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2309 - dense_4_loss: 0.0825 - dense_3_loss: 0.1485\n",
            "Epoch 00045: loss did not improve from 0.23070\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2309 - dense_4_loss: 0.0825 - dense_3_loss: 0.1485\n",
            "Epoch 46/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2292 - dense_4_loss: 0.0805 - dense_3_loss: 0.1487\n",
            "Epoch 00046: loss improved from 0.23070 to 0.22921, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2292 - dense_4_loss: 0.0805 - dense_3_loss: 0.1487\n",
            "Epoch 47/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2299 - dense_4_loss: 0.0813 - dense_3_loss: 0.1486\n",
            "Epoch 00047: loss did not improve from 0.22921\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2299 - dense_4_loss: 0.0813 - dense_3_loss: 0.1486\n",
            "Epoch 48/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2280 - dense_4_loss: 0.0799 - dense_3_loss: 0.1481\n",
            "Epoch 00048: loss improved from 0.22921 to 0.22800, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2280 - dense_4_loss: 0.0799 - dense_3_loss: 0.1481\n",
            "Epoch 49/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2268 - dense_4_loss: 0.0799 - dense_3_loss: 0.1469\n",
            "Epoch 00049: loss improved from 0.22800 to 0.22683, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2268 - dense_4_loss: 0.0799 - dense_3_loss: 0.1469\n",
            "Epoch 50/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2222 - dense_4_loss: 0.0782 - dense_3_loss: 0.1440\n",
            "Epoch 00050: loss improved from 0.22683 to 0.22223, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2222 - dense_4_loss: 0.0782 - dense_3_loss: 0.1440\n",
            "Epoch 51/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2254 - dense_4_loss: 0.0796 - dense_3_loss: 0.1458\n",
            "Epoch 00051: loss did not improve from 0.22223\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2254 - dense_4_loss: 0.0796 - dense_3_loss: 0.1458\n",
            "Epoch 52/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2227 - dense_4_loss: 0.0788 - dense_3_loss: 0.1439\n",
            "Epoch 00052: loss did not improve from 0.22223\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2227 - dense_4_loss: 0.0788 - dense_3_loss: 0.1439\n",
            "Epoch 53/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2255 - dense_4_loss: 0.0796 - dense_3_loss: 0.1460\n",
            "Epoch 00053: loss did not improve from 0.22223\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2255 - dense_4_loss: 0.0796 - dense_3_loss: 0.1460\n",
            "Epoch 54/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2203 - dense_4_loss: 0.0780 - dense_3_loss: 0.1424\n",
            "Epoch 00054: loss improved from 0.22223 to 0.22035, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2203 - dense_4_loss: 0.0780 - dense_3_loss: 0.1424\n",
            "Epoch 55/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2208 - dense_4_loss: 0.0776 - dense_3_loss: 0.1432\n",
            "Epoch 00055: loss did not improve from 0.22035\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2208 - dense_4_loss: 0.0776 - dense_3_loss: 0.1432\n",
            "Epoch 56/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2199 - dense_4_loss: 0.0774 - dense_3_loss: 0.1425\n",
            "Epoch 00056: loss improved from 0.22035 to 0.21992, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2199 - dense_4_loss: 0.0774 - dense_3_loss: 0.1425\n",
            "Epoch 57/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2177 - dense_4_loss: 0.0775 - dense_3_loss: 0.1402\n",
            "Epoch 00057: loss improved from 0.21992 to 0.21766, saving model to ae.model\n",
            "535/535 [==============================] - 47s 88ms/step - loss: 0.2177 - dense_4_loss: 0.0775 - dense_3_loss: 0.1402\n",
            "Epoch 58/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2198 - dense_4_loss: 0.0775 - dense_3_loss: 0.1423\n",
            "Epoch 00058: loss did not improve from 0.21766\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2198 - dense_4_loss: 0.0775 - dense_3_loss: 0.1423\n",
            "Epoch 59/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2198 - dense_4_loss: 0.0768 - dense_3_loss: 0.1430\n",
            "Epoch 00059: loss did not improve from 0.21766\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2198 - dense_4_loss: 0.0768 - dense_3_loss: 0.1430\n",
            "Epoch 60/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2189 - dense_4_loss: 0.0766 - dense_3_loss: 0.1423\n",
            "Epoch 00060: loss did not improve from 0.21766\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2189 - dense_4_loss: 0.0766 - dense_3_loss: 0.1423\n",
            "Epoch 61/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2154 - dense_4_loss: 0.0769 - dense_3_loss: 0.1386\n",
            "Epoch 00061: loss improved from 0.21766 to 0.21544, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2154 - dense_4_loss: 0.0769 - dense_3_loss: 0.1386\n",
            "Epoch 62/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2190 - dense_4_loss: 0.0785 - dense_3_loss: 0.1406\n",
            "Epoch 00062: loss did not improve from 0.21544\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2190 - dense_4_loss: 0.0785 - dense_3_loss: 0.1406\n",
            "Epoch 63/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2148 - dense_4_loss: 0.0753 - dense_3_loss: 0.1394\n",
            "Epoch 00063: loss improved from 0.21544 to 0.21479, saving model to ae.model\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2148 - dense_4_loss: 0.0753 - dense_3_loss: 0.1394\n",
            "Epoch 64/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2135 - dense_4_loss: 0.0766 - dense_3_loss: 0.1369\n",
            "Epoch 00064: loss improved from 0.21479 to 0.21348, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2135 - dense_4_loss: 0.0766 - dense_3_loss: 0.1369\n",
            "Epoch 65/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2163 - dense_4_loss: 0.0767 - dense_3_loss: 0.1396\n",
            "Epoch 00065: loss did not improve from 0.21348\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2163 - dense_4_loss: 0.0767 - dense_3_loss: 0.1396\n",
            "Epoch 66/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2131 - dense_4_loss: 0.0748 - dense_3_loss: 0.1383\n",
            "Epoch 00066: loss improved from 0.21348 to 0.21308, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2131 - dense_4_loss: 0.0748 - dense_3_loss: 0.1383\n",
            "Epoch 67/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2143 - dense_4_loss: 0.0772 - dense_3_loss: 0.1371\n",
            "Epoch 00067: loss did not improve from 0.21308\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2143 - dense_4_loss: 0.0772 - dense_3_loss: 0.1371\n",
            "Epoch 68/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2141 - dense_4_loss: 0.0753 - dense_3_loss: 0.1388\n",
            "Epoch 00068: loss did not improve from 0.21308\n",
            "535/535 [==============================] - 46s 86ms/step - loss: 0.2141 - dense_4_loss: 0.0753 - dense_3_loss: 0.1388\n",
            "Epoch 69/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2121 - dense_4_loss: 0.0761 - dense_3_loss: 0.1360\n",
            "Epoch 00069: loss improved from 0.21308 to 0.21214, saving model to ae.model\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2121 - dense_4_loss: 0.0761 - dense_3_loss: 0.1360\n",
            "Epoch 70/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2129 - dense_4_loss: 0.0756 - dense_3_loss: 0.1373\n",
            "Epoch 00070: loss did not improve from 0.21214\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2129 - dense_4_loss: 0.0756 - dense_3_loss: 0.1373\n",
            "Epoch 71/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2124 - dense_4_loss: 0.0763 - dense_3_loss: 0.1361\n",
            "Epoch 00071: loss did not improve from 0.21214\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2124 - dense_4_loss: 0.0763 - dense_3_loss: 0.1361\n",
            "Epoch 72/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2081 - dense_4_loss: 0.0733 - dense_3_loss: 0.1348\n",
            "Epoch 00072: loss improved from 0.21214 to 0.20813, saving model to ae.model\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2081 - dense_4_loss: 0.0733 - dense_3_loss: 0.1348\n",
            "Epoch 73/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2094 - dense_4_loss: 0.0749 - dense_3_loss: 0.1345\n",
            "Epoch 00073: loss did not improve from 0.20813\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2094 - dense_4_loss: 0.0749 - dense_3_loss: 0.1345\n",
            "Epoch 74/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2088 - dense_4_loss: 0.0745 - dense_3_loss: 0.1343\n",
            "Epoch 00074: loss did not improve from 0.20813\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2088 - dense_4_loss: 0.0745 - dense_3_loss: 0.1343\n",
            "Epoch 75/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2082 - dense_4_loss: 0.0736 - dense_3_loss: 0.1346\n",
            "Epoch 00075: loss did not improve from 0.20813\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2082 - dense_4_loss: 0.0736 - dense_3_loss: 0.1346\n",
            "Epoch 76/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2059 - dense_4_loss: 0.0730 - dense_3_loss: 0.1328\n",
            "Epoch 00076: loss improved from 0.20813 to 0.20586, saving model to ae.model\n",
            "535/535 [==============================] - 45s 83ms/step - loss: 0.2059 - dense_4_loss: 0.0730 - dense_3_loss: 0.1328\n",
            "Epoch 77/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2077 - dense_4_loss: 0.0751 - dense_3_loss: 0.1326\n",
            "Epoch 00077: loss did not improve from 0.20586\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2077 - dense_4_loss: 0.0751 - dense_3_loss: 0.1326\n",
            "Epoch 78/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2054 - dense_4_loss: 0.0733 - dense_3_loss: 0.1321\n",
            "Epoch 00078: loss improved from 0.20586 to 0.20542, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2054 - dense_4_loss: 0.0733 - dense_3_loss: 0.1321\n",
            "Epoch 79/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2042 - dense_4_loss: 0.0731 - dense_3_loss: 0.1311\n",
            "Epoch 00079: loss improved from 0.20542 to 0.20421, saving model to ae.model\n",
            "535/535 [==============================] - 45s 84ms/step - loss: 0.2042 - dense_4_loss: 0.0731 - dense_3_loss: 0.1311\n",
            "Epoch 80/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2061 - dense_4_loss: 0.0726 - dense_3_loss: 0.1336\n",
            "Epoch 00080: loss did not improve from 0.20421\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2061 - dense_4_loss: 0.0726 - dense_3_loss: 0.1336\n",
            "Epoch 81/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2055 - dense_4_loss: 0.0734 - dense_3_loss: 0.1320\n",
            "Epoch 00081: loss did not improve from 0.20421\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2055 - dense_4_loss: 0.0734 - dense_3_loss: 0.1320\n",
            "Epoch 82/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2045 - dense_4_loss: 0.0732 - dense_3_loss: 0.1313\n",
            "Epoch 00082: loss did not improve from 0.20421\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2045 - dense_4_loss: 0.0732 - dense_3_loss: 0.1313\n",
            "Epoch 83/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2036 - dense_4_loss: 0.0720 - dense_3_loss: 0.1316\n",
            "Epoch 00083: loss improved from 0.20421 to 0.20362, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2036 - dense_4_loss: 0.0720 - dense_3_loss: 0.1316\n",
            "Epoch 84/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2038 - dense_4_loss: 0.0719 - dense_3_loss: 0.1319\n",
            "Epoch 00084: loss did not improve from 0.20362\n",
            "535/535 [==============================] - 47s 88ms/step - loss: 0.2038 - dense_4_loss: 0.0719 - dense_3_loss: 0.1319\n",
            "Epoch 85/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2022 - dense_4_loss: 0.0722 - dense_3_loss: 0.1300\n",
            "Epoch 00085: loss improved from 0.20362 to 0.20225, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2022 - dense_4_loss: 0.0722 - dense_3_loss: 0.1300\n",
            "Epoch 86/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2035 - dense_4_loss: 0.0735 - dense_3_loss: 0.1300\n",
            "Epoch 00086: loss did not improve from 0.20225\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.2035 - dense_4_loss: 0.0735 - dense_3_loss: 0.1300\n",
            "Epoch 87/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2026 - dense_4_loss: 0.0710 - dense_3_loss: 0.1316\n",
            "Epoch 00087: loss did not improve from 0.20225\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2026 - dense_4_loss: 0.0710 - dense_3_loss: 0.1316\n",
            "Epoch 88/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2013 - dense_4_loss: 0.0709 - dense_3_loss: 0.1304\n",
            "Epoch 00088: loss improved from 0.20225 to 0.20125, saving model to ae.model\n",
            "535/535 [==============================] - 44s 82ms/step - loss: 0.2013 - dense_4_loss: 0.0709 - dense_3_loss: 0.1304\n",
            "Epoch 89/100\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.1993 - dense_4_loss: 0.0717 - dense_3_loss: 0.1276\n",
            "Epoch 00089: loss improved from 0.20125 to 0.19930, saving model to ae.model\n",
            "535/535 [==============================] - 44s 83ms/step - loss: 0.1993 - dense_4_loss: 0.0717 - dense_3_loss: 0.1276\n",
            "Epoch 90/100\n",
            "265/535 [=============>................] - ETA: 22s - loss: 0.1951 - dense_4_loss: 0.0697 - dense_3_loss: 0.1254"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNx0SXIhhb0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "668ae9a2-70a0-4104-fc56-ac90cb9380c1"
      },
      "source": [
        "path = F\"/content/gdrive/My Drive/autoenc.hdf5\" \n",
        "model_mse.save_weights(path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-58a6efb47779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mF\"/content/gdrive/My Drive/autoenc.hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_mse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_mse' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMgaSIGq7Ux1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldPyXXg2s7I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u2bD5IT3XFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod=Model(inputs=model_mse.inputs,outputs=model_mse.layers[4].output)\n",
        "mod.summary()\n",
        "pre=mod.predict((X[num],X[tot]))\n",
        "pre.shape\n",
        "np.save('/content/gdrive/My Drive/fraud/without_id_withoutV.npy',pre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvZpVVMA3dYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
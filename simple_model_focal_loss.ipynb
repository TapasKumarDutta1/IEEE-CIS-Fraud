{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "906bc660-de6c-4444-cee9-b2207a219171"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9b906b8e-d986-4831-9ef7-feac9f47c716"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 81% 47.0M/58.3M [00:00<00:00, 101MB/s] \n",
            "100% 58.3M/58.3M [00:00<00:00, 168MB/s]\n",
            "test_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDrCIAqHzl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9f2af116-aad0-4c38-a0b8-03f9b6fe1c78"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWJ-hzcznam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5OQjb1zo6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "44032739-0fac-4f8a-a27b-3c7a3c0a95cc"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 1671.53 MB\n",
            "Memory usage after optimization is: 417.88 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 1430.33 MB\n",
            "Memory usage after optimization is: 357.58 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01ad3abc-cfa3-4709-fad3-b0362face158"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "430"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0r3SuH1K97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQ20JqWATak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4dLndq4CQC2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ba31553-2e6f-4452-9710-040b03574fb6"
      },
      "source": [
        "20663/569877"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03625870143908247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVegeBsUCNjJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "892a64ae-cd2f-4e3d-f61c-63389f760ce5"
      },
      "source": [
        "trn['isFraud'].value_counts()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    569877\n",
              "1.0     20663\n",
              "Name: isFraud, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80a67232-5495-4be4-abdc-2116e9a0ab65"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=2\n",
        "        alpha=1-0.036\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((593,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.00001,decay=1e-3),loss=fl())\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  if en==0:\n",
        "    pre=mod.predict(tst)/5\n",
        "    tot=mod.predict(trn.loc[trn['month']==6].drop(['month','isFraud'],1))/5\n",
        "  else:\n",
        "    pre+=mod.predict(tst)/5\n",
        "    tot+=mod.predict(trn.loc[trn['month']==6].drop(['month','isFraud'],1))/5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 62.4923 - val_loss: 28.1993\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 50.6542 - val_loss: 29.0182\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 46.9577 - val_loss: 28.2409\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 44.5102 - val_loss: 27.0914\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 42.9227 - val_loss: 26.3814\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 42.2515 - val_loss: 25.5772\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 41.0506 - val_loss: 25.1324\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39.9987 - val_loss: 24.6044\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 39.4491 - val_loss: 24.2190\n",
            "Epoch 10/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 39.1523roc-auc_val: 0.7102\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 39.1060 - val_loss: 23.8731\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38.4654 - val_loss: 23.5534\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 38.1579 - val_loss: 23.2950\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37.1372 - val_loss: 23.1672\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 37.1808 - val_loss: 22.9072\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36.6122 - val_loss: 22.7877\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 36.5193 - val_loss: 22.6552\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35.8898 - val_loss: 22.5337\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35.8104 - val_loss: 22.4133\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35.6862 - val_loss: 22.3178\n",
            "Epoch 20/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 35.4409roc-auc_val: 0.7237\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 35.3856 - val_loss: 22.2265\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35.1876 - val_loss: 22.1788\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34.9982 - val_loss: 22.0064\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 35.0232 - val_loss: 21.9244\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34.7484 - val_loss: 21.9032\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34.7342 - val_loss: 21.8180\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34.2488 - val_loss: 21.7243\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 34.1836 - val_loss: 21.6150\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33.9502 - val_loss: 21.5850\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33.7661 - val_loss: 21.4922\n",
            "Epoch 30/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 34.0263roc-auc_val: 0.7308\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 34.0774 - val_loss: 21.4715\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33.7301 - val_loss: 21.4139\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33.4421 - val_loss: 21.3625\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33.3399 - val_loss: 21.3309\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33.4280 - val_loss: 21.3004\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 32.9339 - val_loss: 21.1815\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33.2261 - val_loss: 21.2775\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.9066 - val_loss: 21.1407\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 33.1498 - val_loss: 20.9616\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.7887 - val_loss: 21.0887\n",
            "Epoch 40/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 32.6854roc-auc_val: 0.7357\n",
            "225/225 [==============================] - 5s 23ms/step - loss: 32.6238 - val_loss: 21.0580\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.9124 - val_loss: 21.0270\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.6257 - val_loss: 21.0634\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.3898 - val_loss: 21.0295\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.2291 - val_loss: 20.9941\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.8579 - val_loss: 20.9225\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.9478 - val_loss: 20.9105\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.2166 - val_loss: 20.8863\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.9276 - val_loss: 20.8436\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.0254 - val_loss: 20.9223\n",
            "Epoch 50/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 31.9129roc-auc_val: 0.7382\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 31.9500 - val_loss: 20.8808\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.1566 - val_loss: 20.8279\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 32.1073 - val_loss: 20.7670\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.6899 - val_loss: 20.7972\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.4336 - val_loss: 20.7697\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.6941 - val_loss: 20.7437\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.4753 - val_loss: 20.7403\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.5909 - val_loss: 20.7447\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.5650 - val_loss: 20.7174\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.5489 - val_loss: 20.7238\n",
            "Epoch 60/1000\n",
            "214/225 [===========================>..] - ETA: 0s - loss: 31.5963roc-auc_val: 0.7406\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 31.6210 - val_loss: 20.6673\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.5183 - val_loss: 20.7474\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.2340 - val_loss: 20.7028\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.9573 - val_loss: 20.6531\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.1568 - val_loss: 20.5888\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.0548 - val_loss: 20.6178\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.2619 - val_loss: 20.6056\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.0559 - val_loss: 20.5826\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.5546 - val_loss: 20.5845\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.8945 - val_loss: 20.5893\n",
            "Epoch 70/1000\n",
            "214/225 [===========================>..] - ETA: 0s - loss: 31.0460roc-auc_val: 0.7423\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 30.9802 - val_loss: 20.5684\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.9684 - val_loss: 20.5234\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.0621 - val_loss: 20.5748\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 31.0610 - val_loss: 20.6155\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.6648 - val_loss: 20.5767\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.5012 - val_loss: 20.5740\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.6973 - val_loss: 20.5127\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.6984 - val_loss: 20.4674\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.5614 - val_loss: 20.4377\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.4499 - val_loss: 20.4040\n",
            "Epoch 80/1000\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 30.5746roc-auc_val: 0.7438\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 30.5051 - val_loss: 20.4538\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.4275 - val_loss: 20.3993\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.3435 - val_loss: 20.4076\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.3117 - val_loss: 20.4180\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.1038 - val_loss: 20.3829\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.1680 - val_loss: 20.3577\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.0598 - val_loss: 20.3636\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.2473 - val_loss: 20.3754\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.3641 - val_loss: 20.3359\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.1444 - val_loss: 20.3816\n",
            "Epoch 90/1000\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 30.0398roc-auc_val: 0.7453\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 30.0345 - val_loss: 20.4082\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.2784 - val_loss: 20.3649\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.0615 - val_loss: 20.3432\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.0635 - val_loss: 20.3911\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.1049 - val_loss: 20.2984\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.9737 - val_loss: 20.3269\n",
            "Epoch 96/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.0374 - val_loss: 20.2613\n",
            "Epoch 97/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.1902 - val_loss: 20.3824\n",
            "Epoch 98/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 30.1325 - val_loss: 20.3695\n",
            "Epoch 99/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.9323 - val_loss: 20.3130\n",
            "Epoch 100/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 30.1121roc-auc_val: 0.7462\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 30.1121 - val_loss: 20.2944\n",
            "Epoch 101/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.9754 - val_loss: 20.3476\n",
            "Epoch 102/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.8344 - val_loss: 20.3007\n",
            "Epoch 103/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.7396 - val_loss: 20.2906\n",
            "Epoch 104/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.8251 - val_loss: 20.3004\n",
            "Epoch 105/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.7531 - val_loss: 20.2856\n",
            "Epoch 106/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.7705 - val_loss: 20.2329\n",
            "Epoch 107/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.8486 - val_loss: 20.2338\n",
            "Epoch 108/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.5057 - val_loss: 20.2439\n",
            "Epoch 109/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.8867 - val_loss: 20.2344\n",
            "Epoch 110/1000\n",
            "214/225 [===========================>..] - ETA: 0s - loss: 29.6552roc-auc_val: 0.7474\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 29.6301 - val_loss: 20.2547\n",
            "Epoch 111/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.9017 - val_loss: 20.2959\n",
            "Epoch 112/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.3978 - val_loss: 20.2227\n",
            "Epoch 113/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.7313 - val_loss: 20.2514\n",
            "Epoch 114/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.3997 - val_loss: 20.1753\n",
            "Epoch 115/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.5177 - val_loss: 20.2246\n",
            "Epoch 116/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.4776 - val_loss: 20.1890\n",
            "Epoch 117/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.0957 - val_loss: 20.1248\n",
            "Epoch 118/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.5995 - val_loss: 20.1943\n",
            "Epoch 119/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.2729 - val_loss: 20.1875\n",
            "Epoch 120/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 29.5878roc-auc_val: 0.7483\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 29.4975 - val_loss: 20.1842\n",
            "Epoch 121/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.3566 - val_loss: 20.1726\n",
            "Epoch 122/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.2923 - val_loss: 20.2266\n",
            "Epoch 123/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.2288 - val_loss: 20.1521\n",
            "Epoch 124/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.3218 - val_loss: 20.1284\n",
            "Epoch 125/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.1137 - val_loss: 20.0870\n",
            "Epoch 126/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.0200 - val_loss: 20.1913\n",
            "Epoch 127/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.0333 - val_loss: 20.1187\n",
            "Epoch 128/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.3881 - val_loss: 20.1567\n",
            "Epoch 129/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.3580 - val_loss: 20.1605\n",
            "Epoch 130/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 29.2593roc-auc_val: 0.7484\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 29.2405 - val_loss: 20.1534\n",
            "Epoch 131/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.3567 - val_loss: 20.2318\n",
            "Epoch 132/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.1979 - val_loss: 20.1847\n",
            "Epoch 133/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.3256 - val_loss: 20.0590\n",
            "Epoch 134/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.2090 - val_loss: 20.1211\n",
            "Epoch 135/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.0333 - val_loss: 20.1472\n",
            "Epoch 136/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.0684 - val_loss: 20.1402\n",
            "Epoch 137/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.0110 - val_loss: 20.1326\n",
            "Epoch 138/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.1766 - val_loss: 20.1106\n",
            "Epoch 139/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.9333 - val_loss: 20.1046\n",
            "Epoch 140/1000\n",
            "214/225 [===========================>..] - ETA: 0s - loss: 29.3100roc-auc_val: 0.7489\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 29.3046 - val_loss: 20.1835\n",
            "Epoch 141/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.2021 - val_loss: 20.1380\n",
            "Epoch 142/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.1491 - val_loss: 20.1216\n",
            "Epoch 143/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.8450 - val_loss: 20.1601\n",
            "Epoch 144/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.0211 - val_loss: 20.1465\n",
            "Epoch 145/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.8096 - val_loss: 20.1164\n",
            "Epoch 146/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.0281 - val_loss: 20.0798\n",
            "Epoch 147/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.1173 - val_loss: 20.0294\n",
            "Epoch 148/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.0664 - val_loss: 20.1000\n",
            "Epoch 149/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 29.1385 - val_loss: 20.0842\n",
            "Epoch 150/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 28.7868roc-auc_val: 0.7501\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 28.7427 - val_loss: 20.0022\n",
            "Epoch 151/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.7212 - val_loss: 20.1000\n",
            "Epoch 152/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.6265 - val_loss: 20.0767\n",
            "Epoch 153/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5321 - val_loss: 20.0722\n",
            "Epoch 154/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.9925 - val_loss: 20.0927\n",
            "Epoch 155/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.9198 - val_loss: 20.0804\n",
            "Epoch 156/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.6077 - val_loss: 20.0233\n",
            "Epoch 157/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5967 - val_loss: 20.0932\n",
            "Epoch 158/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5034 - val_loss: 20.0676\n",
            "Epoch 159/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.7300 - val_loss: 19.9686\n",
            "Epoch 160/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 28.5475roc-auc_val: 0.7505\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 28.4467 - val_loss: 20.0638\n",
            "Epoch 161/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.9705 - val_loss: 20.0704\n",
            "Epoch 162/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5985 - val_loss: 19.9761\n",
            "Epoch 163/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.7897 - val_loss: 20.0322\n",
            "Epoch 164/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.7282 - val_loss: 20.0539\n",
            "Epoch 165/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.9012 - val_loss: 20.0228\n",
            "Epoch 166/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5864 - val_loss: 20.0408\n",
            "Epoch 167/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.6488 - val_loss: 20.0307\n",
            "Epoch 168/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.6077 - val_loss: 19.9981\n",
            "Epoch 169/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.4447 - val_loss: 20.0147\n",
            "Epoch 170/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 28.8413roc-auc_val: 0.7512\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 28.8413 - val_loss: 20.0129\n",
            "Epoch 171/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5794 - val_loss: 19.9901\n",
            "Epoch 172/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.4615 - val_loss: 20.0407\n",
            "Epoch 173/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5985 - val_loss: 20.0971\n",
            "Epoch 174/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.4276 - val_loss: 20.0097\n",
            "Epoch 175/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.6157 - val_loss: 20.0213\n",
            "Epoch 176/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.4727 - val_loss: 19.9701\n",
            "Epoch 177/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.4581 - val_loss: 20.0317\n",
            "Epoch 178/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.3577 - val_loss: 20.0024\n",
            "Epoch 179/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5818 - val_loss: 19.9411\n",
            "Epoch 180/1000\n",
            "215/225 [===========================>..] - ETA: 0s - loss: 28.2902roc-auc_val: 0.7513\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 28.2667 - val_loss: 20.0340\n",
            "Epoch 181/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5205 - val_loss: 19.9982\n",
            "Epoch 182/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.4729 - val_loss: 20.0266\n",
            "Epoch 183/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.4227 - val_loss: 20.0077\n",
            "Epoch 184/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.5921 - val_loss: 20.0025\n",
            "Epoch 185/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.4171 - val_loss: 20.0018\n",
            "Epoch 186/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.6167 - val_loss: 20.0059\n",
            "Epoch 187/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.2083 - val_loss: 20.0023\n",
            "Epoch 188/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.4871 - val_loss: 19.9999\n",
            "Epoch 189/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.3823 - val_loss: 19.9488\n",
            "Epoch 190/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 28.3808roc-auc_val: 0.7522\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 28.3808 - val_loss: 20.0336\n",
            "Epoch 191/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.3263 - val_loss: 19.9580\n",
            "Epoch 192/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.0886 - val_loss: 20.0423\n",
            "Epoch 193/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.3633 - val_loss: 19.9941\n",
            "Epoch 194/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.3728 - val_loss: 20.0302\n",
            "Epoch 195/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.3653 - val_loss: 19.9681\n",
            "Epoch 196/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.6994 - val_loss: 19.9644\n",
            "Epoch 197/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.3979 - val_loss: 19.9473\n",
            "Epoch 198/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.3501 - val_loss: 19.9603\n",
            "Epoch 199/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 27.8799 - val_loss: 20.0334\n",
            "Epoch 200/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 28.1901roc-auc_val: 0.7524\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 28.1205 - val_loss: 19.9458\n",
            "Epoch 201/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.1153 - val_loss: 19.9759\n",
            "Epoch 202/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.3810 - val_loss: 19.9582\n",
            "Epoch 203/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.2647 - val_loss: 19.9456\n",
            "Epoch 204/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 27.9587 - val_loss: 19.9660\n",
            "Epoch 205/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.1738 - val_loss: 19.9787\n",
            "Epoch 206/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 27.9944 - val_loss: 19.9598\n",
            "Epoch 207/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 27.8564 - val_loss: 20.0316\n",
            "Epoch 208/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 27.8838 - val_loss: 19.9939\n",
            "Epoch 209/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.1885 - val_loss: 19.9748\n",
            "Epoch 210/1000\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 28.0235roc-auc_val: 0.7525\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 28.0334 - val_loss: 19.9671\n",
            "Epoch 211/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.2555 - val_loss: 19.9821\n",
            "Epoch 212/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 28.0578 - val_loss: 19.9751\n",
            "Epoch 213/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 27.7249 - val_loss: 19.9380\n",
            "Epoch 214/1000\n",
            " 73/225 [========>.....................] - ETA: 0s - loss: 27.9917"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for en,month in enumerate(range(2,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month-1]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.00001,decay=1e-5),loss=focal_loss(),metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es])\n",
        "  del([train,test])\n",
        "  if en==0:\n",
        "    pre_2=mod.predict(tst)/4\n",
        "    tot_2=mod.predict(trn.loc[trn['month']==6].drop(['month','isFraud'],1))/4\n",
        "  else:\n",
        "    pre_2+=mod.predict(tst)/4\n",
        "    tot_2+=mod.predict(trn.loc[trn['month']==6].drop(['month','isFraud'],1))/4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08sp39NR1Iqz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ab09ee0-ecd9-4587-f7df-dfad07ec0e14"
      },
      "source": [
        "\n",
        "for en,month in enumerate(range(3,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month-2]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.00001,decay=1e-5),loss=focal_loss(),metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  if en==0:\n",
        "    pre_4=mod.predict(tst)/3\n",
        "    tot_4=mod.predict(trn.loc[trn['month']==6].drop(['month','isFraud'],1))/3\n",
        "  else:\n",
        "    pre_4+=mod.predict(tst)/3\n",
        "    tot_4+=mod.predict(trn.loc[trn['month']==6].drop(['month','isFraud'],1))/3"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 0.3982 - accuracy: 0.5162 - val_loss: 0.1604 - val_accuracy: 0.6351\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3461 - accuracy: 0.5176 - val_loss: 0.1534 - val_accuracy: 0.6586\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.3121 - accuracy: 0.5258 - val_loss: 0.1499 - val_accuracy: 0.6660\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.2867 - accuracy: 0.5373 - val_loss: 0.1454 - val_accuracy: 0.6806\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.2653 - accuracy: 0.5498 - val_loss: 0.1397 - val_accuracy: 0.7057\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.2478 - accuracy: 0.5624 - val_loss: 0.1333 - val_accuracy: 0.7399\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.2316 - accuracy: 0.5774 - val_loss: 0.1282 - val_accuracy: 0.7746\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.2183 - accuracy: 0.5905 - val_loss: 0.1237 - val_accuracy: 0.8015\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.2056 - accuracy: 0.6069 - val_loss: 0.1180 - val_accuracy: 0.8369\n",
            "Epoch 10/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1945 - accuracy: 0.6225 - val_loss: 0.1134 - val_accuracy: 0.8673\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1842 - accuracy: 0.6405 - val_loss: 0.1095 - val_accuracy: 0.8926\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1747 - accuracy: 0.6570 - val_loss: 0.1051 - val_accuracy: 0.9168\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1652 - accuracy: 0.6770 - val_loss: 0.1012 - val_accuracy: 0.9340\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1570 - accuracy: 0.6938 - val_loss: 0.0976 - val_accuracy: 0.9468\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1487 - accuracy: 0.7152 - val_loss: 0.0939 - val_accuracy: 0.9578\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1414 - accuracy: 0.7356 - val_loss: 0.0904 - val_accuracy: 0.9640\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1340 - accuracy: 0.7547 - val_loss: 0.0870 - val_accuracy: 0.9683\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1289 - accuracy: 0.7733 - val_loss: 0.0836 - val_accuracy: 0.9708\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1221 - accuracy: 0.7915 - val_loss: 0.0801 - val_accuracy: 0.9724\n",
            "Epoch 20/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1163 - accuracy: 0.8103 - val_loss: 0.0772 - val_accuracy: 0.9735\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1118 - accuracy: 0.8261 - val_loss: 0.0744 - val_accuracy: 0.9742\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1057 - accuracy: 0.8454 - val_loss: 0.0717 - val_accuracy: 0.9748\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.1007 - accuracy: 0.8595 - val_loss: 0.0679 - val_accuracy: 0.9752\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0966 - accuracy: 0.8725 - val_loss: 0.0657 - val_accuracy: 0.9755\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0914 - accuracy: 0.8859 - val_loss: 0.0630 - val_accuracy: 0.9756\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0873 - accuracy: 0.8957 - val_loss: 0.0600 - val_accuracy: 0.9757\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0826 - accuracy: 0.9054 - val_loss: 0.0573 - val_accuracy: 0.9757\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0795 - accuracy: 0.9132 - val_loss: 0.0553 - val_accuracy: 0.9757\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0759 - accuracy: 0.9194 - val_loss: 0.0533 - val_accuracy: 0.9757\n",
            "Epoch 30/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0724 - accuracy: 0.9255 - val_loss: 0.0512 - val_accuracy: 0.9759\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0688 - accuracy: 0.9307 - val_loss: 0.0485 - val_accuracy: 0.9759\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0652 - accuracy: 0.9351 - val_loss: 0.0456 - val_accuracy: 0.9758\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0625 - accuracy: 0.9387 - val_loss: 0.0444 - val_accuracy: 0.9759\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0589 - accuracy: 0.9421 - val_loss: 0.0419 - val_accuracy: 0.9759\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0564 - accuracy: 0.9439 - val_loss: 0.0398 - val_accuracy: 0.9758\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0533 - accuracy: 0.9473 - val_loss: 0.0378 - val_accuracy: 0.9758\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0508 - accuracy: 0.9488 - val_loss: 0.0362 - val_accuracy: 0.9758\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0481 - accuracy: 0.9510 - val_loss: 0.0344 - val_accuracy: 0.9758\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0459 - accuracy: 0.9528 - val_loss: 0.0328 - val_accuracy: 0.9758\n",
            "Epoch 40/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0432 - accuracy: 0.9537 - val_loss: 0.0307 - val_accuracy: 0.9758\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0420 - accuracy: 0.9552 - val_loss: 0.0296 - val_accuracy: 0.9758\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0393 - accuracy: 0.9563 - val_loss: 0.0281 - val_accuracy: 0.9759\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0374 - accuracy: 0.9573 - val_loss: 0.0265 - val_accuracy: 0.9759\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0350 - accuracy: 0.9585 - val_loss: 0.0252 - val_accuracy: 0.9759\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0337 - accuracy: 0.9586 - val_loss: 0.0241 - val_accuracy: 0.9759\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0326 - accuracy: 0.9593 - val_loss: 0.0228 - val_accuracy: 0.9759\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0306 - accuracy: 0.9600 - val_loss: 0.0218 - val_accuracy: 0.9758\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0289 - accuracy: 0.9605 - val_loss: 0.0205 - val_accuracy: 0.9758\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0276 - accuracy: 0.9607 - val_loss: 0.0196 - val_accuracy: 0.9757\n",
            "Epoch 50/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0261 - accuracy: 0.9612 - val_loss: 0.0184 - val_accuracy: 0.9757\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0246 - accuracy: 0.9615 - val_loss: 0.0174 - val_accuracy: 0.9757\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0234 - accuracy: 0.9619 - val_loss: 0.0165 - val_accuracy: 0.9757\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0222 - accuracy: 0.9621 - val_loss: 0.0157 - val_accuracy: 0.9757\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0213 - accuracy: 0.9621 - val_loss: 0.0150 - val_accuracy: 0.9756\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0203 - accuracy: 0.9627 - val_loss: 0.0143 - val_accuracy: 0.9756\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0190 - accuracy: 0.9627 - val_loss: 0.0134 - val_accuracy: 0.9756\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0182 - accuracy: 0.9629 - val_loss: 0.0129 - val_accuracy: 0.9756\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0173 - accuracy: 0.9631 - val_loss: 0.0123 - val_accuracy: 0.9756\n",
            "Epoch 59/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0164 - accuracy: 0.9633 - val_loss: 0.0117 - val_accuracy: 0.9756\n",
            "Epoch 60/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0158 - accuracy: 0.9632 - val_loss: 0.0111 - val_accuracy: 0.9756\n",
            "Epoch 61/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0150 - accuracy: 0.9634 - val_loss: 0.0105 - val_accuracy: 0.9756\n",
            "Epoch 62/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0140 - accuracy: 0.9637 - val_loss: 0.0100 - val_accuracy: 0.9755\n",
            "Epoch 63/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0138 - accuracy: 0.9637 - val_loss: 0.0096 - val_accuracy: 0.9755\n",
            "Epoch 64/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0131 - accuracy: 0.9636 - val_loss: 0.0092 - val_accuracy: 0.9755\n",
            "Epoch 65/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0124 - accuracy: 0.9635 - val_loss: 0.0087 - val_accuracy: 0.9755\n",
            "Epoch 66/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0118 - accuracy: 0.9638 - val_loss: 0.0083 - val_accuracy: 0.9755\n",
            "Epoch 67/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0112 - accuracy: 0.9637 - val_loss: 0.0079 - val_accuracy: 0.9755\n",
            "Epoch 68/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0109 - accuracy: 0.9638 - val_loss: 0.0076 - val_accuracy: 0.9755\n",
            "Epoch 69/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0103 - accuracy: 0.9640 - val_loss: 0.0072 - val_accuracy: 0.9755\n",
            "Epoch 70/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0097 - accuracy: 0.9639 - val_loss: 0.0069 - val_accuracy: 0.9755\n",
            "Epoch 71/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0094 - accuracy: 0.9640 - val_loss: 0.0066 - val_accuracy: 0.9756\n",
            "Epoch 72/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0092 - accuracy: 0.9640 - val_loss: 0.0064 - val_accuracy: 0.9755\n",
            "Epoch 73/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0086 - accuracy: 0.9641 - val_loss: 0.0060 - val_accuracy: 0.9755\n",
            "Epoch 74/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0084 - accuracy: 0.9640 - val_loss: 0.0058 - val_accuracy: 0.9755\n",
            "Epoch 75/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0079 - accuracy: 0.9641 - val_loss: 0.0056 - val_accuracy: 0.9755\n",
            "Epoch 76/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0079 - accuracy: 0.9640 - val_loss: 0.0054 - val_accuracy: 0.9755\n",
            "Epoch 77/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0076 - accuracy: 0.9641 - val_loss: 0.0052 - val_accuracy: 0.9755\n",
            "Epoch 78/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0072 - accuracy: 0.9641 - val_loss: 0.0050 - val_accuracy: 0.9755\n",
            "Epoch 79/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0070 - accuracy: 0.9641 - val_loss: 0.0049 - val_accuracy: 0.9755\n",
            "Epoch 80/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0067 - accuracy: 0.9640 - val_loss: 0.0047 - val_accuracy: 0.9755\n",
            "Epoch 81/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0065 - accuracy: 0.9641 - val_loss: 0.0045 - val_accuracy: 0.9754\n",
            "Epoch 82/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0062 - accuracy: 0.9642 - val_loss: 0.0043 - val_accuracy: 0.9755\n",
            "Epoch 83/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0061 - accuracy: 0.9643 - val_loss: 0.0042 - val_accuracy: 0.9755\n",
            "Epoch 84/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9642 - val_loss: 0.0041 - val_accuracy: 0.9755\n",
            "Epoch 85/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9642 - val_loss: 0.0039 - val_accuracy: 0.9755\n",
            "Epoch 86/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0055 - accuracy: 0.9644 - val_loss: 0.0038 - val_accuracy: 0.9755\n",
            "Epoch 87/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0054 - accuracy: 0.9642 - val_loss: 0.0037 - val_accuracy: 0.9754\n",
            "Epoch 88/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0052 - accuracy: 0.9643 - val_loss: 0.0036 - val_accuracy: 0.9755\n",
            "Epoch 89/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0051 - accuracy: 0.9643 - val_loss: 0.0035 - val_accuracy: 0.9755\n",
            "Epoch 90/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0049 - accuracy: 0.9644 - val_loss: 0.0034 - val_accuracy: 0.9755\n",
            "Epoch 91/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0048 - accuracy: 0.9644 - val_loss: 0.0033 - val_accuracy: 0.9755\n",
            "Epoch 92/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0048 - accuracy: 0.9643 - val_loss: 0.0033 - val_accuracy: 0.9755\n",
            "Epoch 93/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0046 - accuracy: 0.9642 - val_loss: 0.0032 - val_accuracy: 0.9754\n",
            "Epoch 94/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9642 - val_loss: 0.0031 - val_accuracy: 0.9754\n",
            "Epoch 95/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9643 - val_loss: 0.0031 - val_accuracy: 0.9754\n",
            "Epoch 96/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 0.9643 - val_loss: 0.0030 - val_accuracy: 0.9755\n",
            "Epoch 97/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0043 - accuracy: 0.9644 - val_loss: 0.0030 - val_accuracy: 0.9755\n",
            "Epoch 98/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9642 - val_loss: 0.0029 - val_accuracy: 0.9755\n",
            "Epoch 99/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9641 - val_loss: 0.0029 - val_accuracy: 0.9755\n",
            "Epoch 100/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9643 - val_loss: 0.0028 - val_accuracy: 0.9755\n",
            "Epoch 101/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.9643 - val_loss: 0.0028 - val_accuracy: 0.9755\n",
            "Epoch 102/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.9643 - val_loss: 0.0027 - val_accuracy: 0.9755\n",
            "Epoch 103/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9644 - val_loss: 0.0027 - val_accuracy: 0.9755\n",
            "Epoch 104/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9645 - val_loss: 0.0026 - val_accuracy: 0.9756\n",
            "Epoch 105/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9644 - val_loss: 0.0026 - val_accuracy: 0.9756\n",
            "Epoch 106/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9645 - val_loss: 0.0026 - val_accuracy: 0.9755\n",
            "Epoch 107/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9644 - val_loss: 0.0025 - val_accuracy: 0.9756\n",
            "Epoch 108/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9643 - val_loss: 0.0025 - val_accuracy: 0.9755\n",
            "Epoch 109/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9644 - val_loss: 0.0025 - val_accuracy: 0.9755\n",
            "Epoch 110/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9644 - val_loss: 0.0025 - val_accuracy: 0.9756\n",
            "Epoch 111/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0035 - accuracy: 0.9644 - val_loss: 0.0025 - val_accuracy: 0.9756\n",
            "Epoch 112/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0035 - accuracy: 0.9644 - val_loss: 0.0024 - val_accuracy: 0.9755\n",
            "Epoch 113/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9645 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 114/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9644 - val_loss: 0.0024 - val_accuracy: 0.9755\n",
            "Epoch 115/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9645 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 116/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9645 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 117/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9645 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 118/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9645 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 119/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9645 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 120/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9645 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 121/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9645 - val_loss: 0.0023 - val_accuracy: 0.9756\n",
            "Epoch 122/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9645 - val_loss: 0.0023 - val_accuracy: 0.9756\n",
            "Epoch 123/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9644 - val_loss: 0.0023 - val_accuracy: 0.9756\n",
            "Epoch 124/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9646 - val_loss: 0.0023 - val_accuracy: 0.9756\n",
            "Epoch 125/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9645 - val_loss: 0.0023 - val_accuracy: 0.9756\n",
            "Epoch 126/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9646 - val_loss: 0.0023 - val_accuracy: 0.9756\n",
            "Epoch 127/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9645 - val_loss: 0.0023 - val_accuracy: 0.9756\n",
            "Epoch 128/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9645 - val_loss: 0.0023 - val_accuracy: 0.9756\n",
            "Epoch 129/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9645 - val_loss: 0.0023 - val_accuracy: 0.9756\n",
            "Epoch 130/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9646 - val_loss: 0.0024 - val_accuracy: 0.9757\n",
            "Epoch 131/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9646 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 132/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9647 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 133/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9646 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 134/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9647 - val_loss: 0.0025 - val_accuracy: 0.9755\n",
            "Epoch 135/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9647 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 136/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9646 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 137/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9645 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 138/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9647 - val_loss: 0.0024 - val_accuracy: 0.9756\n",
            "Epoch 139/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9648 - val_loss: 0.0025 - val_accuracy: 0.9755\n",
            "Epoch 140/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9647 - val_loss: 0.0026 - val_accuracy: 0.9755\n",
            "Epoch 141/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9646 - val_loss: 0.0026 - val_accuracy: 0.9754\n",
            "Epoch 142/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9647 - val_loss: 0.0026 - val_accuracy: 0.9755\n",
            "Epoch 143/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9646 - val_loss: 0.0025 - val_accuracy: 0.9755\n",
            "Epoch 144/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9646 - val_loss: 0.0026 - val_accuracy: 0.9755\n",
            "Epoch 145/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9646 - val_loss: 0.0027 - val_accuracy: 0.9753\n",
            "Epoch 146/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9647 - val_loss: 0.0027 - val_accuracy: 0.9752\n",
            "Epoch 147/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9647 - val_loss: 0.0027 - val_accuracy: 0.9752\n",
            "Epoch 148/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9646 - val_loss: 0.0028 - val_accuracy: 0.9751\n",
            "Epoch 149/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9647 - val_loss: 0.0027 - val_accuracy: 0.9751\n",
            "Epoch 150/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9646 - val_loss: 0.0028 - val_accuracy: 0.9748\n",
            "Epoch 151/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9646 - val_loss: 0.0030 - val_accuracy: 0.9743\n",
            "Epoch 152/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9647 - val_loss: 0.0032 - val_accuracy: 0.9736\n",
            "Epoch 153/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9647 - val_loss: 0.0032 - val_accuracy: 0.9736\n",
            "Epoch 154/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9647 - val_loss: 0.0033 - val_accuracy: 0.9733\n",
            "Epoch 155/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9647 - val_loss: 0.0034 - val_accuracy: 0.9732\n",
            "Epoch 156/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9649 - val_loss: 0.0036 - val_accuracy: 0.9729\n",
            "Epoch 157/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9648 - val_loss: 0.0034 - val_accuracy: 0.9732\n",
            "Epoch 158/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9648 - val_loss: 0.0038 - val_accuracy: 0.9727\n",
            "Epoch 159/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9649 - val_loss: 0.0040 - val_accuracy: 0.9724\n",
            "Epoch 160/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9648 - val_loss: 0.0042 - val_accuracy: 0.9722\n",
            "Epoch 161/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9648 - val_loss: 0.0043 - val_accuracy: 0.9722\n",
            "Epoch 162/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9647 - val_loss: 0.0045 - val_accuracy: 0.9720\n",
            "Epoch 163/1000\n",
            "136/136 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9648 - val_loss: 0.0045 - val_accuracy: 0.9720\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 17ms/step - loss: 0.3990 - accuracy: 0.5223 - val_loss: 0.1662 - val_accuracy: 0.5497\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.3590 - accuracy: 0.5160 - val_loss: 0.1527 - val_accuracy: 0.6356\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.3329 - accuracy: 0.5189 - val_loss: 0.1458 - val_accuracy: 0.6672\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.3133 - accuracy: 0.5242 - val_loss: 0.1381 - val_accuracy: 0.7049\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2973 - accuracy: 0.5329 - val_loss: 0.1330 - val_accuracy: 0.7284\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2832 - accuracy: 0.5384 - val_loss: 0.1278 - val_accuracy: 0.7554\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2691 - accuracy: 0.5473 - val_loss: 0.1228 - val_accuracy: 0.7843\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2584 - accuracy: 0.5547 - val_loss: 0.1211 - val_accuracy: 0.7940\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2482 - accuracy: 0.5612 - val_loss: 0.1180 - val_accuracy: 0.8149\n",
            "Epoch 10/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2375 - accuracy: 0.5721 - val_loss: 0.1149 - val_accuracy: 0.8345\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2299 - accuracy: 0.5810 - val_loss: 0.1144 - val_accuracy: 0.8416\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2218 - accuracy: 0.5882 - val_loss: 0.1115 - val_accuracy: 0.8600\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2144 - accuracy: 0.5967 - val_loss: 0.1098 - val_accuracy: 0.8720\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.2053 - accuracy: 0.6079 - val_loss: 0.1078 - val_accuracy: 0.8839\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1990 - accuracy: 0.6200 - val_loss: 0.1045 - val_accuracy: 0.8991\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1916 - accuracy: 0.6273 - val_loss: 0.1028 - val_accuracy: 0.9083\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1848 - accuracy: 0.6399 - val_loss: 0.1015 - val_accuracy: 0.9158\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1777 - accuracy: 0.6492 - val_loss: 0.0992 - val_accuracy: 0.9258\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1740 - accuracy: 0.6599 - val_loss: 0.0963 - val_accuracy: 0.9344\n",
            "Epoch 20/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1671 - accuracy: 0.6722 - val_loss: 0.0941 - val_accuracy: 0.9412\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1620 - accuracy: 0.6843 - val_loss: 0.0922 - val_accuracy: 0.9468\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1561 - accuracy: 0.6940 - val_loss: 0.0898 - val_accuracy: 0.9519\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1517 - accuracy: 0.7091 - val_loss: 0.0875 - val_accuracy: 0.9563\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1467 - accuracy: 0.7211 - val_loss: 0.0852 - val_accuracy: 0.9600\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1425 - accuracy: 0.7317 - val_loss: 0.0836 - val_accuracy: 0.9625\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1379 - accuracy: 0.7435 - val_loss: 0.0819 - val_accuracy: 0.9643\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1337 - accuracy: 0.7549 - val_loss: 0.0801 - val_accuracy: 0.9657\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1288 - accuracy: 0.7669 - val_loss: 0.0770 - val_accuracy: 0.9669\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1255 - accuracy: 0.7811 - val_loss: 0.0749 - val_accuracy: 0.9677\n",
            "Epoch 30/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1210 - accuracy: 0.7928 - val_loss: 0.0736 - val_accuracy: 0.9681\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1176 - accuracy: 0.8052 - val_loss: 0.0719 - val_accuracy: 0.9688\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1137 - accuracy: 0.8161 - val_loss: 0.0695 - val_accuracy: 0.9692\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1104 - accuracy: 0.8265 - val_loss: 0.0677 - val_accuracy: 0.9695\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1063 - accuracy: 0.8405 - val_loss: 0.0658 - val_accuracy: 0.9697\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1036 - accuracy: 0.8493 - val_loss: 0.0647 - val_accuracy: 0.9699\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.1005 - accuracy: 0.8589 - val_loss: 0.0627 - val_accuracy: 0.9703\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0978 - accuracy: 0.8680 - val_loss: 0.0610 - val_accuracy: 0.9703\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0942 - accuracy: 0.8762 - val_loss: 0.0593 - val_accuracy: 0.9705\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0913 - accuracy: 0.8837 - val_loss: 0.0575 - val_accuracy: 0.9705\n",
            "Epoch 40/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0899 - accuracy: 0.8898 - val_loss: 0.0564 - val_accuracy: 0.9706\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0862 - accuracy: 0.8961 - val_loss: 0.0551 - val_accuracy: 0.9706\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0830 - accuracy: 0.9034 - val_loss: 0.0528 - val_accuracy: 0.9707\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0805 - accuracy: 0.9104 - val_loss: 0.0514 - val_accuracy: 0.9707\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0787 - accuracy: 0.9154 - val_loss: 0.0504 - val_accuracy: 0.9708\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0759 - accuracy: 0.9194 - val_loss: 0.0487 - val_accuracy: 0.9709\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0741 - accuracy: 0.9230 - val_loss: 0.0475 - val_accuracy: 0.9709\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0713 - accuracy: 0.9271 - val_loss: 0.0460 - val_accuracy: 0.9710\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0688 - accuracy: 0.9313 - val_loss: 0.0448 - val_accuracy: 0.9710\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0671 - accuracy: 0.9341 - val_loss: 0.0435 - val_accuracy: 0.9710\n",
            "Epoch 50/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0653 - accuracy: 0.9366 - val_loss: 0.0424 - val_accuracy: 0.9709\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0629 - accuracy: 0.9394 - val_loss: 0.0408 - val_accuracy: 0.9710\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0607 - accuracy: 0.9416 - val_loss: 0.0398 - val_accuracy: 0.9710\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0593 - accuracy: 0.9443 - val_loss: 0.0387 - val_accuracy: 0.9709\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0569 - accuracy: 0.9456 - val_loss: 0.0374 - val_accuracy: 0.9709\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0553 - accuracy: 0.9480 - val_loss: 0.0360 - val_accuracy: 0.9709\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0541 - accuracy: 0.9490 - val_loss: 0.0353 - val_accuracy: 0.9707\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0517 - accuracy: 0.9507 - val_loss: 0.0342 - val_accuracy: 0.9708\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0498 - accuracy: 0.9517 - val_loss: 0.0330 - val_accuracy: 0.9709\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0484 - accuracy: 0.9532 - val_loss: 0.0320 - val_accuracy: 0.9708\n",
            "Epoch 60/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0464 - accuracy: 0.9538 - val_loss: 0.0310 - val_accuracy: 0.9708\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0449 - accuracy: 0.9549 - val_loss: 0.0299 - val_accuracy: 0.9708\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0435 - accuracy: 0.9565 - val_loss: 0.0291 - val_accuracy: 0.9708\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0424 - accuracy: 0.9570 - val_loss: 0.0282 - val_accuracy: 0.9707\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0410 - accuracy: 0.9573 - val_loss: 0.0272 - val_accuracy: 0.9707\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0393 - accuracy: 0.9580 - val_loss: 0.0262 - val_accuracy: 0.9707\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0386 - accuracy: 0.9590 - val_loss: 0.0254 - val_accuracy: 0.9706\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0368 - accuracy: 0.9593 - val_loss: 0.0245 - val_accuracy: 0.9705\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0358 - accuracy: 0.9602 - val_loss: 0.0239 - val_accuracy: 0.9706\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0347 - accuracy: 0.9605 - val_loss: 0.0232 - val_accuracy: 0.9705\n",
            "Epoch 70/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0335 - accuracy: 0.9606 - val_loss: 0.0224 - val_accuracy: 0.9705\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0323 - accuracy: 0.9613 - val_loss: 0.0216 - val_accuracy: 0.9704\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0311 - accuracy: 0.9613 - val_loss: 0.0208 - val_accuracy: 0.9704\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0301 - accuracy: 0.9616 - val_loss: 0.0200 - val_accuracy: 0.9704\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0293 - accuracy: 0.9625 - val_loss: 0.0194 - val_accuracy: 0.9704\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0283 - accuracy: 0.9626 - val_loss: 0.0189 - val_accuracy: 0.9704\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0272 - accuracy: 0.9627 - val_loss: 0.0183 - val_accuracy: 0.9703\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0262 - accuracy: 0.9633 - val_loss: 0.0176 - val_accuracy: 0.9703\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0254 - accuracy: 0.9629 - val_loss: 0.0169 - val_accuracy: 0.9703\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0246 - accuracy: 0.9635 - val_loss: 0.0164 - val_accuracy: 0.9703\n",
            "Epoch 80/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0233 - accuracy: 0.9637 - val_loss: 0.0158 - val_accuracy: 0.9703\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0230 - accuracy: 0.9637 - val_loss: 0.0153 - val_accuracy: 0.9702\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0226 - accuracy: 0.9641 - val_loss: 0.0149 - val_accuracy: 0.9701\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0215 - accuracy: 0.9640 - val_loss: 0.0144 - val_accuracy: 0.9701\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0213 - accuracy: 0.9639 - val_loss: 0.0140 - val_accuracy: 0.9700\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0201 - accuracy: 0.9645 - val_loss: 0.0135 - val_accuracy: 0.9700\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0198 - accuracy: 0.9648 - val_loss: 0.0131 - val_accuracy: 0.9700\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0189 - accuracy: 0.9645 - val_loss: 0.0126 - val_accuracy: 0.9701\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0182 - accuracy: 0.9645 - val_loss: 0.0122 - val_accuracy: 0.9700\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0176 - accuracy: 0.9647 - val_loss: 0.0118 - val_accuracy: 0.9701\n",
            "Epoch 90/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0170 - accuracy: 0.9647 - val_loss: 0.0114 - val_accuracy: 0.9701\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0168 - accuracy: 0.9649 - val_loss: 0.0112 - val_accuracy: 0.9700\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0161 - accuracy: 0.9649 - val_loss: 0.0108 - val_accuracy: 0.9700\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0156 - accuracy: 0.9651 - val_loss: 0.0104 - val_accuracy: 0.9700\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0151 - accuracy: 0.9651 - val_loss: 0.0101 - val_accuracy: 0.9700\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0142 - accuracy: 0.9653 - val_loss: 0.0097 - val_accuracy: 0.9700\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0143 - accuracy: 0.9651 - val_loss: 0.0094 - val_accuracy: 0.9699\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0134 - accuracy: 0.9653 - val_loss: 0.0092 - val_accuracy: 0.9699\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0133 - accuracy: 0.9654 - val_loss: 0.0090 - val_accuracy: 0.9699\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0128 - accuracy: 0.9654 - val_loss: 0.0087 - val_accuracy: 0.9698\n",
            "Epoch 100/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0124 - accuracy: 0.9655 - val_loss: 0.0084 - val_accuracy: 0.9699\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0123 - accuracy: 0.9654 - val_loss: 0.0082 - val_accuracy: 0.9698\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0121 - accuracy: 0.9654 - val_loss: 0.0080 - val_accuracy: 0.9698\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0114 - accuracy: 0.9656 - val_loss: 0.0077 - val_accuracy: 0.9698\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0110 - accuracy: 0.9657 - val_loss: 0.0075 - val_accuracy: 0.9698\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0106 - accuracy: 0.9657 - val_loss: 0.0073 - val_accuracy: 0.9698\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0106 - accuracy: 0.9657 - val_loss: 0.0071 - val_accuracy: 0.9698\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0102 - accuracy: 0.9657 - val_loss: 0.0069 - val_accuracy: 0.9698\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0099 - accuracy: 0.9657 - val_loss: 0.0067 - val_accuracy: 0.9698\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0096 - accuracy: 0.9657 - val_loss: 0.0065 - val_accuracy: 0.9698\n",
            "Epoch 110/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0091 - accuracy: 0.9656 - val_loss: 0.0063 - val_accuracy: 0.9698\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0091 - accuracy: 0.9657 - val_loss: 0.0062 - val_accuracy: 0.9698\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0086 - accuracy: 0.9658 - val_loss: 0.0060 - val_accuracy: 0.9698\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0083 - accuracy: 0.9659 - val_loss: 0.0058 - val_accuracy: 0.9698\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0084 - accuracy: 0.9657 - val_loss: 0.0057 - val_accuracy: 0.9699\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0081 - accuracy: 0.9657 - val_loss: 0.0056 - val_accuracy: 0.9698\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0079 - accuracy: 0.9658 - val_loss: 0.0054 - val_accuracy: 0.9698\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0076 - accuracy: 0.9658 - val_loss: 0.0053 - val_accuracy: 0.9698\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0078 - accuracy: 0.9658 - val_loss: 0.0052 - val_accuracy: 0.9697\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0073 - accuracy: 0.9658 - val_loss: 0.0051 - val_accuracy: 0.9697\n",
            "Epoch 120/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0068 - accuracy: 0.9659 - val_loss: 0.0049 - val_accuracy: 0.9698\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0069 - accuracy: 0.9659 - val_loss: 0.0048 - val_accuracy: 0.9697\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0069 - accuracy: 0.9658 - val_loss: 0.0047 - val_accuracy: 0.9697\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0068 - accuracy: 0.9657 - val_loss: 0.0047 - val_accuracy: 0.9697\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0065 - accuracy: 0.9659 - val_loss: 0.0046 - val_accuracy: 0.9697\n",
            "Epoch 125/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0064 - accuracy: 0.9660 - val_loss: 0.0045 - val_accuracy: 0.9697\n",
            "Epoch 126/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0062 - accuracy: 0.9659 - val_loss: 0.0044 - val_accuracy: 0.9697\n",
            "Epoch 127/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0060 - accuracy: 0.9659 - val_loss: 0.0043 - val_accuracy: 0.9697\n",
            "Epoch 128/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0060 - accuracy: 0.9658 - val_loss: 0.0042 - val_accuracy: 0.9697\n",
            "Epoch 129/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0060 - accuracy: 0.9659 - val_loss: 0.0041 - val_accuracy: 0.9697\n",
            "Epoch 130/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0059 - accuracy: 0.9659 - val_loss: 0.0041 - val_accuracy: 0.9698\n",
            "Epoch 131/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0056 - accuracy: 0.9660 - val_loss: 0.0040 - val_accuracy: 0.9697\n",
            "Epoch 132/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0055 - accuracy: 0.9659 - val_loss: 0.0039 - val_accuracy: 0.9697\n",
            "Epoch 133/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0054 - accuracy: 0.9660 - val_loss: 0.0039 - val_accuracy: 0.9697\n",
            "Epoch 134/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0053 - accuracy: 0.9660 - val_loss: 0.0038 - val_accuracy: 0.9697\n",
            "Epoch 135/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0052 - accuracy: 0.9660 - val_loss: 0.0038 - val_accuracy: 0.9697\n",
            "Epoch 136/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0050 - accuracy: 0.9660 - val_loss: 0.0037 - val_accuracy: 0.9698\n",
            "Epoch 137/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0048 - accuracy: 0.9660 - val_loss: 0.0036 - val_accuracy: 0.9698\n",
            "Epoch 138/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0049 - accuracy: 0.9659 - val_loss: 0.0036 - val_accuracy: 0.9698\n",
            "Epoch 139/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0048 - accuracy: 0.9661 - val_loss: 0.0035 - val_accuracy: 0.9698\n",
            "Epoch 140/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0047 - accuracy: 0.9661 - val_loss: 0.0035 - val_accuracy: 0.9698\n",
            "Epoch 141/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0047 - accuracy: 0.9660 - val_loss: 0.0034 - val_accuracy: 0.9698\n",
            "Epoch 142/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0046 - accuracy: 0.9660 - val_loss: 0.0034 - val_accuracy: 0.9697\n",
            "Epoch 143/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0046 - accuracy: 0.9661 - val_loss: 0.0034 - val_accuracy: 0.9697\n",
            "Epoch 144/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0045 - accuracy: 0.9661 - val_loss: 0.0033 - val_accuracy: 0.9698\n",
            "Epoch 145/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0043 - accuracy: 0.9661 - val_loss: 0.0033 - val_accuracy: 0.9697\n",
            "Epoch 146/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0042 - accuracy: 0.9661 - val_loss: 0.0033 - val_accuracy: 0.9697\n",
            "Epoch 147/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0043 - accuracy: 0.9660 - val_loss: 0.0032 - val_accuracy: 0.9697\n",
            "Epoch 148/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0043 - accuracy: 0.9660 - val_loss: 0.0032 - val_accuracy: 0.9697\n",
            "Epoch 149/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0041 - accuracy: 0.9660 - val_loss: 0.0032 - val_accuracy: 0.9697\n",
            "Epoch 150/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0040 - accuracy: 0.9661 - val_loss: 0.0032 - val_accuracy: 0.9697\n",
            "Epoch 151/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0042 - accuracy: 0.9660 - val_loss: 0.0031 - val_accuracy: 0.9697\n",
            "Epoch 152/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0039 - accuracy: 0.9660 - val_loss: 0.0031 - val_accuracy: 0.9697\n",
            "Epoch 153/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0039 - accuracy: 0.9660 - val_loss: 0.0031 - val_accuracy: 0.9697\n",
            "Epoch 154/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0039 - accuracy: 0.9660 - val_loss: 0.0031 - val_accuracy: 0.9697\n",
            "Epoch 155/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0038 - accuracy: 0.9660 - val_loss: 0.0031 - val_accuracy: 0.9697\n",
            "Epoch 156/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0037 - accuracy: 0.9660 - val_loss: 0.0031 - val_accuracy: 0.9697\n",
            "Epoch 157/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0038 - accuracy: 0.9660 - val_loss: 0.0031 - val_accuracy: 0.9697\n",
            "Epoch 158/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0038 - accuracy: 0.9661 - val_loss: 0.0031 - val_accuracy: 0.9696\n",
            "Epoch 159/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0037 - accuracy: 0.9661 - val_loss: 0.0030 - val_accuracy: 0.9696\n",
            "Epoch 160/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0036 - accuracy: 0.9661 - val_loss: 0.0030 - val_accuracy: 0.9696\n",
            "Epoch 161/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0035 - accuracy: 0.9662 - val_loss: 0.0030 - val_accuracy: 0.9696\n",
            "Epoch 162/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0035 - accuracy: 0.9663 - val_loss: 0.0030 - val_accuracy: 0.9696\n",
            "Epoch 163/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0036 - accuracy: 0.9662 - val_loss: 0.0030 - val_accuracy: 0.9696\n",
            "Epoch 164/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 0.0035 - accuracy: 0.9662 - val_loss: 0.0030 - val_accuracy: 0.9696\n",
            "Epoch 165/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0035 - accuracy: 0.9661 - val_loss: 0.0030 - val_accuracy: 0.9695\n",
            "Epoch 166/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9663 - val_loss: 0.0030 - val_accuracy: 0.9695\n",
            "Epoch 167/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9661 - val_loss: 0.0030 - val_accuracy: 0.9695\n",
            "Epoch 168/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9662 - val_loss: 0.0030 - val_accuracy: 0.9694\n",
            "Epoch 169/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9662 - val_loss: 0.0030 - val_accuracy: 0.9694\n",
            "Epoch 170/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0035 - accuracy: 0.9661 - val_loss: 0.0030 - val_accuracy: 0.9693\n",
            "Epoch 171/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9662 - val_loss: 0.0030 - val_accuracy: 0.9693\n",
            "Epoch 172/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9661 - val_loss: 0.0030 - val_accuracy: 0.9692\n",
            "Epoch 173/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0033 - accuracy: 0.9662 - val_loss: 0.0030 - val_accuracy: 0.9693\n",
            "Epoch 174/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0032 - accuracy: 0.9662 - val_loss: 0.0031 - val_accuracy: 0.9692\n",
            "Epoch 175/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0033 - accuracy: 0.9661 - val_loss: 0.0031 - val_accuracy: 0.9691\n",
            "Epoch 176/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0032 - accuracy: 0.9661 - val_loss: 0.0030 - val_accuracy: 0.9692\n",
            "Epoch 177/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0032 - accuracy: 0.9661 - val_loss: 0.0031 - val_accuracy: 0.9691\n",
            "Epoch 178/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0032 - accuracy: 0.9662 - val_loss: 0.0031 - val_accuracy: 0.9691\n",
            "Epoch 179/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0032 - accuracy: 0.9662 - val_loss: 0.0031 - val_accuracy: 0.9690\n",
            "Epoch 180/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9662 - val_loss: 0.0031 - val_accuracy: 0.9690\n",
            "Epoch 181/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0032 - accuracy: 0.9660 - val_loss: 0.0031 - val_accuracy: 0.9690\n",
            "Epoch 182/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9661 - val_loss: 0.0031 - val_accuracy: 0.9689\n",
            "Epoch 183/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9663 - val_loss: 0.0032 - val_accuracy: 0.9688\n",
            "Epoch 184/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0032 - accuracy: 0.9662 - val_loss: 0.0032 - val_accuracy: 0.9688\n",
            "Epoch 185/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9662 - val_loss: 0.0032 - val_accuracy: 0.9687\n",
            "Epoch 186/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9663 - val_loss: 0.0033 - val_accuracy: 0.9686\n",
            "Epoch 187/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9663 - val_loss: 0.0033 - val_accuracy: 0.9686\n",
            "Epoch 188/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9662 - val_loss: 0.0033 - val_accuracy: 0.9685\n",
            "Epoch 189/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9663 - val_loss: 0.0033 - val_accuracy: 0.9684\n",
            "Epoch 190/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9663 - val_loss: 0.0033 - val_accuracy: 0.9684\n",
            "Epoch 191/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9662 - val_loss: 0.0033 - val_accuracy: 0.9684\n",
            "Epoch 192/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9660 - val_loss: 0.0033 - val_accuracy: 0.9684\n",
            "Epoch 193/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9661 - val_loss: 0.0033 - val_accuracy: 0.9684\n",
            "Epoch 194/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9662 - val_loss: 0.0034 - val_accuracy: 0.9683\n",
            "Epoch 195/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9662 - val_loss: 0.0035 - val_accuracy: 0.9683\n",
            "Epoch 196/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9663 - val_loss: 0.0036 - val_accuracy: 0.9682\n",
            "Epoch 197/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9663 - val_loss: 0.0036 - val_accuracy: 0.9683\n",
            "Epoch 198/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9664 - val_loss: 0.0036 - val_accuracy: 0.9682\n",
            "Epoch 199/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9664 - val_loss: 0.0036 - val_accuracy: 0.9682\n",
            "Epoch 200/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9664 - val_loss: 0.0037 - val_accuracy: 0.9682\n",
            "Epoch 201/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9664 - val_loss: 0.0037 - val_accuracy: 0.9682\n",
            "Epoch 202/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9663 - val_loss: 0.0036 - val_accuracy: 0.9682\n",
            "Epoch 203/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9662 - val_loss: 0.0036 - val_accuracy: 0.9682\n",
            "Epoch 204/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9663 - val_loss: 0.0038 - val_accuracy: 0.9681\n",
            "Epoch 205/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9663 - val_loss: 0.0039 - val_accuracy: 0.9681\n",
            "Epoch 206/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9664 - val_loss: 0.0041 - val_accuracy: 0.9681\n",
            "Epoch 207/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 0.9663 - val_loss: 0.0041 - val_accuracy: 0.9680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7HiCPSH1mUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4cc59b6-8bb1-4567-8fc4-f7bcb16e9472"
      },
      "source": [
        "pre=(pre_2+pre_4+pre)/3\n",
        "pre.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506691, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOAKmlze1ozF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ss=pd.read_csv('sample_submission.csv.zip')\n",
        "ss['isFraud']=pre\n",
        "ss=ss.set_index('TransactionID')\n",
        "ss.to_csv('sub.csv')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSrW9P-Y_ft6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3e3ad44-e45c-4f9f-b182-9224c665cbde"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(trn.loc[trn['month']==6]['isFraud'],tot)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8539109457290902"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTs4Z_CU_zmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
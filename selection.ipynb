{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "selection.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNQiKuqw//vKj4yIm2Ppwxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "outputId": "4953970b-b8c2-40df-ab80-c91dac1a4e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "pip install tensorflow==2.1.0-rc0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.1.0-rc0 in /usr/local/lib/python3.6/dist-packages (2.1.0rc0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (0.9.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (2.0.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (1.18.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (2.0.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (0.34.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (1.28.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (3.2.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (1.7.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (46.1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0-rc0) (2.10.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (0.2.8)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.1.0-rc0) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "outputId": "d8bb1269-005a-47db-9970-def561214374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "pip install keras==2.3.1\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BeDD__pesQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "trn=pd.read_csv('train_transaction.csv.zip')\n",
        "tst=pd.read_csv('test_transaction.csv.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3UJH3B5ezCC",
        "colab_type": "code",
        "outputId": "262944a0-fce6-4c3e-f881-c6073986e491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)\n",
        "trn.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(590540, 55)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV4rJqGZjRI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ext(fnl):\n",
        "\n",
        "  fnl['P_1']=fnl['P_emaildomain'].fillna('nan').str.split('.').str[0]\n",
        "  fnl['P_2']=fnl['P_emaildomain'].fillna('nan').str.split('.').str[1]\n",
        "\n",
        "\n",
        "  fnl['R_1']=fnl['R_emaildomain'].fillna('nan').str.split('.').str[0]\n",
        "  fnl['R_2']=fnl['R_emaildomain'].fillna('nan').str.split('.').str[1]\n",
        "  return fnl\n",
        "trn=ext(trn)\n",
        "tst=ext(tst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOW97hGcpelu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['P_emaildomain','R_emaildomain','TransactionDT','TransactionID'],1)\n",
        "tst=tst.drop(['P_emaildomain','R_emaildomain','TransactionDT','TransactionID'],1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXdmBx1pRQ4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkC1MXQpi1GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def handle_disc(df,df1,cols):\n",
        "#   for col in cols:\n",
        "#     df[col]=df[col].fillna('other')\n",
        "#     cbe=CatBoostEncoder(sigma=0.01)\n",
        "#     df[col]=cbe.fit_transform(df[col].astype(str),df['isFraud'])\n",
        "#     df1[col]=cbe.transform(df1[col])\n",
        "#   return df,df1\n",
        "def handle_disc(df,cols,num):\n",
        "  for col in cols:\n",
        "    par=min(df[col].nunique(),num)\n",
        "    unq = df[col].value_counts().head(par)\n",
        "    dum=pd.get_dummies(df[col])[unq.index]\n",
        "    ls=[]\n",
        "    for i in dum.columns:\n",
        "      ls.append(str(col)+str(i))\n",
        "    dum.columns=ls\n",
        "    df=pd.concat([df,dum],1)\n",
        "\n",
        "  df=df.drop(cols,1)\n",
        "  sp=df.shape[1]\n",
        "  df.columns=list(range(sp))\n",
        "  return df\n",
        "def handle_cont(df,cols):\n",
        "  for col in cols:\n",
        "    df[col]=df[col].fillna(df[col].mean())\n",
        "    mn=df[col].mean()\n",
        "    std=df[col].std()\n",
        "    df[col]=(df[col]-mn)/(std+0.1)\n",
        "    df[col]=np.log10(df[col]+1-min(0,min(df[col])))\n",
        "  return df[cols]\n",
        "disc=['ProductCD','card1','card2','card3','card4','card5','card6','addr1','addr2','P_1','P_2','R_1','R_2','M1','M2','M3','M4','M5','M6','M7','M8','M9']\n",
        "cont=[i for i in tst.columns if i not in disc]\n",
        "trn_cont=handle_cont(trn,cont)\n",
        "tst_cont=handle_cont(tst,cont)\n",
        "trn_cont=trn_cont.replace([np.inf,-np.inf],0)\n",
        "tst_cont=tst_cont.replace([np.inf,-np.inf],0)\n",
        "trn_disc=handle_disc(trn,disc,140)\n",
        "tst_disc=handle_disc(tst,disc,140)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-GXjntws_4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=pd.concat([trn_cont,trn_disc],1).reset_index(drop=True)\n",
        "tst=pd.concat([tst_cont,tst_disc],1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCqKVjlpzag3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls=list(set(list(tst)).intersection(set(list(trn))))\n",
        "trn=trn[ls]\n",
        "tst=tst[ls]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sUgS9Xazr48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.loc[:,~(trn.columns.duplicated())]\n",
        "tst=tst.loc[:,~(tst.columns.duplicated())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFGy6fS5zuxw",
        "colab_type": "code",
        "outputId": "6aa96071-ae66-47ab-be1e-9b85ec93714d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "fnl=pd.concat([trn,tst],0)\n",
        "fnl=fnl.reset_index(drop=True)\n",
        "z=fnl.isna().sum()\n",
        "z[z>0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENKEV7R_6QJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f723ecd8-9ea8-4871-e0c9-0406e8229e41"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDvknW_60DHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b385b122-b282-42b3-9aeb-721bed030589"
      },
      "source": [
        "\n",
        "#input fed to model is original data where 10% columns have their values shuffled within themselves \n",
        "\n",
        "import gc\n",
        "import os\n",
        "gc.collect()\n",
        "import numpy as np\n",
        "import keras \n",
        "from math import ceil\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import Sequence\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Input, Concatenate, Dropout,BatchNormalization\n",
        "from keras.layers import Add\n",
        "import keras.backend as K\n",
        "class DAESequence(Sequence):\n",
        "    def __init__(self,df,batch_size):\n",
        "        self.batch_size=batch_size\n",
        "        self.len_data=df.shape[0]\n",
        "        self.columns=df.shape[1]\n",
        "        self.data=df.values\n",
        "        self.idx=[]\n",
        "    def __len__(self):\n",
        "        return int(ceil(self.len_data/self.batch_size))\n",
        "    def __getitem__(self,idx):\n",
        "        self.idx.append(idx)\n",
        "        last=min((idx+1)*self.batch_size,self.len_data)\n",
        "        idx=idx*self.batch_size\n",
        "        size=last-idx\n",
        "        output_x=self.data[idx:last]\n",
        "        length=last-idx\n",
        "        rnd=np.random.randint(0,self.len_data-size,size)\n",
        "        rnd[rnd>idx]+=size\n",
        "        cols=np.random.randint(0,self.columns,int(self.columns*0.1))\n",
        "        noise_x=output_x.copy()\n",
        "        noise_x[:,cols]=self.data[rnd[:,None],cols]\n",
        "        return noise_x,output_x\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class WarmUpLearningRateScheduler(keras.callbacks.Callback):\n",
        "    \"\"\"Warmup learning rate scheduler\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, warmup_batches, init_lr, verbose=0):\n",
        "        \"\"\"Constructor for warmup learning rate scheduler\n",
        "\n",
        "        Arguments:\n",
        "            warmup_batches {int} -- Number of batch for warmup.\n",
        "            init_lr {float} -- Learning rate after warmup.\n",
        "\n",
        "        Keyword Arguments:\n",
        "            verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n",
        "        \"\"\"\n",
        "\n",
        "        super(WarmUpLearningRateScheduler, self).__init__()\n",
        "        self.warmup_batches = warmup_batches\n",
        "        self.init_lr = init_lr\n",
        "        self.verbose = verbose\n",
        "        self.batch_count = 0\n",
        "        self.learning_rates = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.batch_count = self.batch_count + 1\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.learning_rates.append(lr)\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        if self.batch_count <= self.warmup_batches:\n",
        "            lr = self.batch_count*self.init_lr/self.warmup_batches\n",
        "            K.set_value(self.model.optimizer.lr, lr)\n",
        "            if self.verbose > 0:\n",
        "                print('\\nBatch %05d: WarmUpLearningRateScheduler setting learning '\n",
        "                      'rate to %s.' % (self.batch_count + 1, lr))\n",
        "warm_up_lr = WarmUpLearningRateScheduler(400, init_lr=0.005)\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
        "from keras.layers import BatchNormalization\n",
        "#fnl=pd.concat([fnl_trn,fnl_tst],0).select_dtypes(exclude='uint8')\n",
        "batch_size=1024\n",
        "checkpoint_path = \"training_1.hdf5\" \n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback =ModelCheckpoint(checkpoint_path, \n",
        "monitor='loss',save_best_only=True,save_weights_only=True,verbose=1)\n",
        "len_input_columns=fnl.shape[1]\n",
        "model_dae = Sequential()\n",
        "model_dae.add(Dense(units=len_input_columns,input_shape=(len_input_columns,), activation='relu', dtype='float32', name='Input'))\n",
        "model_dae.add(Dense(units=int(512), activation='relu', dtype='float32',name='Hidden_1'))\n",
        "model_dae.add(BatchNormalization())\n",
        "model_dae.add(Dense(units=int(256), activation='relu', dtype='float32',name='Hidden_2'))\n",
        "model_dae.add(BatchNormalization())\n",
        "model_dae.add(Dense(units=int(512), activation='relu', dtype='float32',name='Hidden_3'))\n",
        "model_dae.add(BatchNormalization())\n",
        "model_dae.add(Dense(units=len_input_columns, activation='linear', dtype='float32', name='Output'))\n",
        "model_dae.compile(loss='mean_squared_error',optimizer='adam')\n",
        "path = F\"/content/gdrive/My Drive/autoenc_final.hdf5\" \n",
        "model_dae.load_weights(path)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKwkqfC14s5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2979c3b-6dc1-4fec-dc6c-1d27de14d5f4"
      },
      "source": [
        "from tqdm import tqdm\n",
        "your_new_df=pd.DataFrame()\n",
        "for i in tqdm([3]):\n",
        "    intermediate_layer_model = Model(inputs=model_dae.input, outputs=model_dae.layers[i].output)\n",
        "    z = pd.DataFrame(intermediate_layer_model.predict(fnl))\n",
        "    columns_names = ['Hidden_'+str(i)+'_'+str(l) for l in range(0, z.shape[1])]\n",
        "    z.columns=columns_names\n",
        "    your_new_df=pd.concat([your_new_df,z],1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:56<00:00, 56.45s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2JvbMj7fgZb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9d492eaf-08dd-4b50-8e6d-0e166bfd02a0"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        df[col]=df[col].fillna(-1)\n",
        "        df[col]=df[col].replace([np.inf,-np.inf],-1)\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "your_new_df=reduce_mem_usage(your_new_df)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 1071.51 MB\n",
            "Memory usage after optimization is: 535.76 MB\n",
            "Decreased by 50.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLNPKXhM4uQB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3dd63c13-18c2-4d76-d2a2-ec6dac1a2197"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "path = F\"/content/gdrive/My Drive/autoenc.csv\" \n",
        "your_new_df.to_csv(path)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
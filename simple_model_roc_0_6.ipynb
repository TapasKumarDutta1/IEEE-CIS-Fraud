{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_0.6",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_0_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a0129dee-d063-4160-b6df-a11e32c15a6f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b4244b66-2a8d-4222-f469-545e7ae58468"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 107MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 63% 33.0M/52.2M [00:00<00:00, 32.4MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 82.2MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 158MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 84% 49.0M/58.3M [00:00<00:00, 34.6MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 70.7MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 221MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv80v8W_Ko2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7bd3dbd6-8859-4534-d7af-d94b2172d394"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jt2pcxPije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a8b8c94a-75b4-4d7c-f774-edcb5b18d9af"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvEaxp9jhbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "9902bd17-e9a7-4af3-d6d2-e91cf3ec0fd3"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38009488-078d-43cf-e603-1657cd99a53d"
      },
      "source": [
        "\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkLEBcl6LHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 0.6\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGXOzNdXS9DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fba46b1-a0ba-4237-bc2a-25f0783e7167"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "# trn=trn.drop(['isFraud_isna'],1)\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Adam(0.001,decay=1e-3),loss=rac,metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.00001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 31017.3789 - accuracy: 0.6995 - val_loss: 25521.0176 - val_accuracy: 0.6141\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 25624.4160 - accuracy: 0.7444 - val_loss: 22580.9609 - val_accuracy: 0.6374\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 23802.0762 - accuracy: 0.7625 - val_loss: 22244.9590 - val_accuracy: 0.6599\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 21847.4805 - accuracy: 0.7812 - val_loss: 21914.8477 - val_accuracy: 0.6964\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 20162.3047 - accuracy: 0.7981 - val_loss: 21514.0664 - val_accuracy: 0.7119\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 19360.3770 - accuracy: 0.8092 - val_loss: 21262.9219 - val_accuracy: 0.7010\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 18345.9219 - accuracy: 0.8180 - val_loss: 20620.3555 - val_accuracy: 0.7214\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 17577.2266 - accuracy: 0.8257 - val_loss: 20988.9492 - val_accuracy: 0.7540\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 17004.2949 - accuracy: 0.8306 - val_loss: 20693.8105 - val_accuracy: 0.7741\n",
            "Epoch 10/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 16555.4355 - accuracy: 0.8362roc-auc_val: 0.7923\n",
            "225/225 [==============================] - 5s 22ms/step - loss: 16555.4355 - accuracy: 0.8362 - val_loss: 20691.3418 - val_accuracy: 0.7476\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 15944.1270 - accuracy: 0.8400 - val_loss: 19494.3379 - val_accuracy: 0.7888\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 15749.6699 - accuracy: 0.8429 - val_loss: 20484.1367 - val_accuracy: 0.7408\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 15339.2109 - accuracy: 0.8453 - val_loss: 19941.4336 - val_accuracy: 0.7977\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 15005.3535 - accuracy: 0.8484 - val_loss: 19855.1230 - val_accuracy: 0.7933\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 14840.1299 - accuracy: 0.8483 - val_loss: 19428.1777 - val_accuracy: 0.8071\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 14487.9727 - accuracy: 0.8526 - val_loss: 19840.5254 - val_accuracy: 0.8171\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 14166.1631 - accuracy: 0.8544 - val_loss: 19526.5742 - val_accuracy: 0.7964\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 14026.9717 - accuracy: 0.8564 - val_loss: 19451.5547 - val_accuracy: 0.8045\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 13924.8145 - accuracy: 0.8545 - val_loss: 19225.2637 - val_accuracy: 0.8307\n",
            "Epoch 20/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 13689.1387 - accuracy: 0.8565roc-auc_val: 0.8072\n",
            "225/225 [==============================] - 5s 22ms/step - loss: 13740.3887 - accuracy: 0.8564 - val_loss: 19501.6289 - val_accuracy: 0.8120\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 13502.4219 - accuracy: 0.8590 - val_loss: 19170.6543 - val_accuracy: 0.8118\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 13299.4795 - accuracy: 0.8575 - val_loss: 19108.2910 - val_accuracy: 0.8132\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 13245.3682 - accuracy: 0.8601 - val_loss: 19269.7656 - val_accuracy: 0.8200\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 13028.4932 - accuracy: 0.8603 - val_loss: 19259.0684 - val_accuracy: 0.8036\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12941.6094 - accuracy: 0.8641 - val_loss: 19116.1953 - val_accuracy: 0.8281\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12760.4102 - accuracy: 0.8660 - val_loss: 19324.7051 - val_accuracy: 0.8128\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12700.9580 - accuracy: 0.8639 - val_loss: 19335.9512 - val_accuracy: 0.8285\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12621.6982 - accuracy: 0.8644 - val_loss: 19388.1699 - val_accuracy: 0.8195\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12424.5068 - accuracy: 0.8656 - val_loss: 19160.3047 - val_accuracy: 0.8236\n",
            "Epoch 30/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 12319.2832 - accuracy: 0.8678roc-auc_val: 0.8141\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 12311.7119 - accuracy: 0.8680 - val_loss: 18945.8574 - val_accuracy: 0.8314\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12215.8477 - accuracy: 0.8676 - val_loss: 19282.2852 - val_accuracy: 0.8381\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12131.2568 - accuracy: 0.8669 - val_loss: 19292.1270 - val_accuracy: 0.8271\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 12069.5264 - accuracy: 0.8700 - val_loss: 18993.9062 - val_accuracy: 0.8272\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 12079.0166 - accuracy: 0.8693 - val_loss: 19186.6094 - val_accuracy: 0.8422\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11971.9756 - accuracy: 0.8717 - val_loss: 19079.9961 - val_accuracy: 0.8495\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11785.3213 - accuracy: 0.8706 - val_loss: 19175.7891 - val_accuracy: 0.8347\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11752.8203 - accuracy: 0.8709 - val_loss: 18893.2539 - val_accuracy: 0.8514\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11733.4434 - accuracy: 0.8709 - val_loss: 19166.7715 - val_accuracy: 0.8394\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11753.4756 - accuracy: 0.8679 - val_loss: 19202.1094 - val_accuracy: 0.8415\n",
            "Epoch 40/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 11606.8633 - accuracy: 0.8716roc-auc_val: 0.8137\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 11572.8789 - accuracy: 0.8718 - val_loss: 19099.0898 - val_accuracy: 0.8443\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11564.4521 - accuracy: 0.8729 - val_loss: 19013.0098 - val_accuracy: 0.8426\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11423.6465 - accuracy: 0.8722 - val_loss: 19061.5664 - val_accuracy: 0.8538\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11533.9648 - accuracy: 0.8737 - val_loss: 18931.2383 - val_accuracy: 0.8535\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11375.9453 - accuracy: 0.8737 - val_loss: 18832.1660 - val_accuracy: 0.8532\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11274.6562 - accuracy: 0.8700 - val_loss: 19009.6406 - val_accuracy: 0.8434\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11313.2129 - accuracy: 0.8748 - val_loss: 18908.1211 - val_accuracy: 0.8465\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11089.2959 - accuracy: 0.8737 - val_loss: 18893.2500 - val_accuracy: 0.8505\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11109.4180 - accuracy: 0.8748 - val_loss: 19024.4512 - val_accuracy: 0.8467\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 11061.8955 - accuracy: 0.8742 - val_loss: 19033.5488 - val_accuracy: 0.8586\n",
            "Epoch 50/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 11014.2510 - accuracy: 0.8758roc-auc_val: 0.8185\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 11007.7383 - accuracy: 0.8759 - val_loss: 18831.0840 - val_accuracy: 0.8613\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10940.1807 - accuracy: 0.8770 - val_loss: 19027.0547 - val_accuracy: 0.8547\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10818.8330 - accuracy: 0.8757 - val_loss: 18947.5293 - val_accuracy: 0.8559\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10816.3359 - accuracy: 0.8771 - val_loss: 19134.0898 - val_accuracy: 0.8518\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10838.3359 - accuracy: 0.8766 - val_loss: 19259.6562 - val_accuracy: 0.8376\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10802.8018 - accuracy: 0.8763 - val_loss: 19025.2578 - val_accuracy: 0.8588\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10832.9404 - accuracy: 0.8760 - val_loss: 18967.4062 - val_accuracy: 0.8655\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10672.2520 - accuracy: 0.8771 - val_loss: 18970.0078 - val_accuracy: 0.8576\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10650.3193 - accuracy: 0.8785 - val_loss: 19036.1309 - val_accuracy: 0.8519\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10599.4521 - accuracy: 0.8807 - val_loss: 19019.0742 - val_accuracy: 0.8534\n",
            "Epoch 60/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 10501.5684 - accuracy: 0.8779roc-auc_val: 0.8147\n",
            "225/225 [==============================] - 5s 23ms/step - loss: 10503.6611 - accuracy: 0.8778 - val_loss: 18982.2598 - val_accuracy: 0.8438\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10572.4980 - accuracy: 0.8800 - val_loss: 18984.7520 - val_accuracy: 0.8486\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10536.3516 - accuracy: 0.8779 - val_loss: 19118.5391 - val_accuracy: 0.8458\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10515.0635 - accuracy: 0.8781 - val_loss: 18938.1543 - val_accuracy: 0.8631\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10486.9229 - accuracy: 0.8793 - val_loss: 18999.1582 - val_accuracy: 0.8569\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10482.0498 - accuracy: 0.8777 - val_loss: 19104.4219 - val_accuracy: 0.8562\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10379.7461 - accuracy: 0.8788 - val_loss: 19106.7051 - val_accuracy: 0.8603\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 10256.7891 - accuracy: 0.8836 - val_loss: 19183.9531 - val_accuracy: 0.8589\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10310.3535 - accuracy: 0.8820 - val_loss: 19056.3848 - val_accuracy: 0.8687\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10297.2529 - accuracy: 0.8831 - val_loss: 19030.6406 - val_accuracy: 0.8607\n",
            "Epoch 70/1000\n",
            "216/225 [===========================>..] - ETA: 0s - loss: 10456.1816 - accuracy: 0.8799roc-auc_val: 0.8161\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 10404.6367 - accuracy: 0.8801 - val_loss: 19076.2637 - val_accuracy: 0.8623\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10280.9121 - accuracy: 0.8812 - val_loss: 19154.0078 - val_accuracy: 0.8606\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10303.7754 - accuracy: 0.8828 - val_loss: 19063.8066 - val_accuracy: 0.8642\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10074.9717 - accuracy: 0.8811 - val_loss: 19168.8398 - val_accuracy: 0.8523\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10198.3711 - accuracy: 0.8799 - val_loss: 19250.5859 - val_accuracy: 0.8659\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10236.8916 - accuracy: 0.8827 - val_loss: 19125.4316 - val_accuracy: 0.8575\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10082.5625 - accuracy: 0.8850 - val_loss: 19101.0254 - val_accuracy: 0.8631\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10123.2178 - accuracy: 0.8816 - val_loss: 19070.8496 - val_accuracy: 0.8593\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10065.2803 - accuracy: 0.8818 - val_loss: 18970.6641 - val_accuracy: 0.8590\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 10138.9414 - accuracy: 0.8819 - val_loss: 19025.0098 - val_accuracy: 0.8600\n",
            "Epoch 80/1000\n",
            "218/225 [============================>.] - ETA: 0s - loss: 10064.3066 - accuracy: 0.8828roc-auc_val: 0.8152\n",
            "225/225 [==============================] - 5s 22ms/step - loss: 10114.6758 - accuracy: 0.8825 - val_loss: 19085.2930 - val_accuracy: 0.8551\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9945.0186 - accuracy: 0.8825 - val_loss: 19163.8105 - val_accuracy: 0.8561\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9981.6689 - accuracy: 0.8816 - val_loss: 18989.1016 - val_accuracy: 0.8593\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9979.7178 - accuracy: 0.8843 - val_loss: 19043.1641 - val_accuracy: 0.8655\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9949.0322 - accuracy: 0.8839 - val_loss: 19203.3477 - val_accuracy: 0.8622\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9840.8350 - accuracy: 0.8850 - val_loss: 19194.9531 - val_accuracy: 0.8641\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9824.6758 - accuracy: 0.8852 - val_loss: 19038.4980 - val_accuracy: 0.8641\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9899.7646 - accuracy: 0.8845 - val_loss: 19171.2207 - val_accuracy: 0.8645\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9874.4287 - accuracy: 0.8836 - val_loss: 19096.9805 - val_accuracy: 0.8690\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9840.5029 - accuracy: 0.8845 - val_loss: 19101.0410 - val_accuracy: 0.8711\n",
            "Epoch 90/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 9875.3115 - accuracy: 0.8860roc-auc_val: 0.8153\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 9901.0078 - accuracy: 0.8859 - val_loss: 19141.8359 - val_accuracy: 0.8605\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9747.1064 - accuracy: 0.8856 - val_loss: 19275.0684 - val_accuracy: 0.8636\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9679.9902 - accuracy: 0.8852 - val_loss: 19096.0098 - val_accuracy: 0.8614\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9807.4102 - accuracy: 0.8838 - val_loss: 19162.9785 - val_accuracy: 0.8664\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9852.6787 - accuracy: 0.8821 - val_loss: 19251.1367 - val_accuracy: 0.8667\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9785.2881 - accuracy: 0.8838 - val_loss: 19121.6699 - val_accuracy: 0.8657\n",
            "Epoch 96/1000\n",
            "225/225 [==============================] - 1s 6ms/step - loss: 9648.6777 - accuracy: 0.8865 - val_loss: 19295.6133 - val_accuracy: 0.8713\n",
            "Epoch 97/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9779.9453 - accuracy: 0.8854 - val_loss: 19087.8535 - val_accuracy: 0.8699\n",
            "Epoch 98/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9663.0859 - accuracy: 0.8868 - val_loss: 19075.1758 - val_accuracy: 0.8692\n",
            "Epoch 99/1000\n",
            "225/225 [==============================] - 1s 5ms/step - loss: 9641.2324 - accuracy: 0.8881 - val_loss: 19127.3555 - val_accuracy: 0.8739\n",
            "Epoch 100/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 9679.1914 - accuracy: 0.8856roc-auc_val: 0.8185\n",
            "225/225 [==============================] - 5s 21ms/step - loss: 9686.9375 - accuracy: 0.8856 - val_loss: 19041.7695 - val_accuracy: 0.8748\n",
            "Epoch 1/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 31052.4551 - accuracy: 0.6990 - val_loss: 26721.4512 - val_accuracy: 0.6903\n",
            "Epoch 2/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 25586.4824 - accuracy: 0.7516 - val_loss: 24872.5664 - val_accuracy: 0.7187\n",
            "Epoch 3/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 23002.5508 - accuracy: 0.7715 - val_loss: 24415.3359 - val_accuracy: 0.6745\n",
            "Epoch 4/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 21427.6816 - accuracy: 0.7904 - val_loss: 24132.6367 - val_accuracy: 0.7026\n",
            "Epoch 5/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 19983.0254 - accuracy: 0.8043 - val_loss: 23585.6172 - val_accuracy: 0.7466\n",
            "Epoch 6/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 19044.2559 - accuracy: 0.8121 - val_loss: 23986.4883 - val_accuracy: 0.7276\n",
            "Epoch 7/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 18039.5469 - accuracy: 0.8182 - val_loss: 23049.9961 - val_accuracy: 0.7599\n",
            "Epoch 8/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 17400.9395 - accuracy: 0.8243 - val_loss: 23484.0488 - val_accuracy: 0.7829\n",
            "Epoch 9/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 16835.0254 - accuracy: 0.8332 - val_loss: 22910.0781 - val_accuracy: 0.7811\n",
            "Epoch 10/1000\n",
            "174/181 [===========================>..] - ETA: 0s - loss: 16365.7100 - accuracy: 0.8385roc-auc_val: 0.8159\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 16348.8750 - accuracy: 0.8387 - val_loss: 22740.7324 - val_accuracy: 0.7852\n",
            "Epoch 11/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 15817.6416 - accuracy: 0.8442 - val_loss: 22806.7363 - val_accuracy: 0.7756\n",
            "Epoch 12/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 15421.1035 - accuracy: 0.8458 - val_loss: 22110.2441 - val_accuracy: 0.7766\n",
            "Epoch 13/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 15170.6826 - accuracy: 0.8500 - val_loss: 22338.6973 - val_accuracy: 0.7877\n",
            "Epoch 14/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14624.4111 - accuracy: 0.8525 - val_loss: 22314.2969 - val_accuracy: 0.7625\n",
            "Epoch 15/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14519.2910 - accuracy: 0.8518 - val_loss: 22215.4863 - val_accuracy: 0.7898\n",
            "Epoch 16/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 14278.4180 - accuracy: 0.8546 - val_loss: 22442.8105 - val_accuracy: 0.8048\n",
            "Epoch 17/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13872.8037 - accuracy: 0.8588 - val_loss: 22496.3770 - val_accuracy: 0.7925\n",
            "Epoch 18/1000\n",
            "181/181 [==============================] - 1s 7ms/step - loss: 13727.7354 - accuracy: 0.8622 - val_loss: 22395.8809 - val_accuracy: 0.8033\n",
            "Epoch 19/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13534.4619 - accuracy: 0.8608 - val_loss: 22098.2676 - val_accuracy: 0.8047\n",
            "Epoch 20/1000\n",
            "172/181 [===========================>..] - ETA: 0s - loss: 13515.2051 - accuracy: 0.8620roc-auc_val: 0.8194\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 13467.4912 - accuracy: 0.8624 - val_loss: 22375.6641 - val_accuracy: 0.8096\n",
            "Epoch 21/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 13190.7754 - accuracy: 0.8642 - val_loss: 22175.4668 - val_accuracy: 0.8064\n",
            "Epoch 22/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12944.9062 - accuracy: 0.8649 - val_loss: 22146.7715 - val_accuracy: 0.8132\n",
            "Epoch 23/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12803.8848 - accuracy: 0.8660 - val_loss: 22078.5215 - val_accuracy: 0.8261\n",
            "Epoch 24/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12758.4883 - accuracy: 0.8665 - val_loss: 21848.1777 - val_accuracy: 0.8157\n",
            "Epoch 25/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12426.2080 - accuracy: 0.8684 - val_loss: 22103.2676 - val_accuracy: 0.7968\n",
            "Epoch 26/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12345.5820 - accuracy: 0.8687 - val_loss: 22245.0723 - val_accuracy: 0.8212\n",
            "Epoch 27/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12229.0254 - accuracy: 0.8708 - val_loss: 22301.4199 - val_accuracy: 0.8174\n",
            "Epoch 28/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12310.3008 - accuracy: 0.8704 - val_loss: 22197.5078 - val_accuracy: 0.8228\n",
            "Epoch 29/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12121.2754 - accuracy: 0.8702 - val_loss: 22475.9395 - val_accuracy: 0.8183\n",
            "Epoch 30/1000\n",
            "172/181 [===========================>..] - ETA: 0s - loss: 12046.4531 - accuracy: 0.8720roc-auc_val: 0.8215\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 12023.7148 - accuracy: 0.8722 - val_loss: 22277.9570 - val_accuracy: 0.8210\n",
            "Epoch 31/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 12009.5498 - accuracy: 0.8671 - val_loss: 22016.7344 - val_accuracy: 0.8218\n",
            "Epoch 32/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11751.0342 - accuracy: 0.8703 - val_loss: 22445.5410 - val_accuracy: 0.8225\n",
            "Epoch 33/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11661.7852 - accuracy: 0.8741 - val_loss: 22592.5566 - val_accuracy: 0.8393\n",
            "Epoch 34/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11585.9971 - accuracy: 0.8745 - val_loss: 22152.8496 - val_accuracy: 0.8300\n",
            "Epoch 35/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11532.2871 - accuracy: 0.8735 - val_loss: 22385.5762 - val_accuracy: 0.8356\n",
            "Epoch 36/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11465.3154 - accuracy: 0.8770 - val_loss: 22617.6504 - val_accuracy: 0.8343\n",
            "Epoch 37/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11272.4863 - accuracy: 0.8767 - val_loss: 22261.1641 - val_accuracy: 0.8332\n",
            "Epoch 38/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11109.3086 - accuracy: 0.8787 - val_loss: 22479.0176 - val_accuracy: 0.8229\n",
            "Epoch 39/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11233.8467 - accuracy: 0.8761 - val_loss: 22637.9199 - val_accuracy: 0.8385\n",
            "Epoch 40/1000\n",
            "175/181 [============================>.] - ETA: 0s - loss: 11139.3252 - accuracy: 0.8785roc-auc_val: 0.8183\n",
            "181/181 [==============================] - 7s 39ms/step - loss: 11114.1318 - accuracy: 0.8785 - val_loss: 22605.3477 - val_accuracy: 0.8261\n",
            "Epoch 41/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 11049.2920 - accuracy: 0.8771 - val_loss: 22523.8359 - val_accuracy: 0.8451\n",
            "Epoch 42/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10977.0693 - accuracy: 0.8821 - val_loss: 22390.1816 - val_accuracy: 0.8518\n",
            "Epoch 43/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10844.4004 - accuracy: 0.8789 - val_loss: 22473.2324 - val_accuracy: 0.8373\n",
            "Epoch 44/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10992.7607 - accuracy: 0.8803 - val_loss: 22537.8789 - val_accuracy: 0.8568\n",
            "Epoch 45/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10765.1748 - accuracy: 0.8827 - val_loss: 22169.8906 - val_accuracy: 0.8573\n",
            "Epoch 46/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10719.3125 - accuracy: 0.8824 - val_loss: 22235.5898 - val_accuracy: 0.8414\n",
            "Epoch 47/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10585.0098 - accuracy: 0.8820 - val_loss: 22330.4277 - val_accuracy: 0.8630\n",
            "Epoch 48/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10587.5723 - accuracy: 0.8838 - val_loss: 22563.9551 - val_accuracy: 0.8510\n",
            "Epoch 49/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10565.4531 - accuracy: 0.8816 - val_loss: 22406.2285 - val_accuracy: 0.8370\n",
            "Epoch 50/1000\n",
            "177/181 [============================>.] - ETA: 0s - loss: 10708.2988 - accuracy: 0.8830roc-auc_val: 0.819\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 10650.1357 - accuracy: 0.8829 - val_loss: 22634.9492 - val_accuracy: 0.8415\n",
            "Epoch 51/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10425.5117 - accuracy: 0.8848 - val_loss: 22539.6562 - val_accuracy: 0.8565\n",
            "Epoch 52/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10335.1523 - accuracy: 0.8836 - val_loss: 22742.0625 - val_accuracy: 0.8513\n",
            "Epoch 53/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10447.8848 - accuracy: 0.8825 - val_loss: 22598.5566 - val_accuracy: 0.8429\n",
            "Epoch 54/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10358.7607 - accuracy: 0.8823 - val_loss: 22738.7695 - val_accuracy: 0.8533\n",
            "Epoch 55/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10307.9541 - accuracy: 0.8853 - val_loss: 22537.0781 - val_accuracy: 0.8412\n",
            "Epoch 56/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10295.9082 - accuracy: 0.8863 - val_loss: 22506.4160 - val_accuracy: 0.8462\n",
            "Epoch 57/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10213.7256 - accuracy: 0.8827 - val_loss: 22673.7969 - val_accuracy: 0.8452\n",
            "Epoch 58/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10177.2637 - accuracy: 0.8848 - val_loss: 22675.5137 - val_accuracy: 0.8457\n",
            "Epoch 59/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10139.7021 - accuracy: 0.8867 - val_loss: 22658.3340 - val_accuracy: 0.8464\n",
            "Epoch 60/1000\n",
            "178/181 [============================>.] - ETA: 0s - loss: 10028.7461 - accuracy: 0.8887roc-auc_val: 0.821\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 10009.7227 - accuracy: 0.8885 - val_loss: 22639.1699 - val_accuracy: 0.8498\n",
            "Epoch 61/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10035.5449 - accuracy: 0.8865 - val_loss: 22670.4258 - val_accuracy: 0.8503\n",
            "Epoch 62/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10173.9805 - accuracy: 0.8846 - val_loss: 22783.2852 - val_accuracy: 0.8541\n",
            "Epoch 63/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10036.6670 - accuracy: 0.8853 - val_loss: 22711.5430 - val_accuracy: 0.8459\n",
            "Epoch 64/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9972.2695 - accuracy: 0.8851 - val_loss: 22655.1797 - val_accuracy: 0.8421\n",
            "Epoch 65/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 10040.7783 - accuracy: 0.8864 - val_loss: 22837.7578 - val_accuracy: 0.8596\n",
            "Epoch 66/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9917.4492 - accuracy: 0.8872 - val_loss: 22590.5527 - val_accuracy: 0.8529\n",
            "Epoch 67/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9936.2207 - accuracy: 0.8863 - val_loss: 22662.5859 - val_accuracy: 0.8551\n",
            "Epoch 68/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9725.1992 - accuracy: 0.8893 - val_loss: 22706.4375 - val_accuracy: 0.8563\n",
            "Epoch 69/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9797.5371 - accuracy: 0.8877 - val_loss: 22793.8867 - val_accuracy: 0.8566\n",
            "Epoch 70/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 9773.3291 - accuracy: 0.8876roc-auc_val: 0.8204\n",
            "181/181 [==============================] - 7s 38ms/step - loss: 9733.9795 - accuracy: 0.8876 - val_loss: 22766.2969 - val_accuracy: 0.8615\n",
            "Epoch 71/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9763.9131 - accuracy: 0.8887 - val_loss: 22841.6270 - val_accuracy: 0.8612\n",
            "Epoch 72/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9657.7676 - accuracy: 0.8893 - val_loss: 22758.6465 - val_accuracy: 0.8561\n",
            "Epoch 73/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9749.8975 - accuracy: 0.8888 - val_loss: 22830.5977 - val_accuracy: 0.8591\n",
            "Epoch 74/1000\n",
            "181/181 [==============================] - 1s 6ms/step - loss: 9625.9248 - accuracy: 0.8901 - val_loss: 22441.5391 - val_accuracy: 0.8538\n",
            "Epoch 1/1000\n",
            "136/136 [==============================] - 2s 12ms/step - loss: 31211.5098 - accuracy: 0.6909 - val_loss: 28554.7305 - val_accuracy: 0.6710\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 25547.6953 - accuracy: 0.7383 - val_loss: 27811.7852 - val_accuracy: 0.7056\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 23481.3535 - accuracy: 0.7562 - val_loss: 26762.7520 - val_accuracy: 0.7228\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 21963.2305 - accuracy: 0.7703 - val_loss: 25685.9551 - val_accuracy: 0.7194\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 20373.0020 - accuracy: 0.7927 - val_loss: 25875.3770 - val_accuracy: 0.7507\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 19241.3652 - accuracy: 0.7993 - val_loss: 25432.6641 - val_accuracy: 0.7742\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 18461.0508 - accuracy: 0.8090 - val_loss: 25406.1270 - val_accuracy: 0.7780\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 17538.9023 - accuracy: 0.8191 - val_loss: 25214.0996 - val_accuracy: 0.7683\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 16756.7617 - accuracy: 0.8235 - val_loss: 25714.6309 - val_accuracy: 0.8027\n",
            "Epoch 10/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 16152.3555 - accuracy: 0.8334roc-auc_val: 0.8144\n",
            "136/136 [==============================] - 9s 67ms/step - loss: 16060.1074 - accuracy: 0.8333 - val_loss: 25213.7793 - val_accuracy: 0.7975\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 15508.0391 - accuracy: 0.8360 - val_loss: 25628.8477 - val_accuracy: 0.8163\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 15234.4072 - accuracy: 0.8404 - val_loss: 26023.8984 - val_accuracy: 0.8424\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14661.3867 - accuracy: 0.8444 - val_loss: 25453.6641 - val_accuracy: 0.8062\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14344.1465 - accuracy: 0.8448 - val_loss: 25386.4531 - val_accuracy: 0.8109\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 14203.0957 - accuracy: 0.8457 - val_loss: 25673.2559 - val_accuracy: 0.8196\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13607.2939 - accuracy: 0.8504 - val_loss: 25710.7129 - val_accuracy: 0.8266\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13534.1045 - accuracy: 0.8560 - val_loss: 26031.0176 - val_accuracy: 0.8404\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13203.5254 - accuracy: 0.8547 - val_loss: 26191.6270 - val_accuracy: 0.8197\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 13040.0439 - accuracy: 0.8558 - val_loss: 25833.1152 - val_accuracy: 0.8250\n",
            "Epoch 20/1000\n",
            "128/136 [===========================>..] - ETA: 0s - loss: 12696.4668 - accuracy: 0.8602roc-auc_val: 0.811\n",
            "136/136 [==============================] - 9s 67ms/step - loss: 12724.7773 - accuracy: 0.8592 - val_loss: 25816.3730 - val_accuracy: 0.8106\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12548.6934 - accuracy: 0.8607 - val_loss: 25711.1797 - val_accuracy: 0.8307\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12378.0850 - accuracy: 0.8609 - val_loss: 25948.4355 - val_accuracy: 0.8190\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12274.7285 - accuracy: 0.8631 - val_loss: 25947.6289 - val_accuracy: 0.8337\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12131.6777 - accuracy: 0.8651 - val_loss: 25997.5547 - val_accuracy: 0.8259\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 12014.3457 - accuracy: 0.8644 - val_loss: 26470.0020 - val_accuracy: 0.8346\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11527.9219 - accuracy: 0.8713 - val_loss: 26454.5703 - val_accuracy: 0.8340\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11553.7920 - accuracy: 0.8685 - val_loss: 25897.9453 - val_accuracy: 0.8262\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11380.6533 - accuracy: 0.8710 - val_loss: 26036.6328 - val_accuracy: 0.8338\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 11515.6357 - accuracy: 0.8709 - val_loss: 26034.4961 - val_accuracy: 0.8111\n",
            "Epoch 30/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 11194.7188 - accuracy: 0.8731roc-auc_val: 0.8104\n",
            "136/136 [==============================] - 9s 67ms/step - loss: 11123.7100 - accuracy: 0.8733 - val_loss: 25919.5996 - val_accuracy: 0.8229\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10987.4707 - accuracy: 0.8747 - val_loss: 26548.4238 - val_accuracy: 0.8217\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10951.8301 - accuracy: 0.8747 - val_loss: 26259.6348 - val_accuracy: 0.8355\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10854.3701 - accuracy: 0.8739 - val_loss: 26189.3066 - val_accuracy: 0.8235\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10728.3838 - accuracy: 0.8769 - val_loss: 26346.9707 - val_accuracy: 0.8254\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10603.7969 - accuracy: 0.8777 - val_loss: 26535.1582 - val_accuracy: 0.8326\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10396.4570 - accuracy: 0.8774 - val_loss: 26467.7871 - val_accuracy: 0.8163\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10445.8242 - accuracy: 0.8793 - val_loss: 26073.6875 - val_accuracy: 0.8165\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10279.8604 - accuracy: 0.8801 - val_loss: 26743.6328 - val_accuracy: 0.8507\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10472.3896 - accuracy: 0.8805 - val_loss: 26304.5000 - val_accuracy: 0.8072\n",
            "Epoch 40/1000\n",
            "135/136 [============================>.] - ETA: 0s - loss: 10346.3359 - accuracy: 0.8751roc-auc_val: 0.8062\n",
            "136/136 [==============================] - 9s 67ms/step - loss: 10325.5146 - accuracy: 0.8751 - val_loss: 26605.1484 - val_accuracy: 0.8160\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9965.0752 - accuracy: 0.8818 - val_loss: 26903.0723 - val_accuracy: 0.8233\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 10198.2773 - accuracy: 0.8792 - val_loss: 26842.2070 - val_accuracy: 0.8366\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9978.4619 - accuracy: 0.8824 - val_loss: 26547.4746 - val_accuracy: 0.8449\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9869.3271 - accuracy: 0.8854 - val_loss: 26788.7969 - val_accuracy: 0.8438\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9772.1084 - accuracy: 0.8832 - val_loss: 26817.8711 - val_accuracy: 0.8462\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 9931.0430 - accuracy: 0.8840 - val_loss: 27018.5254 - val_accuracy: 0.8404\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 9784.6680 - accuracy: 0.8855 - val_loss: 26777.8047 - val_accuracy: 0.8526\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 9577.9658 - accuracy: 0.8863 - val_loss: 26936.5605 - val_accuracy: 0.8591\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 8ms/step - loss: 9614.1543 - accuracy: 0.8863 - val_loss: 26689.2090 - val_accuracy: 0.8302\n",
            "Epoch 50/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 9374.5703 - accuracy: 0.8873roc-auc_val: 0.8044\n",
            "136/136 [==============================] - 10s 72ms/step - loss: 9380.1982 - accuracy: 0.8873 - val_loss: 27017.1172 - val_accuracy: 0.8473\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9533.6230 - accuracy: 0.8877 - val_loss: 26975.0957 - val_accuracy: 0.8500\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9460.2236 - accuracy: 0.8871 - val_loss: 26813.2715 - val_accuracy: 0.8339\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9316.8936 - accuracy: 0.8853 - val_loss: 26978.5566 - val_accuracy: 0.8471\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9321.3525 - accuracy: 0.8885 - val_loss: 26564.2363 - val_accuracy: 0.8379\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9147.3125 - accuracy: 0.8884 - val_loss: 27024.7793 - val_accuracy: 0.8582\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9177.3115 - accuracy: 0.8902 - val_loss: 26850.8359 - val_accuracy: 0.8475\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9128.3701 - accuracy: 0.8902 - val_loss: 27184.1973 - val_accuracy: 0.8447\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 9225.5020 - accuracy: 0.8906 - val_loss: 26997.1250 - val_accuracy: 0.8438\n",
            "Epoch 59/1000\n",
            "136/136 [==============================] - 1s 7ms/step - loss: 8909.8027 - accuracy: 0.8896 - val_loss: 27486.3438 - val_accuracy: 0.8659\n",
            "Epoch 60/1000\n",
            "135/136 [============================>.] - ETA: 0s - loss: 9064.0752 - accuracy: 0.8903roc-auc_val: 0.8144\n",
            "136/136 [==============================] - 9s 66ms/step - loss: 9045.4336 - accuracy: 0.8901 - val_loss: 27329.7891 - val_accuracy: 0.8436\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 18ms/step - loss: 31464.3867 - accuracy: 0.6663 - val_loss: 30713.9766 - val_accuracy: 0.6800\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 25305.9922 - accuracy: 0.7245 - val_loss: 29652.7461 - val_accuracy: 0.7074\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 22853.0020 - accuracy: 0.7426 - val_loss: 28998.5664 - val_accuracy: 0.7090\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 21170.7441 - accuracy: 0.7569 - val_loss: 28481.2520 - val_accuracy: 0.7348\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 20160.9277 - accuracy: 0.7656 - val_loss: 28561.4453 - val_accuracy: 0.7069\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 19048.8281 - accuracy: 0.7733 - val_loss: 28210.4297 - val_accuracy: 0.6911\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 18318.2461 - accuracy: 0.7740 - val_loss: 27802.3496 - val_accuracy: 0.6964\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 17352.9434 - accuracy: 0.7901 - val_loss: 27785.9199 - val_accuracy: 0.7633\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 16441.6289 - accuracy: 0.8025 - val_loss: 27162.6934 - val_accuracy: 0.7754\n",
            "Epoch 10/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 15789.0518 - accuracy: 0.8129roc-auc_val: 0.8105\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 15805.3633 - accuracy: 0.8124 - val_loss: 27105.7578 - val_accuracy: 0.7902\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 14894.6123 - accuracy: 0.8257 - val_loss: 26708.6738 - val_accuracy: 0.8039\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 14442.8369 - accuracy: 0.8294 - val_loss: 26716.3613 - val_accuracy: 0.8169\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13628.4482 - accuracy: 0.8368 - val_loss: 26521.6230 - val_accuracy: 0.8300\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13043.2637 - accuracy: 0.8434 - val_loss: 26780.5156 - val_accuracy: 0.8452\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 13035.7607 - accuracy: 0.8451 - val_loss: 26394.1836 - val_accuracy: 0.8316\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12494.4160 - accuracy: 0.8502 - val_loss: 26990.3398 - val_accuracy: 0.8471\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12360.2197 - accuracy: 0.8528 - val_loss: 26459.5820 - val_accuracy: 0.8458\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 12061.3076 - accuracy: 0.8562 - val_loss: 26829.4785 - val_accuracy: 0.8574\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11619.2266 - accuracy: 0.8588 - val_loss: 26956.2051 - val_accuracy: 0.8704\n",
            "Epoch 20/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 11503.3564 - accuracy: 0.8626roc-auc_val: 0.8163\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 11357.5029 - accuracy: 0.8619 - val_loss: 27143.1133 - val_accuracy: 0.8730\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 11034.9121 - accuracy: 0.8597 - val_loss: 26934.4434 - val_accuracy: 0.8630\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10717.5283 - accuracy: 0.8674 - val_loss: 27288.1211 - val_accuracy: 0.8699\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10650.6318 - accuracy: 0.8667 - val_loss: 27320.6797 - val_accuracy: 0.8629\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10556.5645 - accuracy: 0.8684 - val_loss: 27253.9629 - val_accuracy: 0.8719\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10508.3945 - accuracy: 0.8680 - val_loss: 27411.7031 - val_accuracy: 0.8685\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 10118.1582 - accuracy: 0.8693 - val_loss: 27486.5273 - val_accuracy: 0.8677\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9964.7607 - accuracy: 0.8727 - val_loss: 27849.4395 - val_accuracy: 0.8785\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9590.9873 - accuracy: 0.8761 - val_loss: 27899.1289 - val_accuracy: 0.8764\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9604.3994 - accuracy: 0.8760 - val_loss: 27886.5801 - val_accuracy: 0.8671\n",
            "Epoch 30/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 9486.9033 - accuracy: 0.8726roc-auc_val: 0.8096\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 9560.6611 - accuracy: 0.8728 - val_loss: 27896.9238 - val_accuracy: 0.8707\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9391.9092 - accuracy: 0.8750 - val_loss: 27883.0762 - val_accuracy: 0.8810\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9291.1055 - accuracy: 0.8764 - val_loss: 27534.8867 - val_accuracy: 0.8732\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8973.7217 - accuracy: 0.8812 - val_loss: 27675.2578 - val_accuracy: 0.8733\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8974.0176 - accuracy: 0.8807 - val_loss: 28368.6387 - val_accuracy: 0.8844\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 9045.7744 - accuracy: 0.8803 - val_loss: 27928.0762 - val_accuracy: 0.8749\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8763.2090 - accuracy: 0.8817 - val_loss: 28297.4961 - val_accuracy: 0.8834\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8555.1357 - accuracy: 0.8844 - val_loss: 27758.0059 - val_accuracy: 0.8606\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8632.3281 - accuracy: 0.8819 - val_loss: 28280.6680 - val_accuracy: 0.8849\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8284.5967 - accuracy: 0.8845 - val_loss: 28294.4531 - val_accuracy: 0.8892\n",
            "Epoch 40/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 8264.7871 - accuracy: 0.8818roc-auc_val: 0.8069\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 8211.2588 - accuracy: 0.8819 - val_loss: 28317.2344 - val_accuracy: 0.8765\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8325.4023 - accuracy: 0.8866 - val_loss: 28838.6973 - val_accuracy: 0.8852\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 8197.8037 - accuracy: 0.8857 - val_loss: 28935.4004 - val_accuracy: 0.8872\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7949.3320 - accuracy: 0.8885 - val_loss: 28876.4395 - val_accuracy: 0.8869\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7849.1875 - accuracy: 0.8919 - val_loss: 29127.2754 - val_accuracy: 0.8777\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7936.0898 - accuracy: 0.8858 - val_loss: 28028.2637 - val_accuracy: 0.8538\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7975.6904 - accuracy: 0.8886 - val_loss: 28611.0293 - val_accuracy: 0.8775\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7720.4004 - accuracy: 0.8899 - val_loss: 28812.1543 - val_accuracy: 0.8822\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7638.2168 - accuracy: 0.8909 - val_loss: 28907.9727 - val_accuracy: 0.8767\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7628.0732 - accuracy: 0.8949 - val_loss: 28935.5078 - val_accuracy: 0.8839\n",
            "Epoch 50/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 7515.9468 - accuracy: 0.8926roc-auc_val: 0.8038\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 7548.2036 - accuracy: 0.8920 - val_loss: 28735.4082 - val_accuracy: 0.8664\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7505.4165 - accuracy: 0.8896 - val_loss: 28647.3125 - val_accuracy: 0.8755\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7450.7192 - accuracy: 0.8947 - val_loss: 28895.6680 - val_accuracy: 0.8888\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7412.7388 - accuracy: 0.8938 - val_loss: 28722.1973 - val_accuracy: 0.8865\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7246.4487 - accuracy: 0.8951 - val_loss: 29080.4570 - val_accuracy: 0.8863\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7137.9092 - accuracy: 0.8971 - val_loss: 29017.3457 - val_accuracy: 0.8870\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7126.3452 - accuracy: 0.8948 - val_loss: 28997.8906 - val_accuracy: 0.8656\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6968.4399 - accuracy: 0.8955 - val_loss: 29109.7109 - val_accuracy: 0.8823\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7039.3838 - accuracy: 0.8988 - val_loss: 29514.4414 - val_accuracy: 0.8783\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 7022.5527 - accuracy: 0.8944 - val_loss: 29240.3828 - val_accuracy: 0.8851\n",
            "Epoch 60/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 7120.8862 - accuracy: 0.8995roc-auc_val: 0.8021\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 7088.9702 - accuracy: 0.8990 - val_loss: 29619.1426 - val_accuracy: 0.8964\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6987.1450 - accuracy: 0.8972 - val_loss: 29504.2910 - val_accuracy: 0.8877\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6856.0649 - accuracy: 0.8993 - val_loss: 29428.0371 - val_accuracy: 0.8895\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6635.0493 - accuracy: 0.9025 - val_loss: 29872.2773 - val_accuracy: 0.8933\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6717.6235 - accuracy: 0.8999 - val_loss: 29542.0684 - val_accuracy: 0.8880\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 10ms/step - loss: 6883.2656 - accuracy: 0.8982 - val_loss: 29498.1953 - val_accuracy: 0.8778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5PePWLxWIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24480aa2-606c-4c51-f647-1fc8e4be61a1"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNDmjro-2ndD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
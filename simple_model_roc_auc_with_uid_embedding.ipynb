{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_auc_with_uid_embedding",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_auc_with_uid_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIAeHxBw8EEt",
        "colab_type": "text"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstQrkXM8Bz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IonOu6819IW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "f9241d53-8ef9-4a00-e73d-01aa299e807f"
      },
      "source": [
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "test_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvhOLFro71iv",
        "colab_type": "text"
      },
      "source": [
        "loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0ce85211-c71b-46fe-c45f-1ec2bf7b9e63"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZi5I0Va7-Af",
        "colab_type": "text"
      },
      "source": [
        "Loading dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f4f14370-b952-4926-d2f8-41e4a475e9f0"
      },
      "source": [
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train_id.csv',index_col=[0])\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test_id.csv',index_col=[0])\n",
        "trn.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>C9_std_isna</th>\n",
              "      <th>C10_std_isna</th>\n",
              "      <th>C11_std_isna</th>\n",
              "      <th>C12_std_isna</th>\n",
              "      <th>C13_std_isna</th>\n",
              "      <th>C14_std_isna</th>\n",
              "      <th>D1_mean_isna</th>\n",
              "      <th>D1_std_isna</th>\n",
              "      <th>D2_mean_isna</th>\n",
              "      <th>D2_std_isna</th>\n",
              "      <th>D3_mean_isna</th>\n",
              "      <th>D3_std_isna</th>\n",
              "      <th>D4_mean_isna</th>\n",
              "      <th>D4_std_isna</th>\n",
              "      <th>D5_mean_isna</th>\n",
              "      <th>D5_std_isna</th>\n",
              "      <th>D6_mean_isna</th>\n",
              "      <th>D6_std_isna</th>\n",
              "      <th>D7_mean_isna</th>\n",
              "      <th>D7_std_isna</th>\n",
              "      <th>D8_mean_isna</th>\n",
              "      <th>D8_std_isna</th>\n",
              "      <th>D9_mean_isna</th>\n",
              "      <th>D9_std_isna</th>\n",
              "      <th>D10_mean_isna</th>\n",
              "      <th>D10_std_isna</th>\n",
              "      <th>D11_mean_isna</th>\n",
              "      <th>D11_std_isna</th>\n",
              "      <th>D12_mean_isna</th>\n",
              "      <th>D12_std_isna</th>\n",
              "      <th>D13_mean_isna</th>\n",
              "      <th>D13_std_isna</th>\n",
              "      <th>D14_mean_isna</th>\n",
              "      <th>D14_std_isna</th>\n",
              "      <th>D15_mean_isna</th>\n",
              "      <th>D15_std_isna</th>\n",
              "      <th>V1_mean_isna</th>\n",
              "      <th>V1_std_isna</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wnan315.013926-13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wgmail.com325.027551.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Woutlook.com330.046631.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Wyahoo.com476.018132-111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Hgmail.com420.044971.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 619 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2  ...  V1_std_isna  isFraud                          id\n",
              "0  1.0  0.0  0.0  ...            1        0         Wnan315.013926-13.0\n",
              "1  1.0  0.0  0.0  ...            1        0      Wgmail.com325.027551.0\n",
              "2  1.0  0.0  0.0  ...            0        0    Woutlook.com330.046631.0\n",
              "3  1.0  0.0  0.0  ...            1        0  Wyahoo.com476.018132-111.0\n",
              "4  0.0  0.0  0.0  ...            1        0      Hgmail.com420.044971.0\n",
              "\n",
              "[5 rows x 619 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LEIUFVW8iAC",
        "colab_type": "text"
      },
      "source": [
        "Reduce memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "70e08a35-24e7-4c33-98d8-79316f086903"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2793.39 MB\n",
            "Memory usage after optimization is: 678.37 MB\n",
            "Decreased by 75.7%\n",
            "Memory usage of dataframe is 2392.90 MB\n",
            "Memory usage after optimization is: 580.09 MB\n",
            "Decreased by 75.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjn9ePhArDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.replace([np.inf,-np.inf],np.nan)\n",
        "tst=tst.replace([np.inf,-np.inf],np.nan)\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqrD6vz8ol6",
        "colab_type": "text"
      },
      "source": [
        "Making the callbacks and loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dk={}\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data,epochs):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "        self.epochs=epochs\n",
        "        self.val=0\n",
        "        self.wts=[]\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "        if roc_val>self.val:\n",
        "          self.val=roc_val\n",
        "          self.epoch=10\n",
        "          self.wts=self.model.get_weights()\n",
        "        else:\n",
        "          self.epoch-=1\n",
        "        if self.epoch==0:\n",
        "          self.model.set_weights(self.wts)\n",
        "          self.model.stop_training = True\n",
        "        print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "def load_model(dim):\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "  uid=Input((1,))\n",
        "  inp=Input((873,))\n",
        "  emb=Embedding(input_dim=dim,output_dim=4)(uid)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  emb=Flatten()(emb)\n",
        "  x=Concatenate()([emb,x])\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=[inp,uid],outputs=x)\n",
        "  return mod\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZH6xEokzPfj",
        "colab_type": "text"
      },
      "source": [
        "Adding all datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSZw0LXIzPG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "2ab89b61-fdb5-41fe-a7a7-5df35d07e00f"
      },
      "source": [
        "trn_s=trn.shape[0]\n",
        "df=pd.concat([trn,tst],0).reset_index(drop=True)\n",
        "del([trn,tst])\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "autoenc=pd.read_csv('/content/gdrive/My Drive/fraud/without_id.csv',index_col=[0])\n",
        "\n",
        "autoenc.columns=[i for i in range(444,444+autoenc.shape[1])]\n",
        "\n",
        "\n",
        "df=pd.concat([df,autoenc],1)\n",
        "df=reduce_mem_usage(df)\n",
        "del([autoenc])\n",
        "gc.collect()\n",
        "\n",
        "trn=df.loc[:trn_s-1]\n",
        "tst=df.loc[trn_s:].reset_index(drop=True)\n",
        "del([df])\n",
        "gc.collect()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 3384.06 MB\n",
            "Memory usage after optimization is: 1783.79 MB\n",
            "Decreased by 47.3%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxmq8JSOz6Hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical=[str(i) for i in range(444)]\n",
        "trn[categorical]=trn[categorical].astype('uint8')\n",
        "tst[categorical]=tst[categorical].astype('uint8')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVXGvLePKR8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 1.5\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oor5OujA6Bz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37fd45d5-c4a0-4b39-c887-812a82d91bb6"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "splits=KFold(n_splits=5)\n",
        "gc.collect()\n",
        "pre=np.zeros((506691,1))\n",
        "tst=tst.drop(['isFraud'],1)\n",
        "for train_index,test_index in tqdm(splits.split(trn)):\n",
        "  X_train, X_test = trn.loc[train_index], trn.loc[test_index]\n",
        "  y_train, y_test = X_train['isFraud'], X_test['isFraud']\n",
        "  ids={}\n",
        "  for en,id in enumerate(X_train['id'].unique()):\n",
        "    ids[id]=en+2\n",
        "  X_train['id']=X_train['id'].map(lambda x: ids.get(x,1))\n",
        "  X_test['id']=X_test['id'].map(lambda x: ids.get(x,1))\n",
        "  dim=X_train['id'].nunique()+2\n",
        "  gc.collect()\n",
        "  trn_id,tst_id=X_train['id'],X_test['id']\n",
        "  X_train=X_train.drop(['isFraud','id'],1)\n",
        "  X_test=X_test.drop(['isFraud','id'],1)\n",
        "  mod=load_model(dim)\n",
        "  roc = RocCallback(validation_data=([X_test,tst_id], y_test),epochs=10)\n",
        "  mod.compile(optimizer=Nadam(),loss=rac)\n",
        "  es=EarlyStopping(monitor='acu_val',min_delta=0.0001,mode='min',restore_best_weights=True,patience=10)\n",
        "  mod.fit([X_train,trn_id],y_train,validation_data=([X_test,tst_id],y_test),batch_size=2048,epochs=50,callbacks=[roc])\n",
        "  \n",
        "  del[(X_train,y_train)]\n",
        "  gc.collect()\n",
        "\n",
        "  mod.fit([X_test,tst_id],y_test,epochs=2,batch_size=2048)\n",
        "  pre+=mod.predict([tst.drop(['id'],1),tst['id'].map(lambda x: ids.get(x,1))])/5\n",
        "  \n",
        "  del([X_test,y_test,mod])\n",
        "  gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 9188.3018roc-auc_val: 0.8785\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 9171.8438 - val_loss: 5371.7061\n",
            "Epoch 2/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 5563.4214roc-auc_val: 0.8892\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 5545.7339 - val_loss: 4975.9111\n",
            "Epoch 3/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 3627.0779roc-auc_val: 0.909\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 3620.4968 - val_loss: 4269.7017\n",
            "Epoch 4/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 2159.5388roc-auc_val: 0.9152\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 2159.5388 - val_loss: 4137.7759\n",
            "Epoch 5/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1258.5410roc-auc_val: 0.9205\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 1253.8771 - val_loss: 3837.1018\n",
            "Epoch 6/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 736.1911roc-auc_val: 0.9249\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 736.1911 - val_loss: 3846.2046\n",
            "Epoch 7/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 436.6703roc-auc_val: 0.923\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 434.4137 - val_loss: 4094.4812\n",
            "Epoch 8/50\n",
            "226/231 [============================>.] - ETA: 0s - loss: 280.6486roc-auc_val: 0.9237\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 280.1876 - val_loss: 4052.9976\n",
            "Epoch 9/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 201.2558roc-auc_val: 0.9242\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 200.5694 - val_loss: 4136.1401\n",
            "Epoch 10/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 151.1499roc-auc_val: 0.922\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 151.0038 - val_loss: 4279.8643\n",
            "Epoch 11/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 117.7794roc-auc_val: 0.9241\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 117.9821 - val_loss: 4051.3652\n",
            "Epoch 12/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 97.0187roc-auc_val: 0.9207\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 96.8977 - val_loss: 4357.7510\n",
            "Epoch 13/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 83.9238roc-auc_val: 0.924\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 83.7758 - val_loss: 4011.9231\n",
            "Epoch 14/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 76.2442roc-auc_val: 0.9248\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 76.0416 - val_loss: 3879.7319\n",
            "Epoch 15/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 66.5464roc-auc_val: 0.9246\n",
            "231/231 [==============================] - 7s 29ms/step - loss: 66.3329 - val_loss: 3867.7429\n",
            "Epoch 16/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 63.2932roc-auc_val: 0.9255\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 63.2977 - val_loss: 3837.3984\n",
            "Epoch 17/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 55.8706roc-auc_val: 0.9209\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 55.8706 - val_loss: 4028.8132\n",
            "Epoch 18/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 53.3904roc-auc_val: 0.9226\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 53.2973 - val_loss: 3923.1782\n",
            "Epoch 19/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 48.1522roc-auc_val: 0.9239\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 48.1900 - val_loss: 3833.3784\n",
            "Epoch 20/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 45.8450roc-auc_val: 0.921\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 45.8450 - val_loss: 3878.8989\n",
            "Epoch 21/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 43.2628roc-auc_val: 0.9249\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 43.3866 - val_loss: 3731.7769\n",
            "Epoch 22/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 43.2349roc-auc_val: 0.9231\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 43.4143 - val_loss: 3799.1479\n",
            "Epoch 23/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 42.0879roc-auc_val: 0.9223\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 41.9964 - val_loss: 3792.7275\n",
            "Epoch 24/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 38.0149roc-auc_val: 0.9241\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 38.0383 - val_loss: 3673.1577\n",
            "Epoch 25/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 36.1216roc-auc_val: 0.9233\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 36.0656 - val_loss: 3737.5869\n",
            "Epoch 26/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 32.5988roc-auc_val: 0.9272\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 32.5186 - val_loss: 3521.9924\n",
            "Epoch 27/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 31.7878roc-auc_val: 0.9249\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 32.1091 - val_loss: 3590.2180\n",
            "Epoch 28/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 28.5437roc-auc_val: 0.927\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 28.4666 - val_loss: 3572.0647\n",
            "Epoch 29/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 27.8652roc-auc_val: 0.9187\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 27.9357 - val_loss: 3805.6753\n",
            "Epoch 30/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 30.9419roc-auc_val: 0.9213\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 30.8713 - val_loss: 3697.8384\n",
            "Epoch 31/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 29.0307roc-auc_val: 0.9223\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 28.9867 - val_loss: 3615.0107\n",
            "Epoch 32/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 28.6753roc-auc_val: 0.9167\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 28.6682 - val_loss: 3839.7734\n",
            "Epoch 33/50\n",
            "226/231 [============================>.] - ETA: 0s - loss: 25.7237roc-auc_val: 0.9229\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 25.9384 - val_loss: 3548.1750\n",
            "Epoch 34/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 24.8031roc-auc_val: 0.9233\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 24.8430 - val_loss: 3546.8469\n",
            "Epoch 35/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 22.3241roc-auc_val: 0.9233\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 22.2068 - val_loss: 3584.4128\n",
            "Epoch 36/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 22.7643roc-auc_val: 0.9218\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 22.7643 - val_loss: 3643.8884\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 1s 9ms/step - loss: 3169.8281\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2159.6011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r1it [04:24, 264.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 8314.8721roc-auc_val: 0.8998\n",
            "231/231 [==============================] - 7s 32ms/step - loss: 8314.8721 - val_loss: 7328.0669\n",
            "Epoch 2/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 5342.9985roc-auc_val: 0.9161\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 5329.7241 - val_loss: 6178.5898\n",
            "Epoch 3/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 3692.0200roc-auc_val: 0.9248\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 3692.0200 - val_loss: 5824.5498\n",
            "Epoch 4/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 2292.1819roc-auc_val: 0.9364\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 2292.1819 - val_loss: 5318.1074\n",
            "Epoch 5/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 1248.6005roc-auc_val: 0.947\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 1246.1920 - val_loss: 5162.6963\n",
            "Epoch 6/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 678.7776roc-auc_val: 0.948\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 678.7776 - val_loss: 5333.7661\n",
            "Epoch 7/50\n",
            "226/231 [============================>.] - ETA: 0s - loss: 414.9159roc-auc_val: 0.9509\n",
            "231/231 [==============================] - 7s 29ms/step - loss: 411.6350 - val_loss: 5212.4072\n",
            "Epoch 8/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 264.9793roc-auc_val: 0.9501\n",
            "231/231 [==============================] - 7s 31ms/step - loss: 264.9793 - val_loss: 5321.5791\n",
            "Epoch 9/50\n",
            "226/231 [============================>.] - ETA: 0s - loss: 182.9878roc-auc_val: 0.9498\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 181.8236 - val_loss: 5548.9160\n",
            "Epoch 10/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 139.2640roc-auc_val: 0.9513\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 139.0987 - val_loss: 5374.2144\n",
            "Epoch 11/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 105.8536roc-auc_val: 0.9511\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 105.8587 - val_loss: 5247.9155\n",
            "Epoch 12/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 91.2348roc-auc_val: 0.9512\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 91.1660 - val_loss: 5251.8252\n",
            "Epoch 13/50\n",
            "226/231 [============================>.] - ETA: 0s - loss: 82.7967roc-auc_val: 0.9489\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 82.3545 - val_loss: 5260.8662\n",
            "Epoch 14/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 69.5349roc-auc_val: 0.9499\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 69.2048 - val_loss: 5291.7847\n",
            "Epoch 15/50\n",
            "226/231 [============================>.] - ETA: 0s - loss: 63.1144roc-auc_val: 0.95\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 62.8350 - val_loss: 5217.7671\n",
            "Epoch 16/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 57.1776roc-auc_val: 0.9508\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 56.9807 - val_loss: 5010.6055\n",
            "Epoch 17/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 51.1030roc-auc_val: 0.9498\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 50.9133 - val_loss: 5091.8423\n",
            "Epoch 18/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 48.4160roc-auc_val: 0.9494\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 48.3621 - val_loss: 4987.1968\n",
            "Epoch 19/50\n",
            "226/231 [============================>.] - ETA: 0s - loss: 44.0748roc-auc_val: 0.9493\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 44.1832 - val_loss: 5025.2651\n",
            "Epoch 20/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 40.9962roc-auc_val: 0.9498\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 40.9149 - val_loss: 4894.9937\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 9ms/step - loss: 3607.7615\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2572.9290\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r2it [07:00, 232.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 8623.9932roc-auc_val: 0.8923\n",
            "231/231 [==============================] - 7s 31ms/step - loss: 8605.7744 - val_loss: 7268.0322\n",
            "Epoch 2/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 5429.7598roc-auc_val: 0.9108\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 5429.7598 - val_loss: 6121.9492\n",
            "Epoch 3/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 3769.1711roc-auc_val: 0.9248\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 3754.3826 - val_loss: 5472.7227\n",
            "Epoch 4/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 2345.6140roc-auc_val: 0.94\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 2345.6140 - val_loss: 4636.3447\n",
            "Epoch 5/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1421.7640roc-auc_val: 0.9451\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 1414.8931 - val_loss: 4950.9414\n",
            "Epoch 6/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 771.7285roc-auc_val: 0.9474\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 771.7285 - val_loss: 4690.4346\n",
            "Epoch 7/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 458.9143roc-auc_val: 0.9491\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 457.3722 - val_loss: 4673.4819\n",
            "Epoch 8/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 296.4108roc-auc_val: 0.9489\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 296.5229 - val_loss: 4867.6108\n",
            "Epoch 9/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 203.5611roc-auc_val: 0.9492\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 203.0564 - val_loss: 4916.6143\n",
            "Epoch 10/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 152.0080roc-auc_val: 0.949\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 152.0080 - val_loss: 4786.2168\n",
            "Epoch 11/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 116.9274roc-auc_val: 0.9497\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 116.9274 - val_loss: 4783.2456\n",
            "Epoch 12/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 97.7379roc-auc_val: 0.9492\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 97.5497 - val_loss: 4780.1440\n",
            "Epoch 13/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 83.2581roc-auc_val: 0.9502\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 82.8554 - val_loss: 4606.1157\n",
            "Epoch 14/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 74.8324roc-auc_val: 0.95\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 74.8324 - val_loss: 4596.9087\n",
            "Epoch 15/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 67.1269roc-auc_val: 0.949\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 67.0365 - val_loss: 4665.0996\n",
            "Epoch 16/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 60.5335roc-auc_val: 0.9499\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 60.5335 - val_loss: 4471.7354\n",
            "Epoch 17/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 55.8273roc-auc_val: 0.9484\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 56.0518 - val_loss: 4526.8135\n",
            "Epoch 18/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 51.3686roc-auc_val: 0.9492\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 51.9431 - val_loss: 4312.1289\n",
            "Epoch 19/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 48.3629roc-auc_val: 0.9489\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 48.2704 - val_loss: 4479.2041\n",
            "Epoch 20/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 44.4060roc-auc_val: 0.9489\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 44.1589 - val_loss: 4325.7344\n",
            "Epoch 21/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 40.3869roc-auc_val: 0.9488\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 40.5246 - val_loss: 4232.5200\n",
            "Epoch 22/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 41.4406roc-auc_val: 0.9491\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 41.3197 - val_loss: 4227.0801\n",
            "Epoch 23/50\n",
            "226/231 [============================>.] - ETA: 0s - loss: 37.3081roc-auc_val: 0.9491\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 37.1399 - val_loss: 4199.3872\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 3576.9807\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2415.5691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r3it [09:52, 214.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 8339.4102roc-auc_val: 0.9088\n",
            "231/231 [==============================] - 7s 31ms/step - loss: 8339.4102 - val_loss: 6667.9365\n",
            "Epoch 2/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 5447.9438roc-auc_val: 0.9308\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 5447.9438 - val_loss: 5212.0439\n",
            "Epoch 3/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 3852.8379roc-auc_val: 0.9422\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 3839.8955 - val_loss: 4644.8994\n",
            "Epoch 4/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 2328.0874roc-auc_val: 0.9545\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 2315.8804 - val_loss: 4178.3701\n",
            "Epoch 5/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 1264.1403roc-auc_val: 0.9595\n",
            "231/231 [==============================] - 7s 30ms/step - loss: 1264.1403 - val_loss: 4239.8506\n",
            "Epoch 6/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 703.9700roc-auc_val: 0.9616\n",
            "231/231 [==============================] - 7s 30ms/step - loss: 702.9097 - val_loss: 4268.2476\n",
            "Epoch 7/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 425.1203roc-auc_val: 0.9622\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 423.9097 - val_loss: 4277.5112\n",
            "Epoch 8/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 263.3810roc-auc_val: 0.9617\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 264.4566 - val_loss: 4465.0469\n",
            "Epoch 9/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 190.7801roc-auc_val: 0.9611\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 189.4582 - val_loss: 4351.8306\n",
            "Epoch 10/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 134.3548roc-auc_val: 0.9611\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 134.3548 - val_loss: 4454.8271\n",
            "Epoch 11/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 108.5266roc-auc_val: 0.9614\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 108.3627 - val_loss: 4283.1792\n",
            "Epoch 12/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 89.4355roc-auc_val: 0.9616\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 89.3919 - val_loss: 4347.9624\n",
            "Epoch 13/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 79.5100roc-auc_val: 0.9621\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 79.5100 - val_loss: 4168.4907\n",
            "Epoch 14/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 68.0526roc-auc_val: 0.9614\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 67.8468 - val_loss: 4220.5122\n",
            "Epoch 15/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 59.9911roc-auc_val: 0.9609\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 59.9313 - val_loss: 4009.1445\n",
            "Epoch 16/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 54.4116roc-auc_val: 0.9601\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 54.2100 - val_loss: 4148.6968\n",
            "Epoch 17/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 50.6706roc-auc_val: 0.9612\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 50.6881 - val_loss: 3959.2964\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 9ms/step - loss: 3259.2288\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2290.3484\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r4it [12:08, 190.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 8585.0938roc-auc_val: 0.8863\n",
            "231/231 [==============================] - 7s 31ms/step - loss: 8565.0176 - val_loss: 7253.7183\n",
            "Epoch 2/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 5363.0269roc-auc_val: 0.9016\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 5348.4043 - val_loss: 6220.5879\n",
            "Epoch 3/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 3610.2737roc-auc_val: 0.9087\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 3610.2737 - val_loss: 5856.5693\n",
            "Epoch 4/50\n",
            "229/231 [============================>.] - ETA: 0s - loss: 2334.6807roc-auc_val: 0.9265\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 2325.4087 - val_loss: 4952.2632\n",
            "Epoch 5/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 1312.2783roc-auc_val: 0.9339\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 1306.3722 - val_loss: 4966.5557\n",
            "Epoch 6/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 743.9164roc-auc_val: 0.9379\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 740.1691 - val_loss: 5062.3379\n",
            "Epoch 7/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 448.9109roc-auc_val: 0.9395\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 448.9109 - val_loss: 4808.1592\n",
            "Epoch 8/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 290.7753roc-auc_val: 0.9407\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 289.8623 - val_loss: 4919.4419\n",
            "Epoch 9/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 205.6248roc-auc_val: 0.9421\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 204.8920 - val_loss: 4865.6274\n",
            "Epoch 10/50\n",
            "228/231 [============================>.] - ETA: 0s - loss: 153.0769roc-auc_val: 0.9398\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 152.7965 - val_loss: 5076.3730\n",
            "Epoch 11/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 120.4925roc-auc_val: 0.9393\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 120.4925 - val_loss: 5066.5728\n",
            "Epoch 12/50\n",
            "226/231 [============================>.] - ETA: 0s - loss: 100.7697roc-auc_val: 0.94\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 100.2884 - val_loss: 5014.5503\n",
            "Epoch 13/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 85.8672roc-auc_val: 0.9403\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 85.6955 - val_loss: 4977.9009\n",
            "Epoch 14/50\n",
            "227/231 [============================>.] - ETA: 0s - loss: 73.3296roc-auc_val: 0.9398\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 73.1381 - val_loss: 4973.5630\n",
            "Epoch 15/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 71.6598roc-auc_val: 0.94\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 71.6598 - val_loss: 4917.3140\n",
            "Epoch 16/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 60.6208roc-auc_val: 0.9408\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 60.6208 - val_loss: 4633.1045\n",
            "Epoch 17/50\n",
            "230/231 [============================>.] - ETA: 0s - loss: 56.3955roc-auc_val: 0.94\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 56.3031 - val_loss: 4856.6904\n",
            "Epoch 18/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 53.9111roc-auc_val: 0.9404\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 53.9111 - val_loss: 4770.6084\n",
            "Epoch 19/50\n",
            "231/231 [==============================] - ETA: 0s - loss: 50.8262roc-auc_val: 0.9411\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 50.8262 - val_loss: 4582.3584\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 9ms/step - loss: 3878.5134\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 8ms/step - loss: 2798.0566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5it [14:34, 174.87s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVYgkVd5_NVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "8839b875-a916-4e66-e6fc-70d27e945587"
      },
      "source": [
        "sub=pd.read_csv('sample_submission.csv.zip')\n",
        "sub['isFraud']=pre\n",
        "sub=sub.set_index('TransactionID')\n",
        "sub.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TransactionID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3663549</th>\n",
              "      <td>0.052569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663550</th>\n",
              "      <td>0.101970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663551</th>\n",
              "      <td>0.261123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663552</th>\n",
              "      <td>0.153851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663553</th>\n",
              "      <td>0.128886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                isFraud\n",
              "TransactionID          \n",
              "3663549        0.052569\n",
              "3663550        0.101970\n",
              "3663551        0.261123\n",
              "3663552        0.153851\n",
              "3663553        0.128886"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZlw01oHayo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub.to_csv('sub.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FeqwiR2HcSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "da125586-d96a-4f00-f9de-76d051927053"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(pre)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc43ba55828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAeWMkwJIWng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}
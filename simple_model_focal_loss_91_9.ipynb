{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_91.9",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_91_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIAeHxBw8EEt",
        "colab_type": "text"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstQrkXM8Bz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IonOu6819IW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "0eb70aec-ba46-482b-90dc-9936a87be46f"
      },
      "source": [
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 76.0MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 106MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 98% 57.0M/58.3M [00:02<00:00, 17.5MB/s]\n",
            "100% 58.3M/58.3M [00:02<00:00, 24.3MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 30.1MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 94% 49.0M/52.2M [00:02<00:00, 18.2MB/s]\n",
            "100% 52.2M/52.2M [00:02<00:00, 24.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvhOLFro71iv",
        "colab_type": "text"
      },
      "source": [
        "loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "f596e011-f123-4142-c58a-de195f44a75f"
      },
      "source": [
        "\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZi5I0Va7-Af",
        "colab_type": "text"
      },
      "source": [
        "Loading dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "75460f37-708e-4824-b9cf-47acc297ef26"
      },
      "source": [
        "\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv',index_col=[0])\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv',index_col=[0])\n",
        "trn.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>V1</th>\n",
              "      <th>isna_sum</th>\n",
              "      <th>dist1_isna</th>\n",
              "      <th>dist2_isna</th>\n",
              "      <th>D1_isna</th>\n",
              "      <th>D2_isna</th>\n",
              "      <th>D3_isna</th>\n",
              "      <th>D4_isna</th>\n",
              "      <th>D5_isna</th>\n",
              "      <th>D6_isna</th>\n",
              "      <th>D7_isna</th>\n",
              "      <th>D8_isna</th>\n",
              "      <th>D9_isna</th>\n",
              "      <th>D10_isna</th>\n",
              "      <th>D11_isna</th>\n",
              "      <th>D12_isna</th>\n",
              "      <th>D13_isna</th>\n",
              "      <th>D14_isna</th>\n",
              "      <th>D15_isna</th>\n",
              "      <th>V1_isna</th>\n",
              "      <th>id</th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>-0.752856</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>0.155508</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312997</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.012922</td>\n",
              "      <td>-0.737091</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.807711</td>\n",
              "      <td>0.00739</td>\n",
              "      <td>-0.167776</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.01.0nan315.013926-13.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>-0.752856</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>-0.942090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.038619</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.807711</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.349188</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.01.0gmail.com325.027551.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>-0.752856</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>-0.942090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.038619</td>\n",
              "      <td>1.132473</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.746105</td>\n",
              "      <td>0.00739</td>\n",
              "      <td>0.211465</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.01.0outlook.com330.046631.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>1.013560</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>0.973965</td>\n",
              "      <td>0.292248</td>\n",
              "      <td>-1.335951</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.301712</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.688064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.260176</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.211465</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.025.0yahoo.com476.018132-111.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>-0.752856</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>-0.942090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.107669</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.01.0gmail.com420.044971.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 499 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2  ...  V1_isna                                id  isFraud\n",
              "0  1.0  0.0  0.0  ...        0          1.01.0nan315.013926-13.0        0\n",
              "1  1.0  0.0  0.0  ...        1       1.01.0gmail.com325.027551.0        0\n",
              "2  1.0  0.0  0.0  ...        0     1.01.0outlook.com330.046631.0        0\n",
              "3  1.0  0.0  0.0  ...        1  2.025.0yahoo.com476.018132-111.0        0\n",
              "4  0.0  0.0  0.0  ...        1       1.01.0gmail.com420.044971.0        0\n",
              "\n",
              "[5 rows x 499 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LEIUFVW8iAC",
        "colab_type": "text"
      },
      "source": [
        "Reduce memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "2564da1c-f7ae-4f7f-8c98-c125ea5ed2ca"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2252.73 MB\n",
            "Memory usage after optimization is: 580.70 MB\n",
            "Decreased by 74.2%\n",
            "Memory usage of dataframe is 1929.01 MB\n",
            "Memory usage after optimization is: 500.58 MB\n",
            "Decreased by 74.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjn9ePhArDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.replace([np.inf,-np.inf],np.nan)\n",
        "tst=tst.replace([np.inf,-np.inf],np.nan)\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqrD6vz8ol6",
        "colab_type": "text"
      },
      "source": [
        "Making the callbacks and loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dk={}\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "        print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "\n",
        "  inp=Input((497,))\n",
        "  x=Dense(256,activation=custom_gelu)(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation=custom_gelu)(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation=custom_gelu)(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=[inp],outputs=x)\n",
        "  return mod\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXKSLj3p5MY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "splits=KFold(n_splits=5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W07F3czB7b3v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "52590da1-ed64-4689-f014-be7a487376d9"
      },
      "source": [
        "trn=trn.drop(['id'],1)\n",
        "tst=tst.drop(['id'],1)\n",
        "gc.collect()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvn5vEDaDt64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=4\n",
        "        alpha=0.25\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oor5OujA6Bz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d69f60b4-5a3b-4582-a3c4-26cc6e72be83"
      },
      "source": [
        "ln=len(trn)/10\n",
        "for i in range(6,10):\n",
        "  X_train, X_test = trn.loc[:int(ln*i)], trn.loc[int(ln*i):]\n",
        "  y_train, y_test = X_train['isFraud'], X_test['isFraud']\n",
        "  X_train=X_train.drop(['isFraud'],1)\n",
        "  X_test=X_test.drop(['isFraud'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(X_test, y_test))\n",
        "  mod.compile(optimizer=Nadam(),loss=fl())\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=2048,epochs=8,callbacks=[es,roc])\n",
        "  gc.collect()\n",
        "  mod.fit(X_test,y_test,epochs=2,batch_size=2048)\n",
        "  if i ==6:\n",
        "    pre=mod.predict(tst)/5\n",
        "  else:\n",
        "    pre+=mod.predict(tst)/5\n",
        "  del([X_train,X_test,y_train,y_test])\n",
        "  gc.collect()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "166/174 [===========================>..] - ETA: 0s - loss: 14.0604roc-auc_val: 0.8584\n",
            "174/174 [==============================] - 7s 40ms/step - loss: 13.8688 - val_loss: 23.4458\n",
            "Epoch 2/8\n",
            "170/174 [============================>.] - ETA: 0s - loss: 7.4274roc-auc_val: 0.8803\n",
            "174/174 [==============================] - 6s 36ms/step - loss: 7.4207 - val_loss: 9.6608\n",
            "Epoch 3/8\n",
            "166/174 [===========================>..] - ETA: 0s - loss: 6.3434roc-auc_val: 0.8817\n",
            "174/174 [==============================] - 6s 37ms/step - loss: 6.3117 - val_loss: 7.4328\n",
            "Epoch 4/8\n",
            "165/174 [===========================>..] - ETA: 0s - loss: 5.7297roc-auc_val: 0.8896\n",
            "174/174 [==============================] - 6s 37ms/step - loss: 5.7414 - val_loss: 6.4822\n",
            "Epoch 5/8\n",
            "169/174 [============================>.] - ETA: 0s - loss: 5.4266roc-auc_val: 0.8925\n",
            "174/174 [==============================] - 6s 37ms/step - loss: 5.4238 - val_loss: 6.3923\n",
            "Epoch 6/8\n",
            "166/174 [===========================>..] - ETA: 0s - loss: 5.1380roc-auc_val: 0.8923\n",
            "174/174 [==============================] - 6s 37ms/step - loss: 5.1228 - val_loss: 6.3424\n",
            "Epoch 7/8\n",
            "169/174 [============================>.] - ETA: 0s - loss: 4.9064roc-auc_val: 0.8862\n",
            "174/174 [==============================] - 6s 36ms/step - loss: 4.8916 - val_loss: 6.3955\n",
            "Epoch 8/8\n",
            "169/174 [============================>.] - ETA: 0s - loss: 4.6862roc-auc_val: 0.8884\n",
            "174/174 [==============================] - 6s 36ms/step - loss: 4.6816 - val_loss: 6.6436\n",
            "Epoch 1/2\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 6.0645\n",
            "Epoch 2/2\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 5.3308\n",
            "Epoch 1/8\n",
            "193/202 [===========================>..] - ETA: 0s - loss: 20.6360roc-auc_val: 0.8517\n",
            "202/202 [==============================] - 6s 28ms/step - loss: 20.2101 - val_loss: 15.0044\n",
            "Epoch 2/8\n",
            "193/202 [===========================>..] - ETA: 0s - loss: 9.1301roc-auc_val: 0.8646\n",
            "202/202 [==============================] - 5s 25ms/step - loss: 9.0639 - val_loss: 7.0854\n",
            "Epoch 3/8\n",
            "192/202 [===========================>..] - ETA: 0s - loss: 7.3237roc-auc_val: 0.8776\n",
            "202/202 [==============================] - 5s 25ms/step - loss: 7.3015 - val_loss: 6.6205\n",
            "Epoch 4/8\n",
            "193/202 [===========================>..] - ETA: 0s - loss: 6.5901roc-auc_val: 0.8845\n",
            "202/202 [==============================] - 5s 25ms/step - loss: 6.5482 - val_loss: 6.3634\n",
            "Epoch 5/8\n",
            "193/202 [===========================>..] - ETA: 0s - loss: 6.0791roc-auc_val: 0.8934\n",
            "202/202 [==============================] - 5s 25ms/step - loss: 6.0813 - val_loss: 6.0084\n",
            "Epoch 6/8\n",
            "193/202 [===========================>..] - ETA: 0s - loss: 5.6656roc-auc_val: 0.891\n",
            "202/202 [==============================] - 5s 25ms/step - loss: 5.6748 - val_loss: 6.1065\n",
            "Epoch 7/8\n",
            "196/202 [============================>.] - ETA: 0s - loss: 5.4635roc-auc_val: 0.895\n",
            "202/202 [==============================] - 5s 25ms/step - loss: 5.4572 - val_loss: 6.0206\n",
            "Epoch 8/8\n",
            "194/202 [===========================>..] - ETA: 0s - loss: 5.2101roc-auc_val: 0.8952\n",
            "202/202 [==============================] - 5s 25ms/step - loss: 5.2162 - val_loss: 5.9961\n",
            "Epoch 1/2\n",
            "87/87 [==============================] - 0s 4ms/step - loss: 6.2047\n",
            "Epoch 2/2\n",
            "87/87 [==============================] - 0s 4ms/step - loss: 5.4910\n",
            "Epoch 1/8\n",
            "231/231 [==============================] - ETA: 0s - loss: 12.8712roc-auc_val: 0.8597\n",
            "231/231 [==============================] - 4s 18ms/step - loss: 12.8712 - val_loss: 15.6126\n",
            "Epoch 2/8\n",
            "219/231 [===========================>..] - ETA: 0s - loss: 7.1142roc-auc_val: 0.8798\n",
            "231/231 [==============================] - 4s 17ms/step - loss: 7.0534 - val_loss: 7.0007\n",
            "Epoch 3/8\n",
            "229/231 [============================>.] - ETA: 0s - loss: 6.1011roc-auc_val: 0.8936\n",
            "231/231 [==============================] - 4s 17ms/step - loss: 6.0997 - val_loss: 6.1005\n",
            "Epoch 4/8\n",
            "227/231 [============================>.] - ETA: 0s - loss: 5.6735roc-auc_val: 0.8962\n",
            "231/231 [==============================] - 4s 17ms/step - loss: 5.6730 - val_loss: 6.0274\n",
            "Epoch 5/8\n",
            "228/231 [============================>.] - ETA: 0s - loss: 5.3842roc-auc_val: 0.8992\n",
            "231/231 [==============================] - 4s 17ms/step - loss: 5.3878 - val_loss: 6.0443\n",
            "Epoch 6/8\n",
            "226/231 [============================>.] - ETA: 0s - loss: 5.0825roc-auc_val: 0.9019\n",
            "231/231 [==============================] - 4s 17ms/step - loss: 5.0873 - val_loss: 5.9673\n",
            "Epoch 7/8\n",
            "231/231 [==============================] - ETA: 0s - loss: 4.8800roc-auc_val: 0.9044\n",
            "231/231 [==============================] - 4s 17ms/step - loss: 4.8800 - val_loss: 5.7965\n",
            "Epoch 8/8\n",
            "231/231 [==============================] - ETA: 0s - loss: 4.6496roc-auc_val: 0.9045\n",
            "231/231 [==============================] - 4s 17ms/step - loss: 4.6496 - val_loss: 5.8646\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 4ms/step - loss: 5.7085\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 4ms/step - loss: 4.9224\n",
            "Epoch 1/8\n",
            "252/260 [============================>.] - ETA: 0s - loss: 14.4377roc-auc_val: 0.8732\n",
            "260/260 [==============================] - 3s 11ms/step - loss: 14.2364 - val_loss: 10.9477\n",
            "Epoch 2/8\n",
            "250/260 [===========================>..] - ETA: 0s - loss: 7.4169roc-auc_val: 0.8949\n",
            "260/260 [==============================] - 3s 10ms/step - loss: 7.4041 - val_loss: 6.5425\n",
            "Epoch 3/8\n",
            "255/260 [============================>.] - ETA: 0s - loss: 6.3305roc-auc_val: 0.8967\n",
            "260/260 [==============================] - 3s 10ms/step - loss: 6.3214 - val_loss: 6.2775\n",
            "Epoch 4/8\n",
            "256/260 [============================>.] - ETA: 0s - loss: 5.7835roc-auc_val: 0.9004\n",
            "260/260 [==============================] - 3s 10ms/step - loss: 5.7701 - val_loss: 6.2131\n",
            "Epoch 5/8\n",
            "254/260 [============================>.] - ETA: 0s - loss: 5.3922roc-auc_val: 0.906\n",
            "260/260 [==============================] - 3s 10ms/step - loss: 5.3964 - val_loss: 6.1260\n",
            "Epoch 6/8\n",
            "259/260 [============================>.] - ETA: 0s - loss: 5.2182roc-auc_val: 0.9046\n",
            "260/260 [==============================] - 3s 11ms/step - loss: 5.2120 - val_loss: 6.1100\n",
            "Epoch 7/8\n",
            "259/260 [============================>.] - ETA: 0s - loss: 4.9606roc-auc_val: 0.911\n",
            "260/260 [==============================] - 4s 13ms/step - loss: 4.9558 - val_loss: 5.9595\n",
            "Epoch 8/8\n",
            "254/260 [============================>.] - ETA: 0s - loss: 4.7358roc-auc_val: 0.9065\n",
            "260/260 [==============================] - 3s 11ms/step - loss: 4.7247 - val_loss: 6.1270\n",
            "Epoch 1/2\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 6.1029\n",
            "Epoch 2/2\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 5.3078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVYgkVd5_NVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "877a2a9b-332b-4de1-f31a-dd286457b19a"
      },
      "source": [
        "sub=pd.read_csv('sample_submission.csv.zip')\n",
        "sub['isFraud']=pre\n",
        "sub=sub.set_index('TransactionID')\n",
        "sub.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TransactionID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3663549</th>\n",
              "      <td>0.145030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663550</th>\n",
              "      <td>0.136022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663551</th>\n",
              "      <td>0.138083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663552</th>\n",
              "      <td>0.119048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663553</th>\n",
              "      <td>0.108300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                isFraud\n",
              "TransactionID          \n",
              "3663549        0.145030\n",
              "3663550        0.136022\n",
              "3663551        0.138083\n",
              "3663552        0.119048\n",
              "3663553        0.108300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZlw01oHayo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub.to_csv('sub.csv')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FeqwiR2HcSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "d751c97f-20ed-4289-b209-cbe75c221849"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(pre)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbb65a2a6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXDb533n8fcXBwGSAG/wEEmJEinJkmNZluXYie3EVxqnuZpJmknS9NjJ2tPGPTLpTJLddo8e26TdbabpNNvGm7SpWytO4saJ48RuXB/yKcqUdVondVGUSAIkRRLgARDAs3+AkGmaEkEKwO8H4PuawRAkIfAjDOfDH57f83seMcaglFLKvhxWB1BKKXVlWtRKKWVzWtRKKWVzWtRKKWVzWtRKKWVzrlw8aUNDg+no6MjFUyulVFHas2fPsDEmsNj3clLUHR0d9PT05OKplVKqKInI2ct9T4c+lFLK5rSolVLK5rSolVLK5rSolVLK5rSolVLK5rSolVLK5rSolVLK5rSolVLK5pYsahHZKCL75t0mROQL+QhXKr618yT3P9TDSyeG0fXBlVILLXllojHmGLAVQEScwHngsRznKgk7uvt47fQoj+07j8sh/OLwEJtbqvjWr99Ie12F1fGUUjax3KGPu4GTxpjLXuqoMndsMMxP9p9nQ5OPP/7gZj6+rY1zF6d4YMfrROMJq+MppWxiuUX9KeB7i31DRO4XkR4R6QmFQlefrMiNT83yyGt9NFd5+fRNqylzObhxTS3/51ev50D/OP/rZ0esjqiUsomMi1pEyoCPAD9c7PvGmAeNMduNMdsDgUUXgFLzPL7/PNF4ko9ta8Pjdl76+vuvbea+29fy0KtneeLABQsTKqXsYjlH1B8AXjfGDOUqTCn5QU8/LdVeWmvK3/a9L917DTesruGPf3yI4UjUgnRKKTtZTlF/mssMe6jlOTIwwcHz49y4pvZt39vR3ccPe/p5z/oA4ek4n/vua+zo7rMgpVLKLjIqahGpBN4H/Ci3cUrDD3v6KXM62NpWc9nHNFV5uWNjgP394xwbnMhjOqWU3WRU1MaYSWNMvTFmPNeBil0snuTH+85zz+ZGKjxXnh353g0BGv0efrzvAuPTs3lKqJSyG70yMc+ePTrE6GSMX93evuRjXU4HH9/WRnhmli8/ekAvhlGqRGlR59lP9l2gwefh9q6GjB7fXlfB+69t5qk3BvmXXTp9XalSpEWdR+GZWZ45GuRDW1pwOTN/6W/tauDOjQH+/IkjHL6g49VKlRot6jx6+vAQsXiSD1+/aln/ziHCX39yKz6vi68+qRfCKFVqtKjz6Kf7L9BaU8621Zef7XE5dZVlfP6OTl48McyuUyM5SKeUsist6jy5OBnjxRPDfPj6VYjIsv/9ju4+3E4Hfq+Lr/zbAR7edVbnVytVIpZcPU9dvR3dfew+PUo8aXA5ZMUF63Y6uHNjI4/vv0BvMML6Jn+Wkyql7EiPqPPkQP8YDT4PLdXeq3qe7Wtqqalw8+yxYJaSKaXsTos6D2ZmE5wZmeTaVVUrGvaYz+V0sH1NLX0jU0Si8SwlVErZmRZ1HvQGIyQNbMzSUMWGJj8G6A2Gs/J8Sil706LOg+NDYbxuR9Z2bVlVU05FmZPjQ5GsPJ9Syt60qHPMGMPxoTBdAR9Ox9UNe6Q5RNjQ5OfEUJhkUi8rV6rYaVHn2NHBMBMzcTZkeYbGhiYfk7EEhy7oOllKFTst6hx7/lhqW7JsF3VXox8Bdh7Tbc+UKnZa1Dn2/LEgLdVeqsrdWX1en8dFa205zx/Xolaq2GlR51B4ZpY9Zy9m/Wg6bX2jn719Fxmf0rWqlSpmWtQ51HPmIvGkoavRl5Pn39DkI2ng1VPDOXl+pZQ9aFHn0L5zYzgE2mrfvoFtNrTWluN1O9h1ajQnz6+Usgct6hw60D9GV6MPj8uZk+d3ORxsX1Onq+kpVeS0qHPEGMOB/nG2XGED22y4eW0dx4bCjE3FcvpzlFLWyXQX8hoReVREjorIERF5V66DFbrzY9OMTMa4vq06pz/nls56jIHu0zr8oVSxyvSI+hvAU8aYa4DrAd1mZAn7z6UuRMn1EfWWtmo8LgfdOk6tVNFasqhFpBp4D/AdAGNMzBgzlutghe5A/xhlTgfXtOR2zWiPy8mNa2p1nFqpIpbJEfVaIAT8k4jsFZFvi0jlwgeJyP0i0iMiPaGQXoSxv3+MTS3+nJ1InO/mtfUcGZzQ+dRKFalMitoFbAP+3hhzAzAJfGXhg4wxDxpjthtjtgcCgSzHLCyJpOFgHk4kpt2yrg5jYPcZHf5QqhhlUtT9QL8xpnvu80dJFbe6jFOhCJOxBFtyfCIx7fr2Gjwuhw5/KFWkltwz0RgzKCLnRGSjMeYYcDdwOPfRCtf+/tSJxK3tuT+iTu+/2FpTzs8PDtAZSF0F+ZmbV+f8Zyul8iPTWR+/BzwsIgeArcBf5C5S4TvQP0ZlmZN1gdxcOr6YzkYfA+MzTOr2XEoVnYyK2hizb278eYsx5leMMRdzHayQHb4wwaaWqqxtFJCJ9JH0qeHJvP1MpVR+LDn0oTK3o7sPYwwHz4+ztb3m0rBEPrTWlONxOTgZjHBda37GxpVS+aGXkGfZ2PQs0XiSpipvXn+u0yGsbajkZEj3UVSq2GhRZ9nQ+AwALdX5LWpIDX+MTMZ03Q+liowWdZYNTqSKOt9H1ADrAqnrkE6GdJxaqWKiRZ1lgxMz1FS48bpzf0XiQk1VXirLnDr8oVSR0aLOssHxGZotOJoGcIiwLuDjVCiCMcaSDEqp7NOizqJ4IslwJGpZUQN0BXxMzMR1+EOpIqJFnUXBcJSkgWYLTiSmpcepXzmp+ygqVSy0qLNoyMITiWl1lWXUVLh5uVeLWqlioUWdRYPjMzgdQoPPY1kGEaEz4GPXqVESSR2nVqoYaFFn0eDEDI1+T14vHV9MZ6CS8elZDl+YsDSHUio7tKizaHDCuhkf86UXg3pZx6mVKgpa1FkyNhUjPBO3dHw6rcrrZn2jj1dO6vrUShUDLeos6Q2mLjJprLJufHq+d3fW89rpUWLxpNVRlFJXSYs6Sy4Vtd/6I2qAd3c1MD2bYG+frkirVKHTos6S3mAEl0OoqXBbHQWAW9bVIwKv6vZcShU8Leos6Q1FCPg9OMTaGR9p1eVuNjb56TmjR9RKFTot6izpDaaK2k7eubaO1/suEk/oOLVShUyLOgumYwnOj03brqi3d9QxFUtweEDnUytVyHQrriw4GYpgjH1OJEJqW7Dx6VkAvrXzFLd2NQC6O7lShSijI2oROSMiB0Vkn4j05DpUoUmv/2y3I+rqcje1FW7OjOhKekoVsuUcUd9pjNFL3RbRG4zgEGioLLM6ytt01FdyPJhan1pscqJTKbU8OkadBb3BCGvqK3E57fdydtRXMhmNMzKp+ygqVagybRYD/EJE9ojI/bkMVIh6gxE659bXsJs19RUAnBnW4Q+lClWmRX2bMWYb8AHgARF5z8IHiMj9ItIjIj2hUCirIe0snkhyZmSSrkZ7FnXA76GizMnZkSmroyilViijojbGnJ/7GAQeA965yGMeNMZsN8ZsDwQC2U1pY2dHp5hNGNsWtYiwpr5STygqVcCWLGoRqRQRf/o+8EvAoVwHKxTpNT7sWtQAq2vLGZmMMRWNWx1FKbUCmRxRNwEvich+YDfwM2PMU7mNVTjSU/M65/YqtKP2utQ49bmL0xYnUUqtxJLT84wxp4Dr85ClIPUGIzRXefF77bEY02Jaa8sR4NxFHadWqhDZbz5ZgTkZjNh62APA43LSVOXl3KgWtVKFSIv6KhhjOBmy74yP+drryum/OE1SN7xVquBoUV+FwYkZItE4nYVQ1LUVTM8mOK2zP5QqOFrUV+HSjA+bXuwyX/qE4r6+MYuTKKWWS4v6KhTC1Ly0gN+Dx+Vg3zktaqUKjRb1VegNRqgud9Pgs99iTAs5RGirLWfvOd3xRalCo0V9FXrnZnwUyqp07bUVHB0IMx1LWB1FKbUMunHACuzo7gPg0IUJNjX7L31ud+11FcSThkMXxrmpo87qOEqpDOkR9QpNxeJMRuO22yzgSvSEolKFSYt6hULhKACNBVTUPo9Lx6mVKkBa1CsUnCvqgI32SczEDatr9YhaqQKjRb1CoXAUl0OoqbDvGh+L2dpew4XxGYYmZqyOopTKkBb1CgXDMwT8HhwFMuMj7YbVNQDs1aNqpQqGFvUKhcLRgjqRmLa5pQq3U3ScWqkCokW9AvFEkrGpWRp8hVfUXreTzS1VOk6tVAHRol6BkckYBgqyqCF1QvHg+XHiiaTVUZRSGdCiXoGRSGrGRyFcOr6Yre01TMUSHB+KWB1FKZUBvTJxBYYjMaAwj6h3dPdd+kPz4AuneOfa1BWKn7l5tZWxlFJXoEfUKxCKRPF5XHjdTqujrEhdZRkVZU76RnVtaqUKgRb1CgxHogV5NJ0mIqxtqORkaBJjdMcXpexOi3oFhiOxgh2fTusM+BifnmV0MmZ1FKXUEjIuahFxisheEXkil4Hsbnx6lslovKCPqCFV1AC9IT2hqJTdLeeI+g+AI7kKUihOD6fGdQu9qBt8ZVR5XZwM6Ti1UnaXUVGLSBvwQeDbuY1jf6eHU0eghT70ISJ0BnycCkVI6ji1UraW6RH13wBfAi57hYSI3C8iPSLSEwqFshLOjk6HJhFSMycKXWfAx1QsoQs0KWVzSxa1iHwICBpj9lzpccaYB40x240x2wOBQNYC2s2p4UlqK8twOQv/POy6QCWADn8oZXOZtM2twEdE5AzwCHCXiPxrTlPZ2KnQZMEPe6TVVJRRX1nGyaCeUFTKzpYsamPMfzHGtBljOoBPAc8aYz6b82Q2ZIzh9PBkwZ9InK+z0cfpkUlmdd0PpWyr8N+/59HQRJTp2URRFfX6Rh+xeJI9Z3XZU6XsallFbYx53hjzoVyFsbuTofSMj+Ip6s6AD4fA88eK9wSwUoVOj6iXoXduLLeQNrRditftZE19Jc8fC1odRSl1GVrUy3AiGMbvdeH3Fteigxub/BwdDDM4rtP0lLIjLepl6A1G6Gr0IQW2T+JSNjT5Adh5XI+qlbIjLepl6A1GWN/oszpG1jVVeWiu8uo4tVI2pUWdoYuTMYYjMdY3+q2OknUiwh0bA7x0Ylin6SllQ1rUGUqvMtdVhEfUAHdsDBCOxnWanlI2pEWdoRNDxV3Ut3Y14HKIDn8oZUNa1BnqDUYodztprSm3OkpO+L1ublxTq9P0lLIhLeoMnQiG6WysxOEorhkf892xsVGn6SllQ1rUGUrN+Ci+E4lpO7r7mIrFAfirp46yo7vP4kRKqTQt6gyEZ2YZGJ8p2vHptOYqL1VeF8eGwlZHUUrNo0WdgfR6zcVe1CLChiY/vcEIiaTu+qKUXWhRZyC9xkcxXuyy0IYmP9F4kr7RKaujKKXmaFFn4EQwTJnTweq6Cquj5FxXY2o1veM6/KGUbWhRZ6B3KMLahsqi2H5rKV63k9V1lVrUStlIcS0Dl2XpmQ97z43RWlNeMjMh1jf5ePrwECORKPVFtPa2UoWq+A8Rr9JsIsnFyRiBIlqDeildgdRY/CsnRyxOopQCLeolDUeiGIprs4ClrKopx+t28MrJYaujKKXQol5ScCIKQKPfa3GS/HE6hHUNPl7q1aJWyg60qJcQDEcRoMFXZnWUvOps9HFudJq+EZ2mp5TVlixqEfGKyG4R2S8ib4jIn+QjmF0EwzPU+8pKYsbHfOlxaj2qVsp6mbRPFLjLGHM9sBW4V0RuyW0s+wiGowRKaNgjrcFXRku1l5e1qJWy3JJFbVIic5+6524lcX1xImkYiURL6kRimohwa1cDL58cJqmXkytlqYzez4uIU0T2AUHgaWNM9yKPuV9EekSkJxQqjsXnRyJRkqa0ZnzMd1tXA2NTs7xxYcLqKEqVtIyK2hiTMMZsBdqAd4rIOxZ5zIPGmO3GmO2BQCDbOS0RDJfejI/5bu1qAHR3cqWstqwzZMaYMeA54N7cxLGXdFGX0sUu8wX8Hq5rrWbn8eJ4h6RUocpk1kdARGrm7pcD7wOO5jqYHQTDM9RUuClzldaMj/neuyHA631jjE/PWh1FqZKVSQO1AM+JyAHgNVJj1E/kNpY9hMKleSJxvjs2BkgkDS+d0NkfSlllyUWZjDEHgBvykMVWEklDKBylM1D8a1Bfydb2Gqq8LnYeD/LBLS1Wx1GqJJXue/olnBmZJJ40NFeV5onENJfTwe3rA+w8HsIYnaanlBV0mdPLODw3Ja25unSLOr2sq9ftYGgiytefPk5LdTmfuXm1xcmUKi16RH0ZRwYmcEjpzqGeL737+vGhyBKPVErlghb1ZRwemKDR7y25NT4WU1XuZlW1l6MDeuGLUlbQFrqMIwMTtJTwsMdCm1ZV0Tc6RXhGp+kplW9a1IsYiUQZmoiW9Pj0QptbqjDA0UHdS1GpfNOiXsSRgVQZtVSXW5zEPpqrvNRWuC+dZFVK5Y8W9SKOzI3F6tDHm0SEzS1VnAxFiETjVsdRqqRoUS/i8MAEzVVeKj06e3G+zauqiScNL+jaH0rllRb1Io4MTLCpxW91DNtZXVdBRZmTX7wxaHUUpUqKFvUC0XiC3mCETS1VVkexHadD2NRcxTNHg0TjCavjKFUytKgXODEUIZ40bF6lRb2Y69qqCc/Eef6YDn8olS9a1AukTyTqEfXiOgM+Gnwefrz3vNVRlCoZWtQLHDw/TmWZk7X1lVZHsSWnQ/jI9at45khQ16hWKk+0qBc40D/OO1qrcTjE6ii29bEbWoklkjx5cMDqKEqVBC3qeWYTSQ4PTLClrdrqKLb2jtYqOgOVPKbDH0rlhRb1PMeHwsTiSa5rq7E6iq2JCB+7oZXu06OcH5u2Oo5SRU+Lep6D/eMAbGnVI+qlfHRrKwA/7DlncRKlip8W9TwHzo/j97pYU19hdRTba6+r4I6NAXZ09zGbSFodR6miptdI8+ZOJjuPhWj0e/jebj1KvJL067W6roLnj4X4bz8+xJa2Gt35RakcWfKIWkTaReQ5ETksIm+IyB/kI1i+xRNJBsdnaK3Ro+lMbWjyU1dZxq5To1ZHUaqoZTL0EQf+0BizGbgFeEBENuc2Vv4NTsyQMIbWWl3aNFMOEW5eW8eZkUkGxvWkolK5smRRG2MGjDGvz90PA0eA1lwHy7f07IW2Gi3q5bhxTS0uh7Dr1IjVUZQqWss6mSgiHcANQPci37tfRHpEpCcUKrx1IM5fnKaizElNhdvqKAWloszF1vYa9vaNMRKJWh1HqaKUcVGLiA/4N+ALxpi3bfNhjHnQGLPdGLM9EAhkM2Ne9I1O0VpTjohekbhct3U1EE8aHnr1rNVRlCpKGRW1iLhJlfTDxpgf5TZS/k1MzxIMR+kM+KyOUpAaq7xc0+znoVfPMB3T5U+VyrZMZn0I8B3giDHm67mPlH+9oQgAXY1a1Ct1+/oAF6dmeXSPTm1UKtsyOaK+Ffh14C4R2Td3++Uc58qr3mCESo9Ldx2/Ch31FVzfXsO3XzpNImmsjqNUUclk1sdLxhgxxmwxxmydu/08H+HyIZk0nAhG6ApU4tDx6RUTEX77Pes4OzLFEwcuWB1HqaJS8peQHx0MMxmNs75R90i8Wu+/tpmNTX6+8cwJPapWKotKvqhfPJGaSqjj01fP4RC+cM96ToUmeXy/LoGqVLaU/FofL54YptHvoapc509frR3dfSSNobnKy58/cYTITAKnQ3QNEKWuUkkfUc/MJth9ZpT1ejSdNQ4R7tnUyMhkjP3nxqyOo1RRKOmi3n16lFg8SZeOT2fVppYqVlV7efZYUMeqlcqCki7ql3qHKXM6WNugG9lmk4hw96YmRidj7O27aHUcpQpeSRf1C8dDbO+opcxV0i9DTlzT7Ke1ppznjgWJxXVjAaWuRsk2VDA8w9HBMLetb7A6SlESEe7Z1DR3tWK/1XGUKmglW9Qv9w4D8J71hbeAVKHY0OSjvbacv3v2BDOzugaIUitVskX94vFh6irL2NxSZXWUoiUi/NK1zVwYn+GfXj5jdRylClZJFrUxhhd7h7m1qwGHQy8bz6XOgI97NjXyzed6Gdb1qpVakZIs6mNDYULhKLfr+HRefOUDm5ieTfA3/3Hc6ihKFaSSujIxvXt2+rLxkUjs0tdU7nQ1+vi1m1fzcHcfv/GuDjY06bx1pZajJI+oe4MRAn4P1XrZeN584Z4N+Dwu/vtPDmGMXgSj1HKUXFHPJpKcHp7Uy8bzaEd3H08dGuSOjQF2nRrlS48e0HcySi1DyRX12ZEp4kmjRW2BmzrqaK8t5+cHB5iKxa2Oo1TBKLmiPhEM4xRhbYMWdb45RPjo1lamYgn+/Y1Bq+MoVTBKrqh7gxFW11foZeMWWVVTzm1dDbx25iI7j4esjqNUQSiptgrPzDIwPqPDHha7Z3MTAb+HLz96gPHpWavjKGV7JVXUJ3W3cVtwOx386o1thCJR/uTxN6yOo5TtLVnUIvKPIhIUkUP5CJRLvcEIFWVOVtWUWx2l5LXVVvDAnV38aO95/mXXWavjKGVrmRxRfxe4N8c5cs6Y1G7jnQGf7jZuE79/Vxd3X9PI//jJIX6hJxeVuqwli9oY8wIwmocsOXV8KEJ4Jq7j0zbyg55+bl8fYFVNOZ9/+HW+9vMjOr9aqUWUzBi17jZuT2UuB7/xrg6qy93886tnCYV14SalFspaUYvI/SLSIyI9oZD9pl29eGKYBp+Hmooyq6OoBXweF7/17g4cDuG7r5wmGJ6xOpJStpK1ojbGPGiM2W6M2R4I2Gsx/mg8QffpER32sLF6n4fffNcaJqMJ/tM/vUZ4RqftKZVWEkMfe85cZGY2qUVtc221FXzm5tUcGwxz30M9uiuMUnMymZ73PeBVYKOI9IvI53IfK7teODGMyyG623gB2NDk568/eT3dp0f5ve/tJZ7QjXGVymTWx6eNMS3GGLcxps0Y8518BMuml3pDbFtTi8fttDqKysBkNMGHrmvh6cNDfPJbu3hY51mrElf0Qx8jkSiHzk9we5fu5lJI3tXZwN3XNPJ630WePDSoa1irklb0Rf3iidRu47fptlsF565rGrllXT0v9Q7zDztPWR1HKcsUfVE/eWiARr+H69tqrI6ilklE+NCWFra0VfOXTx3le7v1YhhVmop6z8RINM5zx0J85p2rdbfxAuUQ4RM3tlFXWcYfPXaQmnI3H7iuxepYSuVV0R5R7+ju409/ephYPInH5dBLkwuYy+Hg73/tRm5YXcsfPLKPH73eb3UkpfKqaIsa4OD5caq8LtrrKqyOoq5SeZmTf/zNm9i2poYv/mA/X33yCImknmBUpaFohz5mZhMcHwpzy9o6XS2vCKTfEX3wulUkDXxr5ymePRJkx323EPB7LE6nVG4V7RH1kYEJEknDda3VVkdRWeR0CL+ytZWPbW3l9PAkv/y3L15acEupYlW0RX2gf5zqcjdtOuxRlG5aW8fn7+iiutzNr39nN7/zr3voG5myOpZSOVGURT0wPs3xoTBb22t02KOINVd7+env3sYX37eB54+FuOfrO/nqk0d0QSdVdIqyqB/ZfQ6AmzrqLE6icu2xvedp8Hn4/bvX847WKr618xS3/MUz7Oju05ONqmgUXVHPJpI88lof65t81FXq2tOlorrczSdubOeBO7oI+D3818cO8sG/fZFXeoetjqbUVSu6on7mSJChiSg3r623OoqyQGttOffdvo7/+2vbiETjfObb3dz3UA+nhyetjqbUihXd9LyHu8+yqtrLxma/1VGURUSEsalZ7rt9Ha/0DvPc8RDPHgly45pavvbx61gX0HXJVWEpqqI+PhTmxRPDfPF9G/QkosLtdPDejY1sW1PLfxwJsqfvInd/fSd3bmzkYze0cs+mJsrLdOlbZX9FU9TGGP70p4ep8rr47C1reOrQoNWRlE34ve65Ym5kMhrn+z3nePZokIoyJ++/tpmPbF3FbV0NuJ1FNxKoikTRFPUvDg/xUu8w//PDm/UkolqU3+vG73Xze3et58zwJPv7x3jy0ACP7T2P3+vitq4GblvfwHWt1Wxo8uPVjSaUTRRFUc/MJvjznx1mQ5OPz96yxuo4yuYcIqwL+FgX8PHhLas4EYwwm0iy83iIJ+feiTkdQku1l7bacjrqK7mm2c/mVdVsba+hzKVH3iq/Cr6o/3XXWZ44cIFzo9N87ra1/KBHV1ZTmXM5HWxqqQLgutZqRidjDIzPMDA+w+hklIGxGQ70j/PIa6mNdr1uB5uaq/jNd3dwTYufdQ0+LW6VcwVd1LF4kkf39LPv3Bi3dtbTqWfz1VUQEep9Hup9Ht4xb40YYwzhaJz+0SkOD0xweGCCL3x/39y/gbqKMup9ZbTVVtBRX0lrbTl1lW6qy904RDAA8669cTgEl0NwiOBypj46F/maQ5j7KDgcUF/p0ZOfJapgi3rP2Yv85VNH2XdujF/a3MR7NwSsjqSKlIhQ5XWzeVU1m1dVk0gahiNRBsZnCIWjTEbjhKNxDl+Y4MUTIWYTubsisrrcTX1lGV63E6/bQXmZE68rVd6xRBJjUo+prkj9oagpT31M36rS9yvc+D0uRGdHFYSMilpE7gW+ATiBbxtjvpbTVJcRCkd57miQR1/vZ/fpUWoq3Hzixja2ra61Io4qUU6H0FTlpanK+7bvGWOYjiWYiiWYnk1cOpCeX4dJY0ia1Edj0p+n7ieSc/cBY1LPZ4Bk0hCJxhmfnmUqlmA2kWR8epbhSIx4InkpF8DMbJKp2QTTsThXuoreIbxZ3AuL/DK3mgo3DT6PnmjNsyWLWkScwDeB9wH9wGsi8rgx5nCuQhljmIwlGBibpm90it2nR3niwAAXxqYxQE2Fmw9e18L2jlo8Lv2FUfYhIlR4XFR4rH+zaowhlkgyPfdH420f592fmU1wcTL2lq9dqeR9HhcNvjIafB78XhdOx5tDOA6H4EzfF8Hp4NJ9x4Kvpx8bjSeZisUZm5plYHyG4MQMkWicmdkkBoPX5cTjdlJe5sDrclJV7qa2ooy6Sje1lWXUVZRRUebE43LicTvwusYituIAAATiSURBVJ14XA5cDgcOBzjnhpScDgdup+BxOXA7UzeHyFv+WCaNectwVOqjA6fzzc+dInnd3i+T36Z3Ar3GmFMAIvII8FEg60W97c+eJjITZzaZeguX5nYKrTXl3LWpkU3NVbRUe/Utm1JLEJFUcbmcLHdr57eV/GyCmbl3CpG5oZ7ITJyRuZOvb313kPr3879mDCS5zNeNweVw4HE58JY5qS5301ztxeN2UjY3t302kWQ2YYgnksQSScamYlwYm2YyGmcylrBkAS6HcOkPQfoCuwafhxe+dGfWf1YmRd0KnJv3eT9w88IHicj9wP1zn0ZE5NjVx3tTLzTshEJbYacBzZwPmjl/CjF3XjPLl1f8Ty87tzhr78+MMQ8CD2br+RYSkR5jzPZcPX8uaOb80Mz5U4i5CzHzQplMAD0PtM/7vG3ua0oppfIgk6J+DVgvImtFpAz4FPB4bmMppZRKW3LowxgTF5HfBf6d1PS8fzTGvJHzZG+Xs2GVHNLM+aGZ86cQcxdi5rcQY3S7IqWUsjNdpEAppWxOi1oppWzOdkUtIveKyDER6RWRryzyfY+IfH/u+90i0pH/lG/LtFTm94jI6yISF5FPWJFxoQwyf1FEDovIARF5RkQsXz82g8y/LSIHRWSfiLwkIputyLkg0xUzz3vcx0XEiIjl08gyeJ1/S0RCc6/zPhH5z1bkXJBpyddZRD459zv9hojsyHfGq2KMsc2N1MnKk8A6oAzYD2xe8JjPA/8wd/9TwPcLIHMHsAV4CPhEgbzOdwIVc/d/p0Be56p59z8CPGX3zHOP8wMvALuA7XbPDPwW8HdW5lxB5vXAXqB27vNGq3Mv52a3I+pLl6sbY2JA+nL1+T4K/PPc/UeBu8Xa68mXzGyMOWOMOUDqKlo7yCTzc8aYqblPd5GaP2+lTDJPzPu0krcsLmqJTH6fAf4M+EtgJp/hLiPTzHaSSeb7gG8aYy4CGGOCec54VexW1Itdrt56uccYY+LAOFCfl3SLyySz3Sw38+eAJ3OaaGkZZRaRB0TkJPBXwO/nKdvlLJlZRLYB7caYn+Uz2BVk+rvx8blhsUdFpH2R7+dTJpk3ABtE5GUR2TW3ImjBsFtRK5sRkc8C24H/bXWWTBhjvmmM6QS+DPyx1XmuREQcwNeBP7Q6yzL9FOgwxmwBnubNd7h25iI1/HEH8Gng/4nIcteqsozdijqTy9UvPUZEXEA1MJKXdIsrxEvsM8osIvcAfwR8xBgTzVO2y1nu6/wI8Cs5TbS0pTL7gXcAz4vIGeAW4HGLTygu+TobY0bm/T58G7gxT9kuJ5PfjX7gcWPMrDHmNHCcVHEXBqsHyRcM+LuAU8Ba3jwpcO2CxzzAW08m/sDumec99rvY42RiJq/zDaRO0Ky3Ou8yMq+fd//DQI/dMy94/PNYfzIxk9e5Zd79jwG7CiDzvcA/z91vIDVUUm9l7mX9H60OsMiL/suk/tqdBP5o7mt/SuqoDsAL/BDoBXYD6wog802k/qJPkjr6f6MAMv8HMATsm7s9XgCZvwG8MZf3uSuVol0yL3is5UWd4ev81bnXef/c63xNAWQWUsNMh4GDwKeszrycm15CrpRSNme3MWqllFILaFErpZTNaVErpZTNaVErpZTNaVErpZTNaVErpZTNaVErpZTN/X8KRdvGLhqIbgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAeWMkwJIWng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
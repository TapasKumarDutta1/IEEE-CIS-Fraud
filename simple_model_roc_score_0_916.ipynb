{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc_score_0.916",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc_score_0_916.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIAeHxBw8EEt",
        "colab_type": "text"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstQrkXM8Bz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IonOu6819IW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "70da3151-83f5-471b-9957-ca6619af9a83"
      },
      "source": [
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 109MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 164MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 53.0MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 84% 49.0M/58.3M [00:00<00:00, 59.1MB/s]\n",
            "100% 58.3M/58.3M [00:00<00:00, 92.0MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 73% 38.0M/52.2M [00:00<00:00, 71.7MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 132MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvhOLFro71iv",
        "colab_type": "text"
      },
      "source": [
        "loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "1d1815e1-1bde-4fb5-9d75-ba6356ce45f4"
      },
      "source": [
        "\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZi5I0Va7-Af",
        "colab_type": "text"
      },
      "source": [
        "Loading dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5a493d30-cd24-4114-95c7-72ae3298dc13"
      },
      "source": [
        "\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv',index_col=[0])\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv',index_col=[0])\n",
        "trn.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D13</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>V1</th>\n",
              "      <th>isna_sum</th>\n",
              "      <th>dist1_isna</th>\n",
              "      <th>dist2_isna</th>\n",
              "      <th>D1_isna</th>\n",
              "      <th>D2_isna</th>\n",
              "      <th>D3_isna</th>\n",
              "      <th>D4_isna</th>\n",
              "      <th>D5_isna</th>\n",
              "      <th>D6_isna</th>\n",
              "      <th>D7_isna</th>\n",
              "      <th>D8_isna</th>\n",
              "      <th>D9_isna</th>\n",
              "      <th>D10_isna</th>\n",
              "      <th>D11_isna</th>\n",
              "      <th>D12_isna</th>\n",
              "      <th>D13_isna</th>\n",
              "      <th>D14_isna</th>\n",
              "      <th>D15_isna</th>\n",
              "      <th>V1_isna</th>\n",
              "      <th>id</th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>-0.752856</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>0.155508</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.312997</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.012922</td>\n",
              "      <td>-0.737091</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.807711</td>\n",
              "      <td>0.00739</td>\n",
              "      <td>-0.167776</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.01.0nan315.013926-13.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>-0.752856</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>-0.942090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.038619</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.807711</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.349188</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.01.0gmail.com325.027551.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>-0.752856</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>-0.942090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.038619</td>\n",
              "      <td>1.132473</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.746105</td>\n",
              "      <td>0.00739</td>\n",
              "      <td>0.211465</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.01.0outlook.com330.046631.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>1.013560</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>0.973965</td>\n",
              "      <td>0.292248</td>\n",
              "      <td>-1.335951</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.301712</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.688064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.260176</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.211465</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.025.0yahoo.com476.018132-111.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.319736</td>\n",
              "      <td>-0.752856</td>\n",
              "      <td>-0.429362</td>\n",
              "      <td>-0.942090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.107669</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.01.0gmail.com420.044971.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 499 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2  ...  V1_isna                                id  isFraud\n",
              "0  1.0  0.0  0.0  ...        0          1.01.0nan315.013926-13.0        0\n",
              "1  1.0  0.0  0.0  ...        1       1.01.0gmail.com325.027551.0        0\n",
              "2  1.0  0.0  0.0  ...        0     1.01.0outlook.com330.046631.0        0\n",
              "3  1.0  0.0  0.0  ...        1  2.025.0yahoo.com476.018132-111.0        0\n",
              "4  0.0  0.0  0.0  ...        1       1.01.0gmail.com420.044971.0        0\n",
              "\n",
              "[5 rows x 499 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LEIUFVW8iAC",
        "colab_type": "text"
      },
      "source": [
        "Reduce memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "cb55ab4d-b268-4e91-8e9a-f81afe51f9ce"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2252.73 MB\n",
            "Memory usage after optimization is: 580.70 MB\n",
            "Decreased by 74.2%\n",
            "Memory usage of dataframe is 1929.01 MB\n",
            "Memory usage after optimization is: 500.58 MB\n",
            "Decreased by 74.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjn9ePhArDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.replace([np.inf,-np.inf],np.nan)\n",
        "tst=tst.replace([np.inf,-np.inf],np.nan)\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=tst.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(0)\n",
        "  tst[col]=tst[col].fillna(0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqrD6vz8ol6",
        "colab_type": "text"
      },
      "source": [
        "Making the callbacks and loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dk={}\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "        print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "\n",
        "  inp=Input((497,))\n",
        "  x=Dense(256,activation=custom_gelu)(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation=custom_gelu)(inp)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(256,activation=custom_gelu)(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=[inp],outputs=x)\n",
        "  return mod\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXKSLj3p5MY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "splits=KFold(n_splits=5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W07F3czB7b3v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "23522c9f-73c5-4b98-ed3f-7447bd34abda"
      },
      "source": [
        "trn=trn.drop(['id'],1)\n",
        "tst=tst.drop(['id'],1)\n",
        "gc.collect()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBA_Yk6oC_IY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 1.5\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oor5OujA6Bz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d36d246-cfae-4da8-b406-508e1f44a69f"
      },
      "source": [
        "ln=len(trn)/10\n",
        "for i in range(6,10):\n",
        "  X_train, X_test = trn.loc[:int(ln*i)], trn.loc[int(ln*i):]\n",
        "  y_train, y_test = X_train['isFraud'], X_test['isFraud']\n",
        "  X_train=X_train.drop(['isFraud'],1)\n",
        "  X_test=X_test.drop(['isFraud'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(X_test, y_test))\n",
        "  mod.compile(optimizer=Nadam(),loss=rac)\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=2048,epochs=8,callbacks=[es,roc])\n",
        "  gc.collect()\n",
        "  mod.fit(X_test,y_test,epochs=2,batch_size=2048)\n",
        "  if i ==6:\n",
        "    pre=mod.predict(tst)/5\n",
        "  else:\n",
        "    pre+=mod.predict(tst)/5\n",
        "  del([X_train,X_test,y_train,y_test])\n",
        "  gc.collect()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "168/174 [===========================>..] - ETA: 0s - loss: 9438.4395roc-auc_val: 0.8707\n",
            "174/174 [==============================] - 9s 52ms/step - loss: 9356.7441 - val_loss: 9411.3252\n",
            "Epoch 2/8\n",
            "169/174 [============================>.] - ETA: 0s - loss: 6640.8315roc-auc_val: 0.8797\n",
            "174/174 [==============================] - 9s 49ms/step - loss: 6633.5615 - val_loss: 8199.6396\n",
            "Epoch 3/8\n",
            "172/174 [============================>.] - ETA: 0s - loss: 5695.7598roc-auc_val: 0.8855\n",
            "174/174 [==============================] - 8s 49ms/step - loss: 5692.8037 - val_loss: 7716.1030\n",
            "Epoch 4/8\n",
            "170/174 [============================>.] - ETA: 0s - loss: 5083.3174roc-auc_val: 0.8837\n",
            "174/174 [==============================] - 8s 49ms/step - loss: 5077.4521 - val_loss: 7848.3628\n",
            "Epoch 5/8\n",
            "167/174 [===========================>..] - ETA: 0s - loss: 4451.6978roc-auc_val: 0.882\n",
            "174/174 [==============================] - 9s 49ms/step - loss: 4437.7329 - val_loss: 7937.0757\n",
            "Epoch 6/8\n",
            "170/174 [============================>.] - ETA: 0s - loss: 4013.1084roc-auc_val: 0.8828\n",
            "174/174 [==============================] - 9s 50ms/step - loss: 3992.4968 - val_loss: 8007.4771\n",
            "Epoch 7/8\n",
            "172/174 [============================>.] - ETA: 0s - loss: 3635.3997roc-auc_val: 0.8815\n",
            "174/174 [==============================] - 9s 50ms/step - loss: 3632.4844 - val_loss: 8080.8774\n",
            "Epoch 8/8\n",
            "171/174 [============================>.] - ETA: 0s - loss: 3288.1809roc-auc_val: 0.882\n",
            "174/174 [==============================] - 9s 53ms/step - loss: 3288.8589 - val_loss: 8094.1157\n",
            "Epoch 1/2\n",
            "116/116 [==============================] - 1s 7ms/step - loss: 5985.5811\n",
            "Epoch 2/2\n",
            "116/116 [==============================] - 1s 7ms/step - loss: 4341.2144\n",
            "Epoch 1/8\n",
            "199/202 [============================>.] - ETA: 0s - loss: 9371.2842roc-auc_val: 0.8687\n",
            "202/202 [==============================] - 8s 40ms/step - loss: 9335.3926 - val_loss: 8822.0967\n",
            "Epoch 2/8\n",
            "198/202 [============================>.] - ETA: 0s - loss: 6550.7925roc-auc_val: 0.8875\n",
            "202/202 [==============================] - 7s 37ms/step - loss: 6538.2612 - val_loss: 7125.2241\n",
            "Epoch 3/8\n",
            "196/202 [============================>.] - ETA: 0s - loss: 5633.2378roc-auc_val: 0.8859\n",
            "202/202 [==============================] - 7s 37ms/step - loss: 5654.0093 - val_loss: 7200.1699\n",
            "Epoch 4/8\n",
            "202/202 [==============================] - ETA: 0s - loss: 4987.6362roc-auc_val: 0.8931\n",
            "202/202 [==============================] - 7s 36ms/step - loss: 4987.6362 - val_loss: 6809.6519\n",
            "Epoch 5/8\n",
            "198/202 [============================>.] - ETA: 0s - loss: 4532.9243roc-auc_val: 0.8961\n",
            "202/202 [==============================] - 7s 37ms/step - loss: 4523.4897 - val_loss: 6662.1963\n",
            "Epoch 6/8\n",
            "201/202 [============================>.] - ETA: 0s - loss: 4078.0420roc-auc_val: 0.8953\n",
            "202/202 [==============================] - 8s 40ms/step - loss: 4074.0376 - val_loss: 6730.8354\n",
            "Epoch 7/8\n",
            "202/202 [==============================] - ETA: 0s - loss: 3737.8044roc-auc_val: 0.9008\n",
            "202/202 [==============================] - 8s 40ms/step - loss: 3737.8044 - val_loss: 6461.0371\n",
            "Epoch 8/8\n",
            "200/202 [============================>.] - ETA: 0s - loss: 3424.2463roc-auc_val: 0.8983\n",
            "202/202 [==============================] - 7s 37ms/step - loss: 3420.8062 - val_loss: 6551.6562\n",
            "Epoch 1/2\n",
            "87/87 [==============================] - 1s 7ms/step - loss: 5637.2817\n",
            "Epoch 2/2\n",
            "87/87 [==============================] - 1s 7ms/step - loss: 3996.5703\n",
            "Epoch 1/8\n",
            "229/231 [============================>.] - ETA: 0s - loss: 9130.9580roc-auc_val: 0.8647\n",
            "231/231 [==============================] - 6s 27ms/step - loss: 9106.9834 - val_loss: 8604.9941\n",
            "Epoch 2/8\n",
            "224/231 [============================>.] - ETA: 0s - loss: 6502.3901roc-auc_val: 0.8773\n",
            "231/231 [==============================] - 6s 25ms/step - loss: 6471.2866 - val_loss: 7657.8599\n",
            "Epoch 3/8\n",
            "228/231 [============================>.] - ETA: 0s - loss: 5563.8975roc-auc_val: 0.8905\n",
            "231/231 [==============================] - 6s 25ms/step - loss: 5554.5215 - val_loss: 6914.7881\n",
            "Epoch 4/8\n",
            "228/231 [============================>.] - ETA: 0s - loss: 4867.7720roc-auc_val: 0.8858\n",
            "231/231 [==============================] - 6s 24ms/step - loss: 4866.8306 - val_loss: 7241.0244\n",
            "Epoch 5/8\n",
            "225/231 [============================>.] - ETA: 0s - loss: 4334.4834roc-auc_val: 0.9002\n",
            "231/231 [==============================] - 6s 25ms/step - loss: 4351.5322 - val_loss: 6273.1255\n",
            "Epoch 6/8\n",
            "231/231 [==============================] - ETA: 0s - loss: 3903.2954roc-auc_val: 0.8992\n",
            "231/231 [==============================] - 6s 25ms/step - loss: 3903.2954 - val_loss: 6435.8452\n",
            "Epoch 7/8\n",
            "229/231 [============================>.] - ETA: 0s - loss: 3494.6365roc-auc_val: 0.8975\n",
            "231/231 [==============================] - 6s 24ms/step - loss: 3489.9236 - val_loss: 6519.4609\n",
            "Epoch 8/8\n",
            "227/231 [============================>.] - ETA: 0s - loss: 3255.7522roc-auc_val: 0.9024\n",
            "231/231 [==============================] - 6s 24ms/step - loss: 3242.6907 - val_loss: 6240.6558\n",
            "Epoch 1/2\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 5496.0278\n",
            "Epoch 2/2\n",
            "58/58 [==============================] - 0s 7ms/step - loss: 3788.1145\n",
            "Epoch 1/8\n",
            "258/260 [============================>.] - ETA: 0s - loss: 9281.2520roc-auc_val: 0.874\n",
            "260/260 [==============================] - 5s 17ms/step - loss: 9259.3271 - val_loss: 8810.4980\n",
            "Epoch 2/8\n",
            "255/260 [============================>.] - ETA: 0s - loss: 6534.1826roc-auc_val: 0.8873\n",
            "260/260 [==============================] - 4s 15ms/step - loss: 6527.5015 - val_loss: 7529.9375\n",
            "Epoch 3/8\n",
            "257/260 [============================>.] - ETA: 0s - loss: 5506.5020roc-auc_val: 0.8974\n",
            "260/260 [==============================] - 4s 15ms/step - loss: 5487.6577 - val_loss: 7012.3105\n",
            "Epoch 4/8\n",
            "257/260 [============================>.] - ETA: 0s - loss: 4868.4692roc-auc_val: 0.8968\n",
            "260/260 [==============================] - 4s 15ms/step - loss: 4855.2588 - val_loss: 7021.9717\n",
            "Epoch 5/8\n",
            "257/260 [============================>.] - ETA: 0s - loss: 4343.3740roc-auc_val: 0.9032\n",
            "260/260 [==============================] - 4s 15ms/step - loss: 4329.8071 - val_loss: 6652.1582\n",
            "Epoch 6/8\n",
            "256/260 [============================>.] - ETA: 0s - loss: 3931.8271roc-auc_val: 0.9082\n",
            "260/260 [==============================] - 4s 15ms/step - loss: 3931.0657 - val_loss: 6405.7520\n",
            "Epoch 7/8\n",
            "258/260 [============================>.] - ETA: 0s - loss: 3526.8467roc-auc_val: 0.9064\n",
            "260/260 [==============================] - 4s 17ms/step - loss: 3519.6926 - val_loss: 6432.8853\n",
            "Epoch 8/8\n",
            "254/260 [============================>.] - ETA: 0s - loss: 3218.8303roc-auc_val: 0.9083\n",
            "260/260 [==============================] - 4s 15ms/step - loss: 3227.5334 - val_loss: 6394.9790\n",
            "Epoch 1/2\n",
            "29/29 [==============================] - 0s 7ms/step - loss: 5714.2964\n",
            "Epoch 2/2\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 3831.5293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVYgkVd5_NVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "d87cbd3c-2fab-4b52-c6de-02fa07b75c05"
      },
      "source": [
        "sub=pd.read_csv('sample_submission.csv.zip')\n",
        "sub['isFraud']=pre\n",
        "sub=sub.set_index('TransactionID')\n",
        "sub.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TransactionID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3663549</th>\n",
              "      <td>0.006110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663550</th>\n",
              "      <td>0.031689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663551</th>\n",
              "      <td>0.051926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663552</th>\n",
              "      <td>0.012394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3663553</th>\n",
              "      <td>0.023230</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                isFraud\n",
              "TransactionID          \n",
              "3663549        0.006110\n",
              "3663550        0.031689\n",
              "3663551        0.051926\n",
              "3663552        0.012394\n",
              "3663553        0.023230"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZlw01oHayo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub.to_csv('sub.csv')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FeqwiR2HcSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "63e6e6a0-b78f-4309-b970-f89f618ffe1d"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(pre)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd8b5717940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAeWMkwJIWng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
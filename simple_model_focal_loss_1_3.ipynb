{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_1.3",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "a38cf157-c64d-4755-845d-22b679cdddea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "5364d243-24b0-4f31-f4ab-24a6d109b6b6"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 79.4MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 98% 51.0M/52.2M [00:01<00:00, 31.8MB/s]\n",
            "100% 52.2M/52.2M [00:01<00:00, 40.0MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 79% 46.0M/58.3M [00:01<00:00, 43.6MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 52.4MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 218MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 222MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDrCIAqHzl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "d072d785-b1b4-4c76-920f-7babb7c3277e"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWJ-hzcznam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5OQjb1zo6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "07632f0b-1078-49b0-9ad9-f4efb0bcf52f"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "63194746-929e-40e8-876e-11663640b4c4"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0r3SuH1K97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQ20JqWATak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQIVOLKBFIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1fab9f9f-c5db-4ada-e469-50dfdfaeac38"
      },
      "source": [
        "1-0.036"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.964"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "636a6dca-aaab-4f62-a196-d8470895c3e3"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=1.3\n",
        "        alpha=1-0.036\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss\n",
        "dk={}\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "for en,month in enumerate([(4,5),(3,4),(3,5)]):\n",
        "  train=trn.loc[trn['month']>=month[1]]\n",
        "  test=trn.loc[trn['month']<=month[0]]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.0001,decay=1e-3),loss=fl())\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 25ms/step - loss: 62.9840 - val_loss: 36.5711\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 54.8325 - val_loss: 35.2605\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 49.7445 - val_loss: 34.7005\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 48.3546 - val_loss: 34.6330\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 46.9894 - val_loss: 34.6908\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 44.1353 - val_loss: 34.7043\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 43.3248 - val_loss: 34.6988\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 41.8274 - val_loss: 34.6306\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 41.5673 - val_loss: 34.4905\n",
            "Epoch 10/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 40.4849roc-auc_val: 0.7768\n",
            "47/47 [==============================] - 13s 282ms/step - loss: 40.4145 - val_loss: 34.3863\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 40.0797 - val_loss: 34.6091\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 39.2578 - val_loss: 34.5804\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 39.4127 - val_loss: 34.5311\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 38.5019 - val_loss: 34.5124\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 38.0221 - val_loss: 34.4861\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 36.9996 - val_loss: 34.3897\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 36.8541 - val_loss: 34.3915\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 36.4783 - val_loss: 34.5168\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 35.9939 - val_loss: 34.5278\n",
            "Epoch 20/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 35.6388roc-auc_val: 0.7858\n",
            "47/47 [==============================] - 13s 281ms/step - loss: 35.4918 - val_loss: 34.4742\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 35.1246 - val_loss: 34.4277\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 35.2313 - val_loss: 34.3768\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 34.6785 - val_loss: 34.3784\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 34.5068 - val_loss: 34.4113\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.9799 - val_loss: 34.4209\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 34.0924 - val_loss: 34.3813\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.7133 - val_loss: 34.3320\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.9083 - val_loss: 34.3117\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.3145 - val_loss: 34.3254\n",
            "Epoch 30/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 33.2033roc-auc_val: 0.7895\n",
            "47/47 [==============================] - 13s 279ms/step - loss: 33.1721 - val_loss: 34.3425\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.7442 - val_loss: 34.2594\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.7398 - val_loss: 34.2875\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.8123 - val_loss: 34.3099\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.6296 - val_loss: 34.3594\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.5167 - val_loss: 34.3975\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.9057 - val_loss: 34.3345\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.6980 - val_loss: 34.2992\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.8727 - val_loss: 34.2647\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.4800 - val_loss: 34.2518\n",
            "Epoch 40/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 32.2033roc-auc_val: 0.792\n",
            "47/47 [==============================] - 13s 280ms/step - loss: 31.8807 - val_loss: 34.2318\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.3510 - val_loss: 34.2498\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.9505 - val_loss: 34.2140\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.2039 - val_loss: 34.2218\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.6515 - val_loss: 34.1625\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.3905 - val_loss: 34.2101\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.2488 - val_loss: 34.1982\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.4776 - val_loss: 34.2088\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.2041 - val_loss: 34.2184\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.3875 - val_loss: 34.1902\n",
            "Epoch 50/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 30.1480roc-auc_val: 0.7943\n",
            "47/47 [==============================] - 13s 280ms/step - loss: 30.0920 - val_loss: 34.1520\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.2034 - val_loss: 34.1663\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.5551 - val_loss: 34.1184\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.7510 - val_loss: 34.0989\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.2062 - val_loss: 34.0871\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.7498 - val_loss: 34.1111\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.5298 - val_loss: 34.1306\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.7354 - val_loss: 34.1136\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.8395 - val_loss: 34.0960\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.9474 - val_loss: 34.1502\n",
            "Epoch 60/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 29.1667roc-auc_val: 0.7963\n",
            "47/47 [==============================] - 13s 280ms/step - loss: 29.1511 - val_loss: 34.1109\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.8704 - val_loss: 34.1384\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.9033 - val_loss: 34.1282\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.8852 - val_loss: 34.1536\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.6300 - val_loss: 34.1629\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.0984 - val_loss: 34.1445\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.0383 - val_loss: 34.1659\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.8220 - val_loss: 34.1567\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.6696 - val_loss: 34.1283\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.7191 - val_loss: 34.0857\n",
            "Epoch 70/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 28.2746roc-auc_val: 0.7979\n",
            "47/47 [==============================] - 13s 278ms/step - loss: 28.1658 - val_loss: 34.0885\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.7490 - val_loss: 34.0910\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.7119 - val_loss: 34.1490\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.2888 - val_loss: 34.1872\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.2641 - val_loss: 34.1894\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.5642 - val_loss: 34.2244\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.2081 - val_loss: 34.1965\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 28.0199 - val_loss: 34.2206\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 28.2844 - val_loss: 34.2269\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 27.9744 - val_loss: 34.2199\n",
            "Epoch 80/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 28.0957roc-auc_val: 0.7985\n",
            "47/47 [==============================] - 14s 297ms/step - loss: 27.8841 - val_loss: 34.1961\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.0884 - val_loss: 34.2346\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.8484 - val_loss: 34.2308\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.6144 - val_loss: 34.2512\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.8591 - val_loss: 34.2379\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.0698 - val_loss: 34.2257\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.8165 - val_loss: 34.2471\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.7331 - val_loss: 34.2434\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.0076 - val_loss: 34.2465\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.1867 - val_loss: 34.2724\n",
            "Epoch 90/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 27.5286roc-auc_val: 0.7998\n",
            "47/47 [==============================] - 13s 279ms/step - loss: 27.4569 - val_loss: 34.2657\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.9606 - val_loss: 34.2594\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.2703 - val_loss: 34.2222\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.0709 - val_loss: 34.2495\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.1981 - val_loss: 34.2591\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.1058 - val_loss: 34.3050\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.4887 - val_loss: 34.3409\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.3741 - val_loss: 34.3725\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.9387 - val_loss: 34.3658\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.1141 - val_loss: 34.3868\n",
            "Epoch 100/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 27.0350roc-auc_val: 0.8005\n",
            "47/47 [==============================] - 13s 283ms/step - loss: 27.0324 - val_loss: 34.3663\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.0215 - val_loss: 34.3643\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.3444 - val_loss: 34.3616\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.6068 - val_loss: 34.3772\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.0574 - val_loss: 34.3689\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.8800 - val_loss: 34.3445\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.3144 - val_loss: 34.3209\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.4284 - val_loss: 34.3590\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.0071 - val_loss: 34.3639\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.8591 - val_loss: 34.3550\n",
            "Epoch 110/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 26.3721roc-auc_val: 0.8016\n",
            "47/47 [==============================] - 14s 288ms/step - loss: 26.5010 - val_loss: 34.3801\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 26.1484 - val_loss: 34.3600\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.8278 - val_loss: 34.3527\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.3628 - val_loss: 34.3573\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.5066 - val_loss: 34.4277\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.2665 - val_loss: 34.4139\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.4242 - val_loss: 34.4211\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.2698 - val_loss: 34.4420\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.3018 - val_loss: 34.4543\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.1517 - val_loss: 34.4585\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 60.1447 - val_loss: 35.2983\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 50.4394 - val_loss: 34.4019\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 46.8298 - val_loss: 34.5389\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 44.0670 - val_loss: 34.4238\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 42.9747 - val_loss: 34.6747\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 41.5315 - val_loss: 34.4197\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 39.9705 - val_loss: 34.0372\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 38.8448 - val_loss: 34.2358\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 38.9175 - val_loss: 33.9823\n",
            "Epoch 10/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 37.9073roc-auc_val: 0.7913\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 37.7249 - val_loss: 33.7303\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.8330 - val_loss: 33.6829\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.3383 - val_loss: 33.5956\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.1898 - val_loss: 33.5092\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 35.4705 - val_loss: 33.6293\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 35.2330 - val_loss: 33.5571\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 35.0100 - val_loss: 33.5388\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 34.4989 - val_loss: 33.4190\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 34.5512 - val_loss: 33.3515\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.7448 - val_loss: 33.3646\n",
            "Epoch 20/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 33.5162roc-auc_val: 0.7994\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 33.6194 - val_loss: 33.2431\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.0544 - val_loss: 33.2063\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.7869 - val_loss: 33.1773\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.9152 - val_loss: 33.1411\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.8617 - val_loss: 33.1100\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.2975 - val_loss: 33.1188\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.3365 - val_loss: 33.1359\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.9754 - val_loss: 33.1416\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.0447 - val_loss: 33.1332\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.2200 - val_loss: 33.1189\n",
            "Epoch 30/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 31.1896roc-auc_val: 0.803\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 31.1776 - val_loss: 33.0673\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.2946 - val_loss: 33.0518\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.4730 - val_loss: 33.0282\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.9257 - val_loss: 33.0102\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.2189 - val_loss: 32.9957\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.8441 - val_loss: 32.9550\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.3709 - val_loss: 32.8913\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.6262 - val_loss: 32.8711\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.3522 - val_loss: 32.8041\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.5339 - val_loss: 32.7831\n",
            "Epoch 40/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 30.2574roc-auc_val: 0.8061\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 30.2574 - val_loss: 32.8126\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.0038 - val_loss: 32.7832\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.1760 - val_loss: 32.7530\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.1821 - val_loss: 32.7189\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.9577 - val_loss: 32.6859\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.9080 - val_loss: 32.7033\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.8180 - val_loss: 32.6690\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.6804 - val_loss: 32.6734\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.3658 - val_loss: 32.6530\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.4111 - val_loss: 32.6246\n",
            "Epoch 50/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 29.3376roc-auc_val: 0.8087\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 29.2885 - val_loss: 32.6409\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.1872 - val_loss: 32.6101\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.9214 - val_loss: 32.6110\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.8943 - val_loss: 32.5990\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.1253 - val_loss: 32.5755\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.7635 - val_loss: 32.5694\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.7316 - val_loss: 32.5763\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.7808 - val_loss: 32.5203\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.8245 - val_loss: 32.5165\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.6381 - val_loss: 32.4847\n",
            "Epoch 60/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 28.4313roc-auc_val: 0.8108\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 28.4313 - val_loss: 32.4938\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5938 - val_loss: 32.4762\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.7530 - val_loss: 32.4507\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.4502 - val_loss: 32.4456\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.4883 - val_loss: 32.4258\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.4981 - val_loss: 32.4207\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5333 - val_loss: 32.4167\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.4743 - val_loss: 32.4099\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.1350 - val_loss: 32.3895\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.3975 - val_loss: 32.3643\n",
            "Epoch 70/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 27.9584roc-auc_val: 0.8123\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 28.0125 - val_loss: 32.3315\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5519 - val_loss: 32.3348\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.1111 - val_loss: 32.3201\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.0034 - val_loss: 32.3303\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8407 - val_loss: 32.3063\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8906 - val_loss: 32.2898\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.0683 - val_loss: 32.2942\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8383 - val_loss: 32.2676\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9096 - val_loss: 32.2604\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6138 - val_loss: 32.2581\n",
            "Epoch 80/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 27.7319roc-auc_val: 0.8135\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 27.7242 - val_loss: 32.2527\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8743 - val_loss: 32.2620\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6760 - val_loss: 32.2714\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6197 - val_loss: 32.2466\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.7227 - val_loss: 32.2401\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.4223 - val_loss: 32.2257\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.4035 - val_loss: 32.2157\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5450 - val_loss: 32.2122\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2606 - val_loss: 32.2150\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.4608 - val_loss: 32.2099\n",
            "Epoch 90/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 27.3630roc-auc_val: 0.8145\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 27.3944 - val_loss: 32.1940\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.1602 - val_loss: 32.1864\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3518 - val_loss: 32.1707\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2445 - val_loss: 32.1470\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3044 - val_loss: 32.1343\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2300 - val_loss: 32.1347\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.1240 - val_loss: 32.1231\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9758 - val_loss: 32.1246\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9752 - val_loss: 32.1323\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.1859 - val_loss: 32.1294\n",
            "Epoch 100/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 27.5610roc-auc_val: 0.8154\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 27.4304 - val_loss: 32.1222\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9521 - val_loss: 32.1135\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9309 - val_loss: 32.0886\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9509 - val_loss: 32.0892\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9167 - val_loss: 32.0806\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8818 - val_loss: 32.0932\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8113 - val_loss: 32.0907\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8385 - val_loss: 32.0841\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8152 - val_loss: 32.0652\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6693 - val_loss: 32.0412\n",
            "Epoch 110/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 26.8405roc-auc_val: 0.8165\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 26.8328 - val_loss: 32.0448\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7791 - val_loss: 32.0437\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5774 - val_loss: 32.0405\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4554 - val_loss: 32.0467\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7468 - val_loss: 32.0443\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5871 - val_loss: 32.0491\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6463 - val_loss: 32.0523\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3370 - val_loss: 32.0240\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4883 - val_loss: 32.0007\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6424 - val_loss: 32.0077\n",
            "Epoch 120/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 26.4767roc-auc_val: 0.817\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 26.6188 - val_loss: 32.0189\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3097 - val_loss: 32.0076\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2834 - val_loss: 31.9928\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7013 - val_loss: 31.9984\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1581 - val_loss: 31.9877\n",
            "Epoch 125/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.3834 - val_loss: 31.9804\n",
            "Epoch 126/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.4344 - val_loss: 31.9733\n",
            "Epoch 127/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.3006 - val_loss: 31.9832\n",
            "Epoch 128/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.4886 - val_loss: 31.9809\n",
            "Epoch 129/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.1967 - val_loss: 31.9697\n",
            "Epoch 130/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 26.2220roc-auc_val: 0.8175\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 26.1675 - val_loss: 31.9681\n",
            "Epoch 131/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5841 - val_loss: 31.9708\n",
            "Epoch 132/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0677 - val_loss: 31.9585\n",
            "Epoch 133/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2469 - val_loss: 31.9518\n",
            "Epoch 134/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0384 - val_loss: 31.9512\n",
            "Epoch 135/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3810 - val_loss: 31.9319\n",
            "Epoch 136/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0926 - val_loss: 31.9193\n",
            "Epoch 137/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9714 - val_loss: 31.9314\n",
            "Epoch 138/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0137 - val_loss: 31.9360\n",
            "Epoch 139/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1862 - val_loss: 31.9190\n",
            "Epoch 140/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 26.1443roc-auc_val: 0.8183\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 26.1297 - val_loss: 31.9084\n",
            "Epoch 141/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0961 - val_loss: 31.8968\n",
            "Epoch 142/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7517 - val_loss: 31.9027\n",
            "Epoch 143/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8538 - val_loss: 31.9076\n",
            "Epoch 144/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9965 - val_loss: 31.9005\n",
            "Epoch 145/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6817 - val_loss: 31.8955\n",
            "Epoch 146/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7909 - val_loss: 31.8915\n",
            "Epoch 147/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2775 - val_loss: 31.8926\n",
            "Epoch 148/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7958 - val_loss: 31.8983\n",
            "Epoch 149/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6001 - val_loss: 31.8916\n",
            "Epoch 150/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 25.7671roc-auc_val: 0.8188\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 25.7802 - val_loss: 31.8903\n",
            "Epoch 151/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7251 - val_loss: 31.8933\n",
            "Epoch 152/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7530 - val_loss: 31.8789\n",
            "Epoch 153/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7872 - val_loss: 31.8742\n",
            "Epoch 154/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0926 - val_loss: 31.8719\n",
            "Epoch 155/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9247 - val_loss: 31.8802\n",
            "Epoch 156/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1099 - val_loss: 31.8617\n",
            "Epoch 157/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8226 - val_loss: 31.8569\n",
            "Epoch 158/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6830 - val_loss: 31.8430\n",
            "Epoch 159/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6440 - val_loss: 31.8419\n",
            "Epoch 160/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 25.8021roc-auc_val: 0.8192\n",
            "88/88 [==============================] - 12s 131ms/step - loss: 25.6621 - val_loss: 31.8661\n",
            "Epoch 161/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6187 - val_loss: 31.8362\n",
            "Epoch 162/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5978 - val_loss: 31.8373\n",
            "Epoch 163/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5268 - val_loss: 31.8277\n",
            "Epoch 164/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4320 - val_loss: 31.8401\n",
            "Epoch 165/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6646 - val_loss: 31.8452\n",
            "Epoch 166/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4264 - val_loss: 31.8514\n",
            "Epoch 167/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2681 - val_loss: 31.8622\n",
            "Epoch 168/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5097 - val_loss: 31.8600\n",
            "Epoch 169/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6306 - val_loss: 31.8461\n",
            "Epoch 170/1000\n",
            "84/88 [===========================>..] - ETA: 0s - loss: 25.8052roc-auc_val: 0.8196\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 25.7940 - val_loss: 31.8411\n",
            "Epoch 171/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6871 - val_loss: 31.8305\n",
            "Epoch 172/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6935 - val_loss: 31.8192\n",
            "Epoch 173/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4464 - val_loss: 31.8111\n",
            "Epoch 174/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4458 - val_loss: 31.8200\n",
            "Epoch 175/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4966 - val_loss: 31.8057\n",
            "Epoch 176/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4398 - val_loss: 31.8053\n",
            "Epoch 177/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5858 - val_loss: 31.8155\n",
            "Epoch 178/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4724 - val_loss: 31.8024\n",
            "Epoch 179/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5128 - val_loss: 31.7996\n",
            "Epoch 180/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 25.4713roc-auc_val: 0.8202\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 25.4416 - val_loss: 31.8019\n",
            "Epoch 181/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4832 - val_loss: 31.7802\n",
            "Epoch 182/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2255 - val_loss: 31.7954\n",
            "Epoch 183/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2277 - val_loss: 31.7992\n",
            "Epoch 184/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.1977 - val_loss: 31.7877\n",
            "Epoch 185/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.3729 - val_loss: 31.7830\n",
            "Epoch 186/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.3006 - val_loss: 31.7841\n",
            "Epoch 187/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6205 - val_loss: 31.7894\n",
            "Epoch 188/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4630 - val_loss: 31.7834\n",
            "Epoch 189/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2828 - val_loss: 31.7712\n",
            "Epoch 190/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 25.3859roc-auc_val: 0.8206\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 25.2490 - val_loss: 31.7653\n",
            "Epoch 191/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.1645 - val_loss: 31.7615\n",
            "Epoch 192/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2158 - val_loss: 31.7647\n",
            "Epoch 193/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.1712 - val_loss: 31.7780\n",
            "Epoch 194/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.1630 - val_loss: 31.7758\n",
            "Epoch 195/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.1263 - val_loss: 31.7660\n",
            "Epoch 196/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4133 - val_loss: 31.7554\n",
            "Epoch 197/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.1483 - val_loss: 31.7581\n",
            "Epoch 198/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.0817 - val_loss: 31.7548\n",
            "Epoch 199/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2078 - val_loss: 31.7637\n",
            "Epoch 200/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 24.9699roc-auc_val: 0.8209\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 24.9974 - val_loss: 31.7703\n",
            "Epoch 201/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2537 - val_loss: 31.7638\n",
            "Epoch 202/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.1936 - val_loss: 31.7740\n",
            "Epoch 203/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.9624 - val_loss: 31.7612\n",
            "Epoch 204/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.0880 - val_loss: 31.7703\n",
            "Epoch 205/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.9581 - val_loss: 31.7594\n",
            "Epoch 206/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.1801 - val_loss: 31.7695\n",
            "Epoch 207/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.0122 - val_loss: 31.7653\n",
            "Epoch 208/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8974 - val_loss: 31.7698\n",
            "Epoch 209/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7721 - val_loss: 31.7734\n",
            "Epoch 210/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 25.1474roc-auc_val: 0.821\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 25.0338 - val_loss: 31.7795\n",
            "Epoch 211/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.9935 - val_loss: 31.7601\n",
            "Epoch 212/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8652 - val_loss: 31.7647\n",
            "Epoch 213/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6742 - val_loss: 31.7634\n",
            "Epoch 214/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7450 - val_loss: 31.7659\n",
            "Epoch 215/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.9755 - val_loss: 31.7665\n",
            "Epoch 216/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7950 - val_loss: 31.7542\n",
            "Epoch 217/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8393 - val_loss: 31.7487\n",
            "Epoch 218/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.0273 - val_loss: 31.7494\n",
            "Epoch 219/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.9237 - val_loss: 31.7486\n",
            "Epoch 220/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 25.0816roc-auc_val: 0.8212\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 24.9268 - val_loss: 31.7578\n",
            "Epoch 221/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7848 - val_loss: 31.7590\n",
            "Epoch 222/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8625 - val_loss: 31.7625\n",
            "Epoch 223/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.9519 - val_loss: 31.7424\n",
            "Epoch 224/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7957 - val_loss: 31.7419\n",
            "Epoch 225/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.9760 - val_loss: 31.7505\n",
            "Epoch 226/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8296 - val_loss: 31.7474\n",
            "Epoch 227/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7551 - val_loss: 31.7497\n",
            "Epoch 228/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8376 - val_loss: 31.7497\n",
            "Epoch 229/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6537 - val_loss: 31.7515\n",
            "Epoch 230/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 24.7831roc-auc_val: 0.8216\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 24.7674 - val_loss: 31.7522\n",
            "Epoch 231/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5524 - val_loss: 31.7695\n",
            "Epoch 232/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8841 - val_loss: 31.7648\n",
            "Epoch 233/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6250 - val_loss: 31.7572\n",
            "Epoch 234/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4183 - val_loss: 31.7501\n",
            "Epoch 235/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6324 - val_loss: 31.7587\n",
            "Epoch 236/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6108 - val_loss: 31.7493\n",
            "Epoch 237/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4681 - val_loss: 31.7496\n",
            "Epoch 238/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8784 - val_loss: 31.7328\n",
            "Epoch 239/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8613 - val_loss: 31.7437\n",
            "Epoch 240/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 24.6770roc-auc_val: 0.8217\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 24.6219 - val_loss: 31.7516\n",
            "Epoch 241/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3845 - val_loss: 31.7378\n",
            "Epoch 242/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5254 - val_loss: 31.7396\n",
            "Epoch 243/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6033 - val_loss: 31.7452\n",
            "Epoch 244/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7799 - val_loss: 31.7448\n",
            "Epoch 245/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5613 - val_loss: 31.7550\n",
            "Epoch 246/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7043 - val_loss: 31.7512\n",
            "Epoch 247/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6817 - val_loss: 31.7602\n",
            "Epoch 248/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4910 - val_loss: 31.7630\n",
            "Epoch 249/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4380 - val_loss: 31.7398\n",
            "Epoch 250/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 24.4641roc-auc_val: 0.8218\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 24.4336 - val_loss: 31.7492\n",
            "Epoch 251/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5188 - val_loss: 31.7393\n",
            "Epoch 252/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5933 - val_loss: 31.7359\n",
            "Epoch 253/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6992 - val_loss: 31.7327\n",
            "Epoch 254/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5896 - val_loss: 31.7349\n",
            "Epoch 255/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7600 - val_loss: 31.7497\n",
            "Epoch 256/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5235 - val_loss: 31.7358\n",
            "Epoch 257/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2468 - val_loss: 31.7433\n",
            "Epoch 258/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5592 - val_loss: 31.7463\n",
            "Epoch 259/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7044 - val_loss: 31.7428\n",
            "Epoch 260/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 24.3657roc-auc_val: 0.822\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 24.4812 - val_loss: 31.7458\n",
            "Epoch 261/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5219 - val_loss: 31.7535\n",
            "Epoch 262/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2109 - val_loss: 31.7404\n",
            "Epoch 263/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3574 - val_loss: 31.7342\n",
            "Epoch 264/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2383 - val_loss: 31.7381\n",
            "Epoch 265/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6936 - val_loss: 31.7354\n",
            "Epoch 266/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5707 - val_loss: 31.7348\n",
            "Epoch 267/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3306 - val_loss: 31.7450\n",
            "Epoch 268/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3669 - val_loss: 31.7555\n",
            "Epoch 269/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4538 - val_loss: 31.7440\n",
            "Epoch 270/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 24.8236roc-auc_val: 0.8221\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 24.6821 - val_loss: 31.7422\n",
            "Epoch 271/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.1603 - val_loss: 31.7424\n",
            "Epoch 272/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3175 - val_loss: 31.7310\n",
            "Epoch 273/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4626 - val_loss: 31.7345\n",
            "Epoch 274/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4428 - val_loss: 31.7372\n",
            "Epoch 275/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4975 - val_loss: 31.7307\n",
            "Epoch 276/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5270 - val_loss: 31.7212\n",
            "Epoch 277/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2410 - val_loss: 31.7191\n",
            "Epoch 278/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3172 - val_loss: 31.7277\n",
            "Epoch 279/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3681 - val_loss: 31.7355\n",
            "Epoch 280/1000\n",
            "84/88 [===========================>..] - ETA: 0s - loss: 24.1675roc-auc_val: 0.8224\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 24.1960 - val_loss: 31.7347\n",
            "Epoch 281/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4303 - val_loss: 31.7404\n",
            "Epoch 282/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2246 - val_loss: 31.7358\n",
            "Epoch 283/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2900 - val_loss: 31.7338\n",
            "Epoch 284/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4175 - val_loss: 31.7329\n",
            "Epoch 285/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0046 - val_loss: 31.7321\n",
            "Epoch 286/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3555 - val_loss: 31.7288\n",
            "Epoch 287/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2237 - val_loss: 31.7257\n",
            "Epoch 288/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0556 - val_loss: 31.7158\n",
            "Epoch 289/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.1655 - val_loss: 31.7293\n",
            "Epoch 290/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 24.3749roc-auc_val: 0.8227\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 24.2624 - val_loss: 31.7220\n",
            "Epoch 291/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2390 - val_loss: 31.7200\n",
            "Epoch 292/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2427 - val_loss: 31.7195\n",
            "Epoch 293/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0343 - val_loss: 31.7129\n",
            "Epoch 294/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2558 - val_loss: 31.7209\n",
            "Epoch 295/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4259 - val_loss: 31.7349\n",
            "Epoch 296/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0895 - val_loss: 31.7358\n",
            "Epoch 297/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2613 - val_loss: 31.7345\n",
            "Epoch 298/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2439 - val_loss: 31.7212\n",
            "Epoch 299/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3005 - val_loss: 31.7196\n",
            "Epoch 300/1000\n",
            "84/88 [===========================>..] - ETA: 0s - loss: 24.0568roc-auc_val: 0.8228\n",
            "88/88 [==============================] - 12s 131ms/step - loss: 24.0319 - val_loss: 31.7171\n",
            "Epoch 301/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 24.3281 - val_loss: 31.7221\n",
            "Epoch 302/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 24.1415 - val_loss: 31.7173\n",
            "Epoch 303/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 23.9956 - val_loss: 31.7226\n",
            "Epoch 304/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.1142 - val_loss: 31.7153\n",
            "Epoch 305/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 24.1066 - val_loss: 31.7167\n",
            "Epoch 306/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 23.9840 - val_loss: 31.7153\n",
            "Epoch 307/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 24.0646 - val_loss: 31.7207\n",
            "Epoch 308/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 24.0204 - val_loss: 31.7311\n",
            "Epoch 309/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 24.0515 - val_loss: 31.7226\n",
            "Epoch 310/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 24.2960roc-auc_val: 0.8229\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 24.1711 - val_loss: 31.7098\n",
            "Epoch 311/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.1615 - val_loss: 31.7071\n",
            "Epoch 312/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.1312 - val_loss: 31.7129\n",
            "Epoch 313/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9304 - val_loss: 31.7255\n",
            "Epoch 314/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0647 - val_loss: 31.7423\n",
            "Epoch 315/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0829 - val_loss: 31.7353\n",
            "Epoch 316/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0141 - val_loss: 31.7268\n",
            "Epoch 317/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2924 - val_loss: 31.7165\n",
            "Epoch 318/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9888 - val_loss: 31.7198\n",
            "Epoch 319/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9684 - val_loss: 31.7319\n",
            "Epoch 320/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 23.7066roc-auc_val: 0.8229\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 23.7259 - val_loss: 31.7312\n",
            "Epoch 321/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8037 - val_loss: 31.7221\n",
            "Epoch 322/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.1551 - val_loss: 31.7293\n",
            "Epoch 323/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9528 - val_loss: 31.7254\n",
            "Epoch 324/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.1017 - val_loss: 31.7281\n",
            "Epoch 325/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8737 - val_loss: 31.7173\n",
            "Epoch 326/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9415 - val_loss: 31.7348\n",
            "Epoch 327/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9975 - val_loss: 31.7348\n",
            "Epoch 328/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9242 - val_loss: 31.7361\n",
            "Epoch 329/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9036 - val_loss: 31.7374\n",
            "Epoch 330/1000\n",
            "75/88 [========================>.....] - ETA: 0s - loss: 24.1061roc-auc_val: 0.8231\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 24.1338 - val_loss: 31.7237\n",
            "Epoch 331/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9974 - val_loss: 31.7279\n",
            "Epoch 332/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2254 - val_loss: 31.7163\n",
            "Epoch 333/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8094 - val_loss: 31.7251\n",
            "Epoch 334/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 23.8951 - val_loss: 31.7353\n",
            "Epoch 335/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8086 - val_loss: 31.7287\n",
            "Epoch 336/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8750 - val_loss: 31.7351\n",
            "Epoch 337/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8657 - val_loss: 31.7269\n",
            "Epoch 338/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 23.9316 - val_loss: 31.7156\n",
            "Epoch 339/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9421 - val_loss: 31.7226\n",
            "Epoch 340/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 23.7205roc-auc_val: 0.8232\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 23.8293 - val_loss: 31.7193\n",
            "Epoch 341/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0617 - val_loss: 31.7294\n",
            "Epoch 342/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8299 - val_loss: 31.7312\n",
            "Epoch 343/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8116 - val_loss: 31.7128\n",
            "Epoch 344/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9473 - val_loss: 31.7216\n",
            "Epoch 345/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8187 - val_loss: 31.7187\n",
            "Epoch 346/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.6993 - val_loss: 31.7177\n",
            "Epoch 347/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8899 - val_loss: 31.7172\n",
            "Epoch 348/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.7609 - val_loss: 31.7177\n",
            "Epoch 349/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.6689 - val_loss: 31.7217\n",
            "Epoch 350/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 23.9863roc-auc_val: 0.8233\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 23.9555 - val_loss: 31.7160\n",
            "Epoch 351/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.7308 - val_loss: 31.7183\n",
            "Epoch 352/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8150 - val_loss: 31.7174\n",
            "Epoch 353/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.6689 - val_loss: 31.7228\n",
            "Epoch 354/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.7938 - val_loss: 31.7197\n",
            "Epoch 355/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8166 - val_loss: 31.7143\n",
            "Epoch 356/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.7730 - val_loss: 31.7404\n",
            "Epoch 357/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.9241 - val_loss: 31.7553\n",
            "Epoch 358/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8109 - val_loss: 31.7273\n",
            "Epoch 359/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.6724 - val_loss: 31.7286\n",
            "Epoch 360/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 23.8694roc-auc_val: 0.8232\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 23.7472 - val_loss: 31.7447\n",
            "Epoch 361/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8580 - val_loss: 31.7510\n",
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 24ms/step - loss: 62.4318 - val_loss: 35.7867\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 55.2163 - val_loss: 34.9964\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 50.6992 - val_loss: 35.0950\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 48.1508 - val_loss: 35.0023\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 46.2341 - val_loss: 35.2751\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 45.0873 - val_loss: 35.3392\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 42.9147 - val_loss: 35.4236\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 43.5058 - val_loss: 35.4219\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 41.2491 - val_loss: 35.4973\n",
            "Epoch 10/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 41.5544roc-auc_val: 0.7609\n",
            "47/47 [==============================] - 11s 236ms/step - loss: 41.3928 - val_loss: 35.2764\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 39.8629 - val_loss: 35.3039\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 39.9186 - val_loss: 35.5026\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 39.2721 - val_loss: 35.6594\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 39.0434 - val_loss: 35.7337\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 37.7168 - val_loss: 35.7539\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 37.9106 - val_loss: 35.7054\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 37.1791 - val_loss: 35.7309\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 37.1757 - val_loss: 35.4849\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 36.2102 - val_loss: 35.3696\n",
            "Epoch 20/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 35.9744roc-auc_val: 0.768\n",
            "47/47 [==============================] - 11s 235ms/step - loss: 36.1326 - val_loss: 35.3233\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 35.4087 - val_loss: 35.4264\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 35.8489 - val_loss: 35.5035\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 34.5066 - val_loss: 35.3765\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.5254 - val_loss: 35.3699\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 34.7158 - val_loss: 35.3060\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 34.0382 - val_loss: 35.2564\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 33.6382 - val_loss: 35.2342\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 34.3519 - val_loss: 35.2956\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 33.4169 - val_loss: 35.1958\n",
            "Epoch 30/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 33.6725roc-auc_val: 0.7761\n",
            "47/47 [==============================] - 11s 233ms/step - loss: 33.2167 - val_loss: 35.1183\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.6474 - val_loss: 35.1200\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 33.2735 - val_loss: 35.2002\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 32.9099 - val_loss: 35.1455\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.1592 - val_loss: 35.0805\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 32.4598 - val_loss: 35.0135\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 32.0511 - val_loss: 35.0335\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 32.2785 - val_loss: 34.9447\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.4763 - val_loss: 34.9605\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 31.4152 - val_loss: 35.0467\n",
            "Epoch 40/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 32.1251roc-auc_val: 0.7801\n",
            "47/47 [==============================] - 11s 237ms/step - loss: 32.0110 - val_loss: 35.0220\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 32.5710 - val_loss: 35.0400\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 31.6246 - val_loss: 35.0753\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 31.6934 - val_loss: 35.0352\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.8158 - val_loss: 34.9934\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.7656 - val_loss: 34.9870\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.2260 - val_loss: 34.9881\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 31.0335 - val_loss: 35.0547\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.6170 - val_loss: 35.0449\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.7463 - val_loss: 35.0011\n",
            "Epoch 50/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 30.5261roc-auc_val: 0.783\n",
            "47/47 [==============================] - 11s 234ms/step - loss: 30.2351 - val_loss: 34.9845\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.7072 - val_loss: 34.9946\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.7114 - val_loss: 34.9778\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.1903 - val_loss: 34.9962\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.0987 - val_loss: 35.0369\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.5666 - val_loss: 35.0334\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.0956 - val_loss: 35.0271\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.8353 - val_loss: 35.0596\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.4281 - val_loss: 35.0132\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.4900 - val_loss: 35.0160\n",
            "Epoch 60/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 28.8641roc-auc_val: 0.7844\n",
            "47/47 [==============================] - 11s 235ms/step - loss: 28.9142 - val_loss: 35.0705\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.4630 - val_loss: 35.0155\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.3642 - val_loss: 35.0210\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.0984 - val_loss: 34.9587\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.5644 - val_loss: 34.9625\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.1219 - val_loss: 34.9373\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.6752 - val_loss: 34.9631\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.6656 - val_loss: 34.9644\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.1284 - val_loss: 34.9783\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.0066 - val_loss: 34.9270\n",
            "Epoch 70/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 29.3985roc-auc_val: 0.7877\n",
            "47/47 [==============================] - 11s 237ms/step - loss: 29.2336 - val_loss: 34.9240\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.7337 - val_loss: 34.9300\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.8764 - val_loss: 34.9735\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.3045 - val_loss: 34.9678\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.8953 - val_loss: 34.9664\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.0789 - val_loss: 34.9700\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.1150 - val_loss: 34.9830\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.3943 - val_loss: 34.9525\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.0037 - val_loss: 34.9628\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.6713 - val_loss: 34.9843\n",
            "Epoch 80/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 28.3826roc-auc_val: 0.7885\n",
            "47/47 [==============================] - 11s 235ms/step - loss: 28.1632 - val_loss: 34.9928\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.4910 - val_loss: 34.9953\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.0555 - val_loss: 35.0065\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.1697 - val_loss: 35.0346\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.9195 - val_loss: 35.0295\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.5361 - val_loss: 35.0328\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.8899 - val_loss: 35.0277\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.8453 - val_loss: 35.0287\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.5635 - val_loss: 35.0676\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.6559 - val_loss: 35.0822\n",
            "Epoch 90/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 27.7278roc-auc_val: 0.7893\n",
            "47/47 [==============================] - 11s 234ms/step - loss: 27.6011 - val_loss: 35.0479\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.5358 - val_loss: 35.0287\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.3694 - val_loss: 35.0679\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.3424 - val_loss: 35.0708\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.0086 - val_loss: 35.0934\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.3676 - val_loss: 35.1362\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.4053 - val_loss: 35.1820\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.3963 - val_loss: 35.1944\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.1042 - val_loss: 35.2190\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.3798 - val_loss: 35.2465\n",
            "Epoch 100/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 27.3911roc-auc_val: 0.7896\n",
            "47/47 [==============================] - 11s 233ms/step - loss: 27.1183 - val_loss: 35.2638\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.7154 - val_loss: 35.2497\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.1179 - val_loss: 35.2321\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.0340 - val_loss: 35.2500\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.9695 - val_loss: 35.2290\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.8488 - val_loss: 35.2512\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.4109 - val_loss: 35.2516\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.9719 - val_loss: 35.2882\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.8185 - val_loss: 35.3172\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.9246 - val_loss: 35.3090\n",
            "Epoch 110/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 26.8336roc-auc_val: 0.7906\n",
            "47/47 [==============================] - 11s 236ms/step - loss: 27.0400 - val_loss: 35.3023\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.4829 - val_loss: 35.2958\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.9781 - val_loss: 35.3217\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.8449 - val_loss: 35.3605\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.6286 - val_loss: 35.3855\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.8361 - val_loss: 35.3877\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.3568 - val_loss: 35.4051\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.5948 - val_loss: 35.4027\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.3350 - val_loss: 35.3958\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.3951 - val_loss: 35.3972\n",
            "Epoch 120/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 26.6045roc-auc_val: 0.7877\n",
            "47/47 [==============================] - 12s 254ms/step - loss: 26.4464 - val_loss: 35.4357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnpeTPNLkiCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "96a4ef23-a977-40f9-e96d-12f6bd1d5f12"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6S7Pv_rc-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}
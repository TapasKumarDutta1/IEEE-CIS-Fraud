{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_Preprocessingand adding columns_with_id.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapasKumarDutta1/IEEE-CIS-Fraud/blob/master/Cleaning_Preprocessingand_adding_columns_with_id.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "KamagHOZfwrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAd-jYpbhqod"
      },
      "source": [
        "Getting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iio63rQJpTwo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "539b2294-2f19-4d81-804c-6d22078ff777"
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_identity.csv.zip to /content\n",
            "\r  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 52.3MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 84% 49.0M/58.3M [00:02<00:00, 11.7MB/s]\n",
            "100% 58.3M/58.3M [00:02<00:00, 20.6MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 108MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 94% 49.0M/52.2M [00:01<00:00, 20.8MB/s]\n",
            "100% 52.2M/52.2M [00:01<00:00, 33.8MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 155MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLp-FegbHCEy"
      },
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4o8Kitosplu"
      },
      "source": [
        "Loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "a056967c-3898-4e8d-fbc2-56fb73ea3195"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adbp3dS8srOW"
      },
      "source": [
        "Reading datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "# Load the training dataset from 'train_transaction.csv.zip'\n",
        "trn = pd.read_csv('train_transaction.csv.zip')\n",
        "\n",
        "# Extract the 'isFraud' column from the training dataset and store it separately as 'isFraud'\n",
        "isFraud = trn['isFraud']\n",
        "\n",
        "# Drop the 'isFraud' column from the training dataset\n",
        "trn = trn.drop(columns=['isFraud'])\n",
        "\n",
        "# Load the testing dataset from 'test_transaction.csv.zip'\n",
        "tst = pd.read_csv('test_transaction.csv.zip')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLiZLu3Wssz5"
      },
      "source": [
        "Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4"
      },
      "source": [
        "\n",
        "# Extract the column names of the first 55 columns (excluding 'TransactionID') from the train dataset\n",
        "useful_cols = list(trn.iloc[:, :55])\n",
        "useful_cols.remove('TransactionID')\n",
        "\n",
        "# Keep only the useful columns in the train dataset\n",
        "trn = trn[useful_cols]\n",
        "\n",
        "# Keep only the useful columns in the test dataset\n",
        "tst = tst[useful_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ji3GQS9G0LR"
      },
      "source": [
        "Creating unique id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gcZZoZ_G2XF"
      },
      "source": [
        "# Add a new column \"day\" to the train dataset, representing the number of days from \"TransactionDT\"\n",
        "trn[\"day\"] = trn[\"TransactionDT\"] // 86400\n",
        "\n",
        "# Add a new column \"day\" to the test dataset, representing the number of days from \"TransactionDT\"\n",
        "tst[\"day\"] = tst[\"TransactionDT\"] // 86400\n",
        "\n",
        "# Create new empty columns \"id\" in both train and test datasets\n",
        "trn[\"id\"] = \"\"\n",
        "tst[\"id\"] = \"\"\n",
        "\n",
        "# Calculate the difference \"d_1\" by subtracting the value in column \"D1\" from the \"day\"\n",
        "trn[\"d_1\"] = trn[\"day\"] - trn[\"D1\"]\n",
        "tst[\"d_1\"] = tst[\"day\"] - tst[\"D1\"]\n",
        "\n",
        "# Concatenate selected columns to create the new \"id\" for train and test datasets\n",
        "for col in [\"ProductCD\", \"P_emaildomain\", \"addr1\", \"card1\", \"d_1\"]:\n",
        "    trn[\"id\"] += trn[col].astype(str)\n",
        "    tst[\"id\"] += tst[col].astype(str)\n",
        "\n",
        "# Store the \"id\" columns in separate variables for train and test datasets\n",
        "id_trn = trn[\"id\"]\n",
        "id_tst = tst[\"id\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XXuMDeYGWuW"
      },
      "source": [
        "seperate columns in continuous and categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp_n9G6mH0QG"
      },
      "source": [
        "# List of categorical columns\n",
        "categorical = [\n",
        "    \"ProductCD\",\n",
        "    \"card1\",\n",
        "    \"card2\",\n",
        "    \"card3\",\n",
        "    \"card4\",\n",
        "    \"card5\",\n",
        "    \"card6\",\n",
        "    \"addr1\",\n",
        "    \"addr2\",\n",
        "    \"P_emaildomain\",\n",
        "    \"R_emaildomain\",\n",
        "] + [\"M\" + str(i) for i in range(1, 10)]\n",
        "\n",
        "# List of continuous columns (excluding categorical columns)\n",
        "continuous = [i for i in useful_cols if i not in categorical]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO4xzVhmGTra"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHEa18NyJBBs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e0d9d595-4bd6-4f21-dc92-b3cd4bf697b1"
      },
      "source": [
        "\n",
        "class ContinuousConverter:\n",
        "    \"\"\"\n",
        "    A class to transform continuous features using logarithmic transformation and standardization.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): Name of the continuous feature.\n",
        "        skew (float): Skewness of the continuous feature.\n",
        "        log_transform (function): Logarithmic transformation function.\n",
        "\n",
        "    Methods:\n",
        "        transform(feature): Applies logarithmic transformation and standardization to the feature.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, log_transform, feature):\n",
        "        \"\"\"\n",
        "        Initialize the ContinuousConverter.\n",
        "\n",
        "        Args:\n",
        "            name (str): Name of the continuous feature.\n",
        "            log_transform (function): Logarithmic transformation function.\n",
        "            feature (pandas.Series): The continuous feature to be transformed.\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.skew = feature.skew()\n",
        "        self.log_transform = log_transform\n",
        "\n",
        "    def transform(self, feature):\n",
        "        \"\"\"\n",
        "        Applies logarithmic transformation and standardization to the given feature.\n",
        "\n",
        "        Args:\n",
        "            feature (pandas.Series): The continuous feature to be transformed.\n",
        "\n",
        "        Returns:\n",
        "            pandas.Series: The transformed feature.\n",
        "        \"\"\"\n",
        "        if self.skew > 1:\n",
        "            feature = self.log_transform(feature)\n",
        "        mean = feature.mean()\n",
        "        std = feature.std()\n",
        "        feature = (feature - mean) / (std + 1e-6)\n",
        "        return feature\n",
        "\n",
        "# Initialize lists to store transformed continuous features\n",
        "continuous_trn = []\n",
        "continuous_tst = []\n",
        "\n",
        "# List to store the new id column names\n",
        "id_cols = []\n",
        "\n",
        "# Compute the mean and standard deviation for each continuous column grouped by 'id' in the training dataset\n",
        "for col in tqdm(continuous):\n",
        "    id_cols += [col + \"_mean\", col + \"_std\"]\n",
        "    trn[col + \"_mean\"] = trn.groupby([\"id\"])[col].transform(\"mean\")\n",
        "    trn[col + \"_std\"] = trn.groupby([\"id\"])[col].transform(\"std\")\n",
        "\n",
        "    # Compute the mean and standard deviation for each continuous column grouped by 'id' in the testing dataset\n",
        "    tst[col + \"_mean\"] = tst.groupby([\"id\"])[col].transform(\"mean\")\n",
        "    tst[col + \"_std\"] = tst.groupby([\"id\"])[col].transform(\"std\")\n",
        "\n",
        "# Concatenate the new id column names with the continuous columns list\n",
        "continuous += id_cols\n",
        "\n",
        "# Apply log transformation and standardization to the continuous columns in both train and test datasets\n",
        "for col in tqdm(continuous):\n",
        "    log = lambda x: np.log10(x + (1 - min(0, min(x))))\n",
        "    converter = ContinuousConverter(col, log, trn[col])\n",
        "    continuous_trn.append(converter.transform(trn[col]).astype(np.float32))\n",
        "    continuous_tst.append(converter.transform(tst[col]).astype(np.float32))\n",
        "\n",
        "# Convert the transformed continuous features to DataFrames\n",
        "continuous_trn = pd.DataFrame(continuous_trn).T\n",
        "continuous_trn.columns = continuous\n",
        "continuous_tst = pd.DataFrame(continuous_tst).T\n",
        "continuous_tst.columns = continuous"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 34/34 [01:48<00:00,  3.19s/it]\n",
            "100%|██████████| 102/102 [00:13<00:00,  7.49it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF10rHW3P8Gz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAm5VK2_LTuO"
      },
      "source": [
        "# Calculate the sum of missing values for each row in the train dataset\n",
        "continuous_trn[\"isna_sum\"] = continuous_trn.isna().sum(axis=1)\n",
        "\n",
        "# Calculate the sum of missing values for each row in the test dataset\n",
        "continuous_tst[\"isna_sum\"] = continuous_tst.isna().sum(axis=1)\n",
        "\n",
        "# Standardize the 'isna_sum' column in the train dataset\n",
        "mn = continuous_trn[\"isna_sum\"].mean()\n",
        "std = continuous_trn[\"isna_sum\"].std()\n",
        "continuous_trn[\"isna_sum\"] = (continuous_trn[\"isna_sum\"] - mn) / std\n",
        "\n",
        "# Standardize the 'isna_sum' column in the test dataset\n",
        "mn = continuous_tst[\"isna_sum\"].mean()\n",
        "std = continuous_tst[\"isna_sum\"].std()\n",
        "continuous_tst[\"isna_sum\"] = (continuous_tst[\"isna_sum\"] - mn) / std\n",
        "\n",
        "# Initialize a list to store the column names with missing values\n",
        "isna_cols = []\n",
        "\n",
        "# Handle missing values for each continuous column\n",
        "for col in continuous:\n",
        "    # Check if there are any missing values in the column for the train dataset\n",
        "    isna = continuous_trn[col].isna()\n",
        "\n",
        "    # If missing values are present in the column, create a new binary column to indicate missingness\n",
        "    if isna.sum() > 0:\n",
        "        continuous_trn[col + \"_isna\"] = isna.astype(int)\n",
        "        continuous_tst[col + \"_isna\"] = continuous_tst[col].isna().astype(int)\n",
        "\n",
        "    # Fill missing values with the mean of the column in both train and test datasets\n",
        "    continuous_trn[col] = continuous_trn[col].fillna(continuous_trn[col].mean())\n",
        "    continuous_tst[col] = continuous_tst[col].fillna(continuous_tst[col].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gloxSjblyL9R"
      },
      "source": [
        "Handling categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39hx7kZUMrCD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "69ef45ff-b42b-4a56-8c35-678f1bc9ec99"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def handle_categorical(df_trn, df_tst, categorical, n_values):\n",
        "    \"\"\"\n",
        "    Handle categorical features by one-hot encoding and converting to binary features.\n",
        "\n",
        "    Args:\n",
        "        df_trn (pandas.DataFrame): Train dataset containing the categorical features.\n",
        "        df_tst (pandas.DataFrame): Test dataset containing the categorical features.\n",
        "        categorical (list): List of column names with categorical features.\n",
        "        n_values (int): Number of unique values to keep for each categorical feature.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame, pandas.DataFrame: The one-hot encoded train and test datasets.\n",
        "    \"\"\"\n",
        "    all = []\n",
        "    df_trn = df_trn[categorical]\n",
        "    df_tst = df_tst[categorical]\n",
        "    for col in categorical:\n",
        "        all.append(\n",
        "            list(df_trn[col].value_counts().iloc[: n_values - 1].index) + [\"Other\"]\n",
        "        )\n",
        "        df_trn[col] = df_trn[col].map(lambda x: x if x in all[-1] else \"Other\")\n",
        "        df_tst[col] = df_tst[col].map(lambda x: x if x in all[-1] else \"Other\")\n",
        "    ohe = OneHotEncoder(categories=all)\n",
        "    ohe.fit(pd.concat([df_trn, df_tst]))\n",
        "    df_trn = pd.DataFrame(ohe.transform(df_trn).toarray()).astype(np.float16)\n",
        "    df_tst = pd.DataFrame(ohe.transform(df_tst).toarray()).astype(np.float16)\n",
        "    return df_trn, df_tst\n",
        "\n",
        "# Handle categorical features using one-hot encoding and keep 50 unique values for each categorical feature\n",
        "df_trn, df_tst = handle_categorical(trn, tst, categorical, 50)\n",
        "\n",
        "# Display the shapes of the one-hot encoded train and test datasets\n",
        "df_trn.shape, df_tst.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((590540, 444), (506691, 444))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaVRioYwPtcb"
      },
      "source": [
        "Concatenating continuous and categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54FObgPxwmHD"
      },
      "source": [
        "# Concatenate the one-hot encoded categorical features and the transformed continuous features for the train dataset\n",
        "trn = pd.concat([df_trn, continuous_trn], axis=1)\n",
        "\n",
        "# Concatenate the one-hot encoded categorical features and the transformed continuous features for the test dataset\n",
        "tst = pd.concat([df_tst, continuous_tst], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c7bjtcuRKD5"
      },
      "source": [
        "Columns created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZxYP9a7x7Q8"
      },
      "source": [
        "# Add the \"isFraud\" column back to the train dataset\n",
        "trn[\"isFraud\"] = isFraud\n",
        "\n",
        "# Save the test dataset to a CSV file named \"test_id.csv\"\n",
        "tst.to_csv(\"/content/gdrive/My Drive/fraud/test_id.csv\")\n",
        "\n",
        "# Save the train dataset to a CSV file named \"train_id.csv\"\n",
        "trn.to_csv(\"/content/gdrive/My Drive/fraud/train_id.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITI5a2mTTypz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_Preprocessingand adding columns.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapasKumarDutta1/IEEE-CIS-Fraud/blob/master/Cleaning_Preprocessingand_adding_columns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAd-jYpbhqod"
      },
      "source": [
        "Getting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iio63rQJpTwo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "2cec87f0-d256-4d71-a970-4f45246bd528"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "test_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4o8Kitosplu"
      },
      "source": [
        "Loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "56e2178b-334b-4db7-d7fb-6523ef705554"
      },
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Mount Google Drive for data access\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adbp3dS8srOW"
      },
      "source": [
        "Reading datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "# Read the train_transaction.csv.zip file into the 'trn' DataFrame\n",
        "trn = pd.read_csv(\"train_transaction.csv.zip\")\n",
        "\n",
        "# Extract the 'isFraud' column from 'trn' and store it separately in 'isFraud' variable\n",
        "isFraud = trn[\"isFraud\"]\n",
        "\n",
        "# Drop the 'isFraud' column from the 'trn' DataFrame\n",
        "trn = trn.drop([\"isFraud\"], axis=1)\n",
        "\n",
        "# Read the test_transaction.csv.zip file into the 'tst' DataFrame\n",
        "tst = pd.read_csv(\"test_transaction.csv.zip\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLiZLu3Wssz5"
      },
      "source": [
        "Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4"
      },
      "source": [
        "# Extract the names of useful columns (excluding \"TransactionID\") from the train dataset\n",
        "useful_cols = list(trn.iloc[:, :55])\n",
        "useful_cols.remove(\"TransactionID\")\n",
        "\n",
        "# Keep only the useful columns in the train dataset\n",
        "trn = trn[useful_cols]\n",
        "\n",
        "# Keep only the useful columns in the test dataset\n",
        "tst = tst[useful_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ji3GQS9G0LR"
      },
      "source": [
        "Creating unique id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gcZZoZ_G2XF"
      },
      "source": [
        "# Calculate the day from the \"TransactionDT\" in seconds and assign it to a new column \"day\"\n",
        "trn[\"day\"] = trn[\"TransactionDT\"] // 86400\n",
        "tst[\"day\"] = tst[\"TransactionDT\"] // 86400\n",
        "\n",
        "# Create new empty columns \"id\" in both train and test datasets\n",
        "trn[\"id\"] = \"\"\n",
        "tst[\"id\"] = \"\"\n",
        "\n",
        "# Calculate the difference \"d_1\" by subtracting the value in column \"D1\" from the \"day\"\n",
        "trn[\"d_1\"] = trn[\"day\"] - trn[\"D1\"]\n",
        "tst[\"d_1\"] = tst[\"day\"] - tst[\"D1\"]\n",
        "\n",
        "# Concatenate selected columns to create the new \"id\" for train and test datasets\n",
        "for col in [\"C1\", \"C13\", \"P_emaildomain\", \"addr1\", \"card1\", \"d_1\"]:\n",
        "    trn[\"id\"] += trn[col].astype(str)\n",
        "    tst[\"id\"] += tst[col].astype(str)\n",
        "\n",
        "# Store the \"id\" columns in separate variables for train and test datasets\n",
        "id_trn = trn[\"id\"]\n",
        "id_tst = tst[\"id\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XXuMDeYGWuW"
      },
      "source": [
        "seperate columns in continuous and categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp_n9G6mH0QG"
      },
      "source": [
        "# List of categorical features\n",
        "categorical = [\n",
        "    \"ProductCD\",\n",
        "    \"card1\",\n",
        "    \"card2\",\n",
        "    \"card3\",\n",
        "    \"card4\",\n",
        "    \"card5\",\n",
        "    \"card6\",\n",
        "    \"addr1\",\n",
        "    \"addr2\",\n",
        "    \"P_emaildomain\",\n",
        "    \"R_emaildomain\",\n",
        "] + [\"M\" + str(i) for i in range(1, 10)]\n",
        "\n",
        "# List of continuous features\n",
        "continuous = [i for i in useful_cols if i not in categorical]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO4xzVhmGTra"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX1lpuQrGVE7"
      },
      "source": [
        "class ContinuousConverter:\n",
        "    def __init__(self, name, log_transform=False):\n",
        "        \"\"\"\n",
        "        Initialize a ContinuousConverter instance.\n",
        "\n",
        "        Parameters:\n",
        "            name (str): The name of the continuous feature.\n",
        "            log_transform (bool, optional): Whether to apply log-transform on the feature. Default is False.\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.skew = None\n",
        "        self.log_transform = log_transform\n",
        "\n",
        "    def transform(self, feature):\n",
        "        \"\"\"\n",
        "        Transform the continuous feature.\n",
        "\n",
        "        Parameters:\n",
        "            feature (pd.Series): The continuous feature to be transformed.\n",
        "\n",
        "        Returns:\n",
        "            pd.Series: Transformed continuous feature.\n",
        "        \"\"\"\n",
        "        if self.log_transform:\n",
        "            feature = self._apply_log_transform(feature)\n",
        "\n",
        "        mean = feature.mean()\n",
        "        std = feature.std()\n",
        "        feature = (feature - mean) / (std + 1e-6)\n",
        "        return feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHEa18NyJBBs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "aa0ef769-b78d-4aa9-95ec-78c5260b2fc2"
      },
      "source": [
        "continuous_trn = []\n",
        "continuous_tst = []\n",
        "\n",
        "# Iterate through continuous features and perform transformations\n",
        "for col in tqdm(continuous):\n",
        "    # Define the log transformation function\n",
        "    log = lambda x: np.log10(x + (1 - min(0, min(x))))\n",
        "    # Initialize the ContinuousConverter object\n",
        "    converter = ContinuousConverter(col, log, trn[col])\n",
        "    # Apply the transformation on training data and append to continuous_trn list\n",
        "    continuous_trn.append(converter.transform(trn[col]).astype(np.float32))\n",
        "    # Apply the transformation on test data and append to continuous_tst list\n",
        "    continuous_tst.append(converter.transform(tst[col]).astype(np.float32))\n",
        "\n",
        "# Create pandas DataFrame from the transformed lists\n",
        "continuous_trn = pd.DataFrame(continuous_trn).T\n",
        "continuous_trn.columns = continuous\n",
        "continuous_tst = pd.DataFrame(continuous_tst).T\n",
        "continuous_tst.columns = continuous"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 34/34 [00:04<00:00,  7.40it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF10rHW3P8Gz"
      },
      "source": [
        "# Calculate the sum of NaN values for each row in 'continuous_trn' DataFrame and add a new column 'isna_sum'\n",
        "continuous_trn[\"isna_sum\"] = continuous_trn.isna().sum(axis=1)\n",
        "\n",
        "# Calculate the sum of NaN values for each row in 'continuous_tst' DataFrame and add a new column 'isna_sum'\n",
        "continuous_tst[\"isna_sum\"] = continuous_tst.isna().sum(axis=1)\n",
        "\n",
        "# Calculate the mean and standard deviation of 'isna_sum' column in 'continuous_trn' DataFrame\n",
        "mn = continuous_trn[\"isna_sum\"].mean()\n",
        "std = continuous_trn[\"isna_sum\"].std()\n",
        "\n",
        "# Normalize the 'isna_sum' column in 'continuous_trn' DataFrame\n",
        "continuous_trn[\"isna_sum\"] = (continuous_trn[\"isna_sum\"] - mn) / std\n",
        "\n",
        "# Calculate the mean and standard deviation of 'isna_sum' column in 'continuous_tst' DataFrame\n",
        "mn = continuous_tst[\"isna_sum\"].mean()\n",
        "std = continuous_tst[\"isna_sum\"].std()\n",
        "\n",
        "# Normalize the 'isna_sum' column in 'continuous_tst' DataFrame\n",
        "continuous_tst[\"isna_sum\"] = (continuous_tst[\"isna_sum\"] - mn) / std\n",
        "\n",
        "# Create a list 'isna_cols' to store column names with missing values\n",
        "isna_cols = []\n",
        "\n",
        "# Loop through each column in 'continuous' list\n",
        "for col in continuous:\n",
        "    # Check if the column has any missing values\n",
        "    isna = continuous_trn[col].isna()\n",
        "    if isna.sum() > 0:\n",
        "        # Add a new binary column to indicate whether the original column has missing values\n",
        "        continuous_trn[col + \"_isna\"] = isna.astype(int)\n",
        "        continuous_tst[col + \"_isna\"] = continuous_tst[col].isna().astype(int)\n",
        "\n",
        "# Fill missing values in 'continuous_trn' and 'continuous_tst' DataFrames with 0\n",
        "continuous_trn = continuous_trn.fillna(0)\n",
        "continuous_tst = continuous_tst.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gloxSjblyL9R"
      },
      "source": [
        "Handling categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39hx7kZUMrCD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "532a6cf5-ae2f-4371-e1d8-b11af26d36e7"
      },
      "source": [
        "def handle_categorical(df_trn, df_tst, categorical, n_values):\n",
        "    \"\"\"\n",
        "    Encode categorical features using one-hot encoding and handle rare categories.\n",
        "\n",
        "    Parameters:\n",
        "        df_trn (pd.DataFrame): Training data DataFrame.\n",
        "        df_tst (pd.DataFrame): Test data DataFrame.\n",
        "        categorical (list): List of categorical feature names to be encoded.\n",
        "        n_values (int): Number of top frequent values to keep for each categorical feature.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame, pd.DataFrame: Encoded training and test DataFrames.\n",
        "    \"\"\"\n",
        "    all_categories = []\n",
        "    df_trn = df_trn[categorical]\n",
        "    df_tst = df_tst[categorical]\n",
        "\n",
        "    for col in categorical:\n",
        "        # Get top frequent values and \"Other\" for remaining values\n",
        "        all_categories.append(list(df_trn[col].value_counts().iloc[: n_values - 1].index) + [\"Other\"])\n",
        "\n",
        "        # Map values to top frequent values or \"Other\"\n",
        "        df_trn[col] = df_trn[col].map(lambda x: x if x in all_categories[-1] else \"Other\")\n",
        "        df_tst[col] = df_tst[col].map(lambda x: x if x in all_categories[-1] else \"Other\")\n",
        "\n",
        "    # One-hot encoding using top frequent categories for each feature\n",
        "    ohe = OneHotEncoder(categories=all_categories)\n",
        "    ohe.fit(pd.concat([df_trn, df_tst]))\n",
        "\n",
        "    # Transform the data into one-hot encoded DataFrame\n",
        "    df_trn_encoded = pd.DataFrame(ohe.transform(df_trn).toarray()).astype(np.float16)\n",
        "    df_tst_encoded = pd.DataFrame(ohe.transform(df_tst).toarray()).astype(np.float16)\n",
        "\n",
        "    return df_trn_encoded, df_tst_encoded\n",
        "\n",
        "# Example usage\n",
        "# Assuming 'trn' and 'tst' are already defined DataFrames\n",
        "categorical_features = [...]  # List of categorical feature names\n",
        "num_top_values = 50  # Number of top frequent values to keep for each categorical feature\n",
        "df_trn_encoded, df_tst_encoded = handle_categorical(trn, tst, categorical_features, num_top_values)\n",
        "\n",
        "print(\"Encoded Training Data Shape:\", df_trn_encoded.shape)\n",
        "print(\"Encoded Test Data Shape:\", df_tst_encoded.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((590540, 444), (506691, 444))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaVRioYwPtcb"
      },
      "source": [
        "Concatenating continuous and categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54FObgPxwmHD"
      },
      "source": [
        "# Concatenate 'df_trn' and 'continuous_trn' DataFrames horizontally (axis=1)\n",
        "trn = pd.concat([df_trn, continuous_trn], axis=1)\n",
        "\n",
        "# Concatenate 'df_tst' and 'continuous_tst' DataFrames horizontally (axis=1)\n",
        "tst = pd.concat([df_tst, continuous_tst], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c7bjtcuRKD5"
      },
      "source": [
        "Columns created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZxYP9a7x7Q8"
      },
      "source": [
        "\n",
        "# Add 'id', 'isFraud' columns to the DataFrames\n",
        "trn[\"id\"] = id_trn\n",
        "tst[\"id\"] = id_tst\n",
        "trn[\"isFraud\"] = isFraud\n",
        "\n",
        "# Save DataFrames to CSV files\n",
        "tst.to_csv(\"/content/gdrive/My Drive/fraud/test.csv\", index=False)  # Set index=False to exclude the index column\n",
        "trn.to_csv(\"/content/gdrive/My Drive/fraud/train.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoenc_prep_without_id_with_V.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/autoenc_prep_without_id_with_V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRxz3A0w3Nb9",
        "colab_type": "text"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJr746_23M0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "import keras\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import *\n",
        "from keras import backend as K\n",
        "from keras.utils import Sequence\n",
        "from math import *\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "11745b26-2203-4110-a5f6-ecc1baf277fb"
      },
      "source": [
        "\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JScP1jz82u0P",
        "colab_type": "text"
      },
      "source": [
        "Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJem4-mp8otc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a9f8551-5f3a-4999-ad75-055a7b9d20cb"
      },
      "source": [
        "\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "trn=trn.drop(['isFraud','id'],1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (210,222) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxsP2nEt2wJf",
        "colab_type": "text"
      },
      "source": [
        "seperating data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXizGpQm2ZB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats=list(trn.select_dtypes(include=object))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk9P-M622yh7",
        "colab_type": "text"
      },
      "source": [
        "Label Encoding data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkujRQIB_CUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "bf59ad44-bd22-422c-fe74-da472edf6edf"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "for col in tqdm(cats):\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/21 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "100%|██████████| 21/21 [01:27<00:00,  4.16s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOdYYbZQ7IIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats+=list(trn.filter(regex='dum'))\n",
        "no_dum=[i for i in list(trn) if i not in cats]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFXHiTqX21EM",
        "colab_type": "text"
      },
      "source": [
        "Reducing memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrCs8PWA_LWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "7d186ad9-d7bc-4ef5-d064-14c6c738b05d"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2545.59 MB\n",
            "Memory usage after optimization is: 647.66 MB\n",
            "Decreased by 74.6%\n",
            "Memory usage of dataframe is 2188.01 MB\n",
            "Memory usage after optimization is: 573.15 MB\n",
            "Decreased by 73.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8azxXaFE8T0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "np.random.seed(42) # NumPy\n",
        "random.seed(42) # Python"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf25ibJI8T5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "def create_model():\n",
        "    K.clear_session()\n",
        "    num_inp = Input(shape=(num_shape,))\n",
        "    cat_inp = Input(shape=(cat_shape,))\n",
        "    inps = concatenate([num_inp, cat_inp])\n",
        "    x = Dense(512, activation=custom_gelu)(inps)\n",
        "    x = Dense(256, activation=custom_gelu)(x)\n",
        "    x = Dense(512, activation = custom_gelu)(x)\n",
        "    x = Dropout(.2)(x)\n",
        "    cat_out = Dense(cat_shape, activation = \"linear\")(x)\n",
        "    num_out = Dense(num_shape, activation = \"linear\")(x)\n",
        "    model = Model(inputs=[num_inp,cat_inp], outputs=[num_out, cat_out])\n",
        "    model.compile(\n",
        "        optimizer=Adam(.05, clipnorm = 1, clipvalue = 1),\n",
        "        loss=[\"mse\", \"mse\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "class WarmUpLearningRateScheduler(keras.callbacks.Callback):\n",
        "    \"\"\"Warmup learning rate scheduler\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, warmup_batches, init_lr, verbose=0):\n",
        "        \"\"\"Constructor for warmup learning rate scheduler\n",
        "\n",
        "        Arguments:\n",
        "            warmup_batches {int} -- Number of batch for warmup.\n",
        "            init_lr {float} -- Learning rate after warmup.\n",
        "\n",
        "        Keyword Arguments:\n",
        "            verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n",
        "        \"\"\"\n",
        "\n",
        "        super(WarmUpLearningRateScheduler, self).__init__()\n",
        "        self.warmup_batches = warmup_batches\n",
        "        self.init_lr = init_lr\n",
        "        self.verbose = verbose\n",
        "        self.batch_count = 0\n",
        "        self.learning_rates = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.batch_count = self.batch_count + 1\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.learning_rates.append(lr)\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        if self.batch_count <= self.warmup_batches:\n",
        "            lr = self.batch_count*self.init_lr/self.warmup_batches\n",
        "            K.set_value(self.model.optimizer.lr, lr)\n",
        "            if self.verbose > 0:\n",
        "                print('\\nBatch %05d: WarmUpLearningRateScheduler setting learning '\n",
        "                      'rate to %s.' % (self.batch_count + 1, lr))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyjXsW3U8Txj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DAESequence(Sequence):\n",
        "    def __init__(self,df,no_dum,frac=0.15,dumm=range(911),batch_size=2048):\n",
        "        self.batch_size=batch_size\n",
        "        self.frac=0.15\n",
        "        self.dumm=dumm\n",
        "        self.df=df\n",
        "        self.cat_data=df[dumm].values\n",
        "        self.num_data=df[no_dum].values\n",
        "        self.no_dumm=no_dum\n",
        "        self.len_data=df.shape[0]\n",
        "        self.columns=df.shape[1]\n",
        "        self.data=df\n",
        "        self.idx=[]\n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(ceil(self.len_data/self.batch_size))\n",
        "    \n",
        "    \n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        self.idx.append(idx)\n",
        "        last=min((idx+1)*self.batch_size,self.len_data)\n",
        "        idx=idx*self.batch_size\n",
        "        size=last-idx\n",
        "        \n",
        "        \n",
        "        inps=[]\n",
        "        outs=[]\n",
        "        output_x=self.data.iloc[idx:last]\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        data=output_x[self.no_dumm].values\n",
        "        noise_x=data.copy()\n",
        "        for i in range(len(self.no_dumm)):\n",
        "            to=np.random.randint(0,size,int(size*self.frac))\n",
        "            frm=np.random.randint(0,size,int(size*self.frac))\n",
        "            noise_x[to,i]=noise_x[frm,i]\n",
        "            \n",
        "        inps.append(noise_x)\n",
        "        outs.append(data)\n",
        "        \n",
        "        data=output_x[self.dumm].values\n",
        "        noise_x=data.copy()\n",
        "        for i in range(len(self.dumm)):\n",
        "            to=np.random.randint(0,size,int(size*self.frac))\n",
        "            frm=np.random.randint(0,size,int(size*self.frac))\n",
        "            noise_x[to,i]=noise_x[frm,i]\n",
        "        \n",
        "        \n",
        "        \n",
        "        inps.append(noise_x)\n",
        "        outs.append(data)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        return inps,outs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b1uGJH7Iftn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f9557132-89e2-419f-bb50-be6cbf0e6bad"
      },
      "source": [
        "import gc\n",
        "X=pd.concat([trn,tst],0).reset_index(drop=True)\n",
        "del([trn,tst])\n",
        "a=X.isna().sum()\n",
        "a[a>0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "C9    1097231\n",
              "id     590540\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie7lNLkULcU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_dum.remove('C9')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrWN7PjRLN1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=X.drop(['C9','id'],1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21jEN2-QL_fm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "a223c78d-eba7-4f41-82ea-6c7edd552df9"
      },
      "source": [
        "num=X[no_dum]\n",
        "cat=X[cats]\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss=StandardScaler()\n",
        "numerical=pd.DataFrame(ss.fit_transform(X[no_dum]))\n",
        "numerical.columns=no_dum\n",
        "categorical=pd.DataFrame(ss.fit_transform(X[cats]))\n",
        "categorical.columns=cats\n",
        "X=pd.concat([categorical,numerical],1)\n",
        "X.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>M4</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>M9</th>\n",
              "      <th>card3</th>\n",
              "      <th>M5</th>\n",
              "      <th>M2</th>\n",
              "      <th>M6</th>\n",
              "      <th>card6</th>\n",
              "      <th>M7</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card4</th>\n",
              "      <th>M3</th>\n",
              "      <th>card5</th>\n",
              "      <th>card2</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>M8</th>\n",
              "      <th>M1</th>\n",
              "      <th>card1</th>\n",
              "      <th>P_emaildomain_first</th>\n",
              "      <th>P_emaildomain_second</th>\n",
              "      <th>M4M0_dum</th>\n",
              "      <th>M4M1_dum</th>\n",
              "      <th>M4M2_dum</th>\n",
              "      <th>M4nan_dum</th>\n",
              "      <th>P_emaildomainanonymous.com_dum</th>\n",
              "      <th>P_emaildomainaol.com_dum</th>\n",
              "      <th>P_emaildomainatt.net_dum</th>\n",
              "      <th>P_emaildomaincomcast.net_dum</th>\n",
              "      <th>P_emaildomaingmail.com_dum</th>\n",
              "      <th>P_emaildomainhotmail.com_dum</th>\n",
              "      <th>P_emaildomainicloud.com_dum</th>\n",
              "      <th>P_emaildomainnan_dum</th>\n",
              "      <th>P_emaildomainother_dum</th>\n",
              "      <th>P_emaildomainoutlook.com_dum</th>\n",
              "      <th>P_emaildomainyahoo.com_dum</th>\n",
              "      <th>M9F_dum</th>\n",
              "      <th>M9T_dum</th>\n",
              "      <th>M9nan_dum</th>\n",
              "      <th>card3102.0_dum</th>\n",
              "      <th>...</th>\n",
              "      <th>V284</th>\n",
              "      <th>V111</th>\n",
              "      <th>V313</th>\n",
              "      <th>V2</th>\n",
              "      <th>V304</th>\n",
              "      <th>V72</th>\n",
              "      <th>V67</th>\n",
              "      <th>V46</th>\n",
              "      <th>V87</th>\n",
              "      <th>V19</th>\n",
              "      <th>V12</th>\n",
              "      <th>V292</th>\n",
              "      <th>V49</th>\n",
              "      <th>V40</th>\n",
              "      <th>V303</th>\n",
              "      <th>V95</th>\n",
              "      <th>V120</th>\n",
              "      <th>V63</th>\n",
              "      <th>V298</th>\n",
              "      <th>V123</th>\n",
              "      <th>V92</th>\n",
              "      <th>V137</th>\n",
              "      <th>V36</th>\n",
              "      <th>V100</th>\n",
              "      <th>V290</th>\n",
              "      <th>V112</th>\n",
              "      <th>V62</th>\n",
              "      <th>numerical</th>\n",
              "      <th>categorical</th>\n",
              "      <th>day</th>\n",
              "      <th>week</th>\n",
              "      <th>month</th>\n",
              "      <th>d_1</th>\n",
              "      <th>d_2</th>\n",
              "      <th>d_3</th>\n",
              "      <th>d_4</th>\n",
              "      <th>d_5</th>\n",
              "      <th>d_10</th>\n",
              "      <th>d_11</th>\n",
              "      <th>d_15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.006874</td>\n",
              "      <td>0.659180</td>\n",
              "      <td>1.260742</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>-1.500977</td>\n",
              "      <td>-0.651855</td>\n",
              "      <td>-0.066772</td>\n",
              "      <td>-1.628906</td>\n",
              "      <td>1.200195</td>\n",
              "      <td>0.584473</td>\n",
              "      <td>-2.150391</td>\n",
              "      <td>-0.540527</td>\n",
              "      <td>1.296875</td>\n",
              "      <td>0.805176</td>\n",
              "      <td>-0.682129</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>1.196289</td>\n",
              "      <td>-0.771973</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>0.375488</td>\n",
              "      <td>2.283203</td>\n",
              "      <td>-0.695801</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>2.814453</td>\n",
              "      <td>-0.947754</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>-0.811523</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>2.388672</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>-0.095642</td>\n",
              "      <td>-0.447021</td>\n",
              "      <td>-0.269043</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>0.941895</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261828</td>\n",
              "      <td>-0.044793</td>\n",
              "      <td>-0.224162</td>\n",
              "      <td>-0.243964</td>\n",
              "      <td>-0.520047</td>\n",
              "      <td>-0.473865</td>\n",
              "      <td>-0.060660</td>\n",
              "      <td>0.001095</td>\n",
              "      <td>-0.152711</td>\n",
              "      <td>0.413886</td>\n",
              "      <td>0.892006</td>\n",
              "      <td>-0.078406</td>\n",
              "      <td>0.057314</td>\n",
              "      <td>-0.120921</td>\n",
              "      <td>-0.476799</td>\n",
              "      <td>-0.043103</td>\n",
              "      <td>-0.030553</td>\n",
              "      <td>-0.474523</td>\n",
              "      <td>-0.097883</td>\n",
              "      <td>-0.142667</td>\n",
              "      <td>-0.497739</td>\n",
              "      <td>-0.047358</td>\n",
              "      <td>0.006035</td>\n",
              "      <td>-0.303313</td>\n",
              "      <td>-0.152493</td>\n",
              "      <td>-0.071219</td>\n",
              "      <td>0.247929</td>\n",
              "      <td>0.151174</td>\n",
              "      <td>0.102999</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.497518</td>\n",
              "      <td>-1.003158</td>\n",
              "      <td>-1.269802</td>\n",
              "      <td>-0.765257</td>\n",
              "      <td>-1.302708</td>\n",
              "      <td>-0.262279</td>\n",
              "      <td>-0.111830</td>\n",
              "      <td>-0.018393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.225586</td>\n",
              "      <td>-0.274902</td>\n",
              "      <td>1.260742</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>-0.619141</td>\n",
              "      <td>1.522461</td>\n",
              "      <td>-0.066772</td>\n",
              "      <td>-1.628906</td>\n",
              "      <td>1.200195</td>\n",
              "      <td>0.584473</td>\n",
              "      <td>-1.201172</td>\n",
              "      <td>1.477539</td>\n",
              "      <td>-2.544922</td>\n",
              "      <td>0.805176</td>\n",
              "      <td>-0.416016</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>1.196289</td>\n",
              "      <td>1.583984</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>-0.413086</td>\n",
              "      <td>-0.476807</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>-0.355225</td>\n",
              "      <td>-0.947754</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>1.232422</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>-0.418701</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>-0.095642</td>\n",
              "      <td>-0.447021</td>\n",
              "      <td>-0.269043</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>0.941895</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261828</td>\n",
              "      <td>-0.044793</td>\n",
              "      <td>-0.224162</td>\n",
              "      <td>-0.004845</td>\n",
              "      <td>-0.520047</td>\n",
              "      <td>-0.473865</td>\n",
              "      <td>-0.060660</td>\n",
              "      <td>-0.147651</td>\n",
              "      <td>-0.152711</td>\n",
              "      <td>0.413886</td>\n",
              "      <td>-1.155606</td>\n",
              "      <td>-0.078406</td>\n",
              "      <td>-0.793730</td>\n",
              "      <td>-0.466503</td>\n",
              "      <td>-0.476799</td>\n",
              "      <td>-0.043103</td>\n",
              "      <td>-0.030553</td>\n",
              "      <td>-0.474523</td>\n",
              "      <td>-0.097883</td>\n",
              "      <td>-0.142667</td>\n",
              "      <td>-0.497739</td>\n",
              "      <td>-0.047358</td>\n",
              "      <td>-1.214675</td>\n",
              "      <td>-0.303313</td>\n",
              "      <td>-0.152493</td>\n",
              "      <td>-0.071219</td>\n",
              "      <td>0.247929</td>\n",
              "      <td>-0.037020</td>\n",
              "      <td>0.368876</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.428902</td>\n",
              "      <td>-1.003158</td>\n",
              "      <td>-1.384026</td>\n",
              "      <td>-0.142623</td>\n",
              "      <td>-1.302708</td>\n",
              "      <td>-0.206136</td>\n",
              "      <td>-0.807352</td>\n",
              "      <td>-0.018393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.225586</td>\n",
              "      <td>1.281250</td>\n",
              "      <td>-1.835938</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>-1.500977</td>\n",
              "      <td>-0.651855</td>\n",
              "      <td>-1.003906</td>\n",
              "      <td>0.541016</td>\n",
              "      <td>-1.108398</td>\n",
              "      <td>0.584473</td>\n",
              "      <td>0.694824</td>\n",
              "      <td>-0.540527</td>\n",
              "      <td>-0.797852</td>\n",
              "      <td>-0.728516</td>\n",
              "      <td>-0.149658</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>-1.280273</td>\n",
              "      <td>-0.771973</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>0.611816</td>\n",
              "      <td>-0.476807</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>-0.355225</td>\n",
              "      <td>-0.947754</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>-0.811523</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>-0.418701</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>10.460938</td>\n",
              "      <td>-0.447021</td>\n",
              "      <td>3.716797</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>-1.061523</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261828</td>\n",
              "      <td>-0.044793</td>\n",
              "      <td>-0.224162</td>\n",
              "      <td>-0.243964</td>\n",
              "      <td>-0.520047</td>\n",
              "      <td>-0.473865</td>\n",
              "      <td>-0.060660</td>\n",
              "      <td>-0.147651</td>\n",
              "      <td>-0.152711</td>\n",
              "      <td>0.413886</td>\n",
              "      <td>0.892006</td>\n",
              "      <td>-0.078406</td>\n",
              "      <td>-0.793730</td>\n",
              "      <td>-0.466503</td>\n",
              "      <td>-0.476799</td>\n",
              "      <td>-0.043103</td>\n",
              "      <td>-0.030553</td>\n",
              "      <td>-0.474523</td>\n",
              "      <td>-0.097883</td>\n",
              "      <td>-0.142667</td>\n",
              "      <td>-0.497739</td>\n",
              "      <td>-0.047358</td>\n",
              "      <td>0.893264</td>\n",
              "      <td>-0.303313</td>\n",
              "      <td>-0.152493</td>\n",
              "      <td>-0.071219</td>\n",
              "      <td>0.247929</td>\n",
              "      <td>-0.526324</td>\n",
              "      <td>-1.226384</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.428902</td>\n",
              "      <td>-1.003158</td>\n",
              "      <td>-1.384026</td>\n",
              "      <td>-0.142623</td>\n",
              "      <td>-1.302708</td>\n",
              "      <td>-0.206136</td>\n",
              "      <td>-1.683748</td>\n",
              "      <td>-1.276178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.225586</td>\n",
              "      <td>1.592773</td>\n",
              "      <td>1.260742</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>-0.619141</td>\n",
              "      <td>1.522461</td>\n",
              "      <td>-1.003906</td>\n",
              "      <td>0.541016</td>\n",
              "      <td>1.200195</td>\n",
              "      <td>0.584473</td>\n",
              "      <td>-1.201172</td>\n",
              "      <td>1.477539</td>\n",
              "      <td>-2.195312</td>\n",
              "      <td>0.805176</td>\n",
              "      <td>0.915039</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>1.196289</td>\n",
              "      <td>1.583984</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>1.794922</td>\n",
              "      <td>-0.476807</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>-0.355225</td>\n",
              "      <td>-0.947754</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>-0.811523</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>-0.418701</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>-0.095642</td>\n",
              "      <td>2.236328</td>\n",
              "      <td>-0.269043</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>0.941895</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261828</td>\n",
              "      <td>-0.044793</td>\n",
              "      <td>-0.224162</td>\n",
              "      <td>-0.004845</td>\n",
              "      <td>-0.520047</td>\n",
              "      <td>-0.473865</td>\n",
              "      <td>-0.060660</td>\n",
              "      <td>-0.147651</td>\n",
              "      <td>-0.152711</td>\n",
              "      <td>0.413886</td>\n",
              "      <td>0.892006</td>\n",
              "      <td>-0.078406</td>\n",
              "      <td>-0.793730</td>\n",
              "      <td>-0.466503</td>\n",
              "      <td>-0.476799</td>\n",
              "      <td>0.021649</td>\n",
              "      <td>-0.030553</td>\n",
              "      <td>-0.474523</td>\n",
              "      <td>-0.097883</td>\n",
              "      <td>-0.142667</td>\n",
              "      <td>-0.497739</td>\n",
              "      <td>-0.047358</td>\n",
              "      <td>0.893264</td>\n",
              "      <td>4.157576</td>\n",
              "      <td>-0.152493</td>\n",
              "      <td>-0.071219</td>\n",
              "      <td>0.247929</td>\n",
              "      <td>-0.149936</td>\n",
              "      <td>0.368876</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.977829</td>\n",
              "      <td>-0.681109</td>\n",
              "      <td>-1.173026</td>\n",
              "      <td>-0.560677</td>\n",
              "      <td>-1.003359</td>\n",
              "      <td>-0.568908</td>\n",
              "      <td>-0.807352</td>\n",
              "      <td>-0.461612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.239258</td>\n",
              "      <td>-0.274902</td>\n",
              "      <td>1.260742</td>\n",
              "      <td>-0.127808</td>\n",
              "      <td>1.144531</td>\n",
              "      <td>1.522461</td>\n",
              "      <td>1.806641</td>\n",
              "      <td>-1.628906</td>\n",
              "      <td>1.200195</td>\n",
              "      <td>-1.544922</td>\n",
              "      <td>-1.201172</td>\n",
              "      <td>1.477539</td>\n",
              "      <td>-2.544922</td>\n",
              "      <td>-0.473145</td>\n",
              "      <td>0.915039</td>\n",
              "      <td>-0.754883</td>\n",
              "      <td>1.196289</td>\n",
              "      <td>1.583984</td>\n",
              "      <td>0.378906</td>\n",
              "      <td>-0.413086</td>\n",
              "      <td>-0.476807</td>\n",
              "      <td>-0.695801</td>\n",
              "      <td>-0.312012</td>\n",
              "      <td>-0.355225</td>\n",
              "      <td>1.055664</td>\n",
              "      <td>-0.263184</td>\n",
              "      <td>-0.223877</td>\n",
              "      <td>-0.08374</td>\n",
              "      <td>-0.115601</td>\n",
              "      <td>1.232422</td>\n",
              "      <td>-0.291016</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>-0.418701</td>\n",
              "      <td>-0.243896</td>\n",
              "      <td>-0.095642</td>\n",
              "      <td>-0.447021</td>\n",
              "      <td>-0.269043</td>\n",
              "      <td>-0.821289</td>\n",
              "      <td>0.941895</td>\n",
              "      <td>-0.033051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261828</td>\n",
              "      <td>-0.044793</td>\n",
              "      <td>-0.224162</td>\n",
              "      <td>-0.004845</td>\n",
              "      <td>1.327692</td>\n",
              "      <td>-0.130605</td>\n",
              "      <td>-0.069356</td>\n",
              "      <td>0.001095</td>\n",
              "      <td>-0.019714</td>\n",
              "      <td>-0.052885</td>\n",
              "      <td>-0.009823</td>\n",
              "      <td>-0.078406</td>\n",
              "      <td>0.057314</td>\n",
              "      <td>-0.120921</td>\n",
              "      <td>1.107169</td>\n",
              "      <td>-0.043103</td>\n",
              "      <td>-0.030553</td>\n",
              "      <td>-0.146202</td>\n",
              "      <td>-0.097883</td>\n",
              "      <td>-0.142667</td>\n",
              "      <td>-0.125447</td>\n",
              "      <td>-0.047358</td>\n",
              "      <td>0.006035</td>\n",
              "      <td>-0.303313</td>\n",
              "      <td>-0.152493</td>\n",
              "      <td>-0.071219</td>\n",
              "      <td>-0.046284</td>\n",
              "      <td>3.199918</td>\n",
              "      <td>1.166506</td>\n",
              "      <td>-1.503841</td>\n",
              "      <td>-1.488133</td>\n",
              "      <td>-1.398958</td>\n",
              "      <td>-0.428902</td>\n",
              "      <td>-1.003158</td>\n",
              "      <td>-1.384026</td>\n",
              "      <td>-0.765257</td>\n",
              "      <td>-1.302708</td>\n",
              "      <td>-0.741656</td>\n",
              "      <td>-0.807352</td>\n",
              "      <td>-0.672241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 564 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         M4  P_emaildomain        M9  ...      d_10      d_11      d_15\n",
              "0  0.006874       0.659180  1.260742  ... -0.262279 -0.111830 -0.018393\n",
              "1 -1.225586      -0.274902  1.260742  ... -0.206136 -0.807352 -0.018393\n",
              "2 -1.225586       1.281250 -1.835938  ... -0.206136 -1.683748 -1.276178\n",
              "3 -1.225586       1.592773  1.260742  ... -0.568908 -0.807352 -0.461612\n",
              "4  1.239258      -0.274902  1.260742  ... -0.741656 -0.807352 -0.672241\n",
              "\n",
              "[5 rows x 564 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeTh6RLbLxUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXdXM2kpNXgw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3a75695-f487-4fad-c332-7cd872616bf9"
      },
      "source": [
        "num_shape=len(no_dum)\n",
        "cat_shape=len(cats)\n",
        "model_mse = create_model()\n",
        "model_mse.summary()\n",
        "X=X.dropna()\n",
        "batch_size=2048\n",
        "auto_ckpt = ModelCheckpoint(\"ae.model\", monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min', period=1)\n",
        "warm_up_lr = WarmUpLearningRateScheduler(400, init_lr=0.0001)\n",
        "gc.collect()\n",
        "epochs = 1000\n",
        "batch_size=2048\n",
        "train_gen=DAESequence(X,no_dum,batch_size=batch_size,dumm=cats)\n",
        "hist = model_mse.fit_generator(train_gen, steps_per_epoch=len(X)//batch_size, epochs=epochs,\n",
        "                              callbacks=[auto_ckpt, warm_up_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 423)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 141)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 564)          0           input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          289280      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          131328      dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          131584      dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 423)          216999      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 141)          72333       dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 841,524\n",
            "Trainable params: 841,524\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "WARNING:tensorflow:From <ipython-input-15-24df14d8c07e>:14: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 1.6826 - dense_4_loss: 0.8232 - dense_3_loss: 0.8594\n",
            "Epoch 00001: loss improved from inf to 1.68256, saving model to ae.model\n",
            "535/535 [==============================] - 66s 123ms/step - loss: 1.6826 - dense_4_loss: 0.8232 - dense_3_loss: 0.8594\n",
            "Epoch 2/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 1.0181 - dense_4_loss: 0.5061 - dense_3_loss: 0.5120\n",
            "Epoch 00002: loss improved from 1.68256 to 1.01807, saving model to ae.model\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 1.0181 - dense_4_loss: 0.5061 - dense_3_loss: 0.5120\n",
            "Epoch 3/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.7557 - dense_4_loss: 0.3563 - dense_3_loss: 0.3994\n",
            "Epoch 00003: loss improved from 1.01807 to 0.75567, saving model to ae.model\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.7557 - dense_4_loss: 0.3563 - dense_3_loss: 0.3994\n",
            "Epoch 4/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.6377 - dense_4_loss: 0.2976 - dense_3_loss: 0.3401\n",
            "Epoch 00004: loss improved from 0.75567 to 0.63775, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.6377 - dense_4_loss: 0.2976 - dense_3_loss: 0.3401\n",
            "Epoch 5/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.5676 - dense_4_loss: 0.2596 - dense_3_loss: 0.3079\n",
            "Epoch 00005: loss improved from 0.63775 to 0.56756, saving model to ae.model\n",
            "535/535 [==============================] - 67s 126ms/step - loss: 0.5676 - dense_4_loss: 0.2596 - dense_3_loss: 0.3079\n",
            "Epoch 6/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.5229 - dense_4_loss: 0.2373 - dense_3_loss: 0.2856\n",
            "Epoch 00006: loss improved from 0.56756 to 0.52288, saving model to ae.model\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.5229 - dense_4_loss: 0.2373 - dense_3_loss: 0.2856\n",
            "Epoch 7/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.4856 - dense_4_loss: 0.2187 - dense_3_loss: 0.2669\n",
            "Epoch 00007: loss improved from 0.52288 to 0.48557, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.4856 - dense_4_loss: 0.2187 - dense_3_loss: 0.2669\n",
            "Epoch 8/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.4625 - dense_4_loss: 0.2065 - dense_3_loss: 0.2560\n",
            "Epoch 00008: loss improved from 0.48557 to 0.46252, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.4625 - dense_4_loss: 0.2065 - dense_3_loss: 0.2560\n",
            "Epoch 9/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.4428 - dense_4_loss: 0.1987 - dense_3_loss: 0.2441\n",
            "Epoch 00009: loss improved from 0.46252 to 0.44285, saving model to ae.model\n",
            "535/535 [==============================] - 66s 123ms/step - loss: 0.4428 - dense_4_loss: 0.1987 - dense_3_loss: 0.2441\n",
            "Epoch 10/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.4298 - dense_4_loss: 0.1900 - dense_3_loss: 0.2399\n",
            "Epoch 00010: loss improved from 0.44285 to 0.42983, saving model to ae.model\n",
            "535/535 [==============================] - 67s 125ms/step - loss: 0.4298 - dense_4_loss: 0.1900 - dense_3_loss: 0.2399\n",
            "Epoch 11/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.4200 - dense_4_loss: 0.1842 - dense_3_loss: 0.2358\n",
            "Epoch 00011: loss improved from 0.42983 to 0.41999, saving model to ae.model\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.4200 - dense_4_loss: 0.1842 - dense_3_loss: 0.2358\n",
            "Epoch 12/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.4024 - dense_4_loss: 0.1746 - dense_3_loss: 0.2278\n",
            "Epoch 00012: loss improved from 0.41999 to 0.40241, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.4024 - dense_4_loss: 0.1746 - dense_3_loss: 0.2278\n",
            "Epoch 13/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3930 - dense_4_loss: 0.1733 - dense_3_loss: 0.2197\n",
            "Epoch 00013: loss improved from 0.40241 to 0.39298, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.3930 - dense_4_loss: 0.1733 - dense_3_loss: 0.2197\n",
            "Epoch 14/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3834 - dense_4_loss: 0.1677 - dense_3_loss: 0.2156\n",
            "Epoch 00014: loss improved from 0.39298 to 0.38336, saving model to ae.model\n",
            "535/535 [==============================] - 66s 122ms/step - loss: 0.3834 - dense_4_loss: 0.1677 - dense_3_loss: 0.2156\n",
            "Epoch 15/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3786 - dense_4_loss: 0.1646 - dense_3_loss: 0.2140\n",
            "Epoch 00015: loss improved from 0.38336 to 0.37856, saving model to ae.model\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.3786 - dense_4_loss: 0.1646 - dense_3_loss: 0.2140\n",
            "Epoch 16/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3712 - dense_4_loss: 0.1589 - dense_3_loss: 0.2123\n",
            "Epoch 00016: loss improved from 0.37856 to 0.37123, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.3712 - dense_4_loss: 0.1589 - dense_3_loss: 0.2123\n",
            "Epoch 17/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3648 - dense_4_loss: 0.1561 - dense_3_loss: 0.2087\n",
            "Epoch 00017: loss improved from 0.37123 to 0.36479, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.3648 - dense_4_loss: 0.1561 - dense_3_loss: 0.2087\n",
            "Epoch 18/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3610 - dense_4_loss: 0.1554 - dense_3_loss: 0.2056\n",
            "Epoch 00018: loss improved from 0.36479 to 0.36096, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.3610 - dense_4_loss: 0.1554 - dense_3_loss: 0.2056\n",
            "Epoch 19/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3586 - dense_4_loss: 0.1526 - dense_3_loss: 0.2060\n",
            "Epoch 00019: loss improved from 0.36096 to 0.35855, saving model to ae.model\n",
            "535/535 [==============================] - 66s 123ms/step - loss: 0.3586 - dense_4_loss: 0.1526 - dense_3_loss: 0.2060\n",
            "Epoch 20/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3476 - dense_4_loss: 0.1480 - dense_3_loss: 0.1996\n",
            "Epoch 00020: loss improved from 0.35855 to 0.34761, saving model to ae.model\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.3476 - dense_4_loss: 0.1480 - dense_3_loss: 0.1996\n",
            "Epoch 21/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3456 - dense_4_loss: 0.1500 - dense_3_loss: 0.1956\n",
            "Epoch 00021: loss improved from 0.34761 to 0.34561, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.3456 - dense_4_loss: 0.1500 - dense_3_loss: 0.1956\n",
            "Epoch 22/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3400 - dense_4_loss: 0.1466 - dense_3_loss: 0.1934\n",
            "Epoch 00022: loss improved from 0.34561 to 0.34002, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.3400 - dense_4_loss: 0.1466 - dense_3_loss: 0.1934\n",
            "Epoch 23/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3390 - dense_4_loss: 0.1441 - dense_3_loss: 0.1948\n",
            "Epoch 00023: loss improved from 0.34002 to 0.33898, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.3390 - dense_4_loss: 0.1441 - dense_3_loss: 0.1948\n",
            "Epoch 24/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3349 - dense_4_loss: 0.1424 - dense_3_loss: 0.1925\n",
            "Epoch 00024: loss improved from 0.33898 to 0.33491, saving model to ae.model\n",
            "535/535 [==============================] - 66s 123ms/step - loss: 0.3349 - dense_4_loss: 0.1424 - dense_3_loss: 0.1925\n",
            "Epoch 25/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3288 - dense_4_loss: 0.1409 - dense_3_loss: 0.1880\n",
            "Epoch 00025: loss improved from 0.33491 to 0.32884, saving model to ae.model\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.3288 - dense_4_loss: 0.1409 - dense_3_loss: 0.1880\n",
            "Epoch 26/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3279 - dense_4_loss: 0.1389 - dense_3_loss: 0.1890\n",
            "Epoch 00026: loss improved from 0.32884 to 0.32791, saving model to ae.model\n",
            "535/535 [==============================] - 64s 121ms/step - loss: 0.3279 - dense_4_loss: 0.1389 - dense_3_loss: 0.1890\n",
            "Epoch 27/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3268 - dense_4_loss: 0.1398 - dense_3_loss: 0.1870\n",
            "Epoch 00027: loss improved from 0.32791 to 0.32678, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.3268 - dense_4_loss: 0.1398 - dense_3_loss: 0.1870\n",
            "Epoch 28/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3263 - dense_4_loss: 0.1381 - dense_3_loss: 0.1882\n",
            "Epoch 00028: loss improved from 0.32678 to 0.32633, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.3263 - dense_4_loss: 0.1381 - dense_3_loss: 0.1882\n",
            "Epoch 29/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3199 - dense_4_loss: 0.1351 - dense_3_loss: 0.1848\n",
            "Epoch 00029: loss improved from 0.32633 to 0.31994, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.3199 - dense_4_loss: 0.1351 - dense_3_loss: 0.1848\n",
            "Epoch 30/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3207 - dense_4_loss: 0.1368 - dense_3_loss: 0.1840\n",
            "Epoch 00030: loss did not improve from 0.31994\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.3207 - dense_4_loss: 0.1368 - dense_3_loss: 0.1840\n",
            "Epoch 31/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3147 - dense_4_loss: 0.1326 - dense_3_loss: 0.1821\n",
            "Epoch 00031: loss improved from 0.31994 to 0.31472, saving model to ae.model\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.3147 - dense_4_loss: 0.1326 - dense_3_loss: 0.1821\n",
            "Epoch 32/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3150 - dense_4_loss: 0.1341 - dense_3_loss: 0.1809\n",
            "Epoch 00032: loss did not improve from 0.31472\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.3150 - dense_4_loss: 0.1341 - dense_3_loss: 0.1809\n",
            "Epoch 33/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3136 - dense_4_loss: 0.1312 - dense_3_loss: 0.1824\n",
            "Epoch 00033: loss improved from 0.31472 to 0.31359, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.3136 - dense_4_loss: 0.1312 - dense_3_loss: 0.1824\n",
            "Epoch 34/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3067 - dense_4_loss: 0.1299 - dense_3_loss: 0.1768\n",
            "Epoch 00034: loss improved from 0.31359 to 0.30670, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.3067 - dense_4_loss: 0.1299 - dense_3_loss: 0.1768\n",
            "Epoch 35/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3091 - dense_4_loss: 0.1316 - dense_3_loss: 0.1775\n",
            "Epoch 00035: loss did not improve from 0.30670\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.3091 - dense_4_loss: 0.1316 - dense_3_loss: 0.1775\n",
            "Epoch 36/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3080 - dense_4_loss: 0.1309 - dense_3_loss: 0.1772\n",
            "Epoch 00036: loss did not improve from 0.30670\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.3080 - dense_4_loss: 0.1309 - dense_3_loss: 0.1772\n",
            "Epoch 37/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3050 - dense_4_loss: 0.1300 - dense_3_loss: 0.1749\n",
            "Epoch 00037: loss improved from 0.30670 to 0.30497, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.3050 - dense_4_loss: 0.1300 - dense_3_loss: 0.1749\n",
            "Epoch 38/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3055 - dense_4_loss: 0.1300 - dense_3_loss: 0.1755\n",
            "Epoch 00038: loss did not improve from 0.30497\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.3055 - dense_4_loss: 0.1300 - dense_3_loss: 0.1755\n",
            "Epoch 39/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3022 - dense_4_loss: 0.1275 - dense_3_loss: 0.1747\n",
            "Epoch 00039: loss improved from 0.30497 to 0.30217, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.3022 - dense_4_loss: 0.1275 - dense_3_loss: 0.1747\n",
            "Epoch 40/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.3025 - dense_4_loss: 0.1293 - dense_3_loss: 0.1732\n",
            "Epoch 00040: loss did not improve from 0.30217\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.3025 - dense_4_loss: 0.1293 - dense_3_loss: 0.1732\n",
            "Epoch 41/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2992 - dense_4_loss: 0.1265 - dense_3_loss: 0.1726\n",
            "Epoch 00041: loss improved from 0.30217 to 0.29915, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2992 - dense_4_loss: 0.1265 - dense_3_loss: 0.1726\n",
            "Epoch 42/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2984 - dense_4_loss: 0.1271 - dense_3_loss: 0.1712\n",
            "Epoch 00042: loss improved from 0.29915 to 0.29836, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2984 - dense_4_loss: 0.1271 - dense_3_loss: 0.1712\n",
            "Epoch 43/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2995 - dense_4_loss: 0.1278 - dense_3_loss: 0.1717\n",
            "Epoch 00043: loss did not improve from 0.29836\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2995 - dense_4_loss: 0.1278 - dense_3_loss: 0.1717\n",
            "Epoch 44/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2948 - dense_4_loss: 0.1259 - dense_3_loss: 0.1689\n",
            "Epoch 00044: loss improved from 0.29836 to 0.29475, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2948 - dense_4_loss: 0.1259 - dense_3_loss: 0.1689\n",
            "Epoch 45/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2933 - dense_4_loss: 0.1236 - dense_3_loss: 0.1696\n",
            "Epoch 00045: loss improved from 0.29475 to 0.29326, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2933 - dense_4_loss: 0.1236 - dense_3_loss: 0.1696\n",
            "Epoch 46/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2938 - dense_4_loss: 0.1253 - dense_3_loss: 0.1685\n",
            "Epoch 00046: loss did not improve from 0.29326\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2938 - dense_4_loss: 0.1253 - dense_3_loss: 0.1685\n",
            "Epoch 47/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2934 - dense_4_loss: 0.1241 - dense_3_loss: 0.1693\n",
            "Epoch 00047: loss did not improve from 0.29326\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2934 - dense_4_loss: 0.1241 - dense_3_loss: 0.1693\n",
            "Epoch 48/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2912 - dense_4_loss: 0.1242 - dense_3_loss: 0.1670\n",
            "Epoch 00048: loss improved from 0.29326 to 0.29121, saving model to ae.model\n",
            "535/535 [==============================] - 66s 124ms/step - loss: 0.2912 - dense_4_loss: 0.1242 - dense_3_loss: 0.1670\n",
            "Epoch 49/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2887 - dense_4_loss: 0.1228 - dense_3_loss: 0.1659\n",
            "Epoch 00049: loss improved from 0.29121 to 0.28875, saving model to ae.model\n",
            "535/535 [==============================] - 66s 123ms/step - loss: 0.2887 - dense_4_loss: 0.1228 - dense_3_loss: 0.1659\n",
            "Epoch 50/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2915 - dense_4_loss: 0.1269 - dense_3_loss: 0.1645\n",
            "Epoch 00050: loss did not improve from 0.28875\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2915 - dense_4_loss: 0.1269 - dense_3_loss: 0.1645\n",
            "Epoch 51/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2875 - dense_4_loss: 0.1223 - dense_3_loss: 0.1652\n",
            "Epoch 00051: loss improved from 0.28875 to 0.28750, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2875 - dense_4_loss: 0.1223 - dense_3_loss: 0.1652\n",
            "Epoch 52/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2869 - dense_4_loss: 0.1225 - dense_3_loss: 0.1643\n",
            "Epoch 00052: loss improved from 0.28750 to 0.28689, saving model to ae.model\n",
            "535/535 [==============================] - 66s 123ms/step - loss: 0.2869 - dense_4_loss: 0.1225 - dense_3_loss: 0.1643\n",
            "Epoch 53/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2849 - dense_4_loss: 0.1207 - dense_3_loss: 0.1642\n",
            "Epoch 00053: loss improved from 0.28689 to 0.28492, saving model to ae.model\n",
            "535/535 [==============================] - 66s 122ms/step - loss: 0.2849 - dense_4_loss: 0.1207 - dense_3_loss: 0.1642\n",
            "Epoch 54/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2857 - dense_4_loss: 0.1218 - dense_3_loss: 0.1639\n",
            "Epoch 00054: loss did not improve from 0.28492\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2857 - dense_4_loss: 0.1218 - dense_3_loss: 0.1639\n",
            "Epoch 55/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2821 - dense_4_loss: 0.1193 - dense_3_loss: 0.1628\n",
            "Epoch 00055: loss improved from 0.28492 to 0.28213, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2821 - dense_4_loss: 0.1193 - dense_3_loss: 0.1628\n",
            "Epoch 56/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2829 - dense_4_loss: 0.1202 - dense_3_loss: 0.1627\n",
            "Epoch 00056: loss did not improve from 0.28213\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2829 - dense_4_loss: 0.1202 - dense_3_loss: 0.1627\n",
            "Epoch 57/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2809 - dense_4_loss: 0.1195 - dense_3_loss: 0.1614\n",
            "Epoch 00057: loss improved from 0.28213 to 0.28088, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2809 - dense_4_loss: 0.1195 - dense_3_loss: 0.1614\n",
            "Epoch 58/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2797 - dense_4_loss: 0.1210 - dense_3_loss: 0.1587\n",
            "Epoch 00058: loss improved from 0.28088 to 0.27969, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2797 - dense_4_loss: 0.1210 - dense_3_loss: 0.1587\n",
            "Epoch 59/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2784 - dense_4_loss: 0.1182 - dense_3_loss: 0.1602\n",
            "Epoch 00059: loss improved from 0.27969 to 0.27842, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2784 - dense_4_loss: 0.1182 - dense_3_loss: 0.1602\n",
            "Epoch 60/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2807 - dense_4_loss: 0.1205 - dense_3_loss: 0.1602\n",
            "Epoch 00060: loss did not improve from 0.27842\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2807 - dense_4_loss: 0.1205 - dense_3_loss: 0.1602\n",
            "Epoch 61/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2793 - dense_4_loss: 0.1191 - dense_3_loss: 0.1602\n",
            "Epoch 00061: loss did not improve from 0.27842\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2793 - dense_4_loss: 0.1191 - dense_3_loss: 0.1602\n",
            "Epoch 62/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2772 - dense_4_loss: 0.1181 - dense_3_loss: 0.1591\n",
            "Epoch 00062: loss improved from 0.27842 to 0.27723, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2772 - dense_4_loss: 0.1181 - dense_3_loss: 0.1591\n",
            "Epoch 63/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2760 - dense_4_loss: 0.1172 - dense_3_loss: 0.1588\n",
            "Epoch 00063: loss improved from 0.27723 to 0.27602, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2760 - dense_4_loss: 0.1172 - dense_3_loss: 0.1588\n",
            "Epoch 64/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2747 - dense_4_loss: 0.1184 - dense_3_loss: 0.1563\n",
            "Epoch 00064: loss improved from 0.27602 to 0.27468, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2747 - dense_4_loss: 0.1184 - dense_3_loss: 0.1563\n",
            "Epoch 65/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2766 - dense_4_loss: 0.1178 - dense_3_loss: 0.1589\n",
            "Epoch 00065: loss did not improve from 0.27468\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2766 - dense_4_loss: 0.1178 - dense_3_loss: 0.1589\n",
            "Epoch 66/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2721 - dense_4_loss: 0.1147 - dense_3_loss: 0.1575\n",
            "Epoch 00066: loss improved from 0.27468 to 0.27215, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2721 - dense_4_loss: 0.1147 - dense_3_loss: 0.1575\n",
            "Epoch 67/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2759 - dense_4_loss: 0.1177 - dense_3_loss: 0.1582\n",
            "Epoch 00067: loss did not improve from 0.27215\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2759 - dense_4_loss: 0.1177 - dense_3_loss: 0.1582\n",
            "Epoch 68/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2755 - dense_4_loss: 0.1164 - dense_3_loss: 0.1591\n",
            "Epoch 00068: loss did not improve from 0.27215\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2755 - dense_4_loss: 0.1164 - dense_3_loss: 0.1591\n",
            "Epoch 69/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2721 - dense_4_loss: 0.1148 - dense_3_loss: 0.1573\n",
            "Epoch 00069: loss improved from 0.27215 to 0.27205, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2721 - dense_4_loss: 0.1148 - dense_3_loss: 0.1573\n",
            "Epoch 70/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2778 - dense_4_loss: 0.1220 - dense_3_loss: 0.1558\n",
            "Epoch 00070: loss did not improve from 0.27205\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2778 - dense_4_loss: 0.1220 - dense_3_loss: 0.1558\n",
            "Epoch 71/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2696 - dense_4_loss: 0.1174 - dense_3_loss: 0.1522\n",
            "Epoch 00071: loss improved from 0.27205 to 0.26956, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2696 - dense_4_loss: 0.1174 - dense_3_loss: 0.1522\n",
            "Epoch 72/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2703 - dense_4_loss: 0.1167 - dense_3_loss: 0.1536\n",
            "Epoch 00072: loss did not improve from 0.26956\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2703 - dense_4_loss: 0.1167 - dense_3_loss: 0.1536\n",
            "Epoch 73/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2707 - dense_4_loss: 0.1156 - dense_3_loss: 0.1550\n",
            "Epoch 00073: loss did not improve from 0.26956\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2707 - dense_4_loss: 0.1156 - dense_3_loss: 0.1550\n",
            "Epoch 74/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2685 - dense_4_loss: 0.1173 - dense_3_loss: 0.1513\n",
            "Epoch 00074: loss improved from 0.26956 to 0.26853, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2685 - dense_4_loss: 0.1173 - dense_3_loss: 0.1513\n",
            "Epoch 75/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2694 - dense_4_loss: 0.1163 - dense_3_loss: 0.1531\n",
            "Epoch 00075: loss did not improve from 0.26853\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2694 - dense_4_loss: 0.1163 - dense_3_loss: 0.1531\n",
            "Epoch 76/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2673 - dense_4_loss: 0.1139 - dense_3_loss: 0.1535\n",
            "Epoch 00076: loss improved from 0.26853 to 0.26734, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2673 - dense_4_loss: 0.1139 - dense_3_loss: 0.1535\n",
            "Epoch 77/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2675 - dense_4_loss: 0.1152 - dense_3_loss: 0.1522\n",
            "Epoch 00077: loss did not improve from 0.26734\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2675 - dense_4_loss: 0.1152 - dense_3_loss: 0.1522\n",
            "Epoch 78/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2641 - dense_4_loss: 0.1123 - dense_3_loss: 0.1518\n",
            "Epoch 00078: loss improved from 0.26734 to 0.26411, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2641 - dense_4_loss: 0.1123 - dense_3_loss: 0.1518\n",
            "Epoch 79/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2662 - dense_4_loss: 0.1153 - dense_3_loss: 0.1509\n",
            "Epoch 00079: loss did not improve from 0.26411\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2662 - dense_4_loss: 0.1153 - dense_3_loss: 0.1509\n",
            "Epoch 80/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2663 - dense_4_loss: 0.1137 - dense_3_loss: 0.1526\n",
            "Epoch 00080: loss did not improve from 0.26411\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2663 - dense_4_loss: 0.1137 - dense_3_loss: 0.1526\n",
            "Epoch 81/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2643 - dense_4_loss: 0.1140 - dense_3_loss: 0.1503\n",
            "Epoch 00081: loss did not improve from 0.26411\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2643 - dense_4_loss: 0.1140 - dense_3_loss: 0.1503\n",
            "Epoch 82/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2637 - dense_4_loss: 0.1121 - dense_3_loss: 0.1516\n",
            "Epoch 00082: loss improved from 0.26411 to 0.26374, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2637 - dense_4_loss: 0.1121 - dense_3_loss: 0.1516\n",
            "Epoch 83/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2629 - dense_4_loss: 0.1140 - dense_3_loss: 0.1489\n",
            "Epoch 00083: loss improved from 0.26374 to 0.26293, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2629 - dense_4_loss: 0.1140 - dense_3_loss: 0.1489\n",
            "Epoch 84/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2624 - dense_4_loss: 0.1127 - dense_3_loss: 0.1496\n",
            "Epoch 00084: loss improved from 0.26293 to 0.26238, saving model to ae.model\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.2624 - dense_4_loss: 0.1127 - dense_3_loss: 0.1496\n",
            "Epoch 85/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2607 - dense_4_loss: 0.1121 - dense_3_loss: 0.1486\n",
            "Epoch 00085: loss improved from 0.26238 to 0.26067, saving model to ae.model\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.2607 - dense_4_loss: 0.1121 - dense_3_loss: 0.1486\n",
            "Epoch 86/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2601 - dense_4_loss: 0.1120 - dense_3_loss: 0.1481\n",
            "Epoch 00086: loss improved from 0.26067 to 0.26013, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2601 - dense_4_loss: 0.1120 - dense_3_loss: 0.1481\n",
            "Epoch 87/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2622 - dense_4_loss: 0.1131 - dense_3_loss: 0.1491\n",
            "Epoch 00087: loss did not improve from 0.26013\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2622 - dense_4_loss: 0.1131 - dense_3_loss: 0.1491\n",
            "Epoch 88/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2587 - dense_4_loss: 0.1114 - dense_3_loss: 0.1473\n",
            "Epoch 00088: loss improved from 0.26013 to 0.25868, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2587 - dense_4_loss: 0.1114 - dense_3_loss: 0.1473\n",
            "Epoch 89/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2601 - dense_4_loss: 0.1121 - dense_3_loss: 0.1481\n",
            "Epoch 00089: loss did not improve from 0.25868\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2601 - dense_4_loss: 0.1121 - dense_3_loss: 0.1481\n",
            "Epoch 90/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2586 - dense_4_loss: 0.1108 - dense_3_loss: 0.1478\n",
            "Epoch 00090: loss improved from 0.25868 to 0.25860, saving model to ae.model\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2586 - dense_4_loss: 0.1108 - dense_3_loss: 0.1478\n",
            "Epoch 91/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2607 - dense_4_loss: 0.1128 - dense_3_loss: 0.1479\n",
            "Epoch 00091: loss did not improve from 0.25860\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2607 - dense_4_loss: 0.1128 - dense_3_loss: 0.1479\n",
            "Epoch 92/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2576 - dense_4_loss: 0.1112 - dense_3_loss: 0.1463\n",
            "Epoch 00092: loss improved from 0.25860 to 0.25757, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2576 - dense_4_loss: 0.1112 - dense_3_loss: 0.1463\n",
            "Epoch 93/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2558 - dense_4_loss: 0.1108 - dense_3_loss: 0.1450\n",
            "Epoch 00093: loss improved from 0.25757 to 0.25577, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2558 - dense_4_loss: 0.1108 - dense_3_loss: 0.1450\n",
            "Epoch 94/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2554 - dense_4_loss: 0.1099 - dense_3_loss: 0.1455\n",
            "Epoch 00094: loss improved from 0.25577 to 0.25540, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2554 - dense_4_loss: 0.1099 - dense_3_loss: 0.1455\n",
            "Epoch 95/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2554 - dense_4_loss: 0.1103 - dense_3_loss: 0.1451\n",
            "Epoch 00095: loss improved from 0.25540 to 0.25536, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2554 - dense_4_loss: 0.1103 - dense_3_loss: 0.1451\n",
            "Epoch 96/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2541 - dense_4_loss: 0.1097 - dense_3_loss: 0.1444\n",
            "Epoch 00096: loss improved from 0.25536 to 0.25411, saving model to ae.model\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.2541 - dense_4_loss: 0.1097 - dense_3_loss: 0.1444\n",
            "Epoch 97/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2563 - dense_4_loss: 0.1106 - dense_3_loss: 0.1457\n",
            "Epoch 00097: loss did not improve from 0.25411\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2563 - dense_4_loss: 0.1106 - dense_3_loss: 0.1457\n",
            "Epoch 98/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2559 - dense_4_loss: 0.1122 - dense_3_loss: 0.1437\n",
            "Epoch 00098: loss did not improve from 0.25411\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.2559 - dense_4_loss: 0.1122 - dense_3_loss: 0.1437\n",
            "Epoch 99/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2537 - dense_4_loss: 0.1106 - dense_3_loss: 0.1431\n",
            "Epoch 00099: loss improved from 0.25411 to 0.25369, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2537 - dense_4_loss: 0.1106 - dense_3_loss: 0.1431\n",
            "Epoch 100/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2519 - dense_4_loss: 0.1095 - dense_3_loss: 0.1425\n",
            "Epoch 00100: loss improved from 0.25369 to 0.25194, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2519 - dense_4_loss: 0.1095 - dense_3_loss: 0.1425\n",
            "Epoch 101/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2536 - dense_4_loss: 0.1116 - dense_3_loss: 0.1420\n",
            "Epoch 00101: loss did not improve from 0.25194\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2536 - dense_4_loss: 0.1116 - dense_3_loss: 0.1420\n",
            "Epoch 102/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2526 - dense_4_loss: 0.1097 - dense_3_loss: 0.1429\n",
            "Epoch 00102: loss did not improve from 0.25194\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2526 - dense_4_loss: 0.1097 - dense_3_loss: 0.1429\n",
            "Epoch 103/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2486 - dense_4_loss: 0.1091 - dense_3_loss: 0.1396\n",
            "Epoch 00103: loss improved from 0.25194 to 0.24864, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2486 - dense_4_loss: 0.1091 - dense_3_loss: 0.1396\n",
            "Epoch 104/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2534 - dense_4_loss: 0.1101 - dense_3_loss: 0.1433\n",
            "Epoch 00104: loss did not improve from 0.24864\n",
            "535/535 [==============================] - 62s 117ms/step - loss: 0.2534 - dense_4_loss: 0.1101 - dense_3_loss: 0.1433\n",
            "Epoch 105/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2493 - dense_4_loss: 0.1079 - dense_3_loss: 0.1414\n",
            "Epoch 00105: loss did not improve from 0.24864\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2493 - dense_4_loss: 0.1079 - dense_3_loss: 0.1414\n",
            "Epoch 106/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2487 - dense_4_loss: 0.1092 - dense_3_loss: 0.1395\n",
            "Epoch 00106: loss did not improve from 0.24864\n",
            "535/535 [==============================] - 62s 117ms/step - loss: 0.2487 - dense_4_loss: 0.1092 - dense_3_loss: 0.1395\n",
            "Epoch 107/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2516 - dense_4_loss: 0.1102 - dense_3_loss: 0.1414\n",
            "Epoch 00107: loss did not improve from 0.24864\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2516 - dense_4_loss: 0.1102 - dense_3_loss: 0.1414\n",
            "Epoch 108/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2520 - dense_4_loss: 0.1084 - dense_3_loss: 0.1436\n",
            "Epoch 00108: loss did not improve from 0.24864\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2520 - dense_4_loss: 0.1084 - dense_3_loss: 0.1436\n",
            "Epoch 109/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2503 - dense_4_loss: 0.1094 - dense_3_loss: 0.1409\n",
            "Epoch 00109: loss did not improve from 0.24864\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2503 - dense_4_loss: 0.1094 - dense_3_loss: 0.1409\n",
            "Epoch 110/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2475 - dense_4_loss: 0.1099 - dense_3_loss: 0.1377\n",
            "Epoch 00110: loss improved from 0.24864 to 0.24755, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2475 - dense_4_loss: 0.1099 - dense_3_loss: 0.1377\n",
            "Epoch 111/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2479 - dense_4_loss: 0.1096 - dense_3_loss: 0.1383\n",
            "Epoch 00111: loss did not improve from 0.24755\n",
            "535/535 [==============================] - 62s 117ms/step - loss: 0.2479 - dense_4_loss: 0.1096 - dense_3_loss: 0.1383\n",
            "Epoch 112/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2491 - dense_4_loss: 0.1088 - dense_3_loss: 0.1402\n",
            "Epoch 00112: loss did not improve from 0.24755\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2491 - dense_4_loss: 0.1088 - dense_3_loss: 0.1402\n",
            "Epoch 113/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2480 - dense_4_loss: 0.1095 - dense_3_loss: 0.1385\n",
            "Epoch 00113: loss did not improve from 0.24755\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2480 - dense_4_loss: 0.1095 - dense_3_loss: 0.1385\n",
            "Epoch 114/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2470 - dense_4_loss: 0.1085 - dense_3_loss: 0.1384\n",
            "Epoch 00114: loss improved from 0.24755 to 0.24697, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2470 - dense_4_loss: 0.1085 - dense_3_loss: 0.1384\n",
            "Epoch 115/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2460 - dense_4_loss: 0.1077 - dense_3_loss: 0.1383\n",
            "Epoch 00115: loss improved from 0.24697 to 0.24598, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2460 - dense_4_loss: 0.1077 - dense_3_loss: 0.1383\n",
            "Epoch 116/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2453 - dense_4_loss: 0.1077 - dense_3_loss: 0.1376\n",
            "Epoch 00116: loss improved from 0.24598 to 0.24532, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2453 - dense_4_loss: 0.1077 - dense_3_loss: 0.1376\n",
            "Epoch 117/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2457 - dense_4_loss: 0.1082 - dense_3_loss: 0.1375\n",
            "Epoch 00117: loss did not improve from 0.24532\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2457 - dense_4_loss: 0.1082 - dense_3_loss: 0.1375\n",
            "Epoch 118/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2460 - dense_4_loss: 0.1078 - dense_3_loss: 0.1382\n",
            "Epoch 00118: loss did not improve from 0.24532\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2460 - dense_4_loss: 0.1078 - dense_3_loss: 0.1382\n",
            "Epoch 119/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2446 - dense_4_loss: 0.1070 - dense_3_loss: 0.1375\n",
            "Epoch 00119: loss improved from 0.24532 to 0.24455, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2446 - dense_4_loss: 0.1070 - dense_3_loss: 0.1375\n",
            "Epoch 120/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2430 - dense_4_loss: 0.1063 - dense_3_loss: 0.1367\n",
            "Epoch 00120: loss improved from 0.24455 to 0.24299, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2430 - dense_4_loss: 0.1063 - dense_3_loss: 0.1367\n",
            "Epoch 121/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2459 - dense_4_loss: 0.1073 - dense_3_loss: 0.1385\n",
            "Epoch 00121: loss did not improve from 0.24299\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2459 - dense_4_loss: 0.1073 - dense_3_loss: 0.1385\n",
            "Epoch 122/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2445 - dense_4_loss: 0.1084 - dense_3_loss: 0.1362\n",
            "Epoch 00122: loss did not improve from 0.24299\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2445 - dense_4_loss: 0.1084 - dense_3_loss: 0.1362\n",
            "Epoch 123/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2431 - dense_4_loss: 0.1066 - dense_3_loss: 0.1365\n",
            "Epoch 00123: loss did not improve from 0.24299\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2431 - dense_4_loss: 0.1066 - dense_3_loss: 0.1365\n",
            "Epoch 124/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2426 - dense_4_loss: 0.1062 - dense_3_loss: 0.1364\n",
            "Epoch 00124: loss improved from 0.24299 to 0.24259, saving model to ae.model\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.2426 - dense_4_loss: 0.1062 - dense_3_loss: 0.1364\n",
            "Epoch 125/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2417 - dense_4_loss: 0.1064 - dense_3_loss: 0.1353\n",
            "Epoch 00125: loss improved from 0.24259 to 0.24172, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2417 - dense_4_loss: 0.1064 - dense_3_loss: 0.1353\n",
            "Epoch 126/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2408 - dense_4_loss: 0.1056 - dense_3_loss: 0.1353\n",
            "Epoch 00126: loss improved from 0.24172 to 0.24082, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2408 - dense_4_loss: 0.1056 - dense_3_loss: 0.1353\n",
            "Epoch 127/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2418 - dense_4_loss: 0.1072 - dense_3_loss: 0.1346\n",
            "Epoch 00127: loss did not improve from 0.24082\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2418 - dense_4_loss: 0.1072 - dense_3_loss: 0.1346\n",
            "Epoch 128/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2404 - dense_4_loss: 0.1060 - dense_3_loss: 0.1345\n",
            "Epoch 00128: loss improved from 0.24082 to 0.24041, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2404 - dense_4_loss: 0.1060 - dense_3_loss: 0.1345\n",
            "Epoch 129/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2406 - dense_4_loss: 0.1062 - dense_3_loss: 0.1343\n",
            "Epoch 00129: loss did not improve from 0.24041\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2406 - dense_4_loss: 0.1062 - dense_3_loss: 0.1343\n",
            "Epoch 130/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2402 - dense_4_loss: 0.1054 - dense_3_loss: 0.1348\n",
            "Epoch 00130: loss improved from 0.24041 to 0.24016, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2402 - dense_4_loss: 0.1054 - dense_3_loss: 0.1348\n",
            "Epoch 131/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2419 - dense_4_loss: 0.1076 - dense_3_loss: 0.1343\n",
            "Epoch 00131: loss did not improve from 0.24016\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2419 - dense_4_loss: 0.1076 - dense_3_loss: 0.1343\n",
            "Epoch 132/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2419 - dense_4_loss: 0.1073 - dense_3_loss: 0.1346\n",
            "Epoch 00132: loss did not improve from 0.24016\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.2419 - dense_4_loss: 0.1073 - dense_3_loss: 0.1346\n",
            "Epoch 133/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2390 - dense_4_loss: 0.1056 - dense_3_loss: 0.1334\n",
            "Epoch 00133: loss improved from 0.24016 to 0.23902, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2390 - dense_4_loss: 0.1056 - dense_3_loss: 0.1334\n",
            "Epoch 134/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2405 - dense_4_loss: 0.1064 - dense_3_loss: 0.1341\n",
            "Epoch 00134: loss did not improve from 0.23902\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2405 - dense_4_loss: 0.1064 - dense_3_loss: 0.1341\n",
            "Epoch 135/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2392 - dense_4_loss: 0.1064 - dense_3_loss: 0.1327\n",
            "Epoch 00135: loss did not improve from 0.23902\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2392 - dense_4_loss: 0.1064 - dense_3_loss: 0.1327\n",
            "Epoch 136/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2382 - dense_4_loss: 0.1057 - dense_3_loss: 0.1326\n",
            "Epoch 00136: loss improved from 0.23902 to 0.23822, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2382 - dense_4_loss: 0.1057 - dense_3_loss: 0.1326\n",
            "Epoch 137/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2375 - dense_4_loss: 0.1052 - dense_3_loss: 0.1322\n",
            "Epoch 00137: loss improved from 0.23822 to 0.23746, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2375 - dense_4_loss: 0.1052 - dense_3_loss: 0.1322\n",
            "Epoch 138/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2415 - dense_4_loss: 0.1075 - dense_3_loss: 0.1340\n",
            "Epoch 00138: loss did not improve from 0.23746\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2415 - dense_4_loss: 0.1075 - dense_3_loss: 0.1340\n",
            "Epoch 139/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2369 - dense_4_loss: 0.1055 - dense_3_loss: 0.1314\n",
            "Epoch 00139: loss improved from 0.23746 to 0.23690, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2369 - dense_4_loss: 0.1055 - dense_3_loss: 0.1314\n",
            "Epoch 140/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2384 - dense_4_loss: 0.1050 - dense_3_loss: 0.1334\n",
            "Epoch 00140: loss did not improve from 0.23690\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2384 - dense_4_loss: 0.1050 - dense_3_loss: 0.1334\n",
            "Epoch 141/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2370 - dense_4_loss: 0.1056 - dense_3_loss: 0.1314\n",
            "Epoch 00141: loss did not improve from 0.23690\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2370 - dense_4_loss: 0.1056 - dense_3_loss: 0.1314\n",
            "Epoch 142/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2390 - dense_4_loss: 0.1049 - dense_3_loss: 0.1341\n",
            "Epoch 00142: loss did not improve from 0.23690\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2390 - dense_4_loss: 0.1049 - dense_3_loss: 0.1341\n",
            "Epoch 143/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2382 - dense_4_loss: 0.1077 - dense_3_loss: 0.1306\n",
            "Epoch 00143: loss did not improve from 0.23690\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2382 - dense_4_loss: 0.1077 - dense_3_loss: 0.1306\n",
            "Epoch 144/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2354 - dense_4_loss: 0.1042 - dense_3_loss: 0.1312\n",
            "Epoch 00144: loss improved from 0.23690 to 0.23541, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2354 - dense_4_loss: 0.1042 - dense_3_loss: 0.1312\n",
            "Epoch 145/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2362 - dense_4_loss: 0.1044 - dense_3_loss: 0.1319\n",
            "Epoch 00145: loss did not improve from 0.23541\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2362 - dense_4_loss: 0.1044 - dense_3_loss: 0.1319\n",
            "Epoch 146/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2363 - dense_4_loss: 0.1040 - dense_3_loss: 0.1323\n",
            "Epoch 00146: loss did not improve from 0.23541\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2363 - dense_4_loss: 0.1040 - dense_3_loss: 0.1323\n",
            "Epoch 147/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2321 - dense_4_loss: 0.1035 - dense_3_loss: 0.1286\n",
            "Epoch 00147: loss improved from 0.23541 to 0.23208, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2321 - dense_4_loss: 0.1035 - dense_3_loss: 0.1286\n",
            "Epoch 148/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2358 - dense_4_loss: 0.1055 - dense_3_loss: 0.1303\n",
            "Epoch 00148: loss did not improve from 0.23208\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2358 - dense_4_loss: 0.1055 - dense_3_loss: 0.1303\n",
            "Epoch 149/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2334 - dense_4_loss: 0.1031 - dense_3_loss: 0.1303\n",
            "Epoch 00149: loss did not improve from 0.23208\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2334 - dense_4_loss: 0.1031 - dense_3_loss: 0.1303\n",
            "Epoch 150/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2342 - dense_4_loss: 0.1045 - dense_3_loss: 0.1297\n",
            "Epoch 00150: loss did not improve from 0.23208\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2342 - dense_4_loss: 0.1045 - dense_3_loss: 0.1297\n",
            "Epoch 151/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2340 - dense_4_loss: 0.1046 - dense_3_loss: 0.1293\n",
            "Epoch 00151: loss did not improve from 0.23208\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2340 - dense_4_loss: 0.1046 - dense_3_loss: 0.1293\n",
            "Epoch 152/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2323 - dense_4_loss: 0.1019 - dense_3_loss: 0.1304\n",
            "Epoch 00152: loss did not improve from 0.23208\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2323 - dense_4_loss: 0.1019 - dense_3_loss: 0.1304\n",
            "Epoch 153/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2325 - dense_4_loss: 0.1029 - dense_3_loss: 0.1296\n",
            "Epoch 00153: loss did not improve from 0.23208\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2325 - dense_4_loss: 0.1029 - dense_3_loss: 0.1296\n",
            "Epoch 154/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2330 - dense_4_loss: 0.1045 - dense_3_loss: 0.1284\n",
            "Epoch 00154: loss did not improve from 0.23208\n",
            "535/535 [==============================] - 62s 117ms/step - loss: 0.2330 - dense_4_loss: 0.1045 - dense_3_loss: 0.1284\n",
            "Epoch 155/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2323 - dense_4_loss: 0.1038 - dense_3_loss: 0.1284\n",
            "Epoch 00155: loss did not improve from 0.23208\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2323 - dense_4_loss: 0.1038 - dense_3_loss: 0.1284\n",
            "Epoch 156/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2329 - dense_4_loss: 0.1037 - dense_3_loss: 0.1292\n",
            "Epoch 00156: loss did not improve from 0.23208\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2329 - dense_4_loss: 0.1037 - dense_3_loss: 0.1292\n",
            "Epoch 157/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2315 - dense_4_loss: 0.1027 - dense_3_loss: 0.1287\n",
            "Epoch 00157: loss improved from 0.23208 to 0.23147, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2315 - dense_4_loss: 0.1027 - dense_3_loss: 0.1287\n",
            "Epoch 158/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2308 - dense_4_loss: 0.1024 - dense_3_loss: 0.1285\n",
            "Epoch 00158: loss improved from 0.23147 to 0.23084, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2308 - dense_4_loss: 0.1024 - dense_3_loss: 0.1285\n",
            "Epoch 159/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2328 - dense_4_loss: 0.1043 - dense_3_loss: 0.1285\n",
            "Epoch 00159: loss did not improve from 0.23084\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2328 - dense_4_loss: 0.1043 - dense_3_loss: 0.1285\n",
            "Epoch 160/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2305 - dense_4_loss: 0.1025 - dense_3_loss: 0.1280\n",
            "Epoch 00160: loss improved from 0.23084 to 0.23047, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2305 - dense_4_loss: 0.1025 - dense_3_loss: 0.1280\n",
            "Epoch 161/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2296 - dense_4_loss: 0.1018 - dense_3_loss: 0.1278\n",
            "Epoch 00161: loss improved from 0.23047 to 0.22960, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2296 - dense_4_loss: 0.1018 - dense_3_loss: 0.1278\n",
            "Epoch 162/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2302 - dense_4_loss: 0.1027 - dense_3_loss: 0.1275\n",
            "Epoch 00162: loss did not improve from 0.22960\n",
            "535/535 [==============================] - 62s 117ms/step - loss: 0.2302 - dense_4_loss: 0.1027 - dense_3_loss: 0.1275\n",
            "Epoch 163/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2320 - dense_4_loss: 0.1030 - dense_3_loss: 0.1290\n",
            "Epoch 00163: loss did not improve from 0.22960\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2320 - dense_4_loss: 0.1030 - dense_3_loss: 0.1290\n",
            "Epoch 164/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2306 - dense_4_loss: 0.1027 - dense_3_loss: 0.1279\n",
            "Epoch 00164: loss did not improve from 0.22960\n",
            "535/535 [==============================] - 62s 117ms/step - loss: 0.2306 - dense_4_loss: 0.1027 - dense_3_loss: 0.1279\n",
            "Epoch 165/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2321 - dense_4_loss: 0.1040 - dense_3_loss: 0.1281\n",
            "Epoch 00165: loss did not improve from 0.22960\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2321 - dense_4_loss: 0.1040 - dense_3_loss: 0.1281\n",
            "Epoch 166/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2276 - dense_4_loss: 0.0999 - dense_3_loss: 0.1277\n",
            "Epoch 00166: loss improved from 0.22960 to 0.22758, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2276 - dense_4_loss: 0.0999 - dense_3_loss: 0.1277\n",
            "Epoch 167/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2265 - dense_4_loss: 0.1000 - dense_3_loss: 0.1265\n",
            "Epoch 00167: loss improved from 0.22758 to 0.22655, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2265 - dense_4_loss: 0.1000 - dense_3_loss: 0.1265\n",
            "Epoch 168/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2340 - dense_4_loss: 0.1043 - dense_3_loss: 0.1297\n",
            "Epoch 00168: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2340 - dense_4_loss: 0.1043 - dense_3_loss: 0.1297\n",
            "Epoch 169/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2283 - dense_4_loss: 0.1034 - dense_3_loss: 0.1249\n",
            "Epoch 00169: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2283 - dense_4_loss: 0.1034 - dense_3_loss: 0.1249\n",
            "Epoch 170/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2287 - dense_4_loss: 0.1019 - dense_3_loss: 0.1269\n",
            "Epoch 00170: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2287 - dense_4_loss: 0.1019 - dense_3_loss: 0.1269\n",
            "Epoch 171/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2313 - dense_4_loss: 0.1038 - dense_3_loss: 0.1275\n",
            "Epoch 00171: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2313 - dense_4_loss: 0.1038 - dense_3_loss: 0.1275\n",
            "Epoch 172/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2304 - dense_4_loss: 0.1031 - dense_3_loss: 0.1273\n",
            "Epoch 00172: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2304 - dense_4_loss: 0.1031 - dense_3_loss: 0.1273\n",
            "Epoch 173/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2313 - dense_4_loss: 0.1031 - dense_3_loss: 0.1282\n",
            "Epoch 00173: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2313 - dense_4_loss: 0.1031 - dense_3_loss: 0.1282\n",
            "Epoch 174/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2270 - dense_4_loss: 0.1008 - dense_3_loss: 0.1262\n",
            "Epoch 00174: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2270 - dense_4_loss: 0.1008 - dense_3_loss: 0.1262\n",
            "Epoch 175/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2270 - dense_4_loss: 0.1006 - dense_3_loss: 0.1264\n",
            "Epoch 00175: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2270 - dense_4_loss: 0.1006 - dense_3_loss: 0.1264\n",
            "Epoch 176/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2274 - dense_4_loss: 0.1001 - dense_3_loss: 0.1273\n",
            "Epoch 00176: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2274 - dense_4_loss: 0.1001 - dense_3_loss: 0.1273\n",
            "Epoch 177/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2267 - dense_4_loss: 0.1017 - dense_3_loss: 0.1250\n",
            "Epoch 00177: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2267 - dense_4_loss: 0.1017 - dense_3_loss: 0.1250\n",
            "Epoch 178/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2270 - dense_4_loss: 0.1025 - dense_3_loss: 0.1245\n",
            "Epoch 00178: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2270 - dense_4_loss: 0.1025 - dense_3_loss: 0.1245\n",
            "Epoch 179/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2285 - dense_4_loss: 0.1026 - dense_3_loss: 0.1258\n",
            "Epoch 00179: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2285 - dense_4_loss: 0.1026 - dense_3_loss: 0.1258\n",
            "Epoch 180/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2272 - dense_4_loss: 0.1017 - dense_3_loss: 0.1255\n",
            "Epoch 00180: loss did not improve from 0.22655\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2272 - dense_4_loss: 0.1017 - dense_3_loss: 0.1255\n",
            "Epoch 181/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2242 - dense_4_loss: 0.1002 - dense_3_loss: 0.1239\n",
            "Epoch 00181: loss improved from 0.22655 to 0.22417, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2242 - dense_4_loss: 0.1002 - dense_3_loss: 0.1239\n",
            "Epoch 182/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2263 - dense_4_loss: 0.1007 - dense_3_loss: 0.1257\n",
            "Epoch 00182: loss did not improve from 0.22417\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2263 - dense_4_loss: 0.1007 - dense_3_loss: 0.1257\n",
            "Epoch 183/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2249 - dense_4_loss: 0.0998 - dense_3_loss: 0.1251\n",
            "Epoch 00183: loss did not improve from 0.22417\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2249 - dense_4_loss: 0.0998 - dense_3_loss: 0.1251\n",
            "Epoch 184/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2263 - dense_4_loss: 0.1017 - dense_3_loss: 0.1246\n",
            "Epoch 00184: loss did not improve from 0.22417\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2263 - dense_4_loss: 0.1017 - dense_3_loss: 0.1246\n",
            "Epoch 185/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2258 - dense_4_loss: 0.1010 - dense_3_loss: 0.1248\n",
            "Epoch 00185: loss did not improve from 0.22417\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2258 - dense_4_loss: 0.1010 - dense_3_loss: 0.1248\n",
            "Epoch 186/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2258 - dense_4_loss: 0.1008 - dense_3_loss: 0.1250\n",
            "Epoch 00186: loss did not improve from 0.22417\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2258 - dense_4_loss: 0.1008 - dense_3_loss: 0.1250\n",
            "Epoch 187/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2249 - dense_4_loss: 0.1007 - dense_3_loss: 0.1242\n",
            "Epoch 00187: loss did not improve from 0.22417\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2249 - dense_4_loss: 0.1007 - dense_3_loss: 0.1242\n",
            "Epoch 188/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2275 - dense_4_loss: 0.1010 - dense_3_loss: 0.1265\n",
            "Epoch 00188: loss did not improve from 0.22417\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2275 - dense_4_loss: 0.1010 - dense_3_loss: 0.1265\n",
            "Epoch 189/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2255 - dense_4_loss: 0.0997 - dense_3_loss: 0.1257\n",
            "Epoch 00189: loss did not improve from 0.22417\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2255 - dense_4_loss: 0.0997 - dense_3_loss: 0.1257\n",
            "Epoch 190/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2251 - dense_4_loss: 0.1020 - dense_3_loss: 0.1231\n",
            "Epoch 00190: loss did not improve from 0.22417\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2251 - dense_4_loss: 0.1020 - dense_3_loss: 0.1231\n",
            "Epoch 191/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2237 - dense_4_loss: 0.1012 - dense_3_loss: 0.1225\n",
            "Epoch 00191: loss improved from 0.22417 to 0.22372, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2237 - dense_4_loss: 0.1012 - dense_3_loss: 0.1225\n",
            "Epoch 192/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2248 - dense_4_loss: 0.1013 - dense_3_loss: 0.1235\n",
            "Epoch 00192: loss did not improve from 0.22372\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2248 - dense_4_loss: 0.1013 - dense_3_loss: 0.1235\n",
            "Epoch 193/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2233 - dense_4_loss: 0.1004 - dense_3_loss: 0.1229\n",
            "Epoch 00193: loss improved from 0.22372 to 0.22329, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2233 - dense_4_loss: 0.1004 - dense_3_loss: 0.1229\n",
            "Epoch 194/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2217 - dense_4_loss: 0.0997 - dense_3_loss: 0.1220\n",
            "Epoch 00194: loss improved from 0.22329 to 0.22171, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2217 - dense_4_loss: 0.0997 - dense_3_loss: 0.1220\n",
            "Epoch 195/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2227 - dense_4_loss: 0.0993 - dense_3_loss: 0.1234\n",
            "Epoch 00195: loss did not improve from 0.22171\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2227 - dense_4_loss: 0.0993 - dense_3_loss: 0.1234\n",
            "Epoch 196/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2227 - dense_4_loss: 0.0997 - dense_3_loss: 0.1229\n",
            "Epoch 00196: loss did not improve from 0.22171\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2227 - dense_4_loss: 0.0997 - dense_3_loss: 0.1229\n",
            "Epoch 197/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2218 - dense_4_loss: 0.0988 - dense_3_loss: 0.1231\n",
            "Epoch 00197: loss did not improve from 0.22171\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2218 - dense_4_loss: 0.0988 - dense_3_loss: 0.1231\n",
            "Epoch 198/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2234 - dense_4_loss: 0.1000 - dense_3_loss: 0.1234\n",
            "Epoch 00198: loss did not improve from 0.22171\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2234 - dense_4_loss: 0.1000 - dense_3_loss: 0.1234\n",
            "Epoch 199/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2224 - dense_4_loss: 0.0992 - dense_3_loss: 0.1232\n",
            "Epoch 00199: loss did not improve from 0.22171\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2224 - dense_4_loss: 0.0992 - dense_3_loss: 0.1232\n",
            "Epoch 200/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2217 - dense_4_loss: 0.0998 - dense_3_loss: 0.1219\n",
            "Epoch 00200: loss improved from 0.22171 to 0.22169, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2217 - dense_4_loss: 0.0998 - dense_3_loss: 0.1219\n",
            "Epoch 201/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2233 - dense_4_loss: 0.1006 - dense_3_loss: 0.1226\n",
            "Epoch 00201: loss did not improve from 0.22169\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2233 - dense_4_loss: 0.1006 - dense_3_loss: 0.1226\n",
            "Epoch 202/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2218 - dense_4_loss: 0.1000 - dense_3_loss: 0.1218\n",
            "Epoch 00202: loss did not improve from 0.22169\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2218 - dense_4_loss: 0.1000 - dense_3_loss: 0.1218\n",
            "Epoch 203/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2241 - dense_4_loss: 0.1013 - dense_3_loss: 0.1229\n",
            "Epoch 00203: loss did not improve from 0.22169\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2241 - dense_4_loss: 0.1013 - dense_3_loss: 0.1229\n",
            "Epoch 204/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2216 - dense_4_loss: 0.0995 - dense_3_loss: 0.1221\n",
            "Epoch 00204: loss improved from 0.22169 to 0.22163, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2216 - dense_4_loss: 0.0995 - dense_3_loss: 0.1221\n",
            "Epoch 205/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2209 - dense_4_loss: 0.0995 - dense_3_loss: 0.1214\n",
            "Epoch 00205: loss improved from 0.22163 to 0.22087, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2209 - dense_4_loss: 0.0995 - dense_3_loss: 0.1214\n",
            "Epoch 206/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2235 - dense_4_loss: 0.1003 - dense_3_loss: 0.1232\n",
            "Epoch 00206: loss did not improve from 0.22087\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2235 - dense_4_loss: 0.1003 - dense_3_loss: 0.1232\n",
            "Epoch 207/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2196 - dense_4_loss: 0.0987 - dense_3_loss: 0.1209\n",
            "Epoch 00207: loss improved from 0.22087 to 0.21963, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2196 - dense_4_loss: 0.0987 - dense_3_loss: 0.1209\n",
            "Epoch 208/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2195 - dense_4_loss: 0.0992 - dense_3_loss: 0.1204\n",
            "Epoch 00208: loss improved from 0.21963 to 0.21955, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2195 - dense_4_loss: 0.0992 - dense_3_loss: 0.1204\n",
            "Epoch 209/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2199 - dense_4_loss: 0.0991 - dense_3_loss: 0.1207\n",
            "Epoch 00209: loss did not improve from 0.21955\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2199 - dense_4_loss: 0.0991 - dense_3_loss: 0.1207\n",
            "Epoch 210/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2208 - dense_4_loss: 0.1002 - dense_3_loss: 0.1206\n",
            "Epoch 00210: loss did not improve from 0.21955\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2208 - dense_4_loss: 0.1002 - dense_3_loss: 0.1206\n",
            "Epoch 211/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2211 - dense_4_loss: 0.1003 - dense_3_loss: 0.1208\n",
            "Epoch 00211: loss did not improve from 0.21955\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2211 - dense_4_loss: 0.1003 - dense_3_loss: 0.1208\n",
            "Epoch 212/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2200 - dense_4_loss: 0.0993 - dense_3_loss: 0.1207\n",
            "Epoch 00212: loss did not improve from 0.21955\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2200 - dense_4_loss: 0.0993 - dense_3_loss: 0.1207\n",
            "Epoch 213/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2200 - dense_4_loss: 0.0993 - dense_3_loss: 0.1208\n",
            "Epoch 00213: loss did not improve from 0.21955\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2200 - dense_4_loss: 0.0993 - dense_3_loss: 0.1208\n",
            "Epoch 214/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2190 - dense_4_loss: 0.0984 - dense_3_loss: 0.1207\n",
            "Epoch 00214: loss improved from 0.21955 to 0.21903, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2190 - dense_4_loss: 0.0984 - dense_3_loss: 0.1207\n",
            "Epoch 215/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2192 - dense_4_loss: 0.0981 - dense_3_loss: 0.1211\n",
            "Epoch 00215: loss did not improve from 0.21903\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2192 - dense_4_loss: 0.0981 - dense_3_loss: 0.1211\n",
            "Epoch 216/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2197 - dense_4_loss: 0.0984 - dense_3_loss: 0.1213\n",
            "Epoch 00216: loss did not improve from 0.21903\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2197 - dense_4_loss: 0.0984 - dense_3_loss: 0.1213\n",
            "Epoch 217/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2197 - dense_4_loss: 0.0998 - dense_3_loss: 0.1199\n",
            "Epoch 00217: loss did not improve from 0.21903\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2197 - dense_4_loss: 0.0998 - dense_3_loss: 0.1199\n",
            "Epoch 218/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2180 - dense_4_loss: 0.0989 - dense_3_loss: 0.1191\n",
            "Epoch 00218: loss improved from 0.21903 to 0.21802, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2180 - dense_4_loss: 0.0989 - dense_3_loss: 0.1191\n",
            "Epoch 219/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2196 - dense_4_loss: 0.0992 - dense_3_loss: 0.1204\n",
            "Epoch 00219: loss did not improve from 0.21802\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2196 - dense_4_loss: 0.0992 - dense_3_loss: 0.1204\n",
            "Epoch 220/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2217 - dense_4_loss: 0.1016 - dense_3_loss: 0.1201\n",
            "Epoch 00220: loss did not improve from 0.21802\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2217 - dense_4_loss: 0.1016 - dense_3_loss: 0.1201\n",
            "Epoch 221/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2215 - dense_4_loss: 0.1000 - dense_3_loss: 0.1215\n",
            "Epoch 00221: loss did not improve from 0.21802\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2215 - dense_4_loss: 0.1000 - dense_3_loss: 0.1215\n",
            "Epoch 222/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2165 - dense_4_loss: 0.0975 - dense_3_loss: 0.1190\n",
            "Epoch 00222: loss improved from 0.21802 to 0.21650, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2165 - dense_4_loss: 0.0975 - dense_3_loss: 0.1190\n",
            "Epoch 223/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2163 - dense_4_loss: 0.0984 - dense_3_loss: 0.1178\n",
            "Epoch 00223: loss improved from 0.21650 to 0.21626, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2163 - dense_4_loss: 0.0984 - dense_3_loss: 0.1178\n",
            "Epoch 224/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2166 - dense_4_loss: 0.0979 - dense_3_loss: 0.1187\n",
            "Epoch 00224: loss did not improve from 0.21626\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2166 - dense_4_loss: 0.0979 - dense_3_loss: 0.1187\n",
            "Epoch 225/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2200 - dense_4_loss: 0.0997 - dense_3_loss: 0.1203\n",
            "Epoch 00225: loss did not improve from 0.21626\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2200 - dense_4_loss: 0.0997 - dense_3_loss: 0.1203\n",
            "Epoch 226/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2200 - dense_4_loss: 0.0985 - dense_3_loss: 0.1214\n",
            "Epoch 00226: loss did not improve from 0.21626\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2200 - dense_4_loss: 0.0985 - dense_3_loss: 0.1214\n",
            "Epoch 227/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2193 - dense_4_loss: 0.0995 - dense_3_loss: 0.1198\n",
            "Epoch 00227: loss did not improve from 0.21626\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2193 - dense_4_loss: 0.0995 - dense_3_loss: 0.1198\n",
            "Epoch 228/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2184 - dense_4_loss: 0.0991 - dense_3_loss: 0.1193\n",
            "Epoch 00228: loss did not improve from 0.21626\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2184 - dense_4_loss: 0.0991 - dense_3_loss: 0.1193\n",
            "Epoch 229/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2153 - dense_4_loss: 0.0980 - dense_3_loss: 0.1173\n",
            "Epoch 00229: loss improved from 0.21626 to 0.21534, saving model to ae.model\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2153 - dense_4_loss: 0.0980 - dense_3_loss: 0.1173\n",
            "Epoch 230/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2201 - dense_4_loss: 0.1005 - dense_3_loss: 0.1195\n",
            "Epoch 00230: loss did not improve from 0.21534\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2201 - dense_4_loss: 0.1005 - dense_3_loss: 0.1195\n",
            "Epoch 231/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2193 - dense_4_loss: 0.0995 - dense_3_loss: 0.1199\n",
            "Epoch 00231: loss did not improve from 0.21534\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2193 - dense_4_loss: 0.0995 - dense_3_loss: 0.1199\n",
            "Epoch 232/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2139 - dense_4_loss: 0.0969 - dense_3_loss: 0.1170\n",
            "Epoch 00232: loss improved from 0.21534 to 0.21390, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2139 - dense_4_loss: 0.0969 - dense_3_loss: 0.1170\n",
            "Epoch 233/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2177 - dense_4_loss: 0.0991 - dense_3_loss: 0.1187\n",
            "Epoch 00233: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2177 - dense_4_loss: 0.0991 - dense_3_loss: 0.1187\n",
            "Epoch 234/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2173 - dense_4_loss: 0.0982 - dense_3_loss: 0.1190\n",
            "Epoch 00234: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2173 - dense_4_loss: 0.0982 - dense_3_loss: 0.1190\n",
            "Epoch 235/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2152 - dense_4_loss: 0.0975 - dense_3_loss: 0.1177\n",
            "Epoch 00235: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2152 - dense_4_loss: 0.0975 - dense_3_loss: 0.1177\n",
            "Epoch 236/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2148 - dense_4_loss: 0.0966 - dense_3_loss: 0.1182\n",
            "Epoch 00236: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2148 - dense_4_loss: 0.0966 - dense_3_loss: 0.1182\n",
            "Epoch 237/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2164 - dense_4_loss: 0.0994 - dense_3_loss: 0.1170\n",
            "Epoch 00237: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2164 - dense_4_loss: 0.0994 - dense_3_loss: 0.1170\n",
            "Epoch 238/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2141 - dense_4_loss: 0.0968 - dense_3_loss: 0.1173\n",
            "Epoch 00238: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2141 - dense_4_loss: 0.0968 - dense_3_loss: 0.1173\n",
            "Epoch 239/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2149 - dense_4_loss: 0.0975 - dense_3_loss: 0.1174\n",
            "Epoch 00239: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2149 - dense_4_loss: 0.0975 - dense_3_loss: 0.1174\n",
            "Epoch 240/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2141 - dense_4_loss: 0.0959 - dense_3_loss: 0.1182\n",
            "Epoch 00240: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2141 - dense_4_loss: 0.0959 - dense_3_loss: 0.1182\n",
            "Epoch 241/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2158 - dense_4_loss: 0.0977 - dense_3_loss: 0.1180\n",
            "Epoch 00241: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2158 - dense_4_loss: 0.0977 - dense_3_loss: 0.1180\n",
            "Epoch 242/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2156 - dense_4_loss: 0.0971 - dense_3_loss: 0.1186\n",
            "Epoch 00242: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2156 - dense_4_loss: 0.0971 - dense_3_loss: 0.1186\n",
            "Epoch 243/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2168 - dense_4_loss: 0.0994 - dense_3_loss: 0.1174\n",
            "Epoch 00243: loss did not improve from 0.21390\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2168 - dense_4_loss: 0.0994 - dense_3_loss: 0.1174\n",
            "Epoch 244/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2123 - dense_4_loss: 0.0960 - dense_3_loss: 0.1163\n",
            "Epoch 00244: loss improved from 0.21390 to 0.21227, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2123 - dense_4_loss: 0.0960 - dense_3_loss: 0.1163\n",
            "Epoch 245/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2145 - dense_4_loss: 0.0973 - dense_3_loss: 0.1172\n",
            "Epoch 00245: loss did not improve from 0.21227\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2145 - dense_4_loss: 0.0973 - dense_3_loss: 0.1172\n",
            "Epoch 246/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2155 - dense_4_loss: 0.0982 - dense_3_loss: 0.1173\n",
            "Epoch 00246: loss did not improve from 0.21227\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2155 - dense_4_loss: 0.0982 - dense_3_loss: 0.1173\n",
            "Epoch 247/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2138 - dense_4_loss: 0.0971 - dense_3_loss: 0.1167\n",
            "Epoch 00247: loss did not improve from 0.21227\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2138 - dense_4_loss: 0.0971 - dense_3_loss: 0.1167\n",
            "Epoch 248/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2119 - dense_4_loss: 0.0962 - dense_3_loss: 0.1157\n",
            "Epoch 00248: loss improved from 0.21227 to 0.21190, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2119 - dense_4_loss: 0.0962 - dense_3_loss: 0.1157\n",
            "Epoch 249/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2151 - dense_4_loss: 0.0985 - dense_3_loss: 0.1166\n",
            "Epoch 00249: loss did not improve from 0.21190\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2151 - dense_4_loss: 0.0985 - dense_3_loss: 0.1166\n",
            "Epoch 250/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2131 - dense_4_loss: 0.0967 - dense_3_loss: 0.1163\n",
            "Epoch 00250: loss did not improve from 0.21190\n",
            "535/535 [==============================] - 66s 123ms/step - loss: 0.2131 - dense_4_loss: 0.0967 - dense_3_loss: 0.1163\n",
            "Epoch 251/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2133 - dense_4_loss: 0.0969 - dense_3_loss: 0.1163\n",
            "Epoch 00251: loss did not improve from 0.21190\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2133 - dense_4_loss: 0.0969 - dense_3_loss: 0.1163\n",
            "Epoch 252/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2132 - dense_4_loss: 0.0959 - dense_3_loss: 0.1172\n",
            "Epoch 00252: loss did not improve from 0.21190\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2132 - dense_4_loss: 0.0959 - dense_3_loss: 0.1172\n",
            "Epoch 253/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2151 - dense_4_loss: 0.0980 - dense_3_loss: 0.1171\n",
            "Epoch 00253: loss did not improve from 0.21190\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2151 - dense_4_loss: 0.0980 - dense_3_loss: 0.1171\n",
            "Epoch 254/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2145 - dense_4_loss: 0.0980 - dense_3_loss: 0.1165\n",
            "Epoch 00254: loss did not improve from 0.21190\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2145 - dense_4_loss: 0.0980 - dense_3_loss: 0.1165\n",
            "Epoch 255/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2110 - dense_4_loss: 0.0956 - dense_3_loss: 0.1154\n",
            "Epoch 00255: loss improved from 0.21190 to 0.21100, saving model to ae.model\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2110 - dense_4_loss: 0.0956 - dense_3_loss: 0.1154\n",
            "Epoch 256/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2127 - dense_4_loss: 0.0969 - dense_3_loss: 0.1159\n",
            "Epoch 00256: loss did not improve from 0.21100\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2127 - dense_4_loss: 0.0969 - dense_3_loss: 0.1159\n",
            "Epoch 257/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2127 - dense_4_loss: 0.0967 - dense_3_loss: 0.1160\n",
            "Epoch 00257: loss did not improve from 0.21100\n",
            "535/535 [==============================] - 63s 117ms/step - loss: 0.2127 - dense_4_loss: 0.0967 - dense_3_loss: 0.1160\n",
            "Epoch 258/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2108 - dense_4_loss: 0.0965 - dense_3_loss: 0.1143\n",
            "Epoch 00258: loss improved from 0.21100 to 0.21084, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2108 - dense_4_loss: 0.0965 - dense_3_loss: 0.1143\n",
            "Epoch 259/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2099 - dense_4_loss: 0.0956 - dense_3_loss: 0.1143\n",
            "Epoch 00259: loss improved from 0.21084 to 0.20989, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2099 - dense_4_loss: 0.0956 - dense_3_loss: 0.1143\n",
            "Epoch 260/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2128 - dense_4_loss: 0.0965 - dense_3_loss: 0.1163\n",
            "Epoch 00260: loss did not improve from 0.20989\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2128 - dense_4_loss: 0.0965 - dense_3_loss: 0.1163\n",
            "Epoch 261/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2127 - dense_4_loss: 0.0961 - dense_3_loss: 0.1166\n",
            "Epoch 00261: loss did not improve from 0.20989\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2127 - dense_4_loss: 0.0961 - dense_3_loss: 0.1166\n",
            "Epoch 262/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2137 - dense_4_loss: 0.0980 - dense_3_loss: 0.1157\n",
            "Epoch 00262: loss did not improve from 0.20989\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2137 - dense_4_loss: 0.0980 - dense_3_loss: 0.1157\n",
            "Epoch 263/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2106 - dense_4_loss: 0.0964 - dense_3_loss: 0.1142\n",
            "Epoch 00263: loss did not improve from 0.20989\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2106 - dense_4_loss: 0.0964 - dense_3_loss: 0.1142\n",
            "Epoch 264/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2120 - dense_4_loss: 0.0960 - dense_3_loss: 0.1160\n",
            "Epoch 00264: loss did not improve from 0.20989\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2120 - dense_4_loss: 0.0960 - dense_3_loss: 0.1160\n",
            "Epoch 265/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2111 - dense_4_loss: 0.0976 - dense_3_loss: 0.1135\n",
            "Epoch 00265: loss did not improve from 0.20989\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2111 - dense_4_loss: 0.0976 - dense_3_loss: 0.1135\n",
            "Epoch 266/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2081 - dense_4_loss: 0.0955 - dense_3_loss: 0.1126\n",
            "Epoch 00266: loss improved from 0.20989 to 0.20810, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2081 - dense_4_loss: 0.0955 - dense_3_loss: 0.1126\n",
            "Epoch 267/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2094 - dense_4_loss: 0.0966 - dense_3_loss: 0.1127\n",
            "Epoch 00267: loss did not improve from 0.20810\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2094 - dense_4_loss: 0.0966 - dense_3_loss: 0.1127\n",
            "Epoch 268/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2101 - dense_4_loss: 0.0962 - dense_3_loss: 0.1138\n",
            "Epoch 00268: loss did not improve from 0.20810\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2101 - dense_4_loss: 0.0962 - dense_3_loss: 0.1138\n",
            "Epoch 269/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2084 - dense_4_loss: 0.0945 - dense_3_loss: 0.1139\n",
            "Epoch 00269: loss did not improve from 0.20810\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2084 - dense_4_loss: 0.0945 - dense_3_loss: 0.1139\n",
            "Epoch 270/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2074 - dense_4_loss: 0.0940 - dense_3_loss: 0.1134\n",
            "Epoch 00270: loss improved from 0.20810 to 0.20736, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2074 - dense_4_loss: 0.0940 - dense_3_loss: 0.1134\n",
            "Epoch 271/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2093 - dense_4_loss: 0.0954 - dense_3_loss: 0.1139\n",
            "Epoch 00271: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.2093 - dense_4_loss: 0.0954 - dense_3_loss: 0.1139\n",
            "Epoch 272/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2112 - dense_4_loss: 0.0972 - dense_3_loss: 0.1139\n",
            "Epoch 00272: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2112 - dense_4_loss: 0.0972 - dense_3_loss: 0.1139\n",
            "Epoch 273/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2084 - dense_4_loss: 0.0956 - dense_3_loss: 0.1128\n",
            "Epoch 00273: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2084 - dense_4_loss: 0.0956 - dense_3_loss: 0.1128\n",
            "Epoch 274/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2080 - dense_4_loss: 0.0958 - dense_3_loss: 0.1122\n",
            "Epoch 00274: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2080 - dense_4_loss: 0.0958 - dense_3_loss: 0.1122\n",
            "Epoch 275/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2113 - dense_4_loss: 0.0960 - dense_3_loss: 0.1153\n",
            "Epoch 00275: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2113 - dense_4_loss: 0.0960 - dense_3_loss: 0.1153\n",
            "Epoch 276/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2106 - dense_4_loss: 0.0955 - dense_3_loss: 0.1150\n",
            "Epoch 00276: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2106 - dense_4_loss: 0.0955 - dense_3_loss: 0.1150\n",
            "Epoch 277/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2104 - dense_4_loss: 0.0964 - dense_3_loss: 0.1140\n",
            "Epoch 00277: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2104 - dense_4_loss: 0.0964 - dense_3_loss: 0.1140\n",
            "Epoch 278/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2098 - dense_4_loss: 0.0956 - dense_3_loss: 0.1142\n",
            "Epoch 00278: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 66s 123ms/step - loss: 0.2098 - dense_4_loss: 0.0956 - dense_3_loss: 0.1142\n",
            "Epoch 279/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2104 - dense_4_loss: 0.0969 - dense_3_loss: 0.1135\n",
            "Epoch 00279: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2104 - dense_4_loss: 0.0969 - dense_3_loss: 0.1135\n",
            "Epoch 280/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2077 - dense_4_loss: 0.0955 - dense_3_loss: 0.1122\n",
            "Epoch 00280: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2077 - dense_4_loss: 0.0955 - dense_3_loss: 0.1122\n",
            "Epoch 281/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2097 - dense_4_loss: 0.0967 - dense_3_loss: 0.1130\n",
            "Epoch 00281: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2097 - dense_4_loss: 0.0967 - dense_3_loss: 0.1130\n",
            "Epoch 282/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2088 - dense_4_loss: 0.0962 - dense_3_loss: 0.1126\n",
            "Epoch 00282: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2088 - dense_4_loss: 0.0962 - dense_3_loss: 0.1126\n",
            "Epoch 283/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2098 - dense_4_loss: 0.0958 - dense_3_loss: 0.1141\n",
            "Epoch 00283: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2098 - dense_4_loss: 0.0958 - dense_3_loss: 0.1141\n",
            "Epoch 284/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2113 - dense_4_loss: 0.0973 - dense_3_loss: 0.1140\n",
            "Epoch 00284: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2113 - dense_4_loss: 0.0973 - dense_3_loss: 0.1140\n",
            "Epoch 285/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2083 - dense_4_loss: 0.0955 - dense_3_loss: 0.1128\n",
            "Epoch 00285: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2083 - dense_4_loss: 0.0955 - dense_3_loss: 0.1128\n",
            "Epoch 286/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2082 - dense_4_loss: 0.0965 - dense_3_loss: 0.1117\n",
            "Epoch 00286: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2082 - dense_4_loss: 0.0965 - dense_3_loss: 0.1117\n",
            "Epoch 287/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2093 - dense_4_loss: 0.0948 - dense_3_loss: 0.1145\n",
            "Epoch 00287: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.2093 - dense_4_loss: 0.0948 - dense_3_loss: 0.1145\n",
            "Epoch 288/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2088 - dense_4_loss: 0.0958 - dense_3_loss: 0.1131\n",
            "Epoch 00288: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2088 - dense_4_loss: 0.0958 - dense_3_loss: 0.1131\n",
            "Epoch 289/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2108 - dense_4_loss: 0.0990 - dense_3_loss: 0.1118\n",
            "Epoch 00289: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2108 - dense_4_loss: 0.0990 - dense_3_loss: 0.1118\n",
            "Epoch 290/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2077 - dense_4_loss: 0.0944 - dense_3_loss: 0.1133\n",
            "Epoch 00290: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2077 - dense_4_loss: 0.0944 - dense_3_loss: 0.1133\n",
            "Epoch 291/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2100 - dense_4_loss: 0.0965 - dense_3_loss: 0.1135\n",
            "Epoch 00291: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2100 - dense_4_loss: 0.0965 - dense_3_loss: 0.1135\n",
            "Epoch 292/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2099 - dense_4_loss: 0.0960 - dense_3_loss: 0.1140\n",
            "Epoch 00292: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2099 - dense_4_loss: 0.0960 - dense_3_loss: 0.1140\n",
            "Epoch 293/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2074 - dense_4_loss: 0.0937 - dense_3_loss: 0.1136\n",
            "Epoch 00293: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 66s 123ms/step - loss: 0.2074 - dense_4_loss: 0.0937 - dense_3_loss: 0.1136\n",
            "Epoch 294/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2075 - dense_4_loss: 0.0947 - dense_3_loss: 0.1128\n",
            "Epoch 00294: loss did not improve from 0.20736\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2075 - dense_4_loss: 0.0947 - dense_3_loss: 0.1128\n",
            "Epoch 295/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2072 - dense_4_loss: 0.0955 - dense_3_loss: 0.1117\n",
            "Epoch 00295: loss improved from 0.20736 to 0.20723, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2072 - dense_4_loss: 0.0955 - dense_3_loss: 0.1117\n",
            "Epoch 296/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2093 - dense_4_loss: 0.0950 - dense_3_loss: 0.1143\n",
            "Epoch 00296: loss did not improve from 0.20723\n",
            "535/535 [==============================] - 63s 119ms/step - loss: 0.2093 - dense_4_loss: 0.0950 - dense_3_loss: 0.1143\n",
            "Epoch 297/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2088 - dense_4_loss: 0.0974 - dense_3_loss: 0.1114\n",
            "Epoch 00297: loss did not improve from 0.20723\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2088 - dense_4_loss: 0.0974 - dense_3_loss: 0.1114\n",
            "Epoch 298/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2064 - dense_4_loss: 0.0947 - dense_3_loss: 0.1117\n",
            "Epoch 00298: loss improved from 0.20723 to 0.20639, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2064 - dense_4_loss: 0.0947 - dense_3_loss: 0.1117\n",
            "Epoch 299/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2094 - dense_4_loss: 0.0955 - dense_3_loss: 0.1139\n",
            "Epoch 00299: loss did not improve from 0.20639\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2094 - dense_4_loss: 0.0955 - dense_3_loss: 0.1139\n",
            "Epoch 300/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2102 - dense_4_loss: 0.0978 - dense_3_loss: 0.1124\n",
            "Epoch 00300: loss did not improve from 0.20639\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2102 - dense_4_loss: 0.0978 - dense_3_loss: 0.1124\n",
            "Epoch 301/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2063 - dense_4_loss: 0.0938 - dense_3_loss: 0.1125\n",
            "Epoch 00301: loss improved from 0.20639 to 0.20633, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2063 - dense_4_loss: 0.0938 - dense_3_loss: 0.1125\n",
            "Epoch 302/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2061 - dense_4_loss: 0.0947 - dense_3_loss: 0.1114\n",
            "Epoch 00302: loss improved from 0.20633 to 0.20606, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2061 - dense_4_loss: 0.0947 - dense_3_loss: 0.1114\n",
            "Epoch 303/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2058 - dense_4_loss: 0.0941 - dense_3_loss: 0.1117\n",
            "Epoch 00303: loss improved from 0.20606 to 0.20578, saving model to ae.model\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2058 - dense_4_loss: 0.0941 - dense_3_loss: 0.1117\n",
            "Epoch 304/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2079 - dense_4_loss: 0.0936 - dense_3_loss: 0.1143\n",
            "Epoch 00304: loss did not improve from 0.20578\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2079 - dense_4_loss: 0.0936 - dense_3_loss: 0.1143\n",
            "Epoch 305/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2052 - dense_4_loss: 0.0942 - dense_3_loss: 0.1110\n",
            "Epoch 00305: loss improved from 0.20578 to 0.20521, saving model to ae.model\n",
            "535/535 [==============================] - 63s 118ms/step - loss: 0.2052 - dense_4_loss: 0.0942 - dense_3_loss: 0.1110\n",
            "Epoch 306/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2077 - dense_4_loss: 0.0963 - dense_3_loss: 0.1115\n",
            "Epoch 00306: loss did not improve from 0.20521\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2077 - dense_4_loss: 0.0963 - dense_3_loss: 0.1115\n",
            "Epoch 307/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2081 - dense_4_loss: 0.0951 - dense_3_loss: 0.1129\n",
            "Epoch 00307: loss did not improve from 0.20521\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2081 - dense_4_loss: 0.0951 - dense_3_loss: 0.1129\n",
            "Epoch 308/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2029 - dense_4_loss: 0.0943 - dense_3_loss: 0.1085\n",
            "Epoch 00308: loss improved from 0.20521 to 0.20287, saving model to ae.model\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2029 - dense_4_loss: 0.0943 - dense_3_loss: 0.1085\n",
            "Epoch 309/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2060 - dense_4_loss: 0.0936 - dense_3_loss: 0.1124\n",
            "Epoch 00309: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2060 - dense_4_loss: 0.0936 - dense_3_loss: 0.1124\n",
            "Epoch 310/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2053 - dense_4_loss: 0.0945 - dense_3_loss: 0.1108\n",
            "Epoch 00310: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2053 - dense_4_loss: 0.0945 - dense_3_loss: 0.1108\n",
            "Epoch 311/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2055 - dense_4_loss: 0.0943 - dense_3_loss: 0.1112\n",
            "Epoch 00311: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2055 - dense_4_loss: 0.0943 - dense_3_loss: 0.1112\n",
            "Epoch 312/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2061 - dense_4_loss: 0.0948 - dense_3_loss: 0.1112\n",
            "Epoch 00312: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2061 - dense_4_loss: 0.0948 - dense_3_loss: 0.1112\n",
            "Epoch 313/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2068 - dense_4_loss: 0.0954 - dense_3_loss: 0.1115\n",
            "Epoch 00313: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2068 - dense_4_loss: 0.0954 - dense_3_loss: 0.1115\n",
            "Epoch 314/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2067 - dense_4_loss: 0.0963 - dense_3_loss: 0.1104\n",
            "Epoch 00314: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 65s 121ms/step - loss: 0.2067 - dense_4_loss: 0.0963 - dense_3_loss: 0.1104\n",
            "Epoch 315/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2043 - dense_4_loss: 0.0945 - dense_3_loss: 0.1098\n",
            "Epoch 00315: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 64s 119ms/step - loss: 0.2043 - dense_4_loss: 0.0945 - dense_3_loss: 0.1098\n",
            "Epoch 316/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2044 - dense_4_loss: 0.0943 - dense_3_loss: 0.1101\n",
            "Epoch 00316: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2044 - dense_4_loss: 0.0943 - dense_3_loss: 0.1101\n",
            "Epoch 317/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2054 - dense_4_loss: 0.0941 - dense_3_loss: 0.1113\n",
            "Epoch 00317: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 64s 120ms/step - loss: 0.2054 - dense_4_loss: 0.0941 - dense_3_loss: 0.1113\n",
            "Epoch 318/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2082 - dense_4_loss: 0.0971 - dense_3_loss: 0.1112\n",
            "Epoch 00318: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2082 - dense_4_loss: 0.0971 - dense_3_loss: 0.1112\n",
            "Epoch 319/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2051 - dense_4_loss: 0.0942 - dense_3_loss: 0.1109\n",
            "Epoch 00319: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2051 - dense_4_loss: 0.0942 - dense_3_loss: 0.1109\n",
            "Epoch 320/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2046 - dense_4_loss: 0.0941 - dense_3_loss: 0.1105\n",
            "Epoch 00320: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2046 - dense_4_loss: 0.0941 - dense_3_loss: 0.1105\n",
            "Epoch 321/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2070 - dense_4_loss: 0.0944 - dense_3_loss: 0.1126\n",
            "Epoch 00321: loss did not improve from 0.20287\n",
            "535/535 [==============================] - 66s 124ms/step - loss: 0.2070 - dense_4_loss: 0.0944 - dense_3_loss: 0.1126\n",
            "Epoch 322/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2021 - dense_4_loss: 0.0927 - dense_3_loss: 0.1094\n",
            "Epoch 00322: loss improved from 0.20287 to 0.20213, saving model to ae.model\n",
            "535/535 [==============================] - 65s 122ms/step - loss: 0.2021 - dense_4_loss: 0.0927 - dense_3_loss: 0.1094\n",
            "Epoch 323/1000\n",
            "535/535 [==============================] - ETA: 0s - loss: 0.2068 - dense_4_loss: 0.0950 - dense_3_loss: 0.1119\n",
            "Epoch 00323: loss did not improve from 0.20213\n",
            "535/535 [==============================] - 64s 121ms/step - loss: 0.2068 - dense_4_loss: 0.0950 - dense_3_loss: 0.1119\n",
            "Epoch 324/1000\n",
            "389/535 [====================>.........] - ETA: 17s - loss: 0.2049 - dense_4_loss: 0.0948 - dense_3_loss: 0.1101"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNx0SXIhhb0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = F\"/content/gdrive/My Drive/autoenc.hdf5\" \n",
        "model_mse.save_weights(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMgaSIGq7Ux1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldPyXXg2s7I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u2bD5IT3XFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod=Model(inputs=model_mse.inputs,outputs=model_mse.layers[4].output)\n",
        "mod.summary()\n",
        "pre=mod.predict((X[num],X[tot]))\n",
        "pre.shape\n",
        "np.save('/content/gdrive/My Drive/fraud/without_id_withoutV.npy',pre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvZpVVMA3dYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
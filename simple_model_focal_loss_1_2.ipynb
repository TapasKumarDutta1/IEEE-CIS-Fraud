{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_1.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "65e642ce-2053-4fc0-b696-f703aba09a62"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "e48cfe38-75a0-4ce4-e490-767430f87692"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 52.8MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 86% 50.0M/58.3M [00:02<00:00, 14.5MB/s]\n",
            "100% 58.3M/58.3M [00:02<00:00, 29.6MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 81% 42.0M/52.2M [00:00<00:00, 63.0MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 88.9MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 108MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 77.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDrCIAqHzl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "accb088e-1ebe-4c26-b38b-c587b933fae3"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWJ-hzcznam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5OQjb1zo6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "ad0df66b-26a5-4ff1-971a-0fcc8028e7fb"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "871aa364-d085-4078-a7a4-5b59ecd285e9"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0r3SuH1K97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQ20JqWATak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQIVOLKBFIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2948d744-e693-418d-c6af-b5d28aa8cbb4"
      },
      "source": [
        "1-0.036"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.964"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97bb7a19-35e7-42d9-f900-a5a64442804a"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=1.2\n",
        "        alpha=1-0.036\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss\n",
        "dk={}\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "for en,month in enumerate([(4,5),(3,4),(3,5)]):\n",
        "  train=trn.loc[trn['month']>=month[1]]\n",
        "  test=trn.loc[trn['month']<=month[0]]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.0001,decay=1e-3),loss=fl())\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 26ms/step - loss: 64.4389 - val_loss: 38.1138\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 54.9840 - val_loss: 37.2575\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 51.9432 - val_loss: 37.0140\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 49.9482 - val_loss: 36.6705\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 46.6326 - val_loss: 36.7056\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 46.0479 - val_loss: 36.7508\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 45.0298 - val_loss: 36.6745\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 44.3881 - val_loss: 36.6572\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 42.6346 - val_loss: 36.6200\n",
            "Epoch 10/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 42.6020roc-auc_val: 0.7805\n",
            "47/47 [==============================] - 14s 294ms/step - loss: 42.3446 - val_loss: 36.4514\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 41.6614 - val_loss: 36.3614\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 41.0715 - val_loss: 36.4207\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 39.4739 - val_loss: 36.3471\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 39.7867 - val_loss: 36.2963\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 39.5363 - val_loss: 36.2410\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 38.3208 - val_loss: 36.3740\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 38.4110 - val_loss: 36.1792\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 38.0009 - val_loss: 36.1270\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 37.5804 - val_loss: 36.0615\n",
            "Epoch 20/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 37.2877roc-auc_val: 0.7914\n",
            "47/47 [==============================] - 14s 292ms/step - loss: 36.7118 - val_loss: 36.0004\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 37.0545 - val_loss: 35.9993\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 36.9254 - val_loss: 36.0296\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 36.7344 - val_loss: 35.9061\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 36.1180 - val_loss: 35.9038\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 36.0692 - val_loss: 35.9303\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 36.0084 - val_loss: 35.9201\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 35.2009 - val_loss: 35.8561\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 35.3989 - val_loss: 35.9573\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 34.9519 - val_loss: 35.9113\n",
            "Epoch 30/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 35.2449roc-auc_val: 0.7959\n",
            "47/47 [==============================] - 14s 295ms/step - loss: 35.1456 - val_loss: 35.8609\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 34.5518 - val_loss: 35.8344\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 34.4287 - val_loss: 35.8441\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 34.2650 - val_loss: 35.8846\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 33.7969 - val_loss: 35.8340\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 33.5441 - val_loss: 35.8153\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 33.4031 - val_loss: 35.7829\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 33.9957 - val_loss: 35.8383\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 33.5597 - val_loss: 35.8384\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.3662 - val_loss: 35.7875\n",
            "Epoch 40/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 33.3783roc-auc_val: 0.799\n",
            "47/47 [==============================] - 14s 297ms/step - loss: 33.2362 - val_loss: 35.7628\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.6987 - val_loss: 35.7160\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.8078 - val_loss: 35.7517\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.8269 - val_loss: 35.7888\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 32.4616 - val_loss: 35.7254\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.4727 - val_loss: 35.7222\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 32.3010 - val_loss: 35.7274\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.3694 - val_loss: 35.7598\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.1375 - val_loss: 35.7004\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.2864 - val_loss: 35.6995\n",
            "Epoch 50/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 32.3061roc-auc_val: 0.8016\n",
            "47/47 [==============================] - 14s 296ms/step - loss: 32.5096 - val_loss: 35.7158\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.4264 - val_loss: 35.6728\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 31.9466 - val_loss: 35.6668\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 31.8018 - val_loss: 35.6533\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 31.5418 - val_loss: 35.6782\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.5123 - val_loss: 35.6981\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 31.5392 - val_loss: 35.7065\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.1838 - val_loss: 35.7014\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.9004 - val_loss: 35.7044\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.5043 - val_loss: 35.7263\n",
            "Epoch 60/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 31.0880roc-auc_val: 0.8028\n",
            "47/47 [==============================] - 14s 305ms/step - loss: 31.2621 - val_loss: 35.7167\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 30.8157 - val_loss: 35.7334\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 31.0351 - val_loss: 35.7331\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 30.3283 - val_loss: 35.6984\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 30.3018 - val_loss: 35.7436\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 30.6239 - val_loss: 35.7923\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.8097 - val_loss: 35.7787\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 30.3712 - val_loss: 35.7888\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.6775 - val_loss: 35.8245\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.4725 - val_loss: 35.8201\n",
            "Epoch 70/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 30.9141roc-auc_val: 0.8044\n",
            "47/47 [==============================] - 14s 291ms/step - loss: 30.7309 - val_loss: 35.8059\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.0742 - val_loss: 35.7698\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.6240 - val_loss: 35.7603\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.0961 - val_loss: 35.7239\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.7005 - val_loss: 35.7149\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.0448 - val_loss: 35.7478\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.3347 - val_loss: 35.7367\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.2221 - val_loss: 35.7404\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.3112 - val_loss: 35.7286\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.7999 - val_loss: 35.7627\n",
            "Epoch 80/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 30.3147roc-auc_val: 0.806\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 29.8335 - val_loss: 35.7491\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.3078 - val_loss: 35.7692\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.3302 - val_loss: 35.7933\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.7933 - val_loss: 35.7683\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.4821 - val_loss: 35.7733\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.2747 - val_loss: 35.7885\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.2432 - val_loss: 35.7823\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.4746 - val_loss: 35.7876\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.4141 - val_loss: 35.7759\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.2745 - val_loss: 35.8150\n",
            "Epoch 90/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 28.9751roc-auc_val: 0.8067\n",
            "47/47 [==============================] - 14s 292ms/step - loss: 28.8735 - val_loss: 35.8337\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.9635 - val_loss: 35.8353\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.0632 - val_loss: 35.8265\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.9792 - val_loss: 35.8166\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.1236 - val_loss: 35.8535\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.0387 - val_loss: 35.8443\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.1192 - val_loss: 35.8432\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.1179 - val_loss: 35.8898\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.7597 - val_loss: 35.8568\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.0593 - val_loss: 35.8635\n",
            "Epoch 100/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 28.8107roc-auc_val: 0.8076\n",
            "47/47 [==============================] - 14s 294ms/step - loss: 29.0106 - val_loss: 35.9093\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.1856 - val_loss: 35.9390\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.1583 - val_loss: 35.9254\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 28.5979 - val_loss: 35.9209\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 60.1798 - val_loss: 38.0264\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 51.6332 - val_loss: 37.3083\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 47.3503 - val_loss: 36.8949\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 44.7866 - val_loss: 36.6684\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 43.2498 - val_loss: 36.4513\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 42.1226 - val_loss: 36.1789\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 40.9983 - val_loss: 36.2819\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 40.5613 - val_loss: 36.1620\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 39.6227 - val_loss: 36.1706\n",
            "Epoch 10/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 39.2733roc-auc_val: 0.7916\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 39.1602 - val_loss: 35.9690\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 37.8835 - val_loss: 35.9119\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 38.0019 - val_loss: 35.8229\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 37.4783 - val_loss: 35.7768\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 37.0675 - val_loss: 35.6375\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.4364 - val_loss: 35.6192\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.1672 - val_loss: 35.6194\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.3483 - val_loss: 35.5348\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 35.5030 - val_loss: 35.4600\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 35.1278 - val_loss: 35.4183\n",
            "Epoch 20/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 35.3640roc-auc_val: 0.7991\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 35.2827 - val_loss: 35.3940\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 34.8857 - val_loss: 35.3585\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 34.3650 - val_loss: 35.2817\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 34.2216 - val_loss: 35.1626\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 34.1694 - val_loss: 35.1544\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.6995 - val_loss: 35.1140\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.7190 - val_loss: 35.1432\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.5168 - val_loss: 35.0676\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.1664 - val_loss: 35.0326\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.3322 - val_loss: 34.9619\n",
            "Epoch 30/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 33.2033roc-auc_val: 0.805\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 33.0675 - val_loss: 34.9133\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.0674 - val_loss: 34.9082\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.6760 - val_loss: 34.8553\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.7330 - val_loss: 34.8295\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.7931 - val_loss: 34.8010\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.3253 - val_loss: 34.7614\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.9847 - val_loss: 34.7296\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.2744 - val_loss: 34.6811\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.0563 - val_loss: 34.6570\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.7732 - val_loss: 34.6845\n",
            "Epoch 40/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 32.0679roc-auc_val: 0.8078\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 32.0679 - val_loss: 34.6748\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.7204 - val_loss: 34.6279\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.7467 - val_loss: 34.6021\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.5540 - val_loss: 34.5417\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.4207 - val_loss: 34.5550\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.4001 - val_loss: 34.5676\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.3441 - val_loss: 34.5569\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.0723 - val_loss: 34.5340\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.9689 - val_loss: 34.4731\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.8955 - val_loss: 34.4706\n",
            "Epoch 50/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 30.9032roc-auc_val: 0.8102\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 30.8736 - val_loss: 34.4498\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.2226 - val_loss: 34.4510\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.9023 - val_loss: 34.4573\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 30.5613 - val_loss: 34.4575\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.5842 - val_loss: 34.4335\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.9126 - val_loss: 34.4513\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.3824 - val_loss: 34.4141\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.6902 - val_loss: 34.4210\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.5382 - val_loss: 34.4038\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.5057 - val_loss: 34.4245\n",
            "Epoch 60/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 30.4842roc-auc_val: 0.8114\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 30.4842 - val_loss: 34.4136\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.1307 - val_loss: 34.3709\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.1740 - val_loss: 34.3941\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.3304 - val_loss: 34.3605\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.9693 - val_loss: 34.3547\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.8863 - val_loss: 34.3293\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.9597 - val_loss: 34.2819\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.8449 - val_loss: 34.2664\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.1253 - val_loss: 34.2865\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.4626 - val_loss: 34.2468\n",
            "Epoch 70/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 29.4770roc-auc_val: 0.8136\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 29.4914 - val_loss: 34.2128\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.8731 - val_loss: 34.2097\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.6412 - val_loss: 34.2086\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.3209 - val_loss: 34.1776\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.4118 - val_loss: 34.1643\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.1925 - val_loss: 34.1619\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.5751 - val_loss: 34.1372\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.4866 - val_loss: 34.1380\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.3427 - val_loss: 34.1303\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.1678 - val_loss: 34.1188\n",
            "Epoch 80/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 29.7378roc-auc_val: 0.8149\n",
            "88/88 [==============================] - 12s 131ms/step - loss: 29.4323 - val_loss: 34.1296\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.3442 - val_loss: 34.1088\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.1340 - val_loss: 34.1024\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.3090 - val_loss: 34.1036\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.2894 - val_loss: 34.0968\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.1898 - val_loss: 34.1018\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.9650 - val_loss: 34.1069\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.9326 - val_loss: 34.1036\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.4295 - val_loss: 34.0768\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.9466 - val_loss: 34.0390\n",
            "Epoch 90/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 29.2116roc-auc_val: 0.8162\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 29.0785 - val_loss: 34.0180\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.7415 - val_loss: 34.0250\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.0195 - val_loss: 34.0165\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.7663 - val_loss: 34.0142\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.9971 - val_loss: 34.0157\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5492 - val_loss: 34.0184\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.7322 - val_loss: 34.0038\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.8767 - val_loss: 33.9878\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.6776 - val_loss: 33.9793\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.7402 - val_loss: 33.9707\n",
            "Epoch 100/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 28.8402roc-auc_val: 0.8167\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 28.6235 - val_loss: 33.9717\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5258 - val_loss: 33.9757\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.7841 - val_loss: 33.9613\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.4940 - val_loss: 33.9483\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.1935 - val_loss: 33.9483\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5684 - val_loss: 33.9589\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.3273 - val_loss: 33.9486\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.3166 - val_loss: 33.9352\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.2396 - val_loss: 33.9424\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.2833 - val_loss: 33.9434\n",
            "Epoch 110/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 28.0178roc-auc_val: 0.8174\n",
            "88/88 [==============================] - 12s 131ms/step - loss: 28.2289 - val_loss: 33.9339\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.3442 - val_loss: 33.9281\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.4555 - val_loss: 33.9179\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.2296 - val_loss: 33.9248\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.4231 - val_loss: 33.9312\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.3316 - val_loss: 33.9152\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.3923 - val_loss: 33.9168\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.3879 - val_loss: 33.9124\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.2631 - val_loss: 33.9159\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.1980 - val_loss: 33.9186\n",
            "Epoch 120/1000\n",
            "84/88 [===========================>..] - ETA: 0s - loss: 28.3806roc-auc_val: 0.8177\n",
            "88/88 [==============================] - 12s 139ms/step - loss: 28.3092 - val_loss: 33.9058\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.0607 - val_loss: 33.9029\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.1209 - val_loss: 33.8786\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.1946 - val_loss: 33.8854\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9841 - val_loss: 33.8800\n",
            "Epoch 125/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9999 - val_loss: 33.8714\n",
            "Epoch 126/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8529 - val_loss: 33.8713\n",
            "Epoch 127/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8231 - val_loss: 33.8773\n",
            "Epoch 128/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.7459 - val_loss: 33.8838\n",
            "Epoch 129/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.0170 - val_loss: 33.8669\n",
            "Epoch 130/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 28.1935roc-auc_val: 0.8185\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 27.9892 - val_loss: 33.8691\n",
            "Epoch 131/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9087 - val_loss: 33.8592\n",
            "Epoch 132/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.7275 - val_loss: 33.8426\n",
            "Epoch 133/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.6695 - val_loss: 33.8391\n",
            "Epoch 134/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.7114 - val_loss: 33.8357\n",
            "Epoch 135/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.6715 - val_loss: 33.8351\n",
            "Epoch 136/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.6898 - val_loss: 33.8389\n",
            "Epoch 137/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.8196 - val_loss: 33.8340\n",
            "Epoch 138/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.7441 - val_loss: 33.8463\n",
            "Epoch 139/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.6493 - val_loss: 33.8350\n",
            "Epoch 140/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 27.4679roc-auc_val: 0.8188\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 27.5617 - val_loss: 33.8280\n",
            "Epoch 141/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6390 - val_loss: 33.8266\n",
            "Epoch 142/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.4496 - val_loss: 33.8378\n",
            "Epoch 143/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6405 - val_loss: 33.8251\n",
            "Epoch 144/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5219 - val_loss: 33.8219\n",
            "Epoch 145/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5759 - val_loss: 33.8323\n",
            "Epoch 146/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9158 - val_loss: 33.8246\n",
            "Epoch 147/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3722 - val_loss: 33.8233\n",
            "Epoch 148/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6806 - val_loss: 33.8212\n",
            "Epoch 149/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5873 - val_loss: 33.8127\n",
            "Epoch 150/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 27.3648roc-auc_val: 0.819\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 27.3648 - val_loss: 33.8186\n",
            "Epoch 151/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6764 - val_loss: 33.8131\n",
            "Epoch 152/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.3875 - val_loss: 33.8077\n",
            "Epoch 153/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6463 - val_loss: 33.7868\n",
            "Epoch 154/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.5273 - val_loss: 33.7884\n",
            "Epoch 155/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3239 - val_loss: 33.8000\n",
            "Epoch 156/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3756 - val_loss: 33.8022\n",
            "Epoch 157/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5276 - val_loss: 33.7942\n",
            "Epoch 158/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2844 - val_loss: 33.7938\n",
            "Epoch 159/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5866 - val_loss: 33.7826\n",
            "Epoch 160/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 27.3627roc-auc_val: 0.8196\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 27.3077 - val_loss: 33.7719\n",
            "Epoch 161/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0679 - val_loss: 33.7676\n",
            "Epoch 162/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2509 - val_loss: 33.7545\n",
            "Epoch 163/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.4047 - val_loss: 33.7579\n",
            "Epoch 164/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9469 - val_loss: 33.7654\n",
            "Epoch 165/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3383 - val_loss: 33.7661\n",
            "Epoch 166/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.1195 - val_loss: 33.7675\n",
            "Epoch 167/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3059 - val_loss: 33.7732\n",
            "Epoch 168/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2001 - val_loss: 33.7708\n",
            "Epoch 169/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2142 - val_loss: 33.7754\n",
            "Epoch 170/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 27.0810roc-auc_val: 0.8197\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 27.1509 - val_loss: 33.7805\n",
            "Epoch 171/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3609 - val_loss: 33.7738\n",
            "Epoch 172/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2474 - val_loss: 33.7608\n",
            "Epoch 173/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2846 - val_loss: 33.7699\n",
            "Epoch 174/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0698 - val_loss: 33.7698\n",
            "Epoch 175/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2720 - val_loss: 33.7747\n",
            "Epoch 176/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0440 - val_loss: 33.7620\n",
            "Epoch 177/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2396 - val_loss: 33.7681\n",
            "Epoch 178/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6124 - val_loss: 33.7615\n",
            "Epoch 179/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9274 - val_loss: 33.7462\n",
            "Epoch 180/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 27.1079roc-auc_val: 0.8201\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 27.1003 - val_loss: 33.7460\n",
            "Epoch 181/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.9681 - val_loss: 33.7437\n",
            "Epoch 182/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0498 - val_loss: 33.7535\n",
            "Epoch 183/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7216 - val_loss: 33.7624\n",
            "Epoch 184/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.8211 - val_loss: 33.7625\n",
            "Epoch 185/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.9046 - val_loss: 33.7560\n",
            "Epoch 186/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9480 - val_loss: 33.7484\n",
            "Epoch 187/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7996 - val_loss: 33.7469\n",
            "Epoch 188/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9431 - val_loss: 33.7553\n",
            "Epoch 189/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9195 - val_loss: 33.7497\n",
            "Epoch 190/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 27.1155roc-auc_val: 0.8203\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 26.9381 - val_loss: 33.7440\n",
            "Epoch 191/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0886 - val_loss: 33.7291\n",
            "Epoch 192/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6156 - val_loss: 33.7295\n",
            "Epoch 193/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8972 - val_loss: 33.7369\n",
            "Epoch 194/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7163 - val_loss: 33.7263\n",
            "Epoch 195/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7979 - val_loss: 33.7297\n",
            "Epoch 196/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6402 - val_loss: 33.7283\n",
            "Epoch 197/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7168 - val_loss: 33.7347\n",
            "Epoch 198/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9202 - val_loss: 33.7330\n",
            "Epoch 199/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7498 - val_loss: 33.7417\n",
            "Epoch 200/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 26.6686roc-auc_val: 0.8205\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 26.6924 - val_loss: 33.7336\n",
            "Epoch 201/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8795 - val_loss: 33.7445\n",
            "Epoch 202/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6658 - val_loss: 33.7357\n",
            "Epoch 203/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7650 - val_loss: 33.7471\n",
            "Epoch 204/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8058 - val_loss: 33.7392\n",
            "Epoch 205/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0569 - val_loss: 33.7278\n",
            "Epoch 206/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3857 - val_loss: 33.7278\n",
            "Epoch 207/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7883 - val_loss: 33.7345\n",
            "Epoch 208/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5368 - val_loss: 33.7310\n",
            "Epoch 209/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6360 - val_loss: 33.7283\n",
            "Epoch 210/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 26.5261roc-auc_val: 0.8206\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 26.5261 - val_loss: 33.7439\n",
            "Epoch 211/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.7992 - val_loss: 33.7340\n",
            "Epoch 212/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5201 - val_loss: 33.7268\n",
            "Epoch 213/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7505 - val_loss: 33.7282\n",
            "Epoch 214/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9120 - val_loss: 33.7477\n",
            "Epoch 215/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6865 - val_loss: 33.7378\n",
            "Epoch 216/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5956 - val_loss: 33.7363\n",
            "Epoch 217/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5255 - val_loss: 33.7364\n",
            "Epoch 218/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6345 - val_loss: 33.7371\n",
            "Epoch 219/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5302 - val_loss: 33.7324\n",
            "Epoch 220/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 26.5718roc-auc_val: 0.8209\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 26.4647 - val_loss: 33.7241\n",
            "Epoch 221/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.5723 - val_loss: 33.7195\n",
            "Epoch 222/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.5861 - val_loss: 33.7196\n",
            "Epoch 223/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5656 - val_loss: 33.7301\n",
            "Epoch 224/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.5775 - val_loss: 33.7331\n",
            "Epoch 225/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5146 - val_loss: 33.7279\n",
            "Epoch 226/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4136 - val_loss: 33.7227\n",
            "Epoch 227/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4715 - val_loss: 33.7192\n",
            "Epoch 228/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3611 - val_loss: 33.7192\n",
            "Epoch 229/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.5614 - val_loss: 33.7117\n",
            "Epoch 230/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 26.2433roc-auc_val: 0.8211\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 26.2433 - val_loss: 33.7149\n",
            "Epoch 231/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3994 - val_loss: 33.7145\n",
            "Epoch 232/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4081 - val_loss: 33.7109\n",
            "Epoch 233/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5849 - val_loss: 33.7127\n",
            "Epoch 234/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4653 - val_loss: 33.7093\n",
            "Epoch 235/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2242 - val_loss: 33.7227\n",
            "Epoch 236/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2112 - val_loss: 33.7209\n",
            "Epoch 237/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3969 - val_loss: 33.7133\n",
            "Epoch 238/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3397 - val_loss: 33.7228\n",
            "Epoch 239/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3430 - val_loss: 33.7239\n",
            "Epoch 240/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 26.5355roc-auc_val: 0.8214\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 26.4417 - val_loss: 33.7169\n",
            "Epoch 241/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4428 - val_loss: 33.7264\n",
            "Epoch 242/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5328 - val_loss: 33.7252\n",
            "Epoch 243/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1965 - val_loss: 33.7258\n",
            "Epoch 244/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4739 - val_loss: 33.7198\n",
            "Epoch 245/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0763 - val_loss: 33.7092\n",
            "Epoch 246/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2404 - val_loss: 33.7048\n",
            "Epoch 247/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3893 - val_loss: 33.7026\n",
            "Epoch 248/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3469 - val_loss: 33.7109\n",
            "Epoch 249/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1172 - val_loss: 33.7044\n",
            "Epoch 250/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 26.4751roc-auc_val: 0.8216\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 26.4295 - val_loss: 33.6983\n",
            "Epoch 251/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.2518 - val_loss: 33.6953\n",
            "Epoch 252/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0669 - val_loss: 33.6916\n",
            "Epoch 253/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.2013 - val_loss: 33.6911\n",
            "Epoch 254/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9677 - val_loss: 33.6970\n",
            "Epoch 255/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0501 - val_loss: 33.6917\n",
            "Epoch 256/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9112 - val_loss: 33.7022\n",
            "Epoch 257/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2873 - val_loss: 33.7001\n",
            "Epoch 258/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1417 - val_loss: 33.7001\n",
            "Epoch 259/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0649 - val_loss: 33.6966\n",
            "Epoch 260/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 26.1845roc-auc_val: 0.8217\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 26.1053 - val_loss: 33.6968\n",
            "Epoch 261/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0114 - val_loss: 33.7048\n",
            "Epoch 262/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0315 - val_loss: 33.7102\n",
            "Epoch 263/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3539 - val_loss: 33.7005\n",
            "Epoch 264/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1543 - val_loss: 33.6971\n",
            "Epoch 265/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9417 - val_loss: 33.7057\n",
            "Epoch 266/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1024 - val_loss: 33.6984\n",
            "Epoch 267/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0772 - val_loss: 33.6891\n",
            "Epoch 268/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1963 - val_loss: 33.7029\n",
            "Epoch 269/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1330 - val_loss: 33.7156\n",
            "Epoch 270/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 26.0255roc-auc_val: 0.8216\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 25.8981 - val_loss: 33.7156\n",
            "Epoch 271/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0085 - val_loss: 33.6960\n",
            "Epoch 272/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0707 - val_loss: 33.6949\n",
            "Epoch 273/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7202 - val_loss: 33.6929\n",
            "Epoch 274/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6308 - val_loss: 33.6983\n",
            "Epoch 275/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.9552 - val_loss: 33.6974\n",
            "Epoch 276/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0092 - val_loss: 33.7043\n",
            "Epoch 277/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9761 - val_loss: 33.6976\n",
            "Epoch 278/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0905 - val_loss: 33.6977\n",
            "Epoch 279/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9135 - val_loss: 33.7021\n",
            "Epoch 280/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 25.8229roc-auc_val: 0.8219\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 25.7559 - val_loss: 33.6927\n",
            "Epoch 281/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.8252 - val_loss: 33.6885\n",
            "Epoch 282/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.0012 - val_loss: 33.6925\n",
            "Epoch 283/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.9298 - val_loss: 33.6935\n",
            "Epoch 284/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.0146 - val_loss: 33.6998\n",
            "Epoch 285/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.9853 - val_loss: 33.7003\n",
            "Epoch 286/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.8266 - val_loss: 33.7026\n",
            "Epoch 287/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.8331 - val_loss: 33.7023\n",
            "Epoch 288/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.8321 - val_loss: 33.7083\n",
            "Epoch 289/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.0133 - val_loss: 33.7109\n",
            "Epoch 290/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 25.8733roc-auc_val: 0.8219\n",
            "88/88 [==============================] - 12s 139ms/step - loss: 25.6996 - val_loss: 33.7053\n",
            "Epoch 291/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8270 - val_loss: 33.7061\n",
            "Epoch 292/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9122 - val_loss: 33.7096\n",
            "Epoch 293/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7689 - val_loss: 33.6990\n",
            "Epoch 294/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7311 - val_loss: 33.7027\n",
            "Epoch 295/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5882 - val_loss: 33.7045\n",
            "Epoch 296/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0248 - val_loss: 33.7024\n",
            "Epoch 297/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6637 - val_loss: 33.7145\n",
            "Epoch 298/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9875 - val_loss: 33.7115\n",
            "Epoch 299/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8563 - val_loss: 33.7119\n",
            "Epoch 300/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 25.8724roc-auc_val: 0.822\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 25.8302 - val_loss: 33.7145\n",
            "Epoch 301/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8187 - val_loss: 33.7099\n",
            "Epoch 302/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.7408 - val_loss: 33.7084\n",
            "Epoch 303/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.6114 - val_loss: 33.7142\n",
            "Epoch 304/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7766 - val_loss: 33.7150\n",
            "Epoch 305/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7854 - val_loss: 33.7176\n",
            "Epoch 306/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8204 - val_loss: 33.7122\n",
            "Epoch 307/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.7880 - val_loss: 33.7179\n",
            "Epoch 308/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6750 - val_loss: 33.7150\n",
            "Epoch 309/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5390 - val_loss: 33.7183\n",
            "Epoch 310/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 25.7783roc-auc_val: 0.8219\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 25.7194 - val_loss: 33.7222\n",
            "Epoch 311/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6687 - val_loss: 33.7184\n",
            "Epoch 312/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7655 - val_loss: 33.7143\n",
            "Epoch 313/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7863 - val_loss: 33.7086\n",
            "Epoch 314/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8132 - val_loss: 33.7137\n",
            "Epoch 315/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5513 - val_loss: 33.7293\n",
            "Epoch 316/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8057 - val_loss: 33.7172\n",
            "Epoch 317/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6053 - val_loss: 33.7278\n",
            "Epoch 318/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7059 - val_loss: 33.7300\n",
            "Epoch 319/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7507 - val_loss: 33.7249\n",
            "Epoch 320/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 25.5703roc-auc_val: 0.8219\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 25.6071 - val_loss: 33.7267\n",
            "Epoch 321/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5767 - val_loss: 33.7253\n",
            "Epoch 322/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4953 - val_loss: 33.7257\n",
            "Epoch 323/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7004 - val_loss: 33.7203\n",
            "Epoch 324/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5249 - val_loss: 33.7251\n",
            "Epoch 325/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9677 - val_loss: 33.7245\n",
            "Epoch 326/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5680 - val_loss: 33.7180\n",
            "Epoch 327/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6087 - val_loss: 33.7278\n",
            "Epoch 328/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6400 - val_loss: 33.7309\n",
            "Epoch 329/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5811 - val_loss: 33.7260\n",
            "Epoch 330/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 25.4864roc-auc_val: 0.8221\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 25.6081 - val_loss: 33.7320\n",
            "Epoch 331/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5286 - val_loss: 33.7357\n",
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 26ms/step - loss: 66.0204 - val_loss: 39.1708\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 57.9079 - val_loss: 38.5969\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 53.2236 - val_loss: 37.9338\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 49.6308 - val_loss: 37.9836\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 48.5053 - val_loss: 38.0673\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 46.6158 - val_loss: 38.0987\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 45.8139 - val_loss: 38.0723\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 44.3761 - val_loss: 38.2137\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 43.2798 - val_loss: 38.2796\n",
            "Epoch 10/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 44.0059roc-auc_val: 0.7689\n",
            "47/47 [==============================] - 12s 246ms/step - loss: 43.5650 - val_loss: 38.2503\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 42.1467 - val_loss: 38.0081\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 41.0071 - val_loss: 38.1195\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 40.5285 - val_loss: 37.9834\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 40.1627 - val_loss: 37.8293\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 39.3135 - val_loss: 37.8853\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 39.1093 - val_loss: 37.9017\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 38.6043 - val_loss: 37.9988\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 38.4184 - val_loss: 38.0270\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 38.4259 - val_loss: 38.1010\n",
            "Epoch 20/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 37.2551roc-auc_val: 0.7793\n",
            "47/47 [==============================] - 11s 244ms/step - loss: 37.1360 - val_loss: 38.1579\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 37.8529 - val_loss: 38.0823\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 37.3180 - val_loss: 37.8681\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 36.5650 - val_loss: 37.9601\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 35.8766 - val_loss: 37.9840\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 35.4975 - val_loss: 37.8165\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 35.6776 - val_loss: 37.6934\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 36.0767 - val_loss: 37.7193\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 35.6624 - val_loss: 37.7472\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.5861 - val_loss: 37.8302\n",
            "Epoch 30/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 34.8934roc-auc_val: 0.7832\n",
            "47/47 [==============================] - 11s 239ms/step - loss: 34.7589 - val_loss: 37.7898\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 35.0544 - val_loss: 37.7772\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 35.1073 - val_loss: 37.7718\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.3372 - val_loss: 37.5595\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.0043 - val_loss: 37.5789\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 34.0275 - val_loss: 37.5646\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 33.9573 - val_loss: 37.5479\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.1085 - val_loss: 37.6176\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 32.9505 - val_loss: 37.6177\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 33.2329 - val_loss: 37.4692\n",
            "Epoch 40/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 33.3244roc-auc_val: 0.7854\n",
            "47/47 [==============================] - 11s 244ms/step - loss: 33.2812 - val_loss: 37.5569\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 33.1416 - val_loss: 37.6431\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 33.5788 - val_loss: 37.6823\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 33.2811 - val_loss: 37.5602\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.6408 - val_loss: 37.6669\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.9927 - val_loss: 37.5914\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.3228 - val_loss: 37.6519\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.0135 - val_loss: 37.7123\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 32.5190 - val_loss: 37.8421\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.1960 - val_loss: 37.9838\n",
            "Epoch 50/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 32.0269roc-auc_val: 0.7872\n",
            "47/47 [==============================] - 11s 239ms/step - loss: 31.8706 - val_loss: 37.9725\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.1440 - val_loss: 37.8634\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.6899 - val_loss: 37.8513\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 31.9463 - val_loss: 37.8956\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.7019 - val_loss: 37.9734\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 31.4191 - val_loss: 37.9274\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 31.4504 - val_loss: 37.9298\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.4909 - val_loss: 37.9865\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.2098 - val_loss: 37.8625\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.2028 - val_loss: 37.9022\n",
            "Epoch 60/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 31.2467roc-auc_val: 0.7902\n",
            "47/47 [==============================] - 11s 241ms/step - loss: 31.3540 - val_loss: 37.8094\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 31.2374 - val_loss: 37.7636\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.4752 - val_loss: 37.7937\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.5534 - val_loss: 37.7924\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.7673 - val_loss: 37.7516\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.9419 - val_loss: 37.7898\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.0496 - val_loss: 37.8256\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.1079 - val_loss: 37.8346\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.0232 - val_loss: 37.8216\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.1694 - val_loss: 37.9025\n",
            "Epoch 70/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 30.3561roc-auc_val: 0.7922\n",
            "47/47 [==============================] - 11s 242ms/step - loss: 30.5650 - val_loss: 37.8792\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.8424 - val_loss: 37.8860\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.7028 - val_loss: 37.9348\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.3759 - val_loss: 37.9560\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.0589 - val_loss: 37.9698\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.0231 - val_loss: 38.0187\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.3697 - val_loss: 38.0618\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.7926 - val_loss: 37.9913\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.3146 - val_loss: 37.9567\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.0065 - val_loss: 37.9667\n",
            "Epoch 80/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 29.3252roc-auc_val: 0.793\n",
            "47/47 [==============================] - 11s 243ms/step - loss: 29.3623 - val_loss: 37.9815\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.6424 - val_loss: 37.9772\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.4516 - val_loss: 37.9821\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.2708 - val_loss: 38.0017\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.4510 - val_loss: 38.0283\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.1135 - val_loss: 38.1039\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.9295 - val_loss: 38.1291\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.4041 - val_loss: 38.1409\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.1874 - val_loss: 38.1871\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.2356 - val_loss: 38.2127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnpeTPNLkiCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "01d95878-1f20-4d39-9624-9c2a96322985"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6S7Pv_rc-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_1.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "65e642ce-2053-4fc0-b696-f703aba09a62"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "e48cfe38-75a0-4ce4-e490-767430f87692"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 52.8MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 86% 50.0M/58.3M [00:02<00:00, 14.5MB/s]\n",
            "100% 58.3M/58.3M [00:02<00:00, 29.6MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 81% 42.0M/52.2M [00:00<00:00, 63.0MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 88.9MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 108MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 77.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDrCIAqHzl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "accb088e-1ebe-4c26-b38b-c587b933fae3"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWJ-hzcznam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5OQjb1zo6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "ad0df66b-26a5-4ff1-971a-0fcc8028e7fb"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "871aa364-d085-4078-a7a4-5b59ecd285e9"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0r3SuH1K97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQ20JqWATak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQIVOLKBFIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2948d744-e693-418d-c6af-b5d28aa8cbb4"
      },
      "source": [
        "1-0.036"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.964"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2e9aff77-af14-4c71-c967-a50a9f9d63af"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=1.2\n",
        "        alpha=1-0.036\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss\n",
        "dk={}\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "for en,month in enumerate([(4,5),(3,4),(3,5)]):\n",
        "  train=trn.loc[trn['month']>=month[1]]\n",
        "  test=trn.loc[trn['month']<=month[0]]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.0001,decay=1e-3),loss=fl())\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 26ms/step - loss: 66.7632 - val_loss: 41.4814\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 56.6398 - val_loss: 40.0875\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 52.9794 - val_loss: 39.2805\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 49.6113 - val_loss: 38.9677\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 48.4326 - val_loss: 38.5640\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 46.6473 - val_loss: 38.5013\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 45.4663 - val_loss: 38.3377\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 44.4692 - val_loss: 38.2655\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 43.1510 - val_loss: 38.4460\n",
            "Epoch 10/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 42.3086roc-auc_val: 0.771\n",
            "47/47 [==============================] - 14s 290ms/step - loss: 42.0183 - val_loss: 38.2308\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 41.4580 - val_loss: 38.1952\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 41.5501 - val_loss: 38.1906\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 40.2222 - val_loss: 37.9999\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 40.2381 - val_loss: 37.9370\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 39.8165 - val_loss: 37.8933\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 39.5959 - val_loss: 37.8746\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 38.8384 - val_loss: 37.6702\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 38.7488 - val_loss: 37.5204\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 37.9166 - val_loss: 37.4537\n",
            "Epoch 20/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 38.2730roc-auc_val: 0.779\n",
            "47/47 [==============================] - 14s 290ms/step - loss: 37.8522 - val_loss: 37.6899\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 37.3341 - val_loss: 37.7118\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 37.1597 - val_loss: 37.6396\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 36.1275 - val_loss: 37.3505\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 35.9414 - val_loss: 37.3032\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 36.2556 - val_loss: 37.2198\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 35.9301 - val_loss: 37.1784\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 36.0398 - val_loss: 37.2045\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 35.2856 - val_loss: 37.1081\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 35.4106 - val_loss: 37.1682\n",
            "Epoch 30/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 34.7303roc-auc_val: 0.7858\n",
            "47/47 [==============================] - 14s 292ms/step - loss: 34.8375 - val_loss: 37.1332\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 34.7244 - val_loss: 37.1515\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 34.7329 - val_loss: 37.0004\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 34.4675 - val_loss: 36.9802\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 34.3950 - val_loss: 36.9656\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 33.7041 - val_loss: 36.9473\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.4635 - val_loss: 36.8659\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.3366 - val_loss: 36.8573\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 33.8899 - val_loss: 36.8254\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.2450 - val_loss: 36.8858\n",
            "Epoch 40/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 33.6611roc-auc_val: 0.7906\n",
            "47/47 [==============================] - 14s 293ms/step - loss: 33.2452 - val_loss: 36.7886\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.5525 - val_loss: 36.7908\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.1390 - val_loss: 36.7775\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 32.5544 - val_loss: 36.7371\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.7975 - val_loss: 36.7453\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.4041 - val_loss: 36.7875\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.5644 - val_loss: 36.7821\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.5100 - val_loss: 36.8045\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.1903 - val_loss: 36.7880\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.1736 - val_loss: 36.8153\n",
            "Epoch 50/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 32.0227roc-auc_val: 0.7924\n",
            "47/47 [==============================] - 14s 288ms/step - loss: 31.8553 - val_loss: 36.7602\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 32.0406 - val_loss: 36.7432\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.1797 - val_loss: 36.7593\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.6876 - val_loss: 36.7699\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.0342 - val_loss: 36.7960\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 31.6825 - val_loss: 36.8098\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.9617 - val_loss: 36.8368\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.0818 - val_loss: 36.8281\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.3144 - val_loss: 36.8446\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.9549 - val_loss: 36.8694\n",
            "Epoch 60/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 30.9378roc-auc_val: 0.7932\n",
            "47/47 [==============================] - 14s 291ms/step - loss: 31.0001 - val_loss: 36.9252\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 31.0666 - val_loss: 36.8792\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.5778 - val_loss: 36.8986\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.0504 - val_loss: 36.9234\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.6150 - val_loss: 36.8860\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.5551 - val_loss: 36.8561\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.6010 - val_loss: 36.8960\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.4811 - val_loss: 36.8373\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.5361 - val_loss: 36.8720\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.4755 - val_loss: 36.8751\n",
            "Epoch 70/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 30.3666roc-auc_val: 0.7952\n",
            "47/47 [==============================] - 14s 290ms/step - loss: 30.2448 - val_loss: 36.8472\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.1313 - val_loss: 36.8516\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.4205 - val_loss: 36.8625\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.6822 - val_loss: 36.8411\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.4938 - val_loss: 36.9016\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.9780 - val_loss: 36.8767\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 30.0725 - val_loss: 36.8835\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.8103 - val_loss: 36.8634\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.4740 - val_loss: 36.8280\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.2652 - val_loss: 36.9040\n",
            "Epoch 80/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 29.9124roc-auc_val: 0.796\n",
            "47/47 [==============================] - 14s 292ms/step - loss: 29.7219 - val_loss: 36.9048\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.3875 - val_loss: 36.9516\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.2790 - val_loss: 36.9755\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.0552 - val_loss: 36.9689\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.2073 - val_loss: 36.9612\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.3614 - val_loss: 37.0061\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.5728 - val_loss: 37.0447\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.5514 - val_loss: 37.0162\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.4779 - val_loss: 37.0917\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.9170 - val_loss: 37.1319\n",
            "Epoch 90/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 29.5696roc-auc_val: 0.7959\n",
            "47/47 [==============================] - 14s 291ms/step - loss: 29.4075 - val_loss: 37.1433\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.2844 - val_loss: 37.1074\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.1113 - val_loss: 37.0799\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.1233 - val_loss: 37.0741\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 61.1002 - val_loss: 37.9407\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 52.7997 - val_loss: 37.7541\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 48.6442 - val_loss: 37.7905\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 46.0946 - val_loss: 37.5753\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 44.5365 - val_loss: 37.3541\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 42.9990 - val_loss: 37.2691\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 42.6798 - val_loss: 37.2014\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 41.0339 - val_loss: 37.0825\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 40.4139 - val_loss: 36.9976\n",
            "Epoch 10/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 39.7132roc-auc_val: 0.7822\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 39.7132 - val_loss: 36.8446\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 38.9943 - val_loss: 36.3548\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 38.7735 - val_loss: 36.4209\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 37.8376 - val_loss: 36.5936\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 37.4139 - val_loss: 36.2169\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 37.5010 - val_loss: 36.1584\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.4975 - val_loss: 36.1740\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.6103 - val_loss: 36.0388\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.3989 - val_loss: 35.9166\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 35.8594 - val_loss: 35.8681\n",
            "Epoch 20/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 35.3140roc-auc_val: 0.7933\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 35.3216 - val_loss: 35.8502\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 34.8808 - val_loss: 35.6294\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 35.0038 - val_loss: 35.6213\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 34.9408 - val_loss: 35.5415\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 34.3088 - val_loss: 35.4742\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 34.7483 - val_loss: 35.4166\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 34.2244 - val_loss: 35.4010\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 34.2525 - val_loss: 35.3634\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 33.9750 - val_loss: 35.3080\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.4479 - val_loss: 35.2802\n",
            "Epoch 30/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 33.6349roc-auc_val: 0.8011\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 33.5705 - val_loss: 35.2115\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.9405 - val_loss: 35.1644\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.2102 - val_loss: 35.1211\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 33.4245 - val_loss: 35.1031\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.5718 - val_loss: 35.1027\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.6278 - val_loss: 35.0788\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.5980 - val_loss: 35.0836\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.5008 - val_loss: 35.0916\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.1752 - val_loss: 35.0527\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 32.2837 - val_loss: 35.0009\n",
            "Epoch 40/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 32.2936roc-auc_val: 0.8047\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 32.4670 - val_loss: 34.9861\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.0961 - val_loss: 34.9684\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 31.8537 - val_loss: 34.9246\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 32.0368 - val_loss: 34.9259\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 31.9422 - val_loss: 34.9223\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 31.8583 - val_loss: 34.8772\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 31.2403 - val_loss: 34.8694\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 31.0939 - val_loss: 34.8259\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 31.5098 - val_loss: 34.8135\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 31.3589 - val_loss: 34.7944\n",
            "Epoch 50/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 31.2702roc-auc_val: 0.8074\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 31.2348 - val_loss: 34.7846\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.8472 - val_loss: 34.7456\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.0650 - val_loss: 34.7507\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.0624 - val_loss: 34.7216\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.8809 - val_loss: 34.7135\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 30.8944 - val_loss: 34.7191\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.4917 - val_loss: 34.6920\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.6925 - val_loss: 34.6765\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.6216 - val_loss: 34.6447\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.6111 - val_loss: 34.6243\n",
            "Epoch 60/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 30.6991roc-auc_val: 0.8092\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 30.4555 - val_loss: 34.6397\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 30.5037 - val_loss: 34.6138\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.5253 - val_loss: 34.6181\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.4733 - val_loss: 34.6108\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.1138 - val_loss: 34.5880\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.4249 - val_loss: 34.5929\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.8389 - val_loss: 34.5868\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 30.1365 - val_loss: 34.5635\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.9448 - val_loss: 34.5344\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.0162 - val_loss: 34.5356\n",
            "Epoch 70/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 29.9486roc-auc_val: 0.8104\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 29.9124 - val_loss: 34.5237\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.8704 - val_loss: 34.5092\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.6777 - val_loss: 34.4875\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.9197 - val_loss: 34.4804\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.9182 - val_loss: 34.4889\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.0407 - val_loss: 34.4813\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.3705 - val_loss: 34.4820\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.4037 - val_loss: 34.4640\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.9315 - val_loss: 34.4531\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.6255 - val_loss: 34.4486\n",
            "Epoch 80/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 29.2355roc-auc_val: 0.8115\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 29.3500 - val_loss: 34.4465\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.2631 - val_loss: 34.4197\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.4634 - val_loss: 34.4173\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.1080 - val_loss: 34.4160\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.2156 - val_loss: 34.4009\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.1086 - val_loss: 34.4085\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.2470 - val_loss: 34.3872\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.9618 - val_loss: 34.3754\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.0528 - val_loss: 34.3764\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.8723 - val_loss: 34.3701\n",
            "Epoch 90/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 28.9292roc-auc_val: 0.8124\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 29.0428 - val_loss: 34.3727\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.6485 - val_loss: 34.3596\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.8175 - val_loss: 34.3609\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.9955 - val_loss: 34.3650\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.8742 - val_loss: 34.3463\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.9110 - val_loss: 34.3467\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.8241 - val_loss: 34.3326\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.6604 - val_loss: 34.3166\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.6135 - val_loss: 34.3112\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.7875 - val_loss: 34.2868\n",
            "Epoch 100/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 28.7211roc-auc_val: 0.8136\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 28.7211 - val_loss: 34.2769\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.5874 - val_loss: 34.2707\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.6751 - val_loss: 34.2844\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5833 - val_loss: 34.2687\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.4836 - val_loss: 34.2584\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5204 - val_loss: 34.2467\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5274 - val_loss: 34.2494\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5822 - val_loss: 34.2465\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5049 - val_loss: 34.2449\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.6649 - val_loss: 34.2401\n",
            "Epoch 110/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 28.5557roc-auc_val: 0.814\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 28.4573 - val_loss: 34.2448\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.3193 - val_loss: 34.2395\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.3307 - val_loss: 34.2382\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.3788 - val_loss: 34.2343\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.3977 - val_loss: 34.2190\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.2972 - val_loss: 34.2181\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.0742 - val_loss: 34.2133\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.6440 - val_loss: 34.2202\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.6216 - val_loss: 34.2130\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.1963 - val_loss: 34.2110\n",
            "Epoch 120/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 27.9555roc-auc_val: 0.8147\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 28.0186 - val_loss: 34.2080\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.4498 - val_loss: 34.1973\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 28.0780 - val_loss: 34.1835\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.2149 - val_loss: 34.1773\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9425 - val_loss: 34.1770\n",
            "Epoch 125/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.3361 - val_loss: 34.1859\n",
            "Epoch 126/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.1809 - val_loss: 34.1767\n",
            "Epoch 127/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8756 - val_loss: 34.1779\n",
            "Epoch 128/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9392 - val_loss: 34.1837\n",
            "Epoch 129/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9597 - val_loss: 34.1842\n",
            "Epoch 130/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 28.0370roc-auc_val: 0.8153\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 27.9216 - val_loss: 34.1787\n",
            "Epoch 131/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6573 - val_loss: 34.1630\n",
            "Epoch 132/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8292 - val_loss: 34.1545\n",
            "Epoch 133/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8796 - val_loss: 34.1506\n",
            "Epoch 134/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.0055 - val_loss: 34.1509\n",
            "Epoch 135/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8040 - val_loss: 34.1452\n",
            "Epoch 136/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9208 - val_loss: 34.1415\n",
            "Epoch 137/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8695 - val_loss: 34.1420\n",
            "Epoch 138/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.0511 - val_loss: 34.1384\n",
            "Epoch 139/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6942 - val_loss: 34.1491\n",
            "Epoch 140/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 27.8015roc-auc_val: 0.8158\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 27.6464 - val_loss: 34.1418\n",
            "Epoch 141/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5311 - val_loss: 34.1349\n",
            "Epoch 142/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.7358 - val_loss: 34.1363\n",
            "Epoch 143/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.7318 - val_loss: 34.1465\n",
            "Epoch 144/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5741 - val_loss: 34.1486\n",
            "Epoch 145/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2565 - val_loss: 34.1504\n",
            "Epoch 146/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.8104 - val_loss: 34.1396\n",
            "Epoch 147/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5876 - val_loss: 34.1371\n",
            "Epoch 148/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.6314 - val_loss: 34.1465\n",
            "Epoch 149/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.7578 - val_loss: 34.1407\n",
            "Epoch 150/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 27.5519roc-auc_val: 0.816\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 27.5519 - val_loss: 34.1465\n",
            "Epoch 151/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.4904 - val_loss: 34.1464\n",
            "Epoch 152/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2864 - val_loss: 34.1367\n",
            "Epoch 153/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.4010 - val_loss: 34.1397\n",
            "Epoch 154/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 27.5057 - val_loss: 34.1249\n",
            "Epoch 155/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3181 - val_loss: 34.1279\n",
            "Epoch 156/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3359 - val_loss: 34.1360\n",
            "Epoch 157/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3254 - val_loss: 34.1367\n",
            "Epoch 158/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2973 - val_loss: 34.1234\n",
            "Epoch 159/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5689 - val_loss: 34.1193\n",
            "Epoch 160/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 27.1433roc-auc_val: 0.8165\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 27.2903 - val_loss: 34.1146\n",
            "Epoch 161/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2947 - val_loss: 34.1166\n",
            "Epoch 162/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.1023 - val_loss: 34.1172\n",
            "Epoch 163/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0722 - val_loss: 34.1106\n",
            "Epoch 164/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2785 - val_loss: 34.1140\n",
            "Epoch 165/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2512 - val_loss: 34.1172\n",
            "Epoch 166/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3051 - val_loss: 34.1278\n",
            "Epoch 167/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.1157 - val_loss: 34.1288\n",
            "Epoch 168/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3976 - val_loss: 34.1261\n",
            "Epoch 169/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.1828 - val_loss: 34.1256\n",
            "Epoch 170/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 27.4800roc-auc_val: 0.8165\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 27.3579 - val_loss: 34.1219\n",
            "Epoch 171/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2639 - val_loss: 34.1264\n",
            "Epoch 172/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2021 - val_loss: 34.1228\n",
            "Epoch 173/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.3758 - val_loss: 34.1284\n",
            "Epoch 174/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0107 - val_loss: 34.1262\n",
            "Epoch 175/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9884 - val_loss: 34.1188\n",
            "Epoch 176/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0512 - val_loss: 34.1188\n",
            "Epoch 177/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4752 - val_loss: 34.1144\n",
            "Epoch 178/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0078 - val_loss: 34.1155\n",
            "Epoch 179/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8436 - val_loss: 34.1115\n",
            "Epoch 180/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 27.2578roc-auc_val: 0.8166\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 27.2468 - val_loss: 34.1151\n",
            "Epoch 181/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9894 - val_loss: 34.1193\n",
            "Epoch 182/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7794 - val_loss: 34.1173\n",
            "Epoch 183/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0374 - val_loss: 34.1228\n",
            "Epoch 184/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9004 - val_loss: 34.1200\n",
            "Epoch 185/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7311 - val_loss: 34.1189\n",
            "Epoch 186/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5848 - val_loss: 34.1210\n",
            "Epoch 187/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8403 - val_loss: 34.1080\n",
            "Epoch 188/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0234 - val_loss: 34.1014\n",
            "Epoch 189/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6806 - val_loss: 34.1053\n",
            "Epoch 190/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 26.8049roc-auc_val: 0.8168\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 26.7125 - val_loss: 34.1181\n",
            "Epoch 191/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7138 - val_loss: 34.1070\n",
            "Epoch 192/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7870 - val_loss: 34.1145\n",
            "Epoch 193/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8915 - val_loss: 34.1178\n",
            "Epoch 194/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7415 - val_loss: 34.1153\n",
            "Epoch 195/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7111 - val_loss: 34.1088\n",
            "Epoch 196/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0028 - val_loss: 34.1027\n",
            "Epoch 197/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7803 - val_loss: 34.1069\n",
            "Epoch 198/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6852 - val_loss: 34.1001\n",
            "Epoch 199/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7026 - val_loss: 34.1084\n",
            "Epoch 200/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 26.8824roc-auc_val: 0.817\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 26.9007 - val_loss: 34.1115\n",
            "Epoch 201/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5780 - val_loss: 34.1065\n",
            "Epoch 202/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6337 - val_loss: 34.1122\n",
            "Epoch 203/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6000 - val_loss: 34.1043\n",
            "Epoch 204/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9258 - val_loss: 34.0941\n",
            "Epoch 205/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9003 - val_loss: 34.0980\n",
            "Epoch 206/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.7847 - val_loss: 34.1021\n",
            "Epoch 207/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.8000 - val_loss: 34.0946\n",
            "Epoch 208/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5555 - val_loss: 34.0945\n",
            "Epoch 209/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5796 - val_loss: 34.1017\n",
            "Epoch 210/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 26.6869roc-auc_val: 0.8172\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 26.6332 - val_loss: 34.1090\n",
            "Epoch 211/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.7231 - val_loss: 34.0990\n",
            "Epoch 212/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.5524 - val_loss: 34.0939\n",
            "Epoch 213/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.7346 - val_loss: 34.1011\n",
            "Epoch 214/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.4242 - val_loss: 34.0972\n",
            "Epoch 215/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.4980 - val_loss: 34.0921\n",
            "Epoch 216/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.6072 - val_loss: 34.0953\n",
            "Epoch 217/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.6491 - val_loss: 34.0912\n",
            "Epoch 218/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.4700 - val_loss: 34.0874\n",
            "Epoch 219/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.6579 - val_loss: 34.0922\n",
            "Epoch 220/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 26.6916roc-auc_val: 0.8175\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 26.6460 - val_loss: 34.0893\n",
            "Epoch 221/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6864 - val_loss: 34.0984\n",
            "Epoch 222/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4096 - val_loss: 34.0926\n",
            "Epoch 223/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3250 - val_loss: 34.0905\n",
            "Epoch 224/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6518 - val_loss: 34.0947\n",
            "Epoch 225/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4212 - val_loss: 34.1005\n",
            "Epoch 226/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5284 - val_loss: 34.1014\n",
            "Epoch 227/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5246 - val_loss: 34.1026\n",
            "Epoch 228/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4869 - val_loss: 34.0910\n",
            "Epoch 229/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5751 - val_loss: 34.0974\n",
            "Epoch 230/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 26.5013roc-auc_val: 0.8175\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 26.5036 - val_loss: 34.0927\n",
            "Epoch 231/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.5660 - val_loss: 34.0903\n",
            "Epoch 232/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.2762 - val_loss: 34.0835\n",
            "Epoch 233/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2583 - val_loss: 34.0899\n",
            "Epoch 234/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1214 - val_loss: 34.0938\n",
            "Epoch 235/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1531 - val_loss: 34.1006\n",
            "Epoch 236/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3981 - val_loss: 34.1059\n",
            "Epoch 237/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.4976 - val_loss: 34.1018\n",
            "Epoch 238/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5007 - val_loss: 34.1028\n",
            "Epoch 239/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2295 - val_loss: 34.1079\n",
            "Epoch 240/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 26.1620roc-auc_val: 0.8177\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 26.1583 - val_loss: 34.0993\n",
            "Epoch 241/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3558 - val_loss: 34.1022\n",
            "Epoch 242/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4410 - val_loss: 34.0988\n",
            "Epoch 243/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2752 - val_loss: 34.0916\n",
            "Epoch 244/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3089 - val_loss: 34.0912\n",
            "Epoch 245/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0915 - val_loss: 34.1006\n",
            "Epoch 246/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4468 - val_loss: 34.0879\n",
            "Epoch 247/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2172 - val_loss: 34.0814\n",
            "Epoch 248/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0818 - val_loss: 34.0892\n",
            "Epoch 249/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1048 - val_loss: 34.0888\n",
            "Epoch 250/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 26.0457roc-auc_val: 0.8179\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 26.1438 - val_loss: 34.0880\n",
            "Epoch 251/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1044 - val_loss: 34.0949\n",
            "Epoch 252/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1133 - val_loss: 34.0869\n",
            "Epoch 253/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2164 - val_loss: 34.0867\n",
            "Epoch 254/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2310 - val_loss: 34.0910\n",
            "Epoch 255/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9913 - val_loss: 34.0989\n",
            "Epoch 256/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2136 - val_loss: 34.0888\n",
            "Epoch 257/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0625 - val_loss: 34.0838\n",
            "Epoch 258/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9256 - val_loss: 34.0949\n",
            "Epoch 259/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0983 - val_loss: 34.0974\n",
            "Epoch 260/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 26.0052roc-auc_val: 0.8179\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 26.0618 - val_loss: 34.1000\n",
            "Epoch 261/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0406 - val_loss: 34.1057\n",
            "Epoch 262/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0119 - val_loss: 34.1198\n",
            "Epoch 263/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0733 - val_loss: 34.1154\n",
            "Epoch 264/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0587 - val_loss: 34.1048\n",
            "Epoch 265/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7993 - val_loss: 34.1031\n",
            "Epoch 266/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.2885 - val_loss: 34.1031\n",
            "Epoch 267/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0218 - val_loss: 34.1160\n",
            "Epoch 268/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1103 - val_loss: 34.1059\n",
            "Epoch 269/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9627 - val_loss: 34.1074\n",
            "Epoch 270/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 26.1534roc-auc_val: 0.818\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 26.0774 - val_loss: 34.1062\n",
            "Epoch 271/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9248 - val_loss: 34.1032\n",
            "Epoch 272/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9040 - val_loss: 34.1070\n",
            "Epoch 273/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1241 - val_loss: 34.1074\n",
            "Epoch 274/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0162 - val_loss: 34.1031\n",
            "Epoch 275/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9815 - val_loss: 34.1121\n",
            "Epoch 276/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9038 - val_loss: 34.1079\n",
            "Epoch 277/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0057 - val_loss: 34.1087\n",
            "Epoch 278/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0640 - val_loss: 34.1067\n",
            "Epoch 279/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7028 - val_loss: 34.1089\n",
            "Epoch 280/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 26.0668roc-auc_val: 0.818\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 25.9798 - val_loss: 34.1094\n",
            "Epoch 281/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7875 - val_loss: 34.1222\n",
            "Epoch 282/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.0733 - val_loss: 34.1192\n",
            "Epoch 283/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8805 - val_loss: 34.1109\n",
            "Epoch 284/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8965 - val_loss: 34.1095\n",
            "Epoch 285/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9111 - val_loss: 34.1036\n",
            "Epoch 286/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9488 - val_loss: 34.0985\n",
            "Epoch 287/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6527 - val_loss: 34.1146\n",
            "Epoch 288/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0312 - val_loss: 34.1038\n",
            "Epoch 289/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8280 - val_loss: 34.1116\n",
            "Epoch 290/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 25.9103roc-auc_val: 0.8182\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 25.9103 - val_loss: 34.1102\n",
            "Epoch 291/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7410 - val_loss: 34.1018\n",
            "Epoch 292/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6856 - val_loss: 34.1093\n",
            "Epoch 293/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.9343 - val_loss: 34.1152\n",
            "Epoch 294/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5493 - val_loss: 34.1211\n",
            "Epoch 295/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5554 - val_loss: 34.1364\n",
            "Epoch 296/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.6682 - val_loss: 34.1279\n",
            "Epoch 297/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.8335 - val_loss: 34.1343\n",
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 25ms/step - loss: 66.4505 - val_loss: 38.5943\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 56.5417 - val_loss: 38.7542\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 51.9366 - val_loss: 39.2031\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 49.8977 - val_loss: 39.4155\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 48.0205 - val_loss: 39.6132\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 46.6726 - val_loss: 39.7533\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 43.6265 - val_loss: 39.5428\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 44.2698 - val_loss: 39.4917\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 43.0635 - val_loss: 39.3324\n",
            "Epoch 10/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 42.2654roc-auc_val: 0.7739\n",
            "47/47 [==============================] - 12s 248ms/step - loss: 42.0091 - val_loss: 39.1171\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 42.1008 - val_loss: 39.1263\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 41.0782 - val_loss: 38.8398\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 40.4157 - val_loss: 38.5994\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 39.3234 - val_loss: 38.3318\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 39.4430 - val_loss: 38.2458\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 39.2376 - val_loss: 38.1556\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 37.6794 - val_loss: 38.1411\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 37.9668 - val_loss: 38.1065\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 37.3670 - val_loss: 37.8761\n",
            "Epoch 20/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 37.7539roc-auc_val: 0.784\n",
            "47/47 [==============================] - 12s 247ms/step - loss: 37.7803 - val_loss: 37.7179\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 36.8016 - val_loss: 37.6575\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 36.4166 - val_loss: 37.5768\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 36.8347 - val_loss: 37.8252\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 36.0900 - val_loss: 37.6614\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 35.5225 - val_loss: 37.6359\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 35.4306 - val_loss: 37.6119\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 36.0455 - val_loss: 37.5172\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 35.0231 - val_loss: 37.4098\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.2380 - val_loss: 37.2964\n",
            "Epoch 30/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 35.7807roc-auc_val: 0.7887\n",
            "47/47 [==============================] - 12s 249ms/step - loss: 35.5927 - val_loss: 37.3745\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.6232 - val_loss: 37.3334\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.3391 - val_loss: 37.3789\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.3907 - val_loss: 37.4301\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.3622 - val_loss: 37.4420\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 33.8901 - val_loss: 37.2694\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 33.7099 - val_loss: 37.2612\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 33.9294 - val_loss: 37.3029\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 33.4750 - val_loss: 37.2456\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.9830 - val_loss: 37.1706\n",
            "Epoch 40/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 33.3865roc-auc_val: 0.7914\n",
            "47/47 [==============================] - 12s 251ms/step - loss: 33.3192 - val_loss: 37.1330\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.9371 - val_loss: 37.1520\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.5584 - val_loss: 37.0796\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.3471 - val_loss: 37.0029\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.3382 - val_loss: 36.9440\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.4522 - val_loss: 36.9782\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.2062 - val_loss: 37.0717\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.2048 - val_loss: 37.0905\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.7820 - val_loss: 36.9912\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.8582 - val_loss: 37.0404\n",
            "Epoch 50/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 31.6555roc-auc_val: 0.7932\n",
            "47/47 [==============================] - 12s 249ms/step - loss: 31.5984 - val_loss: 37.0782\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.8867 - val_loss: 37.0381\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.7436 - val_loss: 37.0668\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.6885 - val_loss: 37.0546\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.7369 - val_loss: 37.0250\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.1669 - val_loss: 36.9420\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.2697 - val_loss: 36.9752\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.7768 - val_loss: 36.9459\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.1893 - val_loss: 36.8990\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.2075 - val_loss: 36.8661\n",
            "Epoch 60/1000\n",
            "46/47 [============================>.] - ETA: 0s - loss: 30.5743roc-auc_val: 0.7954\n",
            "47/47 [==============================] - 12s 249ms/step - loss: 30.4666 - val_loss: 36.8611\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.7387 - val_loss: 36.8403\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.6102 - val_loss: 36.8858\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.7086 - val_loss: 36.9370\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.6317 - val_loss: 36.9204\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.3290 - val_loss: 36.9175\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.4116 - val_loss: 36.8650\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.9880 - val_loss: 36.8510\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.3432 - val_loss: 36.8619\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.1163 - val_loss: 36.8475\n",
            "Epoch 70/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 30.1901roc-auc_val: 0.7966\n",
            "47/47 [==============================] - 12s 245ms/step - loss: 29.9410 - val_loss: 36.8600\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.1386 - val_loss: 36.8331\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.9764 - val_loss: 36.8632\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.8755 - val_loss: 36.8265\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.0129 - val_loss: 36.8421\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.0665 - val_loss: 36.8768\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.5005 - val_loss: 36.8385\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.9508 - val_loss: 36.8510\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.6450 - val_loss: 36.8047\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.3567 - val_loss: 36.7678\n",
            "Epoch 80/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 30.0469roc-auc_val: 0.7978\n",
            "47/47 [==============================] - 12s 249ms/step - loss: 29.7638 - val_loss: 36.7556\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.2472 - val_loss: 36.7584\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.5688 - val_loss: 36.7640\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.4566 - val_loss: 36.8052\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.9971 - val_loss: 36.8010\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.3714 - val_loss: 36.8474\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.8793 - val_loss: 36.7957\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.5634 - val_loss: 36.7826\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.2537 - val_loss: 36.7523\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.0019 - val_loss: 36.7695\n",
            "Epoch 90/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 28.8876roc-auc_val: 0.7987\n",
            "47/47 [==============================] - 12s 264ms/step - loss: 28.8401 - val_loss: 36.7543\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.5612 - val_loss: 36.7988\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.0110 - val_loss: 36.7994\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.9608 - val_loss: 36.8323\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.8559 - val_loss: 36.8695\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.8038 - val_loss: 36.8671\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.3647 - val_loss: 36.8472\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.8804 - val_loss: 36.9126\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.7889 - val_loss: 36.8636\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.9149 - val_loss: 36.8480\n",
            "Epoch 100/1000\n",
            "36/47 [=====================>........] - ETA: 0s - loss: 28.5360roc-auc_val: 0.7992\n",
            "47/47 [==============================] - 11s 244ms/step - loss: 28.5442 - val_loss: 36.8397\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.5271 - val_loss: 36.8826\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.3393 - val_loss: 36.8806\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.5249 - val_loss: 36.9046\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.4055 - val_loss: 36.9380\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.6392 - val_loss: 36.8886\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.8456 - val_loss: 36.9155\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.0021 - val_loss: 36.9032\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.8400 - val_loss: 36.9284\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.1324 - val_loss: 36.9237\n",
            "Epoch 110/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 28.7282roc-auc_val: 0.7995\n",
            "47/47 [==============================] - 11s 245ms/step - loss: 28.4074 - val_loss: 36.9815\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.8092 - val_loss: 36.9753\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.3628 - val_loss: 37.0198\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.2714 - val_loss: 37.0025\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.7164 - val_loss: 36.9900\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.0329 - val_loss: 37.0315\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.0202 - val_loss: 37.0442\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.1715 - val_loss: 37.0298\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.8849 - val_loss: 37.0261\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.8640 - val_loss: 37.0413\n",
            "Epoch 120/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 27.7880roc-auc_val: 0.7998\n",
            "47/47 [==============================] - 12s 249ms/step - loss: 27.4778 - val_loss: 37.0395\n",
            "Epoch 121/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.1870 - val_loss: 37.0611\n",
            "Epoch 122/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.6334 - val_loss: 37.0789\n",
            "Epoch 123/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.0223 - val_loss: 37.0778\n",
            "Epoch 124/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.3860 - val_loss: 37.0976\n",
            "Epoch 125/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.6707 - val_loss: 37.1246\n",
            "Epoch 126/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.5833 - val_loss: 37.1041\n",
            "Epoch 127/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.5801 - val_loss: 37.1050\n",
            "Epoch 128/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.1869 - val_loss: 37.1178\n",
            "Epoch 129/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.4329 - val_loss: 37.1277\n",
            "Epoch 130/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 27.3673roc-auc_val: 0.7998\n",
            "47/47 [==============================] - 12s 248ms/step - loss: 27.2715 - val_loss: 37.1211\n",
            "Epoch 131/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.4442 - val_loss: 37.1462\n",
            "Epoch 132/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.1484 - val_loss: 37.1859\n",
            "Epoch 133/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.9839 - val_loss: 37.2094\n",
            "Epoch 134/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.5383 - val_loss: 37.2163\n",
            "Epoch 135/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.3564 - val_loss: 37.2013\n",
            "Epoch 136/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.1911 - val_loss: 37.2302\n",
            "Epoch 137/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.9013 - val_loss: 37.2236\n",
            "Epoch 138/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.0797 - val_loss: 37.2368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnpeTPNLkiCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "outputId": "13c79322-7df0-4139-bf25-cc665dd5bab8"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6S7Pv_rc-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}
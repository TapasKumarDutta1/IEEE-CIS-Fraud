{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_roc",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_roc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "af06e9ee-da56-4923-9cea-75439da3bd4c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "66e1cf9e-3de0-41a9-f84f-56ccd2409e74"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 82% 43.0M/52.2M [00:00<00:00, 111MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 106MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 105MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 65% 38.0M/58.3M [00:01<00:01, 21.0MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 40.3MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 107MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 76.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.0001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv80v8W_Ko2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "fafaeafd-5f84-4dad-8345-e91de9c86017"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4jt2pcxPije",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "72421bff-05c7-4ace-9af2-0aa301b3d8ce"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvEaxp9jhbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "cf6c8e7f-393e-4f34-c624-4224c25956f2"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>V300</th>\n",
              "      <th>V301</th>\n",
              "      <th>V302</th>\n",
              "      <th>V303</th>\n",
              "      <th>V304</th>\n",
              "      <th>V305</th>\n",
              "      <th>V306</th>\n",
              "      <th>V307</th>\n",
              "      <th>V308</th>\n",
              "      <th>V309</th>\n",
              "      <th>V310</th>\n",
              "      <th>V311</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "      <th>V322</th>\n",
              "      <th>V323</th>\n",
              "      <th>V324</th>\n",
              "      <th>V325</th>\n",
              "      <th>V326</th>\n",
              "      <th>V327</th>\n",
              "      <th>V328</th>\n",
              "      <th>V329</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>debit</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1758.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1404.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>credit</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  V337 V338  V339\n",
              "0        2987000        0          86400  ...   NaN  NaN   NaN\n",
              "1        2987001        0          86401  ...   NaN  NaN   NaN\n",
              "2        2987002        0          86469  ...   NaN  NaN   NaN\n",
              "3        2987003        0          86499  ...   NaN  NaN   NaN\n",
              "4        2987004        0          86506  ...   0.0  0.0   0.0\n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d83c02ae-f1e5-4e2b-bee9-356f8ebfdcac"
      },
      "source": [
        "\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkLEBcl6LHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rac(y_true, y_pred):\n",
        "    \"\"\" ROC AUC Score.\n",
        "    Approximates the Area Under Curve score, using approximation based on\n",
        "    the Wilcoxon-Mann-Whitney U statistic.\n",
        "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
        "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
        "    Measures overall performance for a full range of threshold levels.\n",
        "    Arguments:\n",
        "        y_pred: `Tensor`. Predicted values.\n",
        "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"RocAucScore\"):\n",
        "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
        "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
        "        pos = tf.expand_dims(pos, 0)\n",
        "        neg = tf.expand_dims(neg, 1)\n",
        "        # original paper suggests performance is robust to exact parameter choice\n",
        "        gamma = 0.3\n",
        "        p     = 0.3\n",
        "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
        "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
        "        return tf.reduce_sum(tf.pow(-masked, p))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0716313-9697-40de-defd-f1bbdaa963c5"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "# trn=trn.drop(['isFraud_isna'],1)\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "dk={}\n",
        "for en,month in enumerate(range(1,5)):\n",
        "  train=trn.loc[trn['month']>=month]\n",
        "  test=trn.loc[trn['month']<month]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Adam(0.001,decay=1e-3),loss=rac,metrics='accuracy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.00001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "225/225 [==============================] - 2s 9ms/step - loss: 43002.8516 - accuracy: 0.7053 - val_loss: 35935.8594 - val_accuracy: 0.5773\n",
            "Epoch 2/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 37224.3438 - accuracy: 0.7394 - val_loss: 32695.1328 - val_accuracy: 0.5853\n",
            "Epoch 3/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 34875.9883 - accuracy: 0.7515 - val_loss: 32063.6484 - val_accuracy: 0.5929\n",
            "Epoch 4/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 33647.6211 - accuracy: 0.7584 - val_loss: 30912.8398 - val_accuracy: 0.6393\n",
            "Epoch 5/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 32759.8672 - accuracy: 0.7658 - val_loss: 31320.5039 - val_accuracy: 0.6343\n",
            "Epoch 6/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 31602.4297 - accuracy: 0.7726 - val_loss: 31323.5000 - val_accuracy: 0.6535\n",
            "Epoch 7/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 30564.0469 - accuracy: 0.7778 - val_loss: 29891.0176 - val_accuracy: 0.6814\n",
            "Epoch 8/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 30190.2812 - accuracy: 0.7815 - val_loss: 29884.3184 - val_accuracy: 0.6632\n",
            "Epoch 9/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 28977.7910 - accuracy: 0.7943 - val_loss: 28323.2578 - val_accuracy: 0.7272\n",
            "Epoch 10/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 28570.8203 - accuracy: 0.7985roc-auc_val: 0.7757\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 28510.2988 - accuracy: 0.7986 - val_loss: 29712.0508 - val_accuracy: 0.6617\n",
            "Epoch 11/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 27759.0938 - accuracy: 0.7978 - val_loss: 29562.9375 - val_accuracy: 0.6523\n",
            "Epoch 12/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 27259.4629 - accuracy: 0.7980 - val_loss: 29550.8750 - val_accuracy: 0.6823\n",
            "Epoch 13/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 26579.1797 - accuracy: 0.8071 - val_loss: 29305.4355 - val_accuracy: 0.6471\n",
            "Epoch 14/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 26094.9160 - accuracy: 0.8070 - val_loss: 29003.9082 - val_accuracy: 0.6910\n",
            "Epoch 15/1000\n",
            "225/225 [==============================] - 1s 7ms/step - loss: 25650.2988 - accuracy: 0.8117 - val_loss: 29202.1797 - val_accuracy: 0.6455\n",
            "Epoch 16/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 25378.0957 - accuracy: 0.8126 - val_loss: 28925.1230 - val_accuracy: 0.6942\n",
            "Epoch 17/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 24814.6836 - accuracy: 0.8183 - val_loss: 27710.1602 - val_accuracy: 0.7695\n",
            "Epoch 18/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 24414.3848 - accuracy: 0.8188 - val_loss: 29152.6953 - val_accuracy: 0.6706\n",
            "Epoch 19/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 24200.6270 - accuracy: 0.8167 - val_loss: 28317.5020 - val_accuracy: 0.7412\n",
            "Epoch 20/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 23902.0137 - accuracy: 0.8228roc-auc_val: 0.7804\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 23902.0137 - accuracy: 0.8228 - val_loss: 28817.5977 - val_accuracy: 0.6953\n",
            "Epoch 21/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 23906.9199 - accuracy: 0.8240 - val_loss: 28906.6602 - val_accuracy: 0.7050\n",
            "Epoch 22/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 23442.0039 - accuracy: 0.8260 - val_loss: 28262.3965 - val_accuracy: 0.6778\n",
            "Epoch 23/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 23177.7070 - accuracy: 0.8229 - val_loss: 28247.6719 - val_accuracy: 0.7484\n",
            "Epoch 24/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 23191.7246 - accuracy: 0.8265 - val_loss: 27638.3418 - val_accuracy: 0.7165\n",
            "Epoch 25/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 22960.4043 - accuracy: 0.8305 - val_loss: 28503.4785 - val_accuracy: 0.7206\n",
            "Epoch 26/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 22591.9473 - accuracy: 0.8300 - val_loss: 27434.8320 - val_accuracy: 0.7433\n",
            "Epoch 27/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 22473.4004 - accuracy: 0.8339 - val_loss: 27840.6348 - val_accuracy: 0.7164\n",
            "Epoch 28/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 22261.6152 - accuracy: 0.8316 - val_loss: 27661.4902 - val_accuracy: 0.7362\n",
            "Epoch 29/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 22033.8691 - accuracy: 0.8359 - val_loss: 27173.2422 - val_accuracy: 0.7574\n",
            "Epoch 30/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 22238.0254 - accuracy: 0.8353roc-auc_val: 0.793\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 22196.7109 - accuracy: 0.8352 - val_loss: 27616.0820 - val_accuracy: 0.7157\n",
            "Epoch 31/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 21926.0430 - accuracy: 0.8307 - val_loss: 27732.8750 - val_accuracy: 0.7274\n",
            "Epoch 32/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 21707.5527 - accuracy: 0.8352 - val_loss: 27266.8340 - val_accuracy: 0.7585\n",
            "Epoch 33/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 21680.5039 - accuracy: 0.8300 - val_loss: 27076.1621 - val_accuracy: 0.7457\n",
            "Epoch 34/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 21381.4395 - accuracy: 0.8358 - val_loss: 27176.8145 - val_accuracy: 0.7626\n",
            "Epoch 35/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 21356.1055 - accuracy: 0.8413 - val_loss: 26856.5078 - val_accuracy: 0.7733\n",
            "Epoch 36/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 21337.2793 - accuracy: 0.8411 - val_loss: 26936.4473 - val_accuracy: 0.7697\n",
            "Epoch 37/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 21247.2578 - accuracy: 0.8404 - val_loss: 27094.3887 - val_accuracy: 0.7519\n",
            "Epoch 38/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20936.8945 - accuracy: 0.8411 - val_loss: 27497.1992 - val_accuracy: 0.7449\n",
            "Epoch 39/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20925.4844 - accuracy: 0.8428 - val_loss: 27837.3613 - val_accuracy: 0.7290\n",
            "Epoch 40/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 20960.0020 - accuracy: 0.8460roc-auc_val: 0.7993\n",
            "225/225 [==============================] - 6s 27ms/step - loss: 20914.7500 - accuracy: 0.8461 - val_loss: 26788.9590 - val_accuracy: 0.7544\n",
            "Epoch 41/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20967.1250 - accuracy: 0.8406 - val_loss: 26712.4062 - val_accuracy: 0.7532\n",
            "Epoch 42/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20579.5410 - accuracy: 0.8420 - val_loss: 26719.1406 - val_accuracy: 0.7604\n",
            "Epoch 43/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20618.0059 - accuracy: 0.8414 - val_loss: 27805.4219 - val_accuracy: 0.7732\n",
            "Epoch 44/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20639.2852 - accuracy: 0.8517 - val_loss: 26933.4648 - val_accuracy: 0.7747\n",
            "Epoch 45/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20428.1191 - accuracy: 0.8466 - val_loss: 26709.4980 - val_accuracy: 0.7579\n",
            "Epoch 46/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20408.5391 - accuracy: 0.8459 - val_loss: 26706.7773 - val_accuracy: 0.7859\n",
            "Epoch 47/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20414.6406 - accuracy: 0.8380 - val_loss: 26525.7852 - val_accuracy: 0.7665\n",
            "Epoch 48/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20098.4141 - accuracy: 0.8454 - val_loss: 26883.2656 - val_accuracy: 0.7451\n",
            "Epoch 49/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20272.4062 - accuracy: 0.8405 - val_loss: 26876.7227 - val_accuracy: 0.7654\n",
            "Epoch 50/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 20235.8750 - accuracy: 0.8523roc-auc_val: 0.8023\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 20127.6758 - accuracy: 0.8527 - val_loss: 26776.1465 - val_accuracy: 0.7903\n",
            "Epoch 51/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 20193.8906 - accuracy: 0.8509 - val_loss: 26337.2031 - val_accuracy: 0.7629\n",
            "Epoch 52/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19969.9766 - accuracy: 0.8484 - val_loss: 26686.4062 - val_accuracy: 0.7564\n",
            "Epoch 53/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19904.0020 - accuracy: 0.8453 - val_loss: 26375.3730 - val_accuracy: 0.7666\n",
            "Epoch 54/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19795.7402 - accuracy: 0.8485 - val_loss: 26763.5430 - val_accuracy: 0.7849\n",
            "Epoch 55/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19891.7441 - accuracy: 0.8490 - val_loss: 26768.2188 - val_accuracy: 0.7652\n",
            "Epoch 56/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19825.0879 - accuracy: 0.8501 - val_loss: 26787.1973 - val_accuracy: 0.7622\n",
            "Epoch 57/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19448.1406 - accuracy: 0.8559 - val_loss: 26943.5918 - val_accuracy: 0.7965\n",
            "Epoch 58/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19680.4277 - accuracy: 0.8473 - val_loss: 26782.2676 - val_accuracy: 0.7716\n",
            "Epoch 59/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19609.9570 - accuracy: 0.8503 - val_loss: 26445.9785 - val_accuracy: 0.7719\n",
            "Epoch 60/1000\n",
            "220/225 [============================>.] - ETA: 0s - loss: 19669.7734 - accuracy: 0.8495roc-auc_val: 0.8018\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 19658.5234 - accuracy: 0.8496 - val_loss: 26739.8652 - val_accuracy: 0.7986\n",
            "Epoch 61/1000\n",
            "225/225 [==============================] - 1s 7ms/step - loss: 19643.8203 - accuracy: 0.8469 - val_loss: 26697.7832 - val_accuracy: 0.7711\n",
            "Epoch 62/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19464.5762 - accuracy: 0.8571 - val_loss: 26756.6738 - val_accuracy: 0.7921\n",
            "Epoch 63/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19432.0781 - accuracy: 0.8544 - val_loss: 26332.3008 - val_accuracy: 0.7855\n",
            "Epoch 64/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19354.7832 - accuracy: 0.8530 - val_loss: 26222.6250 - val_accuracy: 0.7712\n",
            "Epoch 65/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19292.2637 - accuracy: 0.8359 - val_loss: 26157.7031 - val_accuracy: 0.7901\n",
            "Epoch 66/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19198.3027 - accuracy: 0.8487 - val_loss: 26375.0762 - val_accuracy: 0.7621\n",
            "Epoch 67/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19279.6152 - accuracy: 0.8493 - val_loss: 26138.9375 - val_accuracy: 0.7854\n",
            "Epoch 68/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19181.4297 - accuracy: 0.8530 - val_loss: 26555.6348 - val_accuracy: 0.7725\n",
            "Epoch 69/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19066.8184 - accuracy: 0.8522 - val_loss: 26363.9883 - val_accuracy: 0.7793\n",
            "Epoch 70/1000\n",
            "219/225 [============================>.] - ETA: 0s - loss: 19114.8301 - accuracy: 0.8493roc-auc_val: 0.8072\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 19085.0312 - accuracy: 0.8494 - val_loss: 26545.5430 - val_accuracy: 0.7909\n",
            "Epoch 71/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19135.3594 - accuracy: 0.8581 - val_loss: 26398.1250 - val_accuracy: 0.7707\n",
            "Epoch 72/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18961.8730 - accuracy: 0.8542 - val_loss: 26362.7812 - val_accuracy: 0.7729\n",
            "Epoch 73/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 19063.5781 - accuracy: 0.8512 - val_loss: 26145.2246 - val_accuracy: 0.7911\n",
            "Epoch 74/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18974.7500 - accuracy: 0.8565 - val_loss: 26132.3203 - val_accuracy: 0.7938\n",
            "Epoch 75/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18893.1035 - accuracy: 0.8557 - val_loss: 26206.5352 - val_accuracy: 0.7916\n",
            "Epoch 76/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18823.2383 - accuracy: 0.8557 - val_loss: 26330.4844 - val_accuracy: 0.7843\n",
            "Epoch 77/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18777.4590 - accuracy: 0.8539 - val_loss: 26332.3320 - val_accuracy: 0.7995\n",
            "Epoch 78/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18601.8066 - accuracy: 0.8560 - val_loss: 26220.8438 - val_accuracy: 0.7846\n",
            "Epoch 79/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18754.7363 - accuracy: 0.8591 - val_loss: 26368.0000 - val_accuracy: 0.7875\n",
            "Epoch 80/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 18875.1504 - accuracy: 0.8589roc-auc_val: 0.8094\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 18817.8027 - accuracy: 0.8586 - val_loss: 26077.2441 - val_accuracy: 0.7760\n",
            "Epoch 81/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18527.5801 - accuracy: 0.8514 - val_loss: 26253.8184 - val_accuracy: 0.7801\n",
            "Epoch 82/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18524.0410 - accuracy: 0.8529 - val_loss: 26454.9043 - val_accuracy: 0.7868\n",
            "Epoch 83/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18706.8594 - accuracy: 0.8542 - val_loss: 25900.1523 - val_accuracy: 0.8274\n",
            "Epoch 84/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18736.2754 - accuracy: 0.8495 - val_loss: 26051.8340 - val_accuracy: 0.7924\n",
            "Epoch 85/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18710.7500 - accuracy: 0.8506 - val_loss: 26133.3457 - val_accuracy: 0.7761\n",
            "Epoch 86/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18695.0898 - accuracy: 0.8536 - val_loss: 26032.0508 - val_accuracy: 0.7650\n",
            "Epoch 87/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18548.5176 - accuracy: 0.8500 - val_loss: 26081.4609 - val_accuracy: 0.7884\n",
            "Epoch 88/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18544.4375 - accuracy: 0.8505 - val_loss: 26167.8301 - val_accuracy: 0.7763\n",
            "Epoch 89/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18498.4219 - accuracy: 0.8518 - val_loss: 26295.1152 - val_accuracy: 0.7895\n",
            "Epoch 90/1000\n",
            "217/225 [===========================>..] - ETA: 0s - loss: 18316.2734 - accuracy: 0.8534roc-auc_val: 0.8085\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 18368.6113 - accuracy: 0.8538 - val_loss: 26372.5781 - val_accuracy: 0.7975\n",
            "Epoch 91/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18402.6230 - accuracy: 0.8568 - val_loss: 26403.7773 - val_accuracy: 0.7811\n",
            "Epoch 92/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18321.4727 - accuracy: 0.8522 - val_loss: 26128.5371 - val_accuracy: 0.7927\n",
            "Epoch 93/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18417.9707 - accuracy: 0.8537 - val_loss: 26248.2402 - val_accuracy: 0.7926\n",
            "Epoch 94/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18251.6699 - accuracy: 0.8579 - val_loss: 26076.8301 - val_accuracy: 0.7942\n",
            "Epoch 95/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18219.4316 - accuracy: 0.8550 - val_loss: 26017.4434 - val_accuracy: 0.7966\n",
            "Epoch 96/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18159.3633 - accuracy: 0.8549 - val_loss: 26034.5371 - val_accuracy: 0.7913\n",
            "Epoch 97/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18231.4082 - accuracy: 0.8553 - val_loss: 26115.8438 - val_accuracy: 0.7891\n",
            "Epoch 98/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18095.1094 - accuracy: 0.8541 - val_loss: 26012.6230 - val_accuracy: 0.8016\n",
            "Epoch 99/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18134.2578 - accuracy: 0.8548 - val_loss: 26209.9863 - val_accuracy: 0.7743\n",
            "Epoch 100/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 18120.3906 - accuracy: 0.8563roc-auc_val: 0.8093\n",
            "225/225 [==============================] - 6s 27ms/step - loss: 18102.9941 - accuracy: 0.8565 - val_loss: 26094.7832 - val_accuracy: 0.7979\n",
            "Epoch 101/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18003.3770 - accuracy: 0.8571 - val_loss: 26167.0820 - val_accuracy: 0.7769\n",
            "Epoch 102/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18223.9609 - accuracy: 0.8557 - val_loss: 25910.2461 - val_accuracy: 0.7820\n",
            "Epoch 103/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18039.8945 - accuracy: 0.8543 - val_loss: 25799.1035 - val_accuracy: 0.7791\n",
            "Epoch 104/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18111.3223 - accuracy: 0.8580 - val_loss: 25768.6875 - val_accuracy: 0.8004\n",
            "Epoch 105/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18156.9180 - accuracy: 0.8550 - val_loss: 25928.8320 - val_accuracy: 0.7814\n",
            "Epoch 106/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18014.3066 - accuracy: 0.8588 - val_loss: 26284.9004 - val_accuracy: 0.7934\n",
            "Epoch 107/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17967.4355 - accuracy: 0.8581 - val_loss: 25876.7363 - val_accuracy: 0.7954\n",
            "Epoch 108/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18041.7305 - accuracy: 0.8549 - val_loss: 25839.3965 - val_accuracy: 0.7875\n",
            "Epoch 109/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18028.5742 - accuracy: 0.8502 - val_loss: 25778.9941 - val_accuracy: 0.7839\n",
            "Epoch 110/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 18028.8047 - accuracy: 0.8462roc-auc_val: 0.8074\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 18031.7129 - accuracy: 0.8463 - val_loss: 26169.1152 - val_accuracy: 0.7734\n",
            "Epoch 111/1000\n",
            "225/225 [==============================] - 1s 7ms/step - loss: 17796.4648 - accuracy: 0.8511 - val_loss: 25899.5977 - val_accuracy: 0.7709\n",
            "Epoch 112/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17894.5117 - accuracy: 0.8500 - val_loss: 25892.9219 - val_accuracy: 0.7883\n",
            "Epoch 113/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 18052.2363 - accuracy: 0.8534 - val_loss: 25880.2070 - val_accuracy: 0.7953\n",
            "Epoch 114/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17934.2324 - accuracy: 0.8594 - val_loss: 25841.5840 - val_accuracy: 0.8069\n",
            "Epoch 115/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17870.9883 - accuracy: 0.8589 - val_loss: 25697.2480 - val_accuracy: 0.7911\n",
            "Epoch 116/1000\n",
            "225/225 [==============================] - 1s 7ms/step - loss: 17783.2109 - accuracy: 0.8588 - val_loss: 25856.7422 - val_accuracy: 0.7936\n",
            "Epoch 117/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17781.5996 - accuracy: 0.8545 - val_loss: 25720.0234 - val_accuracy: 0.7911\n",
            "Epoch 118/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17795.9512 - accuracy: 0.8569 - val_loss: 25763.9785 - val_accuracy: 0.8041\n",
            "Epoch 119/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17690.3984 - accuracy: 0.8600 - val_loss: 25855.4707 - val_accuracy: 0.7914\n",
            "Epoch 120/1000\n",
            "225/225 [==============================] - ETA: 0s - loss: 17729.8164 - accuracy: 0.8602roc-auc_val: 0.8138\n",
            "225/225 [==============================] - 6s 27ms/step - loss: 17729.8164 - accuracy: 0.8602 - val_loss: 25737.8867 - val_accuracy: 0.7940\n",
            "Epoch 121/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 17541.4688 - accuracy: 0.8577 - val_loss: 25870.4180 - val_accuracy: 0.7913\n",
            "Epoch 122/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17699.8613 - accuracy: 0.8584 - val_loss: 25956.9297 - val_accuracy: 0.7924\n",
            "Epoch 123/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17618.2285 - accuracy: 0.8588 - val_loss: 26083.1934 - val_accuracy: 0.8041\n",
            "Epoch 124/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17596.2070 - accuracy: 0.8606 - val_loss: 25914.5879 - val_accuracy: 0.7929\n",
            "Epoch 125/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17796.2832 - accuracy: 0.8556 - val_loss: 25926.4688 - val_accuracy: 0.7896\n",
            "Epoch 126/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17727.4941 - accuracy: 0.8597 - val_loss: 25966.2031 - val_accuracy: 0.8018\n",
            "Epoch 127/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17705.1309 - accuracy: 0.8573 - val_loss: 25978.7891 - val_accuracy: 0.7839\n",
            "Epoch 128/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17535.0254 - accuracy: 0.8599 - val_loss: 26164.8750 - val_accuracy: 0.7907\n",
            "Epoch 129/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17601.6309 - accuracy: 0.8613 - val_loss: 25911.9844 - val_accuracy: 0.8008\n",
            "Epoch 130/1000\n",
            "218/225 [============================>.] - ETA: 0s - loss: 17652.1523 - accuracy: 0.8564roc-auc_val: 0.8112\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 17595.4805 - accuracy: 0.8564 - val_loss: 25941.4004 - val_accuracy: 0.7799\n",
            "Epoch 131/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17550.0488 - accuracy: 0.8563 - val_loss: 25835.3828 - val_accuracy: 0.7959\n",
            "Epoch 132/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17578.7598 - accuracy: 0.8572 - val_loss: 25873.4297 - val_accuracy: 0.8006\n",
            "Epoch 133/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17546.1719 - accuracy: 0.8614 - val_loss: 26006.3887 - val_accuracy: 0.8059\n",
            "Epoch 134/1000\n",
            "225/225 [==============================] - 1s 7ms/step - loss: 17329.8027 - accuracy: 0.8620 - val_loss: 26134.1211 - val_accuracy: 0.7975\n",
            "Epoch 135/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17720.2148 - accuracy: 0.8602 - val_loss: 25932.3574 - val_accuracy: 0.7869\n",
            "Epoch 136/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17470.5527 - accuracy: 0.8659 - val_loss: 26152.8691 - val_accuracy: 0.8081\n",
            "Epoch 137/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17383.0430 - accuracy: 0.8670 - val_loss: 26050.9590 - val_accuracy: 0.8042\n",
            "Epoch 138/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17533.1719 - accuracy: 0.8611 - val_loss: 26138.0977 - val_accuracy: 0.7892\n",
            "Epoch 139/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17223.8418 - accuracy: 0.8617 - val_loss: 26023.8867 - val_accuracy: 0.8046\n",
            "Epoch 140/1000\n",
            "222/225 [============================>.] - ETA: 0s - loss: 17586.9766 - accuracy: 0.8591roc-auc_val: 0.8097\n",
            "225/225 [==============================] - 6s 27ms/step - loss: 17572.0938 - accuracy: 0.8591 - val_loss: 26035.4648 - val_accuracy: 0.7879\n",
            "Epoch 141/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 17262.8125 - accuracy: 0.8588 - val_loss: 25960.3301 - val_accuracy: 0.7862\n",
            "Epoch 142/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 17443.5586 - accuracy: 0.8610 - val_loss: 25921.0508 - val_accuracy: 0.7925\n",
            "Epoch 143/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 17305.9746 - accuracy: 0.8628 - val_loss: 25987.0020 - val_accuracy: 0.7885\n",
            "Epoch 144/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 17444.0957 - accuracy: 0.8579 - val_loss: 25734.3242 - val_accuracy: 0.7985\n",
            "Epoch 145/1000\n",
            "225/225 [==============================] - 2s 8ms/step - loss: 17348.7812 - accuracy: 0.8580 - val_loss: 25843.4922 - val_accuracy: 0.7982\n",
            "Epoch 146/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17154.6074 - accuracy: 0.8606 - val_loss: 25955.6016 - val_accuracy: 0.7862\n",
            "Epoch 147/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17334.6504 - accuracy: 0.8553 - val_loss: 25965.6250 - val_accuracy: 0.7866\n",
            "Epoch 148/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17392.8750 - accuracy: 0.8547 - val_loss: 26024.3594 - val_accuracy: 0.7845\n",
            "Epoch 149/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17333.1777 - accuracy: 0.8593 - val_loss: 25931.8984 - val_accuracy: 0.7936\n",
            "Epoch 150/1000\n",
            "224/225 [============================>.] - ETA: 0s - loss: 17377.2871 - accuracy: 0.8634roc-auc_val: 0.8098\n",
            "225/225 [==============================] - 6s 25ms/step - loss: 17349.8281 - accuracy: 0.8635 - val_loss: 26138.4199 - val_accuracy: 0.8040\n",
            "Epoch 151/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17269.9961 - accuracy: 0.8650 - val_loss: 26151.5723 - val_accuracy: 0.7930\n",
            "Epoch 152/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17343.1523 - accuracy: 0.8588 - val_loss: 25725.6504 - val_accuracy: 0.7987\n",
            "Epoch 153/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17398.3750 - accuracy: 0.8601 - val_loss: 25869.2363 - val_accuracy: 0.7899\n",
            "Epoch 154/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17315.9453 - accuracy: 0.8595 - val_loss: 25998.0098 - val_accuracy: 0.7961\n",
            "Epoch 155/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17294.3711 - accuracy: 0.8584 - val_loss: 25806.3477 - val_accuracy: 0.7889\n",
            "Epoch 156/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17220.7930 - accuracy: 0.8586 - val_loss: 25870.5449 - val_accuracy: 0.7871\n",
            "Epoch 157/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17314.8711 - accuracy: 0.8591 - val_loss: 26013.5312 - val_accuracy: 0.7842\n",
            "Epoch 158/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17273.1328 - accuracy: 0.8577 - val_loss: 26102.4434 - val_accuracy: 0.7905\n",
            "Epoch 159/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17243.0801 - accuracy: 0.8586 - val_loss: 26041.5879 - val_accuracy: 0.7813\n",
            "Epoch 160/1000\n",
            "223/225 [============================>.] - ETA: 0s - loss: 17290.2656 - accuracy: 0.8557roc-auc_val: 0.8107\n",
            "225/225 [==============================] - 6s 26ms/step - loss: 17240.9336 - accuracy: 0.8558 - val_loss: 26024.9824 - val_accuracy: 0.7926\n",
            "Epoch 161/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17090.5312 - accuracy: 0.8596 - val_loss: 25943.3359 - val_accuracy: 0.7938\n",
            "Epoch 162/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17190.1387 - accuracy: 0.8572 - val_loss: 25971.1191 - val_accuracy: 0.7859\n",
            "Epoch 163/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17172.9102 - accuracy: 0.8603 - val_loss: 25993.7715 - val_accuracy: 0.7911\n",
            "Epoch 164/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17079.2188 - accuracy: 0.8633 - val_loss: 25950.3574 - val_accuracy: 0.7952\n",
            "Epoch 165/1000\n",
            "225/225 [==============================] - 2s 7ms/step - loss: 17110.6953 - accuracy: 0.8639 - val_loss: 25938.5918 - val_accuracy: 0.7901\n",
            "Epoch 1/1000\n",
            "181/181 [==============================] - 2s 11ms/step - loss: 42536.6836 - accuracy: 0.7045 - val_loss: 37824.9414 - val_accuracy: 0.6443\n",
            "Epoch 2/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 36174.2930 - accuracy: 0.7450 - val_loss: 34767.2969 - val_accuracy: 0.6539\n",
            "Epoch 3/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 34298.9531 - accuracy: 0.7482 - val_loss: 35192.2734 - val_accuracy: 0.6513\n",
            "Epoch 4/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 32946.9727 - accuracy: 0.7648 - val_loss: 34985.9492 - val_accuracy: 0.6502\n",
            "Epoch 5/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 31788.6152 - accuracy: 0.7668 - val_loss: 34233.3789 - val_accuracy: 0.6546\n",
            "Epoch 6/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 30954.7227 - accuracy: 0.7722 - val_loss: 34779.4297 - val_accuracy: 0.6508\n",
            "Epoch 7/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 30083.7168 - accuracy: 0.7807 - val_loss: 34178.0938 - val_accuracy: 0.6894\n",
            "Epoch 8/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 29586.0645 - accuracy: 0.7904 - val_loss: 33929.3047 - val_accuracy: 0.6259\n",
            "Epoch 9/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 29099.6719 - accuracy: 0.7844 - val_loss: 33404.4219 - val_accuracy: 0.6773\n",
            "Epoch 10/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 28118.1016 - accuracy: 0.7948roc-auc_val: 0.7938\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 28114.9980 - accuracy: 0.7948 - val_loss: 33661.8711 - val_accuracy: 0.7034\n",
            "Epoch 11/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 27647.3496 - accuracy: 0.7950 - val_loss: 33112.7969 - val_accuracy: 0.6899\n",
            "Epoch 12/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 27205.0312 - accuracy: 0.7960 - val_loss: 32771.2031 - val_accuracy: 0.6655\n",
            "Epoch 13/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 26992.9434 - accuracy: 0.7965 - val_loss: 32948.7656 - val_accuracy: 0.6830\n",
            "Epoch 14/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 26391.8574 - accuracy: 0.7980 - val_loss: 32580.4277 - val_accuracy: 0.6930\n",
            "Epoch 15/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 25747.0000 - accuracy: 0.8023 - val_loss: 33406.3828 - val_accuracy: 0.6440\n",
            "Epoch 16/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 25413.1289 - accuracy: 0.8016 - val_loss: 33521.2617 - val_accuracy: 0.7051\n",
            "Epoch 17/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 25449.9102 - accuracy: 0.8105 - val_loss: 33430.3672 - val_accuracy: 0.7047\n",
            "Epoch 18/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 24676.9785 - accuracy: 0.8113 - val_loss: 32802.7656 - val_accuracy: 0.7095\n",
            "Epoch 19/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 24171.7949 - accuracy: 0.8145 - val_loss: 32952.9609 - val_accuracy: 0.7204\n",
            "Epoch 20/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 24082.4629 - accuracy: 0.8198roc-auc_val: 0.8047\n",
            "181/181 [==============================] - 9s 49ms/step - loss: 24027.5664 - accuracy: 0.8196 - val_loss: 32383.6309 - val_accuracy: 0.7052\n",
            "Epoch 21/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 23559.5410 - accuracy: 0.8218 - val_loss: 32570.6309 - val_accuracy: 0.7237\n",
            "Epoch 22/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 23221.1055 - accuracy: 0.8220 - val_loss: 32375.4453 - val_accuracy: 0.7437\n",
            "Epoch 23/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 22972.4355 - accuracy: 0.8259 - val_loss: 32389.5566 - val_accuracy: 0.7103\n",
            "Epoch 24/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 22779.0195 - accuracy: 0.8252 - val_loss: 31493.5000 - val_accuracy: 0.6908\n",
            "Epoch 25/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 22520.7891 - accuracy: 0.8266 - val_loss: 31784.3047 - val_accuracy: 0.6801\n",
            "Epoch 26/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 22096.8203 - accuracy: 0.8267 - val_loss: 31783.9668 - val_accuracy: 0.7220\n",
            "Epoch 27/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 21996.7402 - accuracy: 0.8326 - val_loss: 31569.0840 - val_accuracy: 0.7060\n",
            "Epoch 28/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 21724.1582 - accuracy: 0.8322 - val_loss: 31997.8164 - val_accuracy: 0.6878\n",
            "Epoch 29/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 21776.5059 - accuracy: 0.8355 - val_loss: 31536.4805 - val_accuracy: 0.7242\n",
            "Epoch 30/1000\n",
            "174/181 [===========================>..] - ETA: 0s - loss: 21788.2207 - accuracy: 0.8319roc-auc_val: 0.8155\n",
            "181/181 [==============================] - 9s 49ms/step - loss: 21735.9160 - accuracy: 0.8323 - val_loss: 31466.5449 - val_accuracy: 0.7504\n",
            "Epoch 31/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 21065.6680 - accuracy: 0.8379 - val_loss: 32238.3926 - val_accuracy: 0.7159\n",
            "Epoch 32/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 21045.3301 - accuracy: 0.8371 - val_loss: 32047.6621 - val_accuracy: 0.7322\n",
            "Epoch 33/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20981.5664 - accuracy: 0.8388 - val_loss: 32357.4434 - val_accuracy: 0.7789\n",
            "Epoch 34/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20997.2734 - accuracy: 0.8438 - val_loss: 31493.7969 - val_accuracy: 0.7208\n",
            "Epoch 35/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20981.1289 - accuracy: 0.8325 - val_loss: 31771.1445 - val_accuracy: 0.7266\n",
            "Epoch 36/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20645.7520 - accuracy: 0.8429 - val_loss: 32508.5957 - val_accuracy: 0.7009\n",
            "Epoch 37/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20424.8516 - accuracy: 0.8427 - val_loss: 32105.7031 - val_accuracy: 0.7333\n",
            "Epoch 38/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20418.4141 - accuracy: 0.8473 - val_loss: 31889.0000 - val_accuracy: 0.7476\n",
            "Epoch 39/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20336.3086 - accuracy: 0.8433 - val_loss: 31872.0684 - val_accuracy: 0.7405\n",
            "Epoch 40/1000\n",
            "179/181 [============================>.] - ETA: 0s - loss: 20307.5801 - accuracy: 0.8445roc-auc_val: 0.8102\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 20295.1426 - accuracy: 0.8445 - val_loss: 31981.9082 - val_accuracy: 0.7124\n",
            "Epoch 41/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20180.5605 - accuracy: 0.8423 - val_loss: 31574.3945 - val_accuracy: 0.7543\n",
            "Epoch 42/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 20036.3184 - accuracy: 0.8453 - val_loss: 31756.6328 - val_accuracy: 0.7220\n",
            "Epoch 43/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19964.6094 - accuracy: 0.8394 - val_loss: 31679.9141 - val_accuracy: 0.7258\n",
            "Epoch 44/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 20053.8086 - accuracy: 0.8461 - val_loss: 32352.4961 - val_accuracy: 0.7664\n",
            "Epoch 45/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19796.8223 - accuracy: 0.8464 - val_loss: 32248.1738 - val_accuracy: 0.7604\n",
            "Epoch 46/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19769.3223 - accuracy: 0.8437 - val_loss: 31886.0645 - val_accuracy: 0.7371\n",
            "Epoch 47/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19605.5938 - accuracy: 0.8514 - val_loss: 31832.2930 - val_accuracy: 0.7560\n",
            "Epoch 48/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19398.4844 - accuracy: 0.8452 - val_loss: 31820.5215 - val_accuracy: 0.7305\n",
            "Epoch 49/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19455.1562 - accuracy: 0.8461 - val_loss: 31697.9336 - val_accuracy: 0.7322\n",
            "Epoch 50/1000\n",
            "179/181 [============================>.] - ETA: 0s - loss: 19338.2520 - accuracy: 0.8535roc-auc_val: 0.813\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 19288.0234 - accuracy: 0.8534 - val_loss: 31956.9863 - val_accuracy: 0.7468\n",
            "Epoch 51/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19303.0957 - accuracy: 0.8499 - val_loss: 31845.2285 - val_accuracy: 0.7488\n",
            "Epoch 52/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19382.2910 - accuracy: 0.8441 - val_loss: 31460.4512 - val_accuracy: 0.7076\n",
            "Epoch 53/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19180.5957 - accuracy: 0.8496 - val_loss: 31485.5195 - val_accuracy: 0.7346\n",
            "Epoch 54/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19302.5938 - accuracy: 0.8540 - val_loss: 31671.2832 - val_accuracy: 0.7484\n",
            "Epoch 55/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19029.4297 - accuracy: 0.8457 - val_loss: 31959.5449 - val_accuracy: 0.7548\n",
            "Epoch 56/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18978.8320 - accuracy: 0.8447 - val_loss: 32096.3945 - val_accuracy: 0.7261\n",
            "Epoch 57/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 19024.7598 - accuracy: 0.8521 - val_loss: 31816.3535 - val_accuracy: 0.7553\n",
            "Epoch 58/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18763.4219 - accuracy: 0.8520 - val_loss: 31988.8398 - val_accuracy: 0.7559\n",
            "Epoch 59/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 18831.4941 - accuracy: 0.8534 - val_loss: 32060.8496 - val_accuracy: 0.7458\n",
            "Epoch 60/1000\n",
            "174/181 [===========================>..] - ETA: 0s - loss: 19075.7734 - accuracy: 0.8565roc-auc_val: 0.8169\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 18977.2852 - accuracy: 0.8563 - val_loss: 31693.0410 - val_accuracy: 0.7688\n",
            "Epoch 61/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 18921.7695 - accuracy: 0.8560 - val_loss: 32126.7637 - val_accuracy: 0.7696\n",
            "Epoch 62/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18482.3711 - accuracy: 0.8545 - val_loss: 32273.2637 - val_accuracy: 0.7695\n",
            "Epoch 63/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18600.4023 - accuracy: 0.8546 - val_loss: 32116.6953 - val_accuracy: 0.7634\n",
            "Epoch 64/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18409.6719 - accuracy: 0.8544 - val_loss: 31774.3652 - val_accuracy: 0.7525\n",
            "Epoch 65/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18517.2656 - accuracy: 0.8548 - val_loss: 32145.9824 - val_accuracy: 0.7335\n",
            "Epoch 66/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18462.4062 - accuracy: 0.8553 - val_loss: 32054.9180 - val_accuracy: 0.7446\n",
            "Epoch 67/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18505.8926 - accuracy: 0.8542 - val_loss: 31896.3418 - val_accuracy: 0.7444\n",
            "Epoch 68/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18416.3477 - accuracy: 0.8511 - val_loss: 32295.6543 - val_accuracy: 0.7582\n",
            "Epoch 69/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18237.2285 - accuracy: 0.8577 - val_loss: 32121.6602 - val_accuracy: 0.7793\n",
            "Epoch 70/1000\n",
            "177/181 [============================>.] - ETA: 0s - loss: 18235.6133 - accuracy: 0.8596roc-auc_val: 0.8144\n",
            "181/181 [==============================] - 9s 48ms/step - loss: 18205.8672 - accuracy: 0.8594 - val_loss: 31937.4922 - val_accuracy: 0.7631\n",
            "Epoch 71/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18268.3438 - accuracy: 0.8561 - val_loss: 31938.4160 - val_accuracy: 0.7522\n",
            "Epoch 72/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18074.3184 - accuracy: 0.8539 - val_loss: 32632.6035 - val_accuracy: 0.7900\n",
            "Epoch 73/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18272.0781 - accuracy: 0.8558 - val_loss: 32097.6191 - val_accuracy: 0.7772\n",
            "Epoch 74/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18210.3203 - accuracy: 0.8622 - val_loss: 31668.8438 - val_accuracy: 0.7670\n",
            "Epoch 75/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18182.3594 - accuracy: 0.8545 - val_loss: 32121.7363 - val_accuracy: 0.7548\n",
            "Epoch 76/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17991.2285 - accuracy: 0.8583 - val_loss: 32178.3320 - val_accuracy: 0.7695\n",
            "Epoch 77/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17827.1621 - accuracy: 0.8546 - val_loss: 31994.5352 - val_accuracy: 0.7589\n",
            "Epoch 78/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17950.5547 - accuracy: 0.8591 - val_loss: 31903.1328 - val_accuracy: 0.7566\n",
            "Epoch 79/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 18042.3652 - accuracy: 0.8564 - val_loss: 31803.7637 - val_accuracy: 0.7741\n",
            "Epoch 80/1000\n",
            "178/181 [============================>.] - ETA: 0s - loss: 18028.2598 - accuracy: 0.8611roc-auc_val: 0.8164\n",
            "181/181 [==============================] - 9s 49ms/step - loss: 18036.2578 - accuracy: 0.8609 - val_loss: 31724.2734 - val_accuracy: 0.7466\n",
            "Epoch 81/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17951.7324 - accuracy: 0.8588 - val_loss: 31834.1758 - val_accuracy: 0.7442\n",
            "Epoch 82/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17761.5996 - accuracy: 0.8605 - val_loss: 31674.5645 - val_accuracy: 0.7583\n",
            "Epoch 83/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17768.7246 - accuracy: 0.8549 - val_loss: 31671.3320 - val_accuracy: 0.7423\n",
            "Epoch 84/1000\n",
            "181/181 [==============================] - 2s 8ms/step - loss: 17804.0703 - accuracy: 0.8578 - val_loss: 31822.7637 - val_accuracy: 0.7612\n",
            "Epoch 85/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17595.3848 - accuracy: 0.8567 - val_loss: 31738.7090 - val_accuracy: 0.7655\n",
            "Epoch 86/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17592.8633 - accuracy: 0.8602 - val_loss: 32016.4648 - val_accuracy: 0.7738\n",
            "Epoch 87/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17650.5117 - accuracy: 0.8538 - val_loss: 32152.7207 - val_accuracy: 0.7418\n",
            "Epoch 88/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17531.3926 - accuracy: 0.8556 - val_loss: 31662.3691 - val_accuracy: 0.7443\n",
            "Epoch 89/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17528.7188 - accuracy: 0.8604 - val_loss: 31536.9727 - val_accuracy: 0.7522\n",
            "Epoch 90/1000\n",
            "176/181 [============================>.] - ETA: 0s - loss: 17658.1738 - accuracy: 0.8578roc-auc_val: 0.8171\n",
            "181/181 [==============================] - 9s 50ms/step - loss: 17620.3828 - accuracy: 0.8578 - val_loss: 31525.7734 - val_accuracy: 0.7534\n",
            "Epoch 91/1000\n",
            "181/181 [==============================] - 2s 9ms/step - loss: 17590.0312 - accuracy: 0.8566 - val_loss: 31879.4453 - val_accuracy: 0.7702\n",
            "Epoch 92/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17391.0352 - accuracy: 0.8624 - val_loss: 31993.8320 - val_accuracy: 0.7692\n",
            "Epoch 93/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17367.2480 - accuracy: 0.8635 - val_loss: 32005.2793 - val_accuracy: 0.7705\n",
            "Epoch 94/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17329.9336 - accuracy: 0.8557 - val_loss: 31707.1992 - val_accuracy: 0.7556\n",
            "Epoch 95/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17459.1621 - accuracy: 0.8526 - val_loss: 31982.9277 - val_accuracy: 0.7513\n",
            "Epoch 96/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17369.8945 - accuracy: 0.8559 - val_loss: 31956.5742 - val_accuracy: 0.7709\n",
            "Epoch 97/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17445.3633 - accuracy: 0.8592 - val_loss: 31718.5547 - val_accuracy: 0.7552\n",
            "Epoch 98/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17235.4219 - accuracy: 0.8582 - val_loss: 32025.8848 - val_accuracy: 0.7754\n",
            "Epoch 99/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17307.4609 - accuracy: 0.8636 - val_loss: 31984.3711 - val_accuracy: 0.7713\n",
            "Epoch 100/1000\n",
            "174/181 [===========================>..] - ETA: 0s - loss: 17309.7109 - accuracy: 0.8536roc-auc_val: 0.816\n",
            "181/181 [==============================] - 9s 49ms/step - loss: 17304.1758 - accuracy: 0.8537 - val_loss: 31718.0859 - val_accuracy: 0.7495\n",
            "Epoch 101/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17420.1797 - accuracy: 0.8523 - val_loss: 31688.2188 - val_accuracy: 0.7628\n",
            "Epoch 102/1000\n",
            "181/181 [==============================] - 1s 8ms/step - loss: 17171.8086 - accuracy: 0.8600 - val_loss: 31850.6914 - val_accuracy: 0.7668\n",
            "Epoch 1/1000\n",
            "136/136 [==============================] - 2s 16ms/step - loss: 43273.6055 - accuracy: 0.6946 - val_loss: 40741.8594 - val_accuracy: 0.6270\n",
            "Epoch 2/1000\n",
            "136/136 [==============================] - 1s 11ms/step - loss: 36493.9453 - accuracy: 0.7440 - val_loss: 39236.6602 - val_accuracy: 0.6312\n",
            "Epoch 3/1000\n",
            "136/136 [==============================] - 2s 11ms/step - loss: 33911.8555 - accuracy: 0.7472 - val_loss: 37223.1484 - val_accuracy: 0.6874\n",
            "Epoch 4/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 32043.5820 - accuracy: 0.7651 - val_loss: 36874.1250 - val_accuracy: 0.6707\n",
            "Epoch 5/1000\n",
            "136/136 [==============================] - 1s 11ms/step - loss: 31000.9492 - accuracy: 0.7682 - val_loss: 36365.0156 - val_accuracy: 0.7236\n",
            "Epoch 6/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 30434.0117 - accuracy: 0.7768 - val_loss: 36484.2383 - val_accuracy: 0.6465\n",
            "Epoch 7/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 29595.2422 - accuracy: 0.7756 - val_loss: 36419.7461 - val_accuracy: 0.6835\n",
            "Epoch 8/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 29019.5059 - accuracy: 0.7812 - val_loss: 36872.0820 - val_accuracy: 0.7656\n",
            "Epoch 9/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 28274.1680 - accuracy: 0.7880 - val_loss: 36246.5234 - val_accuracy: 0.6721\n",
            "Epoch 10/1000\n",
            "129/136 [===========================>..] - ETA: 0s - loss: 27785.1387 - accuracy: 0.7903roc-auc_val: 0.7927\n",
            "136/136 [==============================] - 12s 85ms/step - loss: 27702.7188 - accuracy: 0.7908 - val_loss: 36007.5469 - val_accuracy: 0.7187\n",
            "Epoch 11/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 27328.6582 - accuracy: 0.7907 - val_loss: 35789.4844 - val_accuracy: 0.7371\n",
            "Epoch 12/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 26870.1074 - accuracy: 0.7894 - val_loss: 35934.7148 - val_accuracy: 0.7346\n",
            "Epoch 13/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 26526.6445 - accuracy: 0.7943 - val_loss: 36169.4648 - val_accuracy: 0.7146\n",
            "Epoch 14/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 26282.4551 - accuracy: 0.7921 - val_loss: 36308.2578 - val_accuracy: 0.6985\n",
            "Epoch 15/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 25690.6582 - accuracy: 0.7986 - val_loss: 35822.8945 - val_accuracy: 0.7406\n",
            "Epoch 16/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 25393.5195 - accuracy: 0.7992 - val_loss: 35962.5312 - val_accuracy: 0.7205\n",
            "Epoch 17/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 24980.5039 - accuracy: 0.8021 - val_loss: 36033.4961 - val_accuracy: 0.7248\n",
            "Epoch 18/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 24671.8340 - accuracy: 0.8087 - val_loss: 35769.2148 - val_accuracy: 0.7578\n",
            "Epoch 19/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 24468.9648 - accuracy: 0.8056 - val_loss: 35856.5469 - val_accuracy: 0.7143\n",
            "Epoch 20/1000\n",
            "128/136 [===========================>..] - ETA: 0s - loss: 24187.3809 - accuracy: 0.8166roc-auc_val: 0.8009\n",
            "136/136 [==============================] - 11s 84ms/step - loss: 24160.6367 - accuracy: 0.8158 - val_loss: 35317.4023 - val_accuracy: 0.7436\n",
            "Epoch 21/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 23759.0137 - accuracy: 0.8123 - val_loss: 35872.2891 - val_accuracy: 0.7326\n",
            "Epoch 22/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 23431.9277 - accuracy: 0.8196 - val_loss: 35702.5234 - val_accuracy: 0.7495\n",
            "Epoch 23/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 23050.9453 - accuracy: 0.8215 - val_loss: 36273.0664 - val_accuracy: 0.7509\n",
            "Epoch 24/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 22625.8926 - accuracy: 0.8201 - val_loss: 36623.8711 - val_accuracy: 0.7784\n",
            "Epoch 25/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 22466.1699 - accuracy: 0.8218 - val_loss: 36129.4844 - val_accuracy: 0.6668\n",
            "Epoch 26/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 22402.1680 - accuracy: 0.8190 - val_loss: 36701.9766 - val_accuracy: 0.7301\n",
            "Epoch 27/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 21999.7559 - accuracy: 0.8194 - val_loss: 36876.7578 - val_accuracy: 0.7657\n",
            "Epoch 28/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 21544.9844 - accuracy: 0.8275 - val_loss: 37192.8359 - val_accuracy: 0.7860\n",
            "Epoch 29/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 21321.5039 - accuracy: 0.8285 - val_loss: 36268.5508 - val_accuracy: 0.7574\n",
            "Epoch 30/1000\n",
            "131/136 [===========================>..] - ETA: 0s - loss: 21043.6250 - accuracy: 0.8323roc-auc_val: 0.8002\n",
            "136/136 [==============================] - 12s 87ms/step - loss: 21013.6777 - accuracy: 0.8324 - val_loss: 36648.9297 - val_accuracy: 0.7389\n",
            "Epoch 31/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 20835.8027 - accuracy: 0.8318 - val_loss: 36900.3828 - val_accuracy: 0.7446\n",
            "Epoch 32/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 20673.6836 - accuracy: 0.8273 - val_loss: 36977.1133 - val_accuracy: 0.7474\n",
            "Epoch 33/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 20269.9277 - accuracy: 0.8355 - val_loss: 36938.0547 - val_accuracy: 0.7371\n",
            "Epoch 34/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 20222.1934 - accuracy: 0.8333 - val_loss: 36792.1250 - val_accuracy: 0.7139\n",
            "Epoch 35/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 20102.5684 - accuracy: 0.8365 - val_loss: 36213.4258 - val_accuracy: 0.7291\n",
            "Epoch 36/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 19916.7246 - accuracy: 0.8393 - val_loss: 36436.0234 - val_accuracy: 0.7225\n",
            "Epoch 37/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 19601.6875 - accuracy: 0.8364 - val_loss: 35761.5859 - val_accuracy: 0.7230\n",
            "Epoch 38/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 19582.8594 - accuracy: 0.8433 - val_loss: 36361.9375 - val_accuracy: 0.7631\n",
            "Epoch 39/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 19407.3242 - accuracy: 0.8434 - val_loss: 36372.8984 - val_accuracy: 0.7286\n",
            "Epoch 40/1000\n",
            "134/136 [============================>.] - ETA: 0s - loss: 19191.9688 - accuracy: 0.8431roc-auc_val: 0.8031\n",
            "136/136 [==============================] - 12s 86ms/step - loss: 19187.3809 - accuracy: 0.8431 - val_loss: 36771.4727 - val_accuracy: 0.7290\n",
            "Epoch 41/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 18808.2969 - accuracy: 0.8456 - val_loss: 36659.8633 - val_accuracy: 0.7247\n",
            "Epoch 42/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 18923.0918 - accuracy: 0.8532 - val_loss: 37531.0430 - val_accuracy: 0.7059\n",
            "Epoch 43/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 18773.4160 - accuracy: 0.8420 - val_loss: 37096.7930 - val_accuracy: 0.7369\n",
            "Epoch 44/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 18661.7383 - accuracy: 0.8482 - val_loss: 36885.7227 - val_accuracy: 0.7140\n",
            "Epoch 45/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 18541.5684 - accuracy: 0.8539 - val_loss: 37222.6016 - val_accuracy: 0.7419\n",
            "Epoch 46/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 18430.8984 - accuracy: 0.8502 - val_loss: 37211.3945 - val_accuracy: 0.7254\n",
            "Epoch 47/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 18340.3496 - accuracy: 0.8514 - val_loss: 37054.7773 - val_accuracy: 0.6997\n",
            "Epoch 48/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 18056.2891 - accuracy: 0.8497 - val_loss: 36905.8359 - val_accuracy: 0.7108\n",
            "Epoch 49/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 18222.1113 - accuracy: 0.8444 - val_loss: 37108.1250 - val_accuracy: 0.7231\n",
            "Epoch 50/1000\n",
            "132/136 [============================>.] - ETA: 0s - loss: 17991.8340 - accuracy: 0.8533roc-auc_val: 0.8021\n",
            "136/136 [==============================] - 11s 83ms/step - loss: 17981.1523 - accuracy: 0.8535 - val_loss: 37142.4805 - val_accuracy: 0.7315\n",
            "Epoch 51/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17998.6270 - accuracy: 0.8459 - val_loss: 37035.7500 - val_accuracy: 0.7112\n",
            "Epoch 52/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 17831.6328 - accuracy: 0.8508 - val_loss: 37728.5547 - val_accuracy: 0.7080\n",
            "Epoch 53/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 17786.5898 - accuracy: 0.8494 - val_loss: 37712.5312 - val_accuracy: 0.7250\n",
            "Epoch 54/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 17799.3242 - accuracy: 0.8559 - val_loss: 37406.6289 - val_accuracy: 0.7450\n",
            "Epoch 55/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17703.7188 - accuracy: 0.8603 - val_loss: 37283.4961 - val_accuracy: 0.7430\n",
            "Epoch 56/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 17609.8633 - accuracy: 0.8542 - val_loss: 37234.5234 - val_accuracy: 0.7165\n",
            "Epoch 57/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17535.9609 - accuracy: 0.8464 - val_loss: 37202.1602 - val_accuracy: 0.7230\n",
            "Epoch 58/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17597.3984 - accuracy: 0.8484 - val_loss: 36817.0430 - val_accuracy: 0.6922\n",
            "Epoch 59/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17499.1543 - accuracy: 0.8430 - val_loss: 36987.9258 - val_accuracy: 0.7224\n",
            "Epoch 60/1000\n",
            "129/136 [===========================>..] - ETA: 0s - loss: 17304.5156 - accuracy: 0.8612roc-auc_val: 0.803\n",
            "136/136 [==============================] - 12s 86ms/step - loss: 17200.7793 - accuracy: 0.8610 - val_loss: 37049.5195 - val_accuracy: 0.7386\n",
            "Epoch 61/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 17436.1289 - accuracy: 0.8535 - val_loss: 37004.4102 - val_accuracy: 0.7345\n",
            "Epoch 62/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17313.5801 - accuracy: 0.8511 - val_loss: 38302.3633 - val_accuracy: 0.7492\n",
            "Epoch 63/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 17238.4922 - accuracy: 0.8535 - val_loss: 37550.9766 - val_accuracy: 0.7638\n",
            "Epoch 64/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17218.7441 - accuracy: 0.8605 - val_loss: 38541.5938 - val_accuracy: 0.7573\n",
            "Epoch 65/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 17013.4863 - accuracy: 0.8605 - val_loss: 37493.2539 - val_accuracy: 0.7488\n",
            "Epoch 66/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 17118.1270 - accuracy: 0.8578 - val_loss: 37861.7852 - val_accuracy: 0.7333\n",
            "Epoch 67/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 16792.1426 - accuracy: 0.8586 - val_loss: 38099.0859 - val_accuracy: 0.7532\n",
            "Epoch 68/1000\n",
            "136/136 [==============================] - 1s 10ms/step - loss: 16924.4727 - accuracy: 0.8579 - val_loss: 37590.2031 - val_accuracy: 0.7364\n",
            "Epoch 69/1000\n",
            "136/136 [==============================] - 1s 9ms/step - loss: 16885.4863 - accuracy: 0.8601 - val_loss: 37753.0391 - val_accuracy: 0.7553\n",
            "Epoch 70/1000\n",
            "133/136 [============================>.] - ETA: 0s - loss: 17002.4336 - accuracy: 0.8578roc-auc_val: 0.8009\n",
            "136/136 [==============================] - 12s 87ms/step - loss: 16897.3359 - accuracy: 0.8579 - val_loss: 37642.7578 - val_accuracy: 0.7594\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 2s 23ms/step - loss: 41217.1719 - accuracy: 0.6814 - val_loss: 41574.3984 - val_accuracy: 0.7146\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 34537.8398 - accuracy: 0.7405 - val_loss: 39739.8008 - val_accuracy: 0.7127\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 31625.7930 - accuracy: 0.7579 - val_loss: 39336.0547 - val_accuracy: 0.7126\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 29874.6543 - accuracy: 0.7656 - val_loss: 39179.0156 - val_accuracy: 0.7016\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 28865.1699 - accuracy: 0.7758 - val_loss: 39248.8711 - val_accuracy: 0.7528\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 27337.1738 - accuracy: 0.7879 - val_loss: 38221.5742 - val_accuracy: 0.7088\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 27295.0898 - accuracy: 0.7827 - val_loss: 38050.8672 - val_accuracy: 0.7343\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 26932.6211 - accuracy: 0.7735 - val_loss: 38558.5898 - val_accuracy: 0.7110\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 26326.6797 - accuracy: 0.7924 - val_loss: 38093.2773 - val_accuracy: 0.7445\n",
            "Epoch 10/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 25511.6836 - accuracy: 0.7993roc-auc_val: 0.7887\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 25258.2637 - accuracy: 0.7980 - val_loss: 38832.7695 - val_accuracy: 0.7260\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 24756.1602 - accuracy: 0.8012 - val_loss: 38820.8320 - val_accuracy: 0.7503\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 24386.4531 - accuracy: 0.8022 - val_loss: 38139.8086 - val_accuracy: 0.7460\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 23929.1074 - accuracy: 0.8033 - val_loss: 38382.9141 - val_accuracy: 0.7472\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 23749.5488 - accuracy: 0.8074 - val_loss: 37991.7695 - val_accuracy: 0.7614\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 23325.6895 - accuracy: 0.8086 - val_loss: 38032.2500 - val_accuracy: 0.7552\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 22928.8887 - accuracy: 0.8180 - val_loss: 39077.3516 - val_accuracy: 0.7793\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 22597.2695 - accuracy: 0.8120 - val_loss: 37887.3438 - val_accuracy: 0.7683\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 21750.9961 - accuracy: 0.8219 - val_loss: 37765.2188 - val_accuracy: 0.7840\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 21683.3438 - accuracy: 0.8186 - val_loss: 38168.3359 - val_accuracy: 0.7991\n",
            "Epoch 20/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 21794.5215 - accuracy: 0.8224roc-auc_val: 0.8009\n",
            "88/88 [==============================] - 15s 165ms/step - loss: 21479.6641 - accuracy: 0.8220 - val_loss: 37915.3008 - val_accuracy: 0.7940\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 21093.2090 - accuracy: 0.8336 - val_loss: 37809.9297 - val_accuracy: 0.7562\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 20845.0020 - accuracy: 0.8253 - val_loss: 37832.9375 - val_accuracy: 0.7871\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 20634.0117 - accuracy: 0.8368 - val_loss: 38731.4961 - val_accuracy: 0.8084\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 20188.6270 - accuracy: 0.8375 - val_loss: 37722.6172 - val_accuracy: 0.7897\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 19876.0469 - accuracy: 0.8339 - val_loss: 37711.2578 - val_accuracy: 0.7938\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 19434.0898 - accuracy: 0.8422 - val_loss: 38366.2656 - val_accuracy: 0.8208\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 19090.6445 - accuracy: 0.8372 - val_loss: 38391.3320 - val_accuracy: 0.8305\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 18887.1348 - accuracy: 0.8429 - val_loss: 37592.7227 - val_accuracy: 0.8203\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 18485.4375 - accuracy: 0.8436 - val_loss: 37545.2891 - val_accuracy: 0.8278\n",
            "Epoch 30/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 18229.3730 - accuracy: 0.8471roc-auc_val: 0.8073\n",
            "88/88 [==============================] - 15s 168ms/step - loss: 18128.1191 - accuracy: 0.8469 - val_loss: 37436.7227 - val_accuracy: 0.8251\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 17808.0156 - accuracy: 0.8496 - val_loss: 37958.3906 - val_accuracy: 0.8408\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 17681.7520 - accuracy: 0.8510 - val_loss: 37108.4727 - val_accuracy: 0.8246\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 17380.5703 - accuracy: 0.8520 - val_loss: 37154.6836 - val_accuracy: 0.8193\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 17409.0723 - accuracy: 0.8485 - val_loss: 37530.0586 - val_accuracy: 0.8219\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 12ms/step - loss: 16939.0977 - accuracy: 0.8590 - val_loss: 37986.7539 - val_accuracy: 0.8475\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 12ms/step - loss: 16797.0938 - accuracy: 0.8610 - val_loss: 37541.2227 - val_accuracy: 0.8258\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 16603.8047 - accuracy: 0.8562 - val_loss: 37955.5859 - val_accuracy: 0.8349\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 16616.7480 - accuracy: 0.8514 - val_loss: 37957.3047 - val_accuracy: 0.8555\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 16569.3672 - accuracy: 0.8552 - val_loss: 38520.0820 - val_accuracy: 0.8639\n",
            "Epoch 40/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 16430.8203 - accuracy: 0.8624roc-auc_val: 0.8083\n",
            "88/88 [==============================] - 15s 168ms/step - loss: 16464.8828 - accuracy: 0.8625 - val_loss: 38990.1172 - val_accuracy: 0.8662\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 12ms/step - loss: 16262.2148 - accuracy: 0.8638 - val_loss: 38622.0547 - val_accuracy: 0.8584\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 15908.3271 - accuracy: 0.8639 - val_loss: 38214.4102 - val_accuracy: 0.8529\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 15807.4502 - accuracy: 0.8639 - val_loss: 37925.2773 - val_accuracy: 0.8394\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 12ms/step - loss: 15871.8857 - accuracy: 0.8602 - val_loss: 39262.2266 - val_accuracy: 0.8879\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 15735.2480 - accuracy: 0.8671 - val_loss: 38692.8320 - val_accuracy: 0.8669\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 15580.5605 - accuracy: 0.8627 - val_loss: 38477.5820 - val_accuracy: 0.8745\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 15362.3799 - accuracy: 0.8722 - val_loss: 39169.4766 - val_accuracy: 0.8862\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 15052.6045 - accuracy: 0.8670 - val_loss: 38961.9414 - val_accuracy: 0.8598\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 15141.0420 - accuracy: 0.8736 - val_loss: 39480.4219 - val_accuracy: 0.8844\n",
            "Epoch 50/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 14861.0732 - accuracy: 0.8691roc-auc_val: 0.8087\n",
            "88/88 [==============================] - 15s 172ms/step - loss: 14807.5713 - accuracy: 0.8691 - val_loss: 39238.5781 - val_accuracy: 0.8869\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 14724.0869 - accuracy: 0.8724 - val_loss: 38964.8242 - val_accuracy: 0.8764\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 15038.5498 - accuracy: 0.8692 - val_loss: 39196.3945 - val_accuracy: 0.8486\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 14742.1113 - accuracy: 0.8781 - val_loss: 40022.4727 - val_accuracy: 0.9006\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 14547.5850 - accuracy: 0.8740 - val_loss: 39201.0625 - val_accuracy: 0.8829\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 14826.1729 - accuracy: 0.8726 - val_loss: 38832.3828 - val_accuracy: 0.8728\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 14662.9932 - accuracy: 0.8705 - val_loss: 38713.7852 - val_accuracy: 0.8728\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 14476.4678 - accuracy: 0.8726 - val_loss: 38856.1953 - val_accuracy: 0.8601\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 14346.1875 - accuracy: 0.8733 - val_loss: 39085.5938 - val_accuracy: 0.8825\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13974.0723 - accuracy: 0.8806 - val_loss: 39610.5352 - val_accuracy: 0.8996\n",
            "Epoch 60/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 14143.6006 - accuracy: 0.8813roc-auc_val: 0.8079\n",
            "88/88 [==============================] - 15s 169ms/step - loss: 14174.6396 - accuracy: 0.8814 - val_loss: 40421.2500 - val_accuracy: 0.9040\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 12ms/step - loss: 14175.1543 - accuracy: 0.8812 - val_loss: 40416.2227 - val_accuracy: 0.9074\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13956.9541 - accuracy: 0.8837 - val_loss: 40062.5586 - val_accuracy: 0.9034\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13881.0039 - accuracy: 0.8856 - val_loss: 40146.5039 - val_accuracy: 0.8738\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13605.3037 - accuracy: 0.8883 - val_loss: 40259.0391 - val_accuracy: 0.8747\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 12ms/step - loss: 13691.7754 - accuracy: 0.8824 - val_loss: 40192.1055 - val_accuracy: 0.8629\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13794.4678 - accuracy: 0.8791 - val_loss: 39502.0664 - val_accuracy: 0.8539\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13801.4160 - accuracy: 0.8834 - val_loss: 40358.4844 - val_accuracy: 0.8792\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13709.1250 - accuracy: 0.8813 - val_loss: 40646.6406 - val_accuracy: 0.8872\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13635.4834 - accuracy: 0.8824 - val_loss: 40543.1172 - val_accuracy: 0.8979\n",
            "Epoch 70/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 13199.7363 - accuracy: 0.8812roc-auc_val: 0.8043\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 13268.6875 - accuracy: 0.8805 - val_loss: 40162.0195 - val_accuracy: 0.8463\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13403.6377 - accuracy: 0.8859 - val_loss: 41150.0898 - val_accuracy: 0.8960\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 14ms/step - loss: 13477.9629 - accuracy: 0.8747 - val_loss: 40780.0273 - val_accuracy: 0.8622\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13230.1758 - accuracy: 0.8905 - val_loss: 40952.4570 - val_accuracy: 0.9083\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13180.1719 - accuracy: 0.8876 - val_loss: 41049.7148 - val_accuracy: 0.9080\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13251.1992 - accuracy: 0.8833 - val_loss: 40751.2344 - val_accuracy: 0.9014\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13138.4004 - accuracy: 0.8887 - val_loss: 41195.1836 - val_accuracy: 0.9078\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13072.8135 - accuracy: 0.8921 - val_loss: 41286.0742 - val_accuracy: 0.9116\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12845.2900 - accuracy: 0.8871 - val_loss: 41102.7578 - val_accuracy: 0.8946\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 13178.3955 - accuracy: 0.8834 - val_loss: 41335.6289 - val_accuracy: 0.8903\n",
            "Epoch 80/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 13120.2764 - accuracy: 0.8836roc-auc_val: 0.8037\n",
            "88/88 [==============================] - 15s 165ms/step - loss: 13056.3486 - accuracy: 0.8836 - val_loss: 41158.0859 - val_accuracy: 0.9051\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12837.7373 - accuracy: 0.8911 - val_loss: 41283.3594 - val_accuracy: 0.9048\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 13ms/step - loss: 12671.3975 - accuracy: 0.8876 - val_loss: 41354.6875 - val_accuracy: 0.9052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5PePWLxWIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c258dfac-85d1-4513-8c36-a25f36dae82a"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhcd33v8fd3Rqu1WLIly7K8yGuIExLbUZxAgJBmIaS5SSlcSKBtgLRuQthueW4fWnqB0ue5l97ewi2EQkPxJTQkhBYS3CYQ/ISUAFllx05sJ17jTV4k27IWa9d87x9zZCbySB7NjKTR0ef1PPPMWX5zzu94+cyZ3+93zjF3R0REwisy2RUQEZHxpaAXEQk5Bb2ISMgp6EVEQk5BLyIScnmTXYFkqqqqvL6+frKrISIyZWzatOmEu1cnW5eTQV9fX09jY+NkV0NEZMowswMjrVPTjYhIyCnoRURC7rxBb2YLzOwpM9thZtvN7FPB8llmttHMdgfvlSN8/o6gzG4zuyPbByAiIqNL5Yx+APiMu68ErgTuMbOVwGeBJ919OfBkMP8GZjYL+AJwBbAW+MJIXwgiIjI+zhv07n7U3TcH0x3Aq0AdcCtwf1DsfuD3knz8XcBGdz/l7q3ARuDGbFRcRERSM6Y2ejOrB1YDzwM17n40WHUMqEnykTrgUML84WBZsm2vM7NGM2tsaWkZS7VERGQUKQe9mZUCPwI+7e7ties8fgvMjG6D6e73uXuDuzdUVycdCioiImlIKejNLJ94yH/f3X8cLD5uZrXB+lqgOclHm4AFCfPzg2UiIjJBUhl1Y8B3gFfd/SsJqzYAQ6No7gB+kuTjTwA3mFll0Al7Q7BMREQmSCpXxl4F/CHwipltCZb9JfBl4IdmdidwAHg/gJk1AHe5+x+7+ykz+xvgxeBzX3L3U1k9gnHy4PMHky7/4BULJ7gmIiKZOW/Qu/uvARth9bVJyjcCf5wwvx5Yn24FRUQkM7oyVkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iEnIJeRCTkzvuEKTNbD9wMNLv7xcGyh4ELgiIVwGl3X5Xks/uBDmAQGHD3hizVW0REUpTKM2O/C9wLfG9ogbt/YGjazP4eaBvl89e4+4l0KygiIplJ5ZmxT5tZfbJ1ZmbEHwr+O9mtloiIZEumbfRvB467++4R1jvwczPbZGbrRtuQma0zs0Yza2xpacmwWiIiMiTToL8deGiU9W9z9zXAu4F7zOwdIxV09/vcvcHdG6qrqzOsloiIDEk76M0sD/h94OGRyrh7U/DeDDwCrE13fyIikp5MzuivA15z98PJVppZiZmVDU0DNwDbMtifiIik4bxBb2YPAc8CF5jZYTO7M1h1G8Oabcxsnpk9HszWAL82s63AC8Bj7v6z7FVdRERSkcqom9tHWP7hJMuOADcF0/uASzOsn4iIZEhXxoqIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iEXCqPElxvZs1mti1h2RfNrMnMtgSvm0b47I1mttPM9pjZZ7NZcRERSU0qZ/TfBW5Msvyr7r4qeD0+fKWZRYFvAO8GVgK3m9nKTCorIiJjd96gd/engVNpbHstsMfd97l7H/AD4NY0tiMiIhnIpI3+42b2ctC0U5lkfR1wKGH+cLAsKTNbZ2aNZtbY0tKSQbVERCRRukH/TWApsAo4Cvx9phVx9/vcvcHdG6qrqzPdnIiIBNIKenc/7u6D7h4Dvk28mWa4JmBBwvz8YJmIiEygtILezGoTZt8DbEtS7EVguZktNrMC4DZgQzr7ExGR9OWdr4CZPQS8E6gys8PAF4B3mtkqwIH9wJ8GZecB/+zuN7n7gJl9HHgCiALr3X37uByFiIiM6LxB7+63J1n8nRHKHgFuSph/HDhn6KWIiEwcXRkrIhJyCnoRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkDtv0JvZejNrNrNtCcv+zsxeM7OXzewRM6sY4bP7zewVM9tiZo3ZrLiIiKQmlTP67wI3Dlu2EbjY3S8BdgF/Mcrnr3H3Ve7ekF4VRUQkE+cNend/Gjg1bNnP3X0gmH0OmD8OdRMRkSzIRhv9R4GfjrDOgZ+b2SYzWzfaRsxsnZk1mlljS0tLFqolIiKQYdCb2eeAAeD7IxR5m7uvAd4N3GNm7xhpW+5+n7s3uHtDdXV1JtUSEZEEaQe9mX0YuBn4kLt7sjLu3hS8NwOPAGvT3Z+IiKQnraA3sxuBPwducfeuEcqUmFnZ0DRwA7AtWVkRERk/qQyvfAh4FrjAzA6b2Z3AvUAZsDEYOvmtoOw8M3s8+GgN8Gsz2wq8ADzm7j8bl6MQEZER5Z2vgLvfnmTxd0YoewS4KZjeB1yaUe1ERCRjujJWRCTkFPQiIiGnoBcRCTkFvYhIyCnoRURCTkEvIhJyCnoRkZBT0IuIhJyCXkQk5BT0IiIhp6AXEQk5Bb2ISMgp6EVEQk5BLyIScgp6EZGQU9CLiIScgl5EJORSCnozW29mzWa2LWHZLDPbaGa7g/fKET57R1Bmt5ndka2Ki4hIalI9o/8ucOOwZZ8FnnT35cCTwfwbmNks4AvAFcBa4AsjfSGIiMj4SCno3f1p4NSwxbcC9wfT9wO/l+Sj7wI2uvspd28FNnLuF4aIiIyjTNroa9z9aDB9DKhJUqYOOJQwfzhYdg4zW2dmjWbW2NLSkkG1REQkUVY6Y93dAc9wG/e5e4O7N1RXV2ejWiIiQmZBf9zMagGC9+YkZZqABQnz84NlIiIyQTIJ+g3A0CiaO4CfJCnzBHCDmVUGnbA3BMtERGSCpDq88iHgWeACMztsZncCXwauN7PdwHXBPGbWYGb/DODup4C/AV4MXl8KlomIyATJS6WQu98+wqprk5RtBP44YX49sD6t2omISMZ0ZayISMgp6EVEQk5BLyIScgp6EZGQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iEnIJeRCTkFPQiIiGnoBcRCTkFvYhIyKUd9GZ2gZltSXi1m9mnh5V5p5m1JZT5fOZVFhGRsUjpUYLJuPtOYBWAmUWBJuCRJEV/5e43p7sfERHJTLaabq4F9rr7gSxtT0REsiRbQX8b8NAI695iZlvN7KdmdlGW9iciIinKOOjNrAC4BfjXJKs3A4vc/VLg68Cjo2xnnZk1mlljS0tLptUSEZFANs7o3w1sdvfjw1e4e7u7dwbTjwP5ZlaVbCPufp+7N7h7Q3V1dRaqlZkjp7vZevg0R9u6GRiMTXZ1RETSlnZnbILbGaHZxszmAsfd3c1sLfEvlpNZ2Oe4cncefOEgp870AVCUH+Ez119ASWE2/rhERCZWRmf0ZlYCXA/8OGHZXWZ2VzD7PmCbmW0Fvgbc5u6eyT4nwv6TXZw608fVK6q5+ZJaevpj7DreMdnVEhFJS0anqO5+Bpg9bNm3EqbvBe7NZB+T4eld8T6ChkWVVJYU8NTOFnYd72D1wspJrpmIyNjpytgkfrmrhVklBcwuLSRixoo5pexu7iSW+z9GRETOoaAfpndgkGf3nmT5nNKzy1bUlNHVN8jh1u5JrJmISHoU9MM07m+lu3+QFTVlZ5ctn1OKgdrpRWRKUtAP8/SuFvKjxpLqkrPLZhTmMb+yWEEvIlOSgn6YX+5qoWHRLArzom9YvmJuGU2t3Zzs7J2kmomIpEdBn6C5vYfXjnXwjhXnXrB1QU0ZDjy9W1ftisjUoqBPsP1IOwCXLTp3GOW8imKK8iM07m+d6GqJiGREQZ9gb0snAMsSRtwMiZgxt7yYV4+2T3S1REQyoqBPsKe5k1klBcwqKUi6vraiiNeOdTAY03h6EZk6FPQJ9jR3sqz63LP5IbXlRXT1DXLg5JkJrJWISGYU9An2tnSydE7JiOtrK4oBePWohlmKyNShoA+c7OyltaufpaOc0c8pKyQvYuw42jaBNRMRyYyCPrCneeSO2CH50QhLq0t1Ri8iU4qCPrBnlBE3iVbOK2fHEY28EZGpQ0Ef2Nt8huL8KPNmFo9a7sLaMo6195x9KImISK5T0Af2tHSypLqESMRGLbeydiaAxtOLyJShoA/sbe48b7MNxM/oQUEvIlOHgh7o6hug6XT3qCNuhswuLaSmvFDt9CIyZWQc9Ga238xeMbMtZtaYZL2Z2dfMbI+ZvWxmazLdZ7bta4lfAJXKGT3AhbXl7NAZvYhMERk9MzbBNe5+YoR17waWB68rgG8G7zkjlaGViVbWlvPr3SfoHRg853bGIiK5ZiKabm4FvudxzwEVZlY7AftN2d6WTqIRY9HsGSmVXzmvnIGYs/t45zjXTEQkc9kIegd+bmabzGxdkvV1wKGE+cPBsjcws3Vm1mhmjS0tE3vP930nzjC/sjjls/MLa8sBdciKyNSQjaB/m7uvId5Ec4+ZvSOdjbj7fe7e4O4N1dXnPvhjPB04eYb62SPf42a4+tklFOdH1U4vIlNCxkHv7k3BezPwCLB2WJEmYEHC/PxgWU5wdw6c6KI+xWYbgGjEuGBumc7oRWRKyCjozazEzMqGpoEbgG3Dim0A/igYfXMl0ObuRzPZbzadOtNHR+8Ai8ZwRg+/vRWCu+5NLyK5LdMz+hrg12a2FXgBeMzdf2Zmd5nZXUGZx4F9wB7g28DHMtxnVu0/2QVAfVXqZ/QQb6dv7xngSFvPeFRLRCRrMhpe6e77gEuTLP9WwrQD92Syn/E09BCRsbTRQ3yIJcCOI+3UVYx+fxwRkck07a+M3X/iDBGD+ZVjO6N/09wyzDTyRkRyn4L+ZBd1lcUU5I3tj6KkMI/62SW6FYKI5LxpH/RjHVqZaGVtOa8eU9CLSG6b9kG//2RXylfEDndhbRkHTnbR0dOf5VqJiGTPtA760119tHX3p39GPy/eIfvaMT1aUERy17QO+rNDK9MM+ovr4g8h2XrodNbqJCKSbdM66M8OrRzjGPohc8qKWDCrmE0HWrNZLRGRrJrWQb//RBeWxtDKRJctrGTTgVZdISsiOWtaB/2Bk2eYN7OYovz07ym/ZlElzR29NJ3uzmLNRESyZ1oH/f6TZ9IecTNkzcJKADXfiEjOmuZB3zXmm5kN96a5ZcwoiPLSQXXIikhumrZB33qmj1Nn+lhSlVnQ50UjXDq/Qmf0IpKzpm3Q72kJnhNbk9pzYkezZlEFO46209U3kPG2RESybdoG/dDzXpen+EDw0Vy2qJLBmPPy4baMtyUikm3TN+ibO5hREGXezMxvMbx6gTpkRSR3ZXQ/+qlsT3Mny+aUEonYmD734PMHz1n2wSsWsrS6hBdeP8U912SrhiIi2TGtg/4tS2ZnbXvXXDCH7z17gI6efsqK8rO2XRHJHclO9CB+spfL0m66MbMFZvaUme0ws+1m9qkkZd5pZm1mtiV4fT6z6mZHR08/R9t6stIRO+TGi+fSNxjjqZ0tWdumiEg2ZHJGPwB8xt03Bw8I32RmG919x7Byv3L3mzPYT9btaR7qiC3L2jbXLKykqrSQJ7Yd45ZL52VtuyIimUr7jN7dj7r75mC6A3gVqMtWxcbTUNAvy8KImyGRiHHDRTU8tbOZnv7BrG1XRCRTWRl1Y2b1wGrg+SSr32JmW83sp2Z20SjbWGdmjWbW2NIyvs0fe5o7KciLsKAyuw/1vvGiuXT1DfKr3Seyul0RkUxkHPRmVgr8CPi0uw9/rt5mYJG7Xwp8HXh0pO24+33u3uDuDdXV1ZlWa1S7mztZUlVCXjS7o0uvXDKb8qI8nth+LKvbFRHJREZJZ2b5xEP+++7+4+Hr3b3d3TuD6ceBfDOrymSf2bC7uSOrzTZDCvIiXHthDRt3HKd/MJb17YuIpCOTUTcGfAd41d2/MkKZuUE5zGxtsL+T6e4zG7r7Bjnc2p3VjthEt66aR1t3P49sbhqX7YuIjFUmo26uAv4QeMXMtgTL/hJYCODu3wLeB9xtZgNAN3CbT/ITOva2dOIOy7M4tDLR1SuqeXPdTO59ag/vWVNHfpabh0RExirtoHf3XwOjXlbq7vcC96a7j/Gw63j8Qd7j0XQDYGZ88trl/Mn3GvnJliO877L547IfEZFUTbvTzc0HWyktzGNp9fgEPcB1F87honnl3PuL3QyorV5EJtm0uwVC4/5WVi+sIDrGe9yMJtll0Z+8djl/+i+b+NdNh7l9bW5fHi0i4TatzujbuvvZebyDhkWzxn1f119YwxWLZ/HX/76dHUeGjzoVEZk40yroNx9sxR0ur68c931FIsa9H1zDzOJ87npgE21d/eO+TxGRZKZV0G/a30o0YqxaWDEh+6suK+QfP3QZR9u6uefBzZzp1ROoRGTiTaugf3H/KS6aV86MgonrmrhsUSX/8z1v5pm9J3jvN5/hcGvXhO1bRASmUWds30CMrYdPT1jH6PAO2jveUs9DLx7k1nt/wxdvuYibL6kluJZMJtlUvce4SKqmzRn99iNt9PTHuLx+/Dtik1leU8bdVy+jpryITzz0Eh/4p+d46aAePSgi42/anNEPPc+1YdH4d8SOpLqskH//xNt4+MVD/J+f7+Q9//gMqxdW8OG31nPDyrkUF0QnrW7TxYPPH2Qw5uw/eYZDp7o40tZDW1cfDkTMqCotZF5FEUuqxu86CwmPSb7QP2XTJuhfeP0UC2fNYE550aTW4+EXDwHwiWuWselgK8/sPcmnfrCFkoIo162s4boLa3jbsioqSwomtZ5hMxhznn/9JI++1MS2I2109cWfGTCrpIBZJQVEDAZizs7jHWwOfmn9bPtRbl1Vx22XL2B2aeFkVl9yyEAsxqYDrew+3snrJ86QHzV2N3dyy6p5rFk4eSeSo5kWQX+6q4//3NXCbZcvmOyqnFWYH+WtS6u4cslsllSV8O8vH+Wn247yky1HMIOL5pWzakEFl86vYPXCCpZUjf1B5tNdLOZsPtjKf7x8lMdeOUpLRy/5UeNNc8u5ZP5MllaXUpT/xl9R7k57zwCvHm3naFs3f/fETv7hyd28Z1UdH33bYi6YOz43w5Opobm9hx82HuJIWw+VM/JZWVtOd/8gD75wkO8+s58/vXoJf/6uN2X1gsxsmBZB/+PNTfQNxLjt8tzrXIuYsf9kF2+um8nK2nKaWrvY1dxJT/8gj750hAeei3cUlhbm8aa5ZSybU8qyOaUsrY6/184syvp99aey0119PLfvFL/c1cxTr7VwrL2HgrwIv3PBHG6+tJYTHX0U5I3852VmzCzO58rgwfFXLa3imX0n+fFLh3m48RBvX17FR69azNUrqvXFO838aNNhvvGfe8iPRviDKxayct7Ms+tuWTWPL//0Vf7pl/vYdayDf7h9NeVF+ZNY2zeyXGxjamho8MbGxqxsy9254atPU1KYx6P3XJXy50YaiTGRYu6c6OjlcGs3h1q7ON7eQ0fPACfP9J0tE40Yc8uLmFdRxLyK4rOvuooi5pQVUVVayKySglHDbSrq6R/kyOluDpzqYvfxDl471sGWQ6fZ13IGiH8xLpw1g4vryrlwbjmF+Zn1f3T1DvDC/lNsPXya4+29LK0u4SNXLea9a+arbyXk3J2v/2IPX9m4iyVVJbz/8gXnhPjQCK0HnjvAFzds56K6mTxw51rKJjDszWyTuzckXRf2oN904BTv/eaz/O/3XsL7x9B0kwtBP5Ku3gFaOntp6eiltauP0139nO7up627n7aufgaT/J2WFeVRVVrI7KBNunJGAWVFeZQW5VFamEdZUR5lRfmUFuZRUhilIBolP8/Ij0YoiEbIj0bIjxr5eb+dH+vPU3dnMOYMxJyYB+/BfN9AjK6+Qbr7BunqG6Crf5D27n5az/Rxqit4P9NH0+lumk5309LRe87x1VUUs3DWDBbNLmHhrBnj8vN5IBZjW1Mbv9lzkqbT3RTnR/nIVfV86MpF1FVk99GUMvn6BmJ8/ifb+MGLh/j9NXWsWlBBXuTck6bEobgbdxzn7gc2sWpBBfd/dC0lhRPTcDKtg/4zP9zKE9uP8cLnrh3ThVK5HPSjibnT2TPA6a4+OnsH6OwdpLN3gDO9A5zpGzg73dU3SG9/jL4s3F3TLH6/ajMjYmDEF1iwLhaLB2QszX9qBhQXRJlRkEdFcT4VM/KpmFFA5Yx8KmcUMKe8cEIvgoP4l9b+k138Zs8JXj3WjjusXTyLW1fN49o31TB35uR2+kvmmjt6+NgDm2k80MrHr1nGZ25YwUMvHEpadvg1F4+9fJRPPLSZtYtn8Z07Lp+QsB8t6EPdRt/c3sN/vHyE/9owf8KDYLJEzCgvzqe8OLWfjDF3evtj9A4M0hO89w7EGIz5Oa8BT5iOxXCH+HmCE7zhnLssEgm+AMyIBF8GZ98jhpmRF7GzvxQK8iIURI2igiglBXkUF0SJ5NjFZWbG4qoSFleV8PblVTzyUhOPbmnic49s43NsY2VtOVctm83l9bNYvbCSqtICXSA3hTy1s5m/+NErtHX38/XbV/NfLp03ps//7iW19A+u4s9+uIU/Wv8C/+8jl09qm31o0693YJC7v7+ZiBkfuWrxZFcnZ0XMKC6Iqp05A7/afYKq0kLuvGoxxzt62Xmsg53HOrj/mQN8+1evA1A5I5/lc8pYVlPK8jmlLJo9g5ryImrKi5g1o0Aduzli57EO/vZnr/GL15pZUl3C+g9fzsp55Wlt6/dW11GYF+GTP3iJD377OdbfcfmkDe8OZdC7O59/dDubDrTyjQ+uGdeHjIgMMYt3jM8tL+LqFdX0D8Zoao33KTR39HK8vYfXjrXT3vPGm9vlRYw5ZYVUlhRQXpRPeXEeM4vzg+l8ZhbnU1KYR3F+lOKCCEX5UYryo/H5/PiXdHxZvP9EvxxS19M/yI6j7Ww+0MpPthzhlaY2Sgvz+NxNF3LHW+szHsTw7jfXcl9BlLsf2MS7/u/T/K/ffzM3XlybpdqnLqOgN7MbgX8AosA/u/uXh60vBL4HXEb8oeAfcPf9mezzfFo6evnak7t5uPEQH79mGb97ycT/oYoA5Ecj1FeVUF9VcnaZu9PZO0BrVz/t3f109PTT3jNAe3c/3f2DHGnrZm/LID39g3T3D9I/OPaOjbxIvBM9L/rb5rCh6bzo0Lp481jidF4kQn5e0OkeiRCNGlEzopF4k1s0Em9qe+OyhJdZsD4+GiwSsXh/TSDx+yfxq+iNy5MXSizv8NtO/WHvg7EYgzEYjMXi8+4MDsbf+wdjnOkdDEau9dLc3sux9h4Gg86ji+vK+avfvZD3rK5jdmlh1vrprrlgDo998u18+gdbuOuBzbzroho+ctVirlg8a8K+lNMOejOLAt8ArgcOAy+a2QZ335FQ7E6g1d2XmdltwN8CH8ikwiPp6hvgqxt38S/PHaBvIMYfXrmIP7t+xXjsSiRtZkZZUX7Kw+4GYjF6+mP0DcToH4y/+gZj9A/4G+cH4/NDfSixhD6V2FA/S0IfS9/AAB0Jy34blPERUfH3eB9OzB0PphPfc28Yxxv9ti/IiETi04V5EQrzopQW5jGnrJAVNWXUBUOTK2bEr0Z/YvvxrNdlaXUpP/7YW/nHp/ay/jev88T24yytLmHt4llcXDeT6tJCiguilBTmjcvVtZmc0a8F9rj7PgAz+wFwK5AY9LcCXwym/w2418zMx2GoT0E0wpOvNXPTxbV84trlLE44ixKZqvIiEUoLI5CDd2Bwj38ZDL0nfhEMTZ8tO8o2ki4fcSZuqIM/ktjBHzEsYVmuyY9G+NR1y1n3jiVs2NrEf7x8lMdfOfaGkTxVpQU0/tX1Wd93JkFfBySONToMXDFSGXcfMLM2YDZwYvjGzGwdsC6Y7TSznelU6ingq+l88FxVJKlnCITxuMJ4TBDO4wrjMfGhLB3XAcD+R9ofXzTSipzpjHX3+4D7JrseQ8yscaQxqVNZGI8rjMcE4TyuMB4T5P5xZdKl3AQkXmo6P1iWtIyZ5QEziXfKiojIBMkk6F8ElpvZYjMrAG4DNgwrswG4I5h+H/CL8WifFxGRkaXddBO0uX8ceIL48Mr17r7dzL4ENLr7BuA7wL+Y2R7gFPEvg6kiZ5qRsiyMxxXGY4JwHlcYjwly/Lhy8l43IiKSPeG6d62IiJxDQS8iEnLTPujN7EYz22lme8zss0nWF5rZw8H6582sfuJrOTYpHNOfmdkOM3vZzJ40sxHH3+aS8x1XQrn3mpmbWc4OdxuSyjGZ2fuDv6/tZvbgRNcxHSn8G1xoZk+Z2UvBv8ObJqOeY2Fm682s2cy2jbDezOxrwTG/bGZrJrqOI3L3afsi3om8F1gCFABbgZXDynwM+FYwfRvw8GTXOwvHdA0wI5i+O9ePKdXjCsqVAU8DzwENk13vLPxdLQdeAiqD+TmTXe8sHdd9wN3B9Epg/2TXO4XjegewBtg2wvqbgJ8SvzXPlcDzk13nodd0P6M/exsHd+8Dhm7jkOhW4P5g+t+Aay23bw943mNy96fcvSuYfY74NRC5LpW/K4C/IX5PpZ6JrFyaUjmmPwG+4e6tAO7ePMF1TEcqx+XA0P1/ZwJHJrB+aXH3p4mPHhzJrcD3PO45oMLMcuKuitM96JPdxqFupDLuPgAM3cYhV6VyTInuJH4WkuvOe1zBT+UF7v7YRFYsA6n8Xa0AVpjZb8zsueCOsbkuleP6IvAHZnYYeBz4xMRUbVyN9f/ehMmZWyDIxDOzPwAagKsnuy6ZMrMI8BXgw5NclWzLI958807iv7yeNrM3u/vpSa1V5m4Hvuvuf29mbyF+vc3F7p75sy3lHNP9jD6Mt3FI5Zgws+uAzwG3uHvv8PU56HzHVQZcDPynme0n3ka6Icc7ZFP5uzoMbHD3fnd/HeNwrYgAAAETSURBVNhFPPhzWSrHdSfwQwB3fxYoIn5jsKkspf97k2G6B30Yb+Nw3mMys9XAPxEP+anQ5gvnOS53b3P3Knevd/d64n0Pt7h7dp4yPz5S+ff3KPGzecysinhTzr6JrGQaUjmug8C1AGZ2IfGgb5nQWmbfBuCPgtE3VwJt7n50sisF07zpxkN4G4cUj+nvgFLgX4N+5YPufsukVToFKR7XlJLiMT0B3GBmO4BB4L+7ey7/okz1uD4DfNvM/hvxjtkP5/gJFGb2EPEv3aqgb+ELQD6Au3+LeF/DTcAeoAv4yOTU9Fy6BYKISMhN96YbEZHQU9CLiIScgl5EJOQU9CIiIaegFxEJOQW9iEjIKehFRELu/wOT3TUnrbopawAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNDmjro-2ndD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_1.6",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "a38cf157-c64d-4755-845d-22b679cdddea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "5364d243-24b0-4f31-f4ab-24a6d109b6b6"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 79.4MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 98% 51.0M/52.2M [00:01<00:00, 31.8MB/s]\n",
            "100% 52.2M/52.2M [00:01<00:00, 40.0MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 79% 46.0M/58.3M [00:01<00:00, 43.6MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 52.4MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 218MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 222MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDrCIAqHzl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "d072d785-b1b4-4c76-920f-7babb7c3277e"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWJ-hzcznam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5OQjb1zo6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "07632f0b-1078-49b0-9ad9-f4efb0bcf52f"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "63194746-929e-40e8-876e-11663640b4c4"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0r3SuH1K97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQ20JqWATak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQIVOLKBFIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1fab9f9f-c5db-4ada-e469-50dfdfaeac38"
      },
      "source": [
        "1-0.036"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.964"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cafb93f6-3e5b-4573-eb95-88ba54d9dfcc"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=1.6\n",
        "        alpha=1-0.036\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss\n",
        "dk={}\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "for en,month in enumerate([(4,5),(3,4),(3,5)]):\n",
        "  train=trn.loc[trn['month']>=month[1]]\n",
        "  test=trn.loc[trn['month']<=month[0]]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.0001,decay=1e-3),loss=fl())\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 25ms/step - loss: 59.0677 - val_loss: 29.8740\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 49.8623 - val_loss: 29.2505\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 46.5956 - val_loss: 29.0704\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 43.9656 - val_loss: 29.0160\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 41.3604 - val_loss: 29.1058\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 40.1410 - val_loss: 29.3105\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 38.8687 - val_loss: 29.2562\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 37.5938 - val_loss: 29.5063\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 37.5159 - val_loss: 29.6006\n",
            "Epoch 10/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 36.5973roc-auc_val: 0.7632\n",
            "47/47 [==============================] - 13s 279ms/step - loss: 36.3528 - val_loss: 29.4814\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 35.0181 - val_loss: 29.5447\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 35.2067 - val_loss: 29.3126\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 34.3329 - val_loss: 29.4149\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.7076 - val_loss: 29.3311\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.4580 - val_loss: 29.1931\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 33.0379 - val_loss: 29.3170\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.9036 - val_loss: 29.2012\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.9763 - val_loss: 29.1076\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.6412 - val_loss: 29.1070\n",
            "Epoch 20/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 32.1119roc-auc_val: 0.7759\n",
            "47/47 [==============================] - 13s 276ms/step - loss: 31.6822 - val_loss: 29.0095\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 31.2248 - val_loss: 28.9354\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.8054 - val_loss: 28.9179\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.5220 - val_loss: 28.9992\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 30.4216 - val_loss: 28.8758\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.7275 - val_loss: 28.7947\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.3929 - val_loss: 28.7487\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.0173 - val_loss: 28.7219\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 29.0835 - val_loss: 28.6790\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.9167 - val_loss: 28.6780\n",
            "Epoch 30/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 29.2264roc-auc_val: 0.7824\n",
            "47/47 [==============================] - 13s 280ms/step - loss: 28.9789 - val_loss: 28.7095\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.8859 - val_loss: 28.7330\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.2599 - val_loss: 28.6841\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.7309 - val_loss: 28.6686\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 28.1111 - val_loss: 28.6821\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.8042 - val_loss: 28.6597\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.7732 - val_loss: 28.6287\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.7448 - val_loss: 28.6403\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.1339 - val_loss: 28.5955\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.3355 - val_loss: 28.6262\n",
            "Epoch 40/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 27.2021roc-auc_val: 0.7866\n",
            "47/47 [==============================] - 13s 278ms/step - loss: 27.0432 - val_loss: 28.6040\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 27.2450 - val_loss: 28.5258\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.9961 - val_loss: 28.5271\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.7072 - val_loss: 28.5303\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.3441 - val_loss: 28.5116\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 25.9191 - val_loss: 28.4737\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.1774 - val_loss: 28.4415\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.1089 - val_loss: 28.4351\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 25.9938 - val_loss: 28.4461\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 25.9979 - val_loss: 28.4382\n",
            "Epoch 50/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 25.9240roc-auc_val: 0.7894\n",
            "47/47 [==============================] - 13s 284ms/step - loss: 26.1386 - val_loss: 28.4002\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.0025 - val_loss: 28.4047\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.1933 - val_loss: 28.3870\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 25.5918 - val_loss: 28.4181\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 25.7318 - val_loss: 28.3926\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 25.5231 - val_loss: 28.3763\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 25.2209 - val_loss: 28.3681\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 25.5825 - val_loss: 28.3381\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 25.2346 - val_loss: 28.3340\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 25.2907 - val_loss: 28.3291\n",
            "Epoch 60/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 25.1460roc-auc_val: 0.7915\n",
            "47/47 [==============================] - 13s 280ms/step - loss: 25.3433 - val_loss: 28.3104\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.8737 - val_loss: 28.3198\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.8484 - val_loss: 28.3410\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.7749 - val_loss: 28.3362\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.8663 - val_loss: 28.3110\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.6865 - val_loss: 28.2906\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.7337 - val_loss: 28.3056\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.6804 - val_loss: 28.2825\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 24.5174 - val_loss: 28.2927\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.4386 - val_loss: 28.3125\n",
            "Epoch 70/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 24.1744roc-auc_val: 0.7938\n",
            "47/47 [==============================] - 13s 276ms/step - loss: 24.1474 - val_loss: 28.3140\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.2018 - val_loss: 28.3071\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.1612 - val_loss: 28.2963\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.6389 - val_loss: 28.2769\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.3143 - val_loss: 28.2236\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.5519 - val_loss: 28.2342\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.0820 - val_loss: 28.2341\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.0124 - val_loss: 28.2160\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.8726 - val_loss: 28.2491\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.6075 - val_loss: 28.2457\n",
            "Epoch 80/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 23.7175roc-auc_val: 0.7957\n",
            "47/47 [==============================] - 13s 277ms/step - loss: 23.7284 - val_loss: 28.2268\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.8437 - val_loss: 28.2448\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.9300 - val_loss: 28.2449\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.9945 - val_loss: 28.2462\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.4715 - val_loss: 28.2179\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.3731 - val_loss: 28.1983\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.4138 - val_loss: 28.1393\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.1102 - val_loss: 28.1417\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.5076 - val_loss: 28.1447\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.5688 - val_loss: 28.1468\n",
            "Epoch 90/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 23.7322roc-auc_val: 0.7974\n",
            "47/47 [==============================] - 13s 278ms/step - loss: 23.4723 - val_loss: 28.1722\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.1590 - val_loss: 28.1585\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.1053 - val_loss: 28.1781\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.0110 - val_loss: 28.1680\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.0152 - val_loss: 28.1342\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.2973 - val_loss: 28.1211\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.8256 - val_loss: 28.1223\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.9067 - val_loss: 28.1197\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.9719 - val_loss: 28.0809\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.0154 - val_loss: 28.0813\n",
            "Epoch 100/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 22.9486roc-auc_val: 0.7979\n",
            "47/47 [==============================] - 13s 283ms/step - loss: 22.7123 - val_loss: 28.0972\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 23.2929 - val_loss: 28.1071\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.8806 - val_loss: 28.1282\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.9044 - val_loss: 28.1348\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.8432 - val_loss: 28.1680\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 23.1596 - val_loss: 28.1507\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.9644 - val_loss: 28.1469\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.4808 - val_loss: 28.1476\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.7275 - val_loss: 28.1567\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.4064 - val_loss: 28.1831\n",
            "Epoch 110/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 22.3895roc-auc_val: 0.7986\n",
            "47/47 [==============================] - 13s 277ms/step - loss: 22.2648 - val_loss: 28.1725\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.6140 - val_loss: 28.1575\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.7595 - val_loss: 28.1775\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.4532 - val_loss: 28.1893\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.6860 - val_loss: 28.1891\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.3384 - val_loss: 28.1882\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.9700 - val_loss: 28.2084\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.4132 - val_loss: 28.2086\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.2259 - val_loss: 28.1924\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.4068 - val_loss: 28.2103\n",
            "Epoch 120/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 22.6433roc-auc_val: 0.7991\n",
            "47/47 [==============================] - 13s 277ms/step - loss: 22.5716 - val_loss: 28.1995\n",
            "Epoch 121/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.3895 - val_loss: 28.1709\n",
            "Epoch 122/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 22.4847 - val_loss: 28.2009\n",
            "Epoch 123/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.6782 - val_loss: 28.1848\n",
            "Epoch 124/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.3492 - val_loss: 28.1762\n",
            "Epoch 125/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.8602 - val_loss: 28.1767\n",
            "Epoch 126/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.7814 - val_loss: 28.1778\n",
            "Epoch 127/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.8614 - val_loss: 28.1842\n",
            "Epoch 128/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.0863 - val_loss: 28.1575\n",
            "Epoch 129/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 21.9946 - val_loss: 28.1325\n",
            "Epoch 130/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 21.8540roc-auc_val: 0.8\n",
            "47/47 [==============================] - 13s 277ms/step - loss: 21.7976 - val_loss: 28.1208\n",
            "Epoch 131/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.1284 - val_loss: 28.0884\n",
            "Epoch 132/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.1390 - val_loss: 28.0948\n",
            "Epoch 133/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.6811 - val_loss: 28.1294\n",
            "Epoch 134/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.9609 - val_loss: 28.1367\n",
            "Epoch 135/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.0246 - val_loss: 28.1272\n",
            "Epoch 136/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.7390 - val_loss: 28.1432\n",
            "Epoch 137/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.6386 - val_loss: 28.1227\n",
            "Epoch 138/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.7267 - val_loss: 28.1276\n",
            "Epoch 139/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.7096 - val_loss: 28.1218\n",
            "Epoch 140/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 21.7796roc-auc_val: 0.8007\n",
            "47/47 [==============================] - 13s 278ms/step - loss: 21.5762 - val_loss: 28.1254\n",
            "Epoch 141/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.6525 - val_loss: 28.1016\n",
            "Epoch 142/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.3247 - val_loss: 28.1271\n",
            "Epoch 143/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.5259 - val_loss: 28.1364\n",
            "Epoch 144/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.5461 - val_loss: 28.1545\n",
            "Epoch 145/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.3153 - val_loss: 28.1546\n",
            "Epoch 146/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.5821 - val_loss: 28.1597\n",
            "Epoch 147/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.3345 - val_loss: 28.1642\n",
            "Epoch 148/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.6673 - val_loss: 28.1614\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 1s 15ms/step - loss: 59.1591 - val_loss: 31.5412\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 46.2658 - val_loss: 31.8953\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 42.5810 - val_loss: 31.9244\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 40.1529 - val_loss: 31.8861\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 38.6904 - val_loss: 31.8159\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 36.7978 - val_loss: 31.4004\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 35.8965 - val_loss: 31.2604\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 35.2550 - val_loss: 30.7305\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 34.4783 - val_loss: 30.5195\n",
            "Epoch 10/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 33.8845roc-auc_val: 0.7787\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 33.7272 - val_loss: 30.5150\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.4276 - val_loss: 30.1895\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.8733 - val_loss: 30.0328\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.4886 - val_loss: 29.8797\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.2067 - val_loss: 29.8427\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 31.1484 - val_loss: 29.7002\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.5718 - val_loss: 29.4621\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.0350 - val_loss: 29.4192\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.6446 - val_loss: 29.4006\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 29.2416 - val_loss: 29.2848\n",
            "Epoch 20/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 29.0668roc-auc_val: 0.7902\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 28.8460 - val_loss: 29.1409\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.9105 - val_loss: 29.1571\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.5193 - val_loss: 29.1727\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.6149 - val_loss: 28.9815\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9122 - val_loss: 28.8709\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9010 - val_loss: 28.8683\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5000 - val_loss: 28.7745\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.9113 - val_loss: 28.7445\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.5183 - val_loss: 28.7009\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.0815 - val_loss: 28.6561\n",
            "Epoch 30/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 27.2276roc-auc_val: 0.7966\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 27.1067 - val_loss: 28.6679\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.9063 - val_loss: 28.5361\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.6424 - val_loss: 28.4850\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.4953 - val_loss: 28.4974\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.5851 - val_loss: 28.3608\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0769 - val_loss: 28.3240\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.3319 - val_loss: 28.2469\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.0474 - val_loss: 28.2398\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 26.1186 - val_loss: 28.2011\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.8739 - val_loss: 28.1301\n",
            "Epoch 40/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 25.5248roc-auc_val: 0.8005\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 25.5576 - val_loss: 28.1236\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.7731 - val_loss: 28.1310\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.5259 - val_loss: 28.0982\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2247 - val_loss: 28.0906\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4931 - val_loss: 28.0635\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2448 - val_loss: 28.0678\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4891 - val_loss: 28.0013\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.4387 - val_loss: 27.9845\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2352 - val_loss: 27.9427\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2970 - val_loss: 27.9414\n",
            "Epoch 50/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 24.9116roc-auc_val: 0.8032\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 24.9026 - val_loss: 27.8913\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8780 - val_loss: 27.8627\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7503 - val_loss: 27.8472\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4902 - val_loss: 27.8673\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5742 - val_loss: 27.8843\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.3627 - val_loss: 27.8422\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7169 - val_loss: 27.8714\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.8527 - val_loss: 27.8244\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5983 - val_loss: 27.8389\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4857 - val_loss: 27.8059\n",
            "Epoch 60/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 24.5198roc-auc_val: 0.8052\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 24.4503 - val_loss: 27.7771\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.5770 - val_loss: 27.7638\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0191 - val_loss: 27.7320\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0480 - val_loss: 27.7236\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.1224 - val_loss: 27.6920\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2588 - val_loss: 27.6634\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.2108 - val_loss: 27.6062\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.1344 - val_loss: 27.6577\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.0130 - val_loss: 27.7058\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.4460 - val_loss: 27.7076\n",
            "Epoch 70/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 23.8641roc-auc_val: 0.807\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 23.7421 - val_loss: 27.7003\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.6128 - val_loss: 27.6980\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.6763 - val_loss: 27.6089\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.8285 - val_loss: 27.6139\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.3495 - val_loss: 27.6293\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.7148 - val_loss: 27.5584\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.5648 - val_loss: 27.5788\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.4026 - val_loss: 27.5731\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.4420 - val_loss: 27.5882\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.6109 - val_loss: 27.5668\n",
            "Epoch 80/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 23.4207roc-auc_val: 0.8086\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 23.5119 - val_loss: 27.5297\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.3647 - val_loss: 27.5155\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.5291 - val_loss: 27.5144\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.3329 - val_loss: 27.5164\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.4721 - val_loss: 27.5284\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.4227 - val_loss: 27.4951\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.1471 - val_loss: 27.4821\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.2007 - val_loss: 27.4859\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.1601 - val_loss: 27.4849\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.8343 - val_loss: 27.4554\n",
            "Epoch 90/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 22.9556roc-auc_val: 0.8098\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 22.9973 - val_loss: 27.4355\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.2000 - val_loss: 27.4252\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.1005 - val_loss: 27.4147\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.1079 - val_loss: 27.4256\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.1258 - val_loss: 27.4344\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.1708 - val_loss: 27.4312\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.8483 - val_loss: 27.4522\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.9171 - val_loss: 27.4246\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.8546 - val_loss: 27.4181\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.7741 - val_loss: 27.4092\n",
            "Epoch 100/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 22.9889roc-auc_val: 0.811\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 22.8225 - val_loss: 27.3585\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.4794 - val_loss: 27.3586\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.8445 - val_loss: 27.3313\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.8977 - val_loss: 27.2946\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.7756 - val_loss: 27.3012\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.5193 - val_loss: 27.2951\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.8134 - val_loss: 27.2901\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.5538 - val_loss: 27.3150\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.6078 - val_loss: 27.2903\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.4215 - val_loss: 27.3026\n",
            "Epoch 110/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 22.5009roc-auc_val: 0.8118\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 22.4728 - val_loss: 27.3046\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.5508 - val_loss: 27.2751\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.6703 - val_loss: 27.3093\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.7156 - val_loss: 27.2657\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.6050 - val_loss: 27.2604\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.3681 - val_loss: 27.2668\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.2976 - val_loss: 27.2592\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.2767 - val_loss: 27.2323\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.3182 - val_loss: 27.2392\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.3560 - val_loss: 27.2490\n",
            "Epoch 120/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 22.2814roc-auc_val: 0.8124\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 22.3205 - val_loss: 27.2509\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.4520 - val_loss: 27.2308\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.5116 - val_loss: 27.2356\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.2043 - val_loss: 27.2287\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.1497 - val_loss: 27.2110\n",
            "Epoch 125/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.5047 - val_loss: 27.1981\n",
            "Epoch 126/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.3295 - val_loss: 27.2273\n",
            "Epoch 127/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.0665 - val_loss: 27.2249\n",
            "Epoch 128/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.3540 - val_loss: 27.2116\n",
            "Epoch 129/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.0343 - val_loss: 27.2140\n",
            "Epoch 130/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 22.1198roc-auc_val: 0.8132\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 21.9743 - val_loss: 27.2078\n",
            "Epoch 131/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.2194 - val_loss: 27.1914\n",
            "Epoch 132/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.2597 - val_loss: 27.1866\n",
            "Epoch 133/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.1242 - val_loss: 27.1600\n",
            "Epoch 134/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.0705 - val_loss: 27.1613\n",
            "Epoch 135/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.9079 - val_loss: 27.1555\n",
            "Epoch 136/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.9923 - val_loss: 27.1707\n",
            "Epoch 137/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.9575 - val_loss: 27.1643\n",
            "Epoch 138/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.8013 - val_loss: 27.1627\n",
            "Epoch 139/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6536 - val_loss: 27.1443\n",
            "Epoch 140/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 21.7696roc-auc_val: 0.8138\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 21.7797 - val_loss: 27.1267\n",
            "Epoch 141/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.8399 - val_loss: 27.1548\n",
            "Epoch 142/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.8359 - val_loss: 27.1461\n",
            "Epoch 143/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.8360 - val_loss: 27.1377\n",
            "Epoch 144/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6995 - val_loss: 27.1237\n",
            "Epoch 145/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.8626 - val_loss: 27.1367\n",
            "Epoch 146/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.8605 - val_loss: 27.1213\n",
            "Epoch 147/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.9717 - val_loss: 27.1140\n",
            "Epoch 148/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.5847 - val_loss: 27.1011\n",
            "Epoch 149/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.9060 - val_loss: 27.0812\n",
            "Epoch 150/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 21.7750roc-auc_val: 0.8144\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 21.7991 - val_loss: 27.0718\n",
            "Epoch 151/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.7307 - val_loss: 27.0856\n",
            "Epoch 152/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.7923 - val_loss: 27.1034\n",
            "Epoch 153/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.5223 - val_loss: 27.1278\n",
            "Epoch 154/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6549 - val_loss: 27.1080\n",
            "Epoch 155/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6926 - val_loss: 27.0923\n",
            "Epoch 156/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.7902 - val_loss: 27.0883\n",
            "Epoch 157/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.5516 - val_loss: 27.1029\n",
            "Epoch 158/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.5544 - val_loss: 27.0865\n",
            "Epoch 159/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6684 - val_loss: 27.0857\n",
            "Epoch 160/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 21.7232roc-auc_val: 0.8148\n",
            "88/88 [==============================] - 11s 125ms/step - loss: 21.5988 - val_loss: 27.0879\n",
            "Epoch 161/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.5341 - val_loss: 27.0801\n",
            "Epoch 162/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.8229 - val_loss: 27.0697\n",
            "Epoch 163/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6050 - val_loss: 27.0860\n",
            "Epoch 164/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.7653 - val_loss: 27.0727\n",
            "Epoch 165/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.7801 - val_loss: 27.0564\n",
            "Epoch 166/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6844 - val_loss: 27.0526\n",
            "Epoch 167/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6544 - val_loss: 27.0507\n",
            "Epoch 168/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6236 - val_loss: 27.0591\n",
            "Epoch 169/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.6147 - val_loss: 27.0512\n",
            "Epoch 170/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 21.3646roc-auc_val: 0.815\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 21.3833 - val_loss: 27.0655\n",
            "Epoch 171/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.3856 - val_loss: 27.0455\n",
            "Epoch 172/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.4159 - val_loss: 27.0456\n",
            "Epoch 173/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.3810 - val_loss: 27.0288\n",
            "Epoch 174/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.4144 - val_loss: 27.0263\n",
            "Epoch 175/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.1886 - val_loss: 27.0513\n",
            "Epoch 176/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.5330 - val_loss: 27.0333\n",
            "Epoch 177/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.2720 - val_loss: 27.0207\n",
            "Epoch 178/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.4171 - val_loss: 27.0062\n",
            "Epoch 179/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.4482 - val_loss: 27.0105\n",
            "Epoch 180/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 21.1912roc-auc_val: 0.8154\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 21.1420 - val_loss: 27.0360\n",
            "Epoch 181/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.1884 - val_loss: 27.0357\n",
            "Epoch 182/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.3884 - val_loss: 27.0330\n",
            "Epoch 183/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.1842 - val_loss: 27.0138\n",
            "Epoch 184/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.2906 - val_loss: 27.0266\n",
            "Epoch 185/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.1619 - val_loss: 27.0179\n",
            "Epoch 186/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.0975 - val_loss: 27.0313\n",
            "Epoch 187/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.1928 - val_loss: 27.0247\n",
            "Epoch 188/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.4421 - val_loss: 27.0397\n",
            "Epoch 189/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.5054 - val_loss: 27.0102\n",
            "Epoch 190/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 21.1783roc-auc_val: 0.8158\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 21.1394 - val_loss: 27.0191\n",
            "Epoch 191/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.2161 - val_loss: 27.0395\n",
            "Epoch 192/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.0590 - val_loss: 27.0333\n",
            "Epoch 193/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.2097 - val_loss: 27.0199\n",
            "Epoch 194/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.2216 - val_loss: 27.0113\n",
            "Epoch 195/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.1040 - val_loss: 26.9878\n",
            "Epoch 196/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.0252 - val_loss: 26.9842\n",
            "Epoch 197/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.0614 - val_loss: 27.0095\n",
            "Epoch 198/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.9951 - val_loss: 26.9955\n",
            "Epoch 199/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.0096 - val_loss: 27.0105\n",
            "Epoch 200/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 21.3122roc-auc_val: 0.816\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 21.2627 - val_loss: 27.0138\n",
            "Epoch 201/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.3768 - val_loss: 27.0155\n",
            "Epoch 202/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.1063 - val_loss: 27.0050\n",
            "Epoch 203/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.2885 - val_loss: 26.9903\n",
            "Epoch 204/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.0063 - val_loss: 26.9960\n",
            "Epoch 205/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.1824 - val_loss: 27.0060\n",
            "Epoch 206/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8538 - val_loss: 26.9782\n",
            "Epoch 207/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.0876 - val_loss: 26.9670\n",
            "Epoch 208/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8730 - val_loss: 26.9753\n",
            "Epoch 209/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.0015 - val_loss: 26.9662\n",
            "Epoch 210/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 21.2576roc-auc_val: 0.8163\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 21.2350 - val_loss: 26.9616\n",
            "Epoch 211/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.9914 - val_loss: 26.9644\n",
            "Epoch 212/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.1126 - val_loss: 26.9750\n",
            "Epoch 213/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8977 - val_loss: 26.9763\n",
            "Epoch 214/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.9993 - val_loss: 26.9628\n",
            "Epoch 215/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.0697 - val_loss: 26.9576\n",
            "Epoch 216/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.9791 - val_loss: 26.9662\n",
            "Epoch 217/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.9159 - val_loss: 26.9659\n",
            "Epoch 218/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8632 - val_loss: 26.9726\n",
            "Epoch 219/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.9261 - val_loss: 26.9722\n",
            "Epoch 220/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 20.9216roc-auc_val: 0.8166\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 20.9899 - val_loss: 26.9859\n",
            "Epoch 221/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.9690 - val_loss: 26.9693\n",
            "Epoch 222/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.9768 - val_loss: 26.9791\n",
            "Epoch 223/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6511 - val_loss: 26.9905\n",
            "Epoch 224/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7104 - val_loss: 26.9679\n",
            "Epoch 225/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8040 - val_loss: 26.9748\n",
            "Epoch 226/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6715 - val_loss: 26.9460\n",
            "Epoch 227/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8751 - val_loss: 26.9464\n",
            "Epoch 228/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7874 - val_loss: 26.9589\n",
            "Epoch 229/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8605 - val_loss: 26.9777\n",
            "Epoch 230/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 20.7340roc-auc_val: 0.8168\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 20.6816 - val_loss: 26.9650\n",
            "Epoch 231/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8555 - val_loss: 26.9557\n",
            "Epoch 232/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5697 - val_loss: 26.9667\n",
            "Epoch 233/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7155 - val_loss: 26.9630\n",
            "Epoch 234/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6953 - val_loss: 26.9557\n",
            "Epoch 235/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5410 - val_loss: 26.9270\n",
            "Epoch 236/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8942 - val_loss: 26.9233\n",
            "Epoch 237/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7117 - val_loss: 26.9270\n",
            "Epoch 238/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.9398 - val_loss: 26.9423\n",
            "Epoch 239/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6675 - val_loss: 26.9356\n",
            "Epoch 240/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 20.7408roc-auc_val: 0.8172\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 20.7408 - val_loss: 26.9133\n",
            "Epoch 241/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7486 - val_loss: 26.9513\n",
            "Epoch 242/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7949 - val_loss: 26.9387\n",
            "Epoch 243/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8193 - val_loss: 26.9518\n",
            "Epoch 244/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7828 - val_loss: 26.9504\n",
            "Epoch 245/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8078 - val_loss: 26.9332\n",
            "Epoch 246/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5846 - val_loss: 26.9346\n",
            "Epoch 247/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4413 - val_loss: 26.9480\n",
            "Epoch 248/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6499 - val_loss: 26.9454\n",
            "Epoch 249/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6410 - val_loss: 26.9209\n",
            "Epoch 250/1000\n",
            "82/88 [==========================>...] - ETA: 0s - loss: 20.7223roc-auc_val: 0.8175\n",
            "88/88 [==============================] - 11s 125ms/step - loss: 20.6429 - val_loss: 26.9013\n",
            "Epoch 251/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5587 - val_loss: 26.9087\n",
            "Epoch 252/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6919 - val_loss: 26.9223\n",
            "Epoch 253/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7779 - val_loss: 26.9086\n",
            "Epoch 254/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7709 - val_loss: 26.9074\n",
            "Epoch 255/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5794 - val_loss: 26.8945\n",
            "Epoch 256/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6232 - val_loss: 26.9166\n",
            "Epoch 257/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5770 - val_loss: 26.9189\n",
            "Epoch 258/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6208 - val_loss: 26.8959\n",
            "Epoch 259/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7690 - val_loss: 26.9100\n",
            "Epoch 260/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 20.7449roc-auc_val: 0.8178\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 20.7161 - val_loss: 26.8892\n",
            "Epoch 261/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5760 - val_loss: 26.9021\n",
            "Epoch 262/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6212 - val_loss: 26.8880\n",
            "Epoch 263/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5281 - val_loss: 26.9020\n",
            "Epoch 264/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5053 - val_loss: 26.8995\n",
            "Epoch 265/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5126 - val_loss: 26.9001\n",
            "Epoch 266/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5248 - val_loss: 26.9172\n",
            "Epoch 267/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4838 - val_loss: 26.9130\n",
            "Epoch 268/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3218 - val_loss: 26.9138\n",
            "Epoch 269/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6744 - val_loss: 26.8988\n",
            "Epoch 270/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 20.6428roc-auc_val: 0.818\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 20.5210 - val_loss: 26.8835\n",
            "Epoch 271/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.7308 - val_loss: 26.8865\n",
            "Epoch 272/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5715 - val_loss: 26.8865\n",
            "Epoch 273/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4000 - val_loss: 26.8949\n",
            "Epoch 274/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4239 - val_loss: 26.8854\n",
            "Epoch 275/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4104 - val_loss: 26.8828\n",
            "Epoch 276/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.6465 - val_loss: 26.8799\n",
            "Epoch 277/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4214 - val_loss: 26.8792\n",
            "Epoch 278/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3115 - val_loss: 26.8834\n",
            "Epoch 279/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2305 - val_loss: 26.8876\n",
            "Epoch 280/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 20.5438roc-auc_val: 0.8182\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 20.5595 - val_loss: 26.8924\n",
            "Epoch 281/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4545 - val_loss: 26.9042\n",
            "Epoch 282/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3249 - val_loss: 26.8728\n",
            "Epoch 283/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5081 - val_loss: 26.8913\n",
            "Epoch 284/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3708 - val_loss: 26.8887\n",
            "Epoch 285/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5198 - val_loss: 26.8860\n",
            "Epoch 286/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3786 - val_loss: 26.8660\n",
            "Epoch 287/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3113 - val_loss: 26.8929\n",
            "Epoch 288/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4324 - val_loss: 26.8755\n",
            "Epoch 289/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2708 - val_loss: 26.8567\n",
            "Epoch 290/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 20.3899roc-auc_val: 0.8184\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 20.3749 - val_loss: 26.8532\n",
            "Epoch 291/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4545 - val_loss: 26.8614\n",
            "Epoch 292/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4150 - val_loss: 26.8656\n",
            "Epoch 293/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2522 - val_loss: 26.8812\n",
            "Epoch 294/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2282 - val_loss: 26.8896\n",
            "Epoch 295/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2295 - val_loss: 26.8706\n",
            "Epoch 296/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1949 - val_loss: 26.8784\n",
            "Epoch 297/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3539 - val_loss: 26.8817\n",
            "Epoch 298/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2115 - val_loss: 26.8875\n",
            "Epoch 299/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1137 - val_loss: 26.8726\n",
            "Epoch 300/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 20.2493roc-auc_val: 0.8186\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 20.1937 - val_loss: 26.8447\n",
            "Epoch 301/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4430 - val_loss: 26.8359\n",
            "Epoch 302/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2062 - val_loss: 26.8616\n",
            "Epoch 303/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2643 - val_loss: 26.8689\n",
            "Epoch 304/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3167 - val_loss: 26.8521\n",
            "Epoch 305/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2632 - val_loss: 26.8445\n",
            "Epoch 306/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3487 - val_loss: 26.8345\n",
            "Epoch 307/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2731 - val_loss: 26.8425\n",
            "Epoch 308/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0907 - val_loss: 26.8387\n",
            "Epoch 309/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2785 - val_loss: 26.8255\n",
            "Epoch 310/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 20.3164roc-auc_val: 0.8188\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 20.2503 - val_loss: 26.8418\n",
            "Epoch 311/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4278 - val_loss: 26.8472\n",
            "Epoch 312/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1099 - val_loss: 26.8408\n",
            "Epoch 313/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0903 - val_loss: 26.8268\n",
            "Epoch 314/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1675 - val_loss: 26.8406\n",
            "Epoch 315/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2811 - val_loss: 26.8460\n",
            "Epoch 316/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2026 - val_loss: 26.8652\n",
            "Epoch 317/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5013 - val_loss: 26.8414\n",
            "Epoch 318/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0370 - val_loss: 26.8513\n",
            "Epoch 319/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1146 - val_loss: 26.8515\n",
            "Epoch 320/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 20.1310roc-auc_val: 0.8191\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 20.1039 - val_loss: 26.8422\n",
            "Epoch 321/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1430 - val_loss: 26.8249\n",
            "Epoch 322/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9702 - val_loss: 26.8377\n",
            "Epoch 323/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2087 - val_loss: 26.8365\n",
            "Epoch 324/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.3370 - val_loss: 26.8375\n",
            "Epoch 325/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2844 - val_loss: 26.8449\n",
            "Epoch 326/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2790 - val_loss: 26.8492\n",
            "Epoch 327/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2798 - val_loss: 26.8284\n",
            "Epoch 328/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1184 - val_loss: 26.8466\n",
            "Epoch 329/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1039 - val_loss: 26.8475\n",
            "Epoch 330/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 20.0659roc-auc_val: 0.8191\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 20.2115 - val_loss: 26.8308\n",
            "Epoch 331/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0649 - val_loss: 26.8227\n",
            "Epoch 332/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9468 - val_loss: 26.8324\n",
            "Epoch 333/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1226 - val_loss: 26.8192\n",
            "Epoch 334/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1416 - val_loss: 26.8145\n",
            "Epoch 335/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1940 - val_loss: 26.8369\n",
            "Epoch 336/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0097 - val_loss: 26.8277\n",
            "Epoch 337/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1239 - val_loss: 26.8448\n",
            "Epoch 338/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9851 - val_loss: 26.8323\n",
            "Epoch 339/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0305 - val_loss: 26.8475\n",
            "Epoch 340/1000\n",
            "81/88 [==========================>...] - ETA: 0s - loss: 20.0027roc-auc_val: 0.8192\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 20.0769 - val_loss: 26.8555\n",
            "Epoch 341/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1548 - val_loss: 26.8484\n",
            "Epoch 342/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9224 - val_loss: 26.8305\n",
            "Epoch 343/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0598 - val_loss: 26.8372\n",
            "Epoch 344/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1565 - val_loss: 26.8365\n",
            "Epoch 345/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9627 - val_loss: 26.8239\n",
            "Epoch 346/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8722 - val_loss: 26.8275\n",
            "Epoch 347/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9427 - val_loss: 26.8136\n",
            "Epoch 348/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0381 - val_loss: 26.8167\n",
            "Epoch 349/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8959 - val_loss: 26.7934\n",
            "Epoch 350/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 20.0910roc-auc_val: 0.8194\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 20.0153 - val_loss: 26.8015\n",
            "Epoch 351/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0115 - val_loss: 26.8230\n",
            "Epoch 352/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9386 - val_loss: 26.8111\n",
            "Epoch 353/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9906 - val_loss: 26.8014\n",
            "Epoch 354/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0647 - val_loss: 26.8187\n",
            "Epoch 355/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9603 - val_loss: 26.8204\n",
            "Epoch 356/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1159 - val_loss: 26.8104\n",
            "Epoch 357/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9447 - val_loss: 26.8142\n",
            "Epoch 358/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1420 - val_loss: 26.7903\n",
            "Epoch 359/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0901 - val_loss: 26.7885\n",
            "Epoch 360/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 19.9058roc-auc_val: 0.8197\n",
            "88/88 [==============================] - 12s 141ms/step - loss: 19.9038 - val_loss: 26.7903\n",
            "Epoch 361/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8736 - val_loss: 26.7918\n",
            "Epoch 362/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9919 - val_loss: 26.8090\n",
            "Epoch 363/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8994 - val_loss: 26.8033\n",
            "Epoch 364/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1191 - val_loss: 26.8026\n",
            "Epoch 365/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9794 - val_loss: 26.8042\n",
            "Epoch 366/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8748 - val_loss: 26.7942\n",
            "Epoch 367/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9437 - val_loss: 26.7960\n",
            "Epoch 368/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.1875 - val_loss: 26.7906\n",
            "Epoch 369/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9683 - val_loss: 26.8214\n",
            "Epoch 370/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 19.8436roc-auc_val: 0.8197\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 19.8328 - val_loss: 26.8121\n",
            "Epoch 371/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7849 - val_loss: 26.8135\n",
            "Epoch 372/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8687 - val_loss: 26.8166\n",
            "Epoch 373/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.0341 - val_loss: 26.8045\n",
            "Epoch 374/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6215 - val_loss: 26.7873\n",
            "Epoch 375/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8643 - val_loss: 26.7707\n",
            "Epoch 376/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7991 - val_loss: 26.7961\n",
            "Epoch 377/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8404 - val_loss: 26.8069\n",
            "Epoch 378/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8255 - val_loss: 26.7977\n",
            "Epoch 379/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8313 - val_loss: 26.7918\n",
            "Epoch 380/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 19.9280roc-auc_val: 0.8198\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 19.8347 - val_loss: 26.7982\n",
            "Epoch 381/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9519 - val_loss: 26.7978\n",
            "Epoch 382/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8591 - val_loss: 26.8039\n",
            "Epoch 383/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8038 - val_loss: 26.8136\n",
            "Epoch 384/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7987 - val_loss: 26.8190\n",
            "Epoch 385/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8001 - val_loss: 26.8287\n",
            "Epoch 386/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7479 - val_loss: 26.8204\n",
            "Epoch 387/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8382 - val_loss: 26.8136\n",
            "Epoch 388/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8035 - val_loss: 26.7974\n",
            "Epoch 389/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8411 - val_loss: 26.8113\n",
            "Epoch 390/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 19.8387roc-auc_val: 0.8199\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 19.8709 - val_loss: 26.8090\n",
            "Epoch 391/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7184 - val_loss: 26.7953\n",
            "Epoch 392/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8783 - val_loss: 26.7897\n",
            "Epoch 393/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6919 - val_loss: 26.7777\n",
            "Epoch 394/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7792 - val_loss: 26.7799\n",
            "Epoch 395/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8204 - val_loss: 26.7808\n",
            "Epoch 396/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6031 - val_loss: 26.7887\n",
            "Epoch 397/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8256 - val_loss: 26.7796\n",
            "Epoch 398/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7792 - val_loss: 26.7947\n",
            "Epoch 399/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8225 - val_loss: 26.7800\n",
            "Epoch 400/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 19.6917roc-auc_val: 0.8201\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 19.5431 - val_loss: 26.7810\n",
            "Epoch 401/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6896 - val_loss: 26.7881\n",
            "Epoch 402/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6685 - val_loss: 26.7812\n",
            "Epoch 403/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8134 - val_loss: 26.7763\n",
            "Epoch 404/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7430 - val_loss: 26.7742\n",
            "Epoch 405/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8437 - val_loss: 26.7751\n",
            "Epoch 406/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6251 - val_loss: 26.7720\n",
            "Epoch 407/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9271 - val_loss: 26.7777\n",
            "Epoch 408/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8671 - val_loss: 26.7795\n",
            "Epoch 409/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7651 - val_loss: 26.7988\n",
            "Epoch 410/1000\n",
            "80/88 [==========================>...] - ETA: 0s - loss: 19.8089roc-auc_val: 0.8201\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 19.7523 - val_loss: 26.7886\n",
            "Epoch 411/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7696 - val_loss: 26.7991\n",
            "Epoch 412/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6403 - val_loss: 26.8098\n",
            "Epoch 413/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6923 - val_loss: 26.7930\n",
            "Epoch 414/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7771 - val_loss: 26.8018\n",
            "Epoch 415/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7275 - val_loss: 26.7832\n",
            "Epoch 416/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.4783 - val_loss: 26.8057\n",
            "Epoch 417/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.5334 - val_loss: 26.7913\n",
            "Epoch 418/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7707 - val_loss: 26.8089\n",
            "Epoch 419/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7275 - val_loss: 26.8001\n",
            "Epoch 420/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 19.7129roc-auc_val: 0.8203\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 19.7406 - val_loss: 26.7783\n",
            "Epoch 421/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.4848 - val_loss: 26.7798\n",
            "Epoch 422/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.5778 - val_loss: 26.7770\n",
            "Epoch 423/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.4806 - val_loss: 26.7867\n",
            "Epoch 424/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6918 - val_loss: 26.8015\n",
            "Epoch 425/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6528 - val_loss: 26.8114\n",
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 24ms/step - loss: 57.1729 - val_loss: 30.9293\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 48.7366 - val_loss: 31.5355\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 44.5792 - val_loss: 31.5845\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 41.5193 - val_loss: 31.5764\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 40.2469 - val_loss: 31.2363\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 38.7732 - val_loss: 31.2246\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 37.7936 - val_loss: 31.1018\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 36.0808 - val_loss: 31.3728\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 35.3549 - val_loss: 31.2006\n",
            "Epoch 10/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 35.5024roc-auc_val: 0.7582\n",
            "47/47 [==============================] - 11s 234ms/step - loss: 35.3926 - val_loss: 31.0783\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 34.9517 - val_loss: 31.1399\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 34.1270 - val_loss: 30.9226\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 33.6166 - val_loss: 30.5042\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 33.2899 - val_loss: 30.3352\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.1355 - val_loss: 30.3669\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 32.2409 - val_loss: 30.0553\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.8895 - val_loss: 29.9234\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.9988 - val_loss: 30.0993\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 30.6790 - val_loss: 30.1857\n",
            "Epoch 20/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 30.1206roc-auc_val: 0.7693\n",
            "47/47 [==============================] - 11s 237ms/step - loss: 29.9872 - val_loss: 29.8769\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.5334 - val_loss: 29.9005\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.5249 - val_loss: 29.7134\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.9772 - val_loss: 29.7687\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 29.2088 - val_loss: 29.7424\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.8890 - val_loss: 29.7014\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.6594 - val_loss: 29.5569\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 29.6573 - val_loss: 29.5518\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.6419 - val_loss: 29.4194\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.9525 - val_loss: 29.4704\n",
            "Epoch 30/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 28.0160roc-auc_val: 0.7756\n",
            "47/47 [==============================] - 11s 232ms/step - loss: 27.9851 - val_loss: 29.2693\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.8568 - val_loss: 29.2551\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 28.2703 - val_loss: 29.1868\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.8906 - val_loss: 29.2040\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.6868 - val_loss: 29.1463\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.9898 - val_loss: 29.1264\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.6980 - val_loss: 29.0329\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.9306 - val_loss: 29.0818\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 27.2032 - val_loss: 29.0894\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.6474 - val_loss: 29.0199\n",
            "Epoch 40/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 26.8977roc-auc_val: 0.7793\n",
            "47/47 [==============================] - 11s 235ms/step - loss: 26.9081 - val_loss: 29.0536\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.5893 - val_loss: 28.9588\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.1551 - val_loss: 28.9339\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.6571 - val_loss: 28.9261\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 25.5477 - val_loss: 28.9802\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 26.0857 - val_loss: 28.9741\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 25.8653 - val_loss: 29.0280\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 25.8056 - val_loss: 28.9765\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 25.5301 - val_loss: 29.0086\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 25.6825 - val_loss: 28.9993\n",
            "Epoch 50/1000\n",
            "42/47 [=========================>....] - ETA: 0s - loss: 25.9223roc-auc_val: 0.782\n",
            "47/47 [==============================] - 11s 233ms/step - loss: 25.6948 - val_loss: 28.9746\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 25.3757 - val_loss: 28.9983\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 25.5218 - val_loss: 29.0220\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 25.5358 - val_loss: 29.0245\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 25.1996 - val_loss: 29.0190\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 24.7602 - val_loss: 29.0116\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 25.1341 - val_loss: 29.0365\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.6777 - val_loss: 29.0768\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.7716 - val_loss: 29.0284\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.8673 - val_loss: 29.0436\n",
            "Epoch 60/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 24.6732roc-auc_val: 0.7833\n",
            "47/47 [==============================] - 11s 234ms/step - loss: 24.5199 - val_loss: 29.0561\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 24.7647 - val_loss: 29.0446\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.3295 - val_loss: 29.0020\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.2987 - val_loss: 28.9990\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.5422 - val_loss: 28.9877\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.5614 - val_loss: 28.9247\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 24.4569 - val_loss: 28.9639\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.3780 - val_loss: 28.9991\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.7020 - val_loss: 29.0050\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.3561 - val_loss: 28.9963\n",
            "Epoch 70/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 24.2600roc-auc_val: 0.7853\n",
            "47/47 [==============================] - 11s 236ms/step - loss: 24.0333 - val_loss: 28.9619\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.9891 - val_loss: 29.0370\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.9493 - val_loss: 29.0664\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.0221 - val_loss: 29.0642\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.6712 - val_loss: 29.0568\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.2102 - val_loss: 29.0985\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.9197 - val_loss: 29.0849\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.7778 - val_loss: 29.0347\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 23.5471 - val_loss: 29.0456\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 24.2253 - val_loss: 29.0941\n",
            "Epoch 80/1000\n",
            "43/47 [==========================>...] - ETA: 0s - loss: 23.5260roc-auc_val: 0.7861\n",
            "47/47 [==============================] - 11s 233ms/step - loss: 23.3004 - val_loss: 29.0935\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.6016 - val_loss: 29.0771\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.2332 - val_loss: 29.0700\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.9825 - val_loss: 29.0857\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.3141 - val_loss: 29.0599\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.6839 - val_loss: 29.1278\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 22.6788 - val_loss: 29.1302\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 23.3628 - val_loss: 29.1313\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.9371 - val_loss: 29.1308\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.8606 - val_loss: 29.1337\n",
            "Epoch 90/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 23.3056roc-auc_val: 0.7876\n",
            "47/47 [==============================] - 11s 235ms/step - loss: 23.1728 - val_loss: 29.1394\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.0731 - val_loss: 29.1552\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.8328 - val_loss: 29.1702\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 23.1146 - val_loss: 29.1795\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.9277 - val_loss: 29.1634\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.9966 - val_loss: 29.1904\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.9732 - val_loss: 29.2018\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.4729 - val_loss: 29.1947\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.9864 - val_loss: 29.2421\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.6393 - val_loss: 29.2413\n",
            "Epoch 100/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 22.8605roc-auc_val: 0.7883\n",
            "47/47 [==============================] - 11s 232ms/step - loss: 22.6727 - val_loss: 29.2807\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.6064 - val_loss: 29.3037\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.4029 - val_loss: 29.3092\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.6649 - val_loss: 29.3226\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.5606 - val_loss: 29.2856\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.6824 - val_loss: 29.2737\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.4469 - val_loss: 29.3061\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.3143 - val_loss: 29.3221\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 21.8241 - val_loss: 29.3248\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.3377 - val_loss: 29.3169\n",
            "Epoch 110/1000\n",
            "41/47 [=========================>....] - ETA: 0s - loss: 22.4752roc-auc_val: 0.7891\n",
            "47/47 [==============================] - 11s 231ms/step - loss: 22.1179 - val_loss: 29.3294\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 1s 11ms/step - loss: 22.7186 - val_loss: 29.3305\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.3079 - val_loss: 29.3062\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.9776 - val_loss: 29.3465\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 21.9134 - val_loss: 29.3568\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 22.7647 - val_loss: 29.3783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnpeTPNLkiCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "outputId": "6339f3f8-b942-453c-c5b4-9234adaddf83"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6S7Pv_rc-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}
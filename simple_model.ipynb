{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIAeHxBw8EEt",
        "colab_type": "text"
      },
      "source": [
        "Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstQrkXM8Bz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import *\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvhOLFro71iv",
        "colab_type": "text"
      },
      "source": [
        "loading drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9860c5b2-717b-4d10-88e6-815de8852ae2"
      },
      "source": [
        "\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vpS1Lgw76SY",
        "colab_type": "text"
      },
      "source": [
        "Loading data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "703a818f-20de-4076-f1e0-ece5e5e035f6"
      },
      "source": [
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "test_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_identity.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_transaction.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZi5I0Va7-Af",
        "colab_type": "text"
      },
      "source": [
        "Loading dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6e35e61f-548c-4d47-e006-21bf12445a0a"
      },
      "source": [
        "\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (210,222) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THUOkL9B8M6e",
        "colab_type": "text"
      },
      "source": [
        "Label encoding data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OTCMdEiOn9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "34f487d2-17f9-4d5f-e0e5-794b281bc82e"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "cols.remove('id')\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JehdJrzj8X-1",
        "colab_type": "text"
      },
      "source": [
        "Getting id features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjtnh0Z-6O-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2d283270-a56e-42a8-a19a-0e270f343d7e"
      },
      "source": [
        "\n",
        "dum=list(trn.filter(regex='dum'))\n",
        "cat=cols+['id']\n",
        "isna=list(trn.filter(regex='isna'))\n",
        "cat.remove('id')\n",
        "categorical = [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\", \"addr1\", \"addr2\",\n",
        "               \"P_emaildomain\",'P_emaildomain_first','P_emaildomain_second', \"R_emaildomain\",\n",
        "              \"DeviceInfo\", \"DeviceType\"] + [\"id_0\" + str(i) for i in range(1, 10)] +\\\n",
        "                [\"id_\" + str(i) for i in range(10, 39)] + \\\n",
        "                 [\"M\" + str(i) for i in range(1, 10)]\n",
        "\n",
        "\n",
        "\n",
        "tot=list(set(dum+cat+isna+['day','month','week']+categorical))\n",
        "num=[ i for i in list(trn) if i not in tot]\n",
        "\n",
        "num.remove('id')\n",
        "num.remove('isFraud')\n",
        "for col in tqdm(num):\n",
        "  cols=[col+'_mean',col+'_std']\n",
        "  trn[col+'_mean']=trn.groupby(['id'])[col].transform('mean')\n",
        "  trn[col+'_std']=trn.groupby(['id'])[col].transform('std')\n",
        "  \n",
        "  tst[col+'_mean']=tst.groupby(['id'])[col].transform('mean')\n",
        "  tst[col+'_std']=tst.groupby(['id'])[col].transform('std')\n",
        "  for c in cols:\n",
        "    mn=trn[c].mean()\n",
        "    std=trn[c].std()\n",
        "    trn[c]=(trn[c]-mn)/std\n",
        "    tst[c]=(tst[c]-mn)/std \n",
        "trn=trn.drop(['id'],1)\n",
        "tst=tst.drop(['id'],1)\n",
        "\n",
        "num=[ i for i in list(trn) if i not in tot]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35/35 [02:12<00:00,  3.79s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfgUSjuf8aUj",
        "colab_type": "text"
      },
      "source": [
        "Loading autoenc features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi5XO-IL6ab2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3ef61f33-6f31-45e0-8ba8-f121b38b2d96"
      },
      "source": [
        "autoenc=np.load('/content/gdrive/My Drive/fraud/with_id_withpoutV.npy')\n",
        "autoenc.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1097231, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO1OnSZJ9GST",
        "colab_type": "text"
      },
      "source": [
        "Merging data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVSmlFta9HsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0776fa86-fc4b-4c6e-962c-8c86d6129da6"
      },
      "source": [
        "autoenc=pd.DataFrame(autoenc)\n",
        "for col in list(autoenc):\n",
        "  mn=autoenc[col].mean()\n",
        "  std=autoenc[col].std()\n",
        "  autoenc[col]=(autoenc[col]-mn)/std  \n",
        "trn_s=trn.shape[0]\n",
        "df=pd.concat([trn,tst],0).reset_index(drop=True)\n",
        "del([trn,tst])\n",
        "gc.collect()\n",
        "df=pd.concat([df,autoenc],1)\n",
        "trn=df.loc[:trn_s-1]\n",
        "tst=df.loc[trn_s:].reset_index(drop=True)\n",
        "del([df])\n",
        "gc.collect()\n",
        "trn=trn.drop(['Unnamed: 0'],1)\n",
        "tst=tst.drop(['Unnamed: 0','isFraud'],1)\n",
        "gc.collect()\n",
        "trn.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>card2_isna</th>\n",
              "      <th>card3_isna</th>\n",
              "      <th>card4_isna</th>\n",
              "      <th>card5_isna</th>\n",
              "      <th>card6_isna</th>\n",
              "      <th>addr1_isna</th>\n",
              "      <th>addr2_isna</th>\n",
              "      <th>dist1_isna</th>\n",
              "      <th>P_emaildomain_isna</th>\n",
              "      <th>D1_isna</th>\n",
              "      <th>D2_isna</th>\n",
              "      <th>D3_isna</th>\n",
              "      <th>D4_isna</th>\n",
              "      <th>D5_isna</th>\n",
              "      <th>D10_isna</th>\n",
              "      <th>D11_isna</th>\n",
              "      <th>D15_isna</th>\n",
              "      <th>M1_isna</th>\n",
              "      <th>M2_isna</th>\n",
              "      <th>M3_isna</th>\n",
              "      <th>M4_isna</th>\n",
              "      <th>M5_isna</th>\n",
              "      <th>M6_isna</th>\n",
              "      <th>M7_isna</th>\n",
              "      <th>M8_isna</th>\n",
              "      <th>M9_isna</th>\n",
              "      <th>M4</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>M9</th>\n",
              "      <th>card3</th>\n",
              "      <th>M5</th>\n",
              "      <th>M2</th>\n",
              "      <th>M6</th>\n",
              "      <th>card6</th>\n",
              "      <th>M7</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card4</th>\n",
              "      <th>M3</th>\n",
              "      <th>card5</th>\n",
              "      <th>card2</th>\n",
              "      <th>...</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>1.242764</td>\n",
              "      <td>-0.515518</td>\n",
              "      <td>-1.070670</td>\n",
              "      <td>0.283700</td>\n",
              "      <td>-0.665361</td>\n",
              "      <td>-0.852507</td>\n",
              "      <td>-1.188304</td>\n",
              "      <td>0.235176</td>\n",
              "      <td>-0.808980</td>\n",
              "      <td>0.537635</td>\n",
              "      <td>2.922530</td>\n",
              "      <td>-0.106844</td>\n",
              "      <td>-0.310748</td>\n",
              "      <td>2.003823</td>\n",
              "      <td>1.206149</td>\n",
              "      <td>0.013289</td>\n",
              "      <td>-0.451956</td>\n",
              "      <td>-1.558216</td>\n",
              "      <td>-0.052969</td>\n",
              "      <td>0.549370</td>\n",
              "      <td>-0.155466</td>\n",
              "      <td>-0.329755</td>\n",
              "      <td>4.310970</td>\n",
              "      <td>-0.076769</td>\n",
              "      <td>-1.207729</td>\n",
              "      <td>-0.193912</td>\n",
              "      <td>1.231712</td>\n",
              "      <td>0.668962</td>\n",
              "      <td>1.233703</td>\n",
              "      <td>0.913392</td>\n",
              "      <td>0.353394</td>\n",
              "      <td>-0.393244</td>\n",
              "      <td>0.981463</td>\n",
              "      <td>-0.800058</td>\n",
              "      <td>0.091785</td>\n",
              "      <td>0.120459</td>\n",
              "      <td>-0.529338</td>\n",
              "      <td>-1.485394</td>\n",
              "      <td>-0.249264</td>\n",
              "      <td>-0.743785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.478335</td>\n",
              "      <td>0.278179</td>\n",
              "      <td>-0.868818</td>\n",
              "      <td>-1.043651</td>\n",
              "      <td>0.801288</td>\n",
              "      <td>0.078223</td>\n",
              "      <td>-0.640398</td>\n",
              "      <td>-0.337298</td>\n",
              "      <td>0.001820</td>\n",
              "      <td>-0.845505</td>\n",
              "      <td>-0.359985</td>\n",
              "      <td>-0.408767</td>\n",
              "      <td>-0.196191</td>\n",
              "      <td>-0.397315</td>\n",
              "      <td>-0.708109</td>\n",
              "      <td>-0.724816</td>\n",
              "      <td>-0.552898</td>\n",
              "      <td>-0.734606</td>\n",
              "      <td>-0.310484</td>\n",
              "      <td>0.382740</td>\n",
              "      <td>-0.549191</td>\n",
              "      <td>-0.849574</td>\n",
              "      <td>0.198847</td>\n",
              "      <td>-0.259470</td>\n",
              "      <td>0.256196</td>\n",
              "      <td>0.807907</td>\n",
              "      <td>0.377824</td>\n",
              "      <td>0.699971</td>\n",
              "      <td>1.761081</td>\n",
              "      <td>0.309027</td>\n",
              "      <td>-0.165418</td>\n",
              "      <td>-0.400574</td>\n",
              "      <td>0.586140</td>\n",
              "      <td>0.229128</td>\n",
              "      <td>-0.713779</td>\n",
              "      <td>0.102720</td>\n",
              "      <td>-0.190755</td>\n",
              "      <td>0.367235</td>\n",
              "      <td>-0.751962</td>\n",
              "      <td>-0.614538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.279917</td>\n",
              "      <td>-0.574741</td>\n",
              "      <td>1.029608</td>\n",
              "      <td>0.457375</td>\n",
              "      <td>-0.584431</td>\n",
              "      <td>-0.589340</td>\n",
              "      <td>2.700197</td>\n",
              "      <td>-0.130066</td>\n",
              "      <td>-0.420789</td>\n",
              "      <td>0.219158</td>\n",
              "      <td>-1.303340</td>\n",
              "      <td>0.558095</td>\n",
              "      <td>-0.327077</td>\n",
              "      <td>1.490529</td>\n",
              "      <td>0.436610</td>\n",
              "      <td>-0.415684</td>\n",
              "      <td>-0.427045</td>\n",
              "      <td>0.136411</td>\n",
              "      <td>-1.589380</td>\n",
              "      <td>-0.439486</td>\n",
              "      <td>1.084522</td>\n",
              "      <td>1.017003</td>\n",
              "      <td>-0.326726</td>\n",
              "      <td>0.418317</td>\n",
              "      <td>0.057420</td>\n",
              "      <td>-0.592301</td>\n",
              "      <td>0.012839</td>\n",
              "      <td>0.497301</td>\n",
              "      <td>-0.945999</td>\n",
              "      <td>0.155201</td>\n",
              "      <td>0.438123</td>\n",
              "      <td>-0.353266</td>\n",
              "      <td>0.837998</td>\n",
              "      <td>0.539504</td>\n",
              "      <td>-0.980364</td>\n",
              "      <td>-0.953120</td>\n",
              "      <td>-0.191095</td>\n",
              "      <td>2.448992</td>\n",
              "      <td>-0.122963</td>\n",
              "      <td>0.135550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>...</td>\n",
              "      <td>0.790510</td>\n",
              "      <td>-0.270352</td>\n",
              "      <td>-0.006041</td>\n",
              "      <td>-0.073428</td>\n",
              "      <td>-1.076433</td>\n",
              "      <td>0.006397</td>\n",
              "      <td>-0.278142</td>\n",
              "      <td>0.044966</td>\n",
              "      <td>-0.439494</td>\n",
              "      <td>-0.794641</td>\n",
              "      <td>-1.078903</td>\n",
              "      <td>-0.158959</td>\n",
              "      <td>-0.303655</td>\n",
              "      <td>-0.836505</td>\n",
              "      <td>-0.306516</td>\n",
              "      <td>-1.054407</td>\n",
              "      <td>-0.214335</td>\n",
              "      <td>0.060106</td>\n",
              "      <td>0.213959</td>\n",
              "      <td>-0.050495</td>\n",
              "      <td>-0.671942</td>\n",
              "      <td>0.124549</td>\n",
              "      <td>-0.507537</td>\n",
              "      <td>-0.649432</td>\n",
              "      <td>-0.507792</td>\n",
              "      <td>-0.606063</td>\n",
              "      <td>0.177337</td>\n",
              "      <td>0.123741</td>\n",
              "      <td>2.553579</td>\n",
              "      <td>-1.065499</td>\n",
              "      <td>0.097979</td>\n",
              "      <td>-0.297628</td>\n",
              "      <td>-0.708184</td>\n",
              "      <td>0.510937</td>\n",
              "      <td>-0.204277</td>\n",
              "      <td>-0.424759</td>\n",
              "      <td>-0.350952</td>\n",
              "      <td>0.162537</td>\n",
              "      <td>-0.261409</td>\n",
              "      <td>0.150569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.040117</td>\n",
              "      <td>0.125807</td>\n",
              "      <td>0.640090</td>\n",
              "      <td>-1.335913</td>\n",
              "      <td>1.071193</td>\n",
              "      <td>-0.229325</td>\n",
              "      <td>-0.147024</td>\n",
              "      <td>-0.218421</td>\n",
              "      <td>0.180540</td>\n",
              "      <td>-0.468195</td>\n",
              "      <td>0.867738</td>\n",
              "      <td>-0.689950</td>\n",
              "      <td>-0.434339</td>\n",
              "      <td>-1.096154</td>\n",
              "      <td>-1.231862</td>\n",
              "      <td>-1.096684</td>\n",
              "      <td>1.020662</td>\n",
              "      <td>-0.510981</td>\n",
              "      <td>-0.023783</td>\n",
              "      <td>-0.609389</td>\n",
              "      <td>0.325300</td>\n",
              "      <td>-0.701391</td>\n",
              "      <td>-0.154755</td>\n",
              "      <td>-0.618930</td>\n",
              "      <td>-0.372212</td>\n",
              "      <td>1.245477</td>\n",
              "      <td>-0.275173</td>\n",
              "      <td>0.924416</td>\n",
              "      <td>-0.024507</td>\n",
              "      <td>-1.042178</td>\n",
              "      <td>2.667389</td>\n",
              "      <td>3.365522</td>\n",
              "      <td>0.725972</td>\n",
              "      <td>-0.521776</td>\n",
              "      <td>-1.169338</td>\n",
              "      <td>0.780491</td>\n",
              "      <td>-0.140265</td>\n",
              "      <td>1.355072</td>\n",
              "      <td>0.048784</td>\n",
              "      <td>-0.236553</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 531 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   card2_isna  card3_isna  card4_isna  ...       253       254       255\n",
              "0         1.0         0.0         0.0  ... -1.485394 -0.249264 -0.743785\n",
              "1         0.0         0.0         0.0  ...  0.367235 -0.751962 -0.614538\n",
              "2         0.0         0.0         0.0  ...  2.448992 -0.122963  0.135550\n",
              "3         0.0         0.0         0.0  ...  0.162537 -0.261409  0.150569\n",
              "4         0.0         0.0         0.0  ...  1.355072  0.048784 -0.236553\n",
              "\n",
              "[5 rows x 531 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LEIUFVW8iAC",
        "colab_type": "text"
      },
      "source": [
        "Reduce memory useage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "5c87e722-2f9b-4554-b595-17fbdf140580"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 2392.40 MB\n",
            "Memory usage after optimization is: 604.86 MB\n",
            "Decreased by 74.7%\n",
            "Memory usage of dataframe is 2048.85 MB\n",
            "Memory usage after optimization is: 517.53 MB\n",
            "Decreased by 74.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZjn9ePhArDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn[col]=trn[col].fillna(trn[col].mean())\n",
        "  tst[col]=tst[col].fillna(tst[col].mean())\n",
        "a=trn.isna().sum()\n",
        "ls=a[a>0].index\n",
        "for col in ls:\n",
        "  trn=trn.drop([col],1)\n",
        "  tst=tst.drop([col],1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqrD6vz8ol6",
        "colab_type": "text"
      },
      "source": [
        "Making the callbacks and loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVzhwjpjEsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e25c388-7d6e-4db7-b4d1-1a90bbe26489"
      },
      "source": [
        "dk={}\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "\n",
        "  inp=Input((523,))\n",
        "  x=Dense(256,activation=custom_gelu)(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation=custom_gelu)(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation=custom_gelu)(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=[inp],outputs=x)\n",
        "  return mod\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "ls=list(trn['month'].unique())\n",
        "for en,month in enumerate([(ls[4],ls[5]),(ls[3],ls[4]),(ls[3],ls[5])]):\n",
        "  train=trn.loc[trn['month']<=month[0]]\n",
        "  test=trn.loc[(trn['month']>=month[1])&(trn['month']<6)]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  mod.compile(optimizer=Nadam(0.0001),loss='binary_crossentropy')\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==ls[6]].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst.drop(['month'],1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "243/243 [==============================] - 2s 9ms/step - loss: 0.6787 - val_loss: 0.4578\n",
            "Epoch 2/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.4638 - val_loss: 0.3154\n",
            "Epoch 3/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.3036 - val_loss: 0.2089\n",
            "Epoch 4/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.2097 - val_loss: 0.1589\n",
            "Epoch 5/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1692 - val_loss: 0.1409\n",
            "Epoch 6/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1511 - val_loss: 0.1302\n",
            "Epoch 7/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1408 - val_loss: 0.1272\n",
            "Epoch 8/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1338 - val_loss: 0.1213\n",
            "Epoch 9/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1299 - val_loss: 0.1198\n",
            "Epoch 10/1000\n",
            "240/243 [============================>.] - ETA: 0s - loss: 0.1263roc-auc_val: 0.8286\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.1265 - val_loss: 0.1183\n",
            "Epoch 11/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1239 - val_loss: 0.1165\n",
            "Epoch 12/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1226 - val_loss: 0.1166\n",
            "Epoch 13/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1209 - val_loss: 0.1166\n",
            "Epoch 14/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1186 - val_loss: 0.1133\n",
            "Epoch 15/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1176 - val_loss: 0.1108\n",
            "Epoch 16/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1154 - val_loss: 0.1116\n",
            "Epoch 17/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1146 - val_loss: 0.1108\n",
            "Epoch 18/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1136 - val_loss: 0.1127\n",
            "Epoch 19/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1132 - val_loss: 0.1147\n",
            "Epoch 20/1000\n",
            "240/243 [============================>.] - ETA: 0s - loss: 0.1120roc-auc_val: 0.8552\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.1121 - val_loss: 0.1081\n",
            "Epoch 21/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1117 - val_loss: 0.1073\n",
            "Epoch 22/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1111 - val_loss: 0.1074\n",
            "Epoch 23/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1093 - val_loss: 0.1124\n",
            "Epoch 24/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1087 - val_loss: 0.1081\n",
            "Epoch 25/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1073 - val_loss: 0.1074\n",
            "Epoch 26/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1067 - val_loss: 0.1061\n",
            "Epoch 27/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1060 - val_loss: 0.1056\n",
            "Epoch 28/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1061 - val_loss: 0.1065\n",
            "Epoch 29/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1039 - val_loss: 0.1048\n",
            "Epoch 30/1000\n",
            "241/243 [============================>.] - ETA: 0s - loss: 0.1033roc-auc_val: 0.861\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.1033 - val_loss: 0.1055\n",
            "Epoch 31/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1021 - val_loss: 0.1040\n",
            "Epoch 32/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.1009 - val_loss: 0.1042\n",
            "Epoch 33/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0997 - val_loss: 0.1025\n",
            "Epoch 34/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0980 - val_loss: 0.1039\n",
            "Epoch 35/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0973 - val_loss: 0.1036\n",
            "Epoch 36/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0962 - val_loss: 0.1019\n",
            "Epoch 37/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0958 - val_loss: 0.1024\n",
            "Epoch 38/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0949 - val_loss: 0.1016\n",
            "Epoch 39/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0942 - val_loss: 0.1014\n",
            "Epoch 40/1000\n",
            "240/243 [============================>.] - ETA: 0s - loss: 0.0943roc-auc_val: 0.8715\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0942 - val_loss: 0.1008\n",
            "Epoch 41/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0930 - val_loss: 0.1012\n",
            "Epoch 42/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0930 - val_loss: 0.1036\n",
            "Epoch 43/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0921 - val_loss: 0.1018\n",
            "Epoch 44/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0918 - val_loss: 0.1011\n",
            "Epoch 45/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0912 - val_loss: 0.1011\n",
            "Epoch 46/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0904 - val_loss: 0.1005\n",
            "Epoch 47/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0902 - val_loss: 0.1010\n",
            "Epoch 48/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0901 - val_loss: 0.0996\n",
            "Epoch 49/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0898 - val_loss: 0.1004\n",
            "Epoch 50/1000\n",
            "240/243 [============================>.] - ETA: 0s - loss: 0.0892roc-auc_val: 0.8704\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0892 - val_loss: 0.1026\n",
            "Epoch 51/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0890 - val_loss: 0.1007\n",
            "Epoch 52/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0882 - val_loss: 0.0996\n",
            "Epoch 53/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0879 - val_loss: 0.1003\n",
            "Epoch 54/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0878 - val_loss: 0.1000\n",
            "Epoch 55/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0874 - val_loss: 0.1008\n",
            "Epoch 56/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0870 - val_loss: 0.1000\n",
            "Epoch 57/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0869 - val_loss: 0.1003\n",
            "Epoch 58/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0862 - val_loss: 0.0991\n",
            "Epoch 59/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0856 - val_loss: 0.1000\n",
            "Epoch 60/1000\n",
            "241/243 [============================>.] - ETA: 0s - loss: 0.0855roc-auc_val: 0.8731\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0854 - val_loss: 0.1013\n",
            "Epoch 61/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0855 - val_loss: 0.0998\n",
            "Epoch 62/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0850 - val_loss: 0.0983\n",
            "Epoch 63/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0848 - val_loss: 0.0982\n",
            "Epoch 64/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0846 - val_loss: 0.1010\n",
            "Epoch 65/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0842 - val_loss: 0.0993\n",
            "Epoch 66/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0837 - val_loss: 0.0984\n",
            "Epoch 67/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0834 - val_loss: 0.1030\n",
            "Epoch 68/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0835 - val_loss: 0.0981\n",
            "Epoch 69/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0832 - val_loss: 0.0982\n",
            "Epoch 70/1000\n",
            "239/243 [============================>.] - ETA: 0s - loss: 0.0830roc-auc_val: 0.8777\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0828 - val_loss: 0.0980\n",
            "Epoch 71/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0827 - val_loss: 0.0994\n",
            "Epoch 72/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0822 - val_loss: 0.0980\n",
            "Epoch 73/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0818 - val_loss: 0.1003\n",
            "Epoch 74/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0819 - val_loss: 0.0978\n",
            "Epoch 75/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0815 - val_loss: 0.1021\n",
            "Epoch 76/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0812 - val_loss: 0.0970\n",
            "Epoch 77/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0810 - val_loss: 0.1002\n",
            "Epoch 78/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0807 - val_loss: 0.0996\n",
            "Epoch 79/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0804 - val_loss: 0.0974\n",
            "Epoch 80/1000\n",
            "238/243 [============================>.] - ETA: 0s - loss: 0.0802roc-auc_val: 0.8737\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0801 - val_loss: 0.1012\n",
            "Epoch 81/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0802 - val_loss: 0.0987\n",
            "Epoch 82/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0795 - val_loss: 0.1015\n",
            "Epoch 83/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0796 - val_loss: 0.1026\n",
            "Epoch 84/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0793 - val_loss: 0.0988\n",
            "Epoch 85/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0794 - val_loss: 0.0989\n",
            "Epoch 86/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0789 - val_loss: 0.0989\n",
            "Epoch 87/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0787 - val_loss: 0.0985\n",
            "Epoch 88/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0784 - val_loss: 0.0970\n",
            "Epoch 89/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0781 - val_loss: 0.0988\n",
            "Epoch 90/1000\n",
            "241/243 [============================>.] - ETA: 0s - loss: 0.0783roc-auc_val: 0.8812\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0783 - val_loss: 0.0975\n",
            "Epoch 91/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0780 - val_loss: 0.0983\n",
            "Epoch 92/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0777 - val_loss: 0.0969\n",
            "Epoch 93/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0777 - val_loss: 0.0975\n",
            "Epoch 94/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0773 - val_loss: 0.0981\n",
            "Epoch 95/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0769 - val_loss: 0.0976\n",
            "Epoch 96/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0769 - val_loss: 0.0974\n",
            "Epoch 97/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0764 - val_loss: 0.0983\n",
            "Epoch 98/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0764 - val_loss: 0.0979\n",
            "Epoch 99/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0764 - val_loss: 0.0983\n",
            "Epoch 100/1000\n",
            "240/243 [============================>.] - ETA: 0s - loss: 0.0760roc-auc_val: 0.8826\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0761 - val_loss: 0.0972\n",
            "Epoch 101/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0757 - val_loss: 0.0985\n",
            "Epoch 102/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0755 - val_loss: 0.0969\n",
            "Epoch 103/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0753 - val_loss: 0.0968\n",
            "Epoch 104/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0754 - val_loss: 0.0964\n",
            "Epoch 105/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0746 - val_loss: 0.0976\n",
            "Epoch 106/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0748 - val_loss: 0.0997\n",
            "Epoch 107/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0746 - val_loss: 0.0979\n",
            "Epoch 108/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0744 - val_loss: 0.0976\n",
            "Epoch 109/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0742 - val_loss: 0.0982\n",
            "Epoch 110/1000\n",
            "240/243 [============================>.] - ETA: 0s - loss: 0.0741roc-auc_val: 0.8801\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0741 - val_loss: 0.0974\n",
            "Epoch 111/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0738 - val_loss: 0.0972\n",
            "Epoch 112/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0737 - val_loss: 0.0975\n",
            "Epoch 113/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0733 - val_loss: 0.0966\n",
            "Epoch 114/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0734 - val_loss: 0.0974\n",
            "Epoch 115/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0736 - val_loss: 0.0968\n",
            "Epoch 116/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0734 - val_loss: 0.0968\n",
            "Epoch 117/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0727 - val_loss: 0.0974\n",
            "Epoch 118/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0729 - val_loss: 0.0974\n",
            "Epoch 119/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0725 - val_loss: 0.0983\n",
            "Epoch 120/1000\n",
            "240/243 [============================>.] - ETA: 0s - loss: 0.0723roc-auc_val: 0.8806\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0722 - val_loss: 0.0979\n",
            "Epoch 121/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0722 - val_loss: 0.0959\n",
            "Epoch 122/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0721 - val_loss: 0.0978\n",
            "Epoch 123/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0718 - val_loss: 0.0965\n",
            "Epoch 124/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0716 - val_loss: 0.0968\n",
            "Epoch 125/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0716 - val_loss: 0.0960\n",
            "Epoch 126/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0712 - val_loss: 0.0966\n",
            "Epoch 127/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0707 - val_loss: 0.0985\n",
            "Epoch 128/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0709 - val_loss: 0.0975\n",
            "Epoch 129/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0710 - val_loss: 0.0972\n",
            "Epoch 130/1000\n",
            "237/243 [============================>.] - ETA: 0s - loss: 0.0706roc-auc_val: 0.8826\n",
            "243/243 [==============================] - 5s 20ms/step - loss: 0.0706 - val_loss: 0.0976\n",
            "Epoch 131/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0701 - val_loss: 0.0977\n",
            "Epoch 132/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0705 - val_loss: 0.0972\n",
            "Epoch 133/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0702 - val_loss: 0.0967\n",
            "Epoch 134/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0697 - val_loss: 0.0988\n",
            "Epoch 135/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0699 - val_loss: 0.0982\n",
            "Epoch 136/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0698 - val_loss: 0.0974\n",
            "Epoch 137/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0696 - val_loss: 0.0965\n",
            "Epoch 138/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0693 - val_loss: 0.0975\n",
            "Epoch 139/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0697 - val_loss: 0.0973\n",
            "Epoch 140/1000\n",
            "238/243 [============================>.] - ETA: 0s - loss: 0.0692roc-auc_val: 0.8849\n",
            "243/243 [==============================] - 5s 19ms/step - loss: 0.0691 - val_loss: 0.0963\n",
            "Epoch 141/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0689 - val_loss: 0.0970\n",
            "Epoch 142/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0688 - val_loss: 0.0974\n",
            "Epoch 143/1000\n",
            "243/243 [==============================] - 2s 9ms/step - loss: 0.0684 - val_loss: 0.0975\n",
            "Epoch 144/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0688 - val_loss: 0.0978\n",
            "Epoch 145/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0684 - val_loss: 0.0973\n",
            "Epoch 146/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0681 - val_loss: 0.0975\n",
            "Epoch 147/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0679 - val_loss: 0.0985\n",
            "Epoch 148/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0679 - val_loss: 0.0980\n",
            "Epoch 149/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0678 - val_loss: 0.0996\n",
            "Epoch 150/1000\n",
            "241/243 [============================>.] - ETA: 0s - loss: 0.0678roc-auc_val: 0.8807\n",
            "243/243 [==============================] - 5s 20ms/step - loss: 0.0678 - val_loss: 0.0980\n",
            "Epoch 151/1000\n",
            "243/243 [==============================] - 2s 7ms/step - loss: 0.0676 - val_loss: 0.0979\n",
            "Epoch 152/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0669 - val_loss: 0.0980\n",
            "Epoch 153/1000\n",
            "243/243 [==============================] - 2s 8ms/step - loss: 0.0673 - val_loss: 0.0986\n",
            "Epoch 154/1000\n",
            "173/243 [====================>.........] - ETA: 0s - loss: 0.0663"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0gL1ZONfsBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
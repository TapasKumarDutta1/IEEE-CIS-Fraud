{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_model_focal_loss_2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/IEEE-CIS-Fraud/blob/master/simple_model_focal_loss_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "59c04b86-705d-4345-99fb-7b2df85b5b7e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "a1c9cdb8-c674-4a4e-940c-ebd65d555f86"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 76.5MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 79% 41.0M/52.2M [00:02<00:01, 9.16MB/s]\n",
            "100% 52.2M/52.2M [00:02<00:00, 18.4MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 84% 49.0M/58.3M [00:02<00:00, 16.9MB/s]\n",
            "100% 58.3M/58.3M [00:03<00:00, 20.3MB/s]\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 106MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 108MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold=5\n",
        "lr=0.001"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "trn=pd.read_csv('/content/gdrive/My Drive/fraud/train.csv')\n",
        "tst=pd.read_csv('/content/gdrive/My Drive/fraud/test.csv')\n",
        "ls=list(trn.filter(regex='V'))\n",
        "trn=trn.drop(ls,1)\n",
        "tst=tst.drop(ls,1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDrCIAqHzl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "5baecd36-ae8c-470b-e086-025e2d16263d"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cols=list(trn.select_dtypes(include=object))\n",
        "for col in cols:\n",
        "  le=LabelEncoderExt()\n",
        "  le.fit(trn[col].astype(str))\n",
        "  trn[col]=le.transform(trn[col].astype(str))\n",
        "  tst[col] = tst[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "  tst[col]=le.transform(tst[col].astype(str))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWJ-hzcznam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "ss=StandardScaler()\n",
        "frd=trn['isFraud']\n",
        "ls=list(trn)\n",
        "trn=ss.fit_transform(trn.drop(['isFraud'],1))\n",
        "trn=pd.DataFrame(trn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF5OQjb1zo6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls.remove('isFraud')\n",
        "trn.columns=ls\n",
        "trn['isFraud']=frd\n",
        "\n",
        "ls=list(tst)\n",
        "tst=ss.fit_transform(tst)\n",
        "tst=pd.DataFrame(tst)\n",
        "tst.columns=ls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "bb08990b-573a-46d1-d3bb-fec482d24699"
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "trn=reduce_mem_usage(trn)\n",
        "tst=reduce_mem_usage(tst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 860.54 MB\n",
            "Memory usage after optimization is: 215.14 MB\n",
            "Decreased by 75.0%\n",
            "Memory usage of dataframe is 734.49 MB\n",
            "Memory usage after optimization is: 183.62 MB\n",
            "Decreased by 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArRiZ5lS0F9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "77b2453c-e4a3-41a8-eaeb-5a9309df7642"
      },
      "source": [
        "trn_n=pd.read_csv('train_transaction.csv.zip')\n",
        "tst_n=pd.read_csv('test_transaction.csv.zip')\n",
        "trn['month']=trn_n['TransactionDT']//(86400*30)\n",
        "trn_n.head()\n",
        "trn_ls=list(trn_n)\n",
        "tst_ls=list(tst_n)\n",
        "for col in trn:\n",
        "  if col in trn_ls:\n",
        "    trn[col+'_isna']=trn_n[col].isna().astype('uint8')\n",
        "for col in tst:\n",
        "  if col in tst_ls:\n",
        "    tst[col+'_isna']=tst_n[col].isna().astype('uint8')\n",
        "import gc\n",
        "del([trn_n,tst_n])\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0r3SuH1K97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn=trn.drop(['isFraud_isna'],1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQ20JqWATak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,validation_data):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.ep=0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.ep+=1\n",
        "        if self.ep%10==0:\n",
        "          y_pred_val = self.model.predict(self.x_val)\n",
        "          roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "          print('roc-auc_val: %s' % str(round(roc_val,4)))\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQIVOLKBFIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e21cf9cc-ec00-465b-afc5-ee355d67d360"
      },
      "source": [
        "1-0.036"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.964"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq6gnpm4CjDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aba93823-475f-4ae5-9e85-e20aa1ebcb6b"
      },
      "source": [
        "def fl():\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        gamma=2\n",
        "        alpha=1-0.036\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
        "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss\n",
        "dk={}\n",
        "def load_model():\n",
        "  K.clear_session()\n",
        "  inp=Input((233,))\n",
        "  x=Dense(256,activation='relu')(inp)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(256,activation='relu')(x)\n",
        "  x=Dropout(0.3)(x)\n",
        "  x=BatchNormalization()(x)\n",
        "  x=Dense(1,activation='sigmoid')(x)\n",
        "  mod=Model(inputs=inp,outputs=x)\n",
        "  return mod\n",
        "for en,month in enumerate([(4,5),(3,4),(3,5)]):\n",
        "  train=trn.loc[trn['month']>=month[1]]\n",
        "  test=trn.loc[trn['month']<=month[0]]\n",
        "  train=train.drop(['month'],1)\n",
        "  test=test.drop(['month'],1)\n",
        "  mod=load_model()\n",
        "  mod.compile(optimizer=Adam(0.0001,decay=1e-3),loss=fl())\n",
        "  roc = RocCallback(\n",
        "                  validation_data=(test.drop(['isFraud'],1), test['isFraud']))\n",
        "  es=EarlyStopping(monitor='val_loss',min_delta=0.0001,mode='min',restore_best_weights=True,patience=50)\n",
        "  mod.fit(train.drop(['isFraud'],1),train['isFraud'],validation_data=(test.drop(['isFraud'],1),test['isFraud']),batch_size=2048,epochs=1000,callbacks=[es,roc])\n",
        "  del([train,test])\n",
        "  gc.collect()\n",
        "  df=trn.loc[trn['month']==6].reset_index(drop=True).drop(['month'],1)\n",
        "  pre=mod.predict(df.drop(['isFraud'],1))\n",
        "  scr=roc_auc_score(df['isFraud'],pre)\n",
        "  dk[str(scr)]=mod.predict(tst)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 27ms/step - loss: 51.1289 - val_loss: 23.4203\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 42.1532 - val_loss: 23.3264\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 40.5394 - val_loss: 23.7762\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 37.4037 - val_loss: 24.4247\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 35.0533 - val_loss: 24.8773\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 33.4858 - val_loss: 24.8637\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 32.0645 - val_loss: 25.0878\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 31.7120 - val_loss: 24.8757\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.9516 - val_loss: 24.0900\n",
            "Epoch 10/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 30.3212roc-auc_val: 0.7608\n",
            "47/47 [==============================] - 14s 297ms/step - loss: 30.0574 - val_loss: 24.4362\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 29.0544 - val_loss: 24.6205\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 28.5648 - val_loss: 24.2164\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 28.0934 - val_loss: 24.2866\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 26.8455 - val_loss: 24.0355\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 26.8114 - val_loss: 23.9075\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 26.4088 - val_loss: 24.0320\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 26.2632 - val_loss: 24.0037\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 26.3757 - val_loss: 23.8780\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 25.3286 - val_loss: 23.6152\n",
            "Epoch 20/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 24.7317roc-auc_val: 0.775\n",
            "47/47 [==============================] - 14s 298ms/step - loss: 24.5576 - val_loss: 23.4732\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.3641 - val_loss: 23.5390\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 24.5946 - val_loss: 23.4480\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 24.2588 - val_loss: 23.4823\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 23.3777 - val_loss: 23.4279\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 23.3578 - val_loss: 23.3077\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 24.0038 - val_loss: 23.3113\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 23.2245 - val_loss: 23.2454\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 23.6017 - val_loss: 23.2416\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 23.1504 - val_loss: 23.1310\n",
            "Epoch 30/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 22.4215roc-auc_val: 0.7806\n",
            "47/47 [==============================] - 14s 298ms/step - loss: 22.6430 - val_loss: 23.0964\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 22.6935 - val_loss: 22.9476\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 22.1843 - val_loss: 22.9268\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 22.4974 - val_loss: 22.9087\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 22.2269 - val_loss: 22.8936\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 22.1479 - val_loss: 22.8578\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 21.7063 - val_loss: 22.8511\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.8642 - val_loss: 22.9818\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.9466 - val_loss: 22.9709\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 21.5850 - val_loss: 22.9403\n",
            "Epoch 40/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 21.0316roc-auc_val: 0.7838\n",
            "47/47 [==============================] - 14s 298ms/step - loss: 20.9457 - val_loss: 22.8935\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 21.0336 - val_loss: 22.8738\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 21.0393 - val_loss: 22.8043\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.9691 - val_loss: 22.7144\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.7858 - val_loss: 22.7093\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 20.4750 - val_loss: 22.7595\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.3767 - val_loss: 22.6904\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.5395 - val_loss: 22.6899\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.9519 - val_loss: 22.6487\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.4926 - val_loss: 22.6067\n",
            "Epoch 50/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 20.4987roc-auc_val: 0.7876\n",
            "47/47 [==============================] - 14s 297ms/step - loss: 20.3642 - val_loss: 22.5933\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.0199 - val_loss: 22.5919\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.3493 - val_loss: 22.6124\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.3110 - val_loss: 22.5768\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.2562 - val_loss: 22.5843\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.7756 - val_loss: 22.5483\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.1393 - val_loss: 22.5038\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.0519 - val_loss: 22.5126\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 20.2234 - val_loss: 22.4433\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.4793 - val_loss: 22.4078\n",
            "Epoch 60/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 19.5901roc-auc_val: 0.7893\n",
            "47/47 [==============================] - 14s 301ms/step - loss: 19.7332 - val_loss: 22.4782\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.6629 - val_loss: 22.5306\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.8106 - val_loss: 22.4706\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.3872 - val_loss: 22.5022\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.0961 - val_loss: 22.4808\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.4911 - val_loss: 22.4652\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.8835 - val_loss: 22.4939\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.3100 - val_loss: 22.4802\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.9553 - val_loss: 22.4611\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 19.3150 - val_loss: 22.4521\n",
            "Epoch 70/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 18.7490roc-auc_val: 0.7911\n",
            "47/47 [==============================] - 14s 299ms/step - loss: 18.8167 - val_loss: 22.4830\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.6214 - val_loss: 22.3884\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.6255 - val_loss: 22.4167\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.6904 - val_loss: 22.4053\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.5351 - val_loss: 22.3687\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.5842 - val_loss: 22.3556\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.9745 - val_loss: 22.3269\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.8330 - val_loss: 22.3809\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.4926 - val_loss: 22.3902\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.5872 - val_loss: 22.4130\n",
            "Epoch 80/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 18.3466roc-auc_val: 0.7929\n",
            "47/47 [==============================] - 14s 298ms/step - loss: 18.3061 - val_loss: 22.3741\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.2822 - val_loss: 22.3287\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.4639 - val_loss: 22.3515\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.5963 - val_loss: 22.3149\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 18.1516 - val_loss: 22.3482\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.1231 - val_loss: 22.3277\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 18.3327 - val_loss: 22.3641\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 18.0014 - val_loss: 22.3562\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.9973 - val_loss: 22.3948\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 17.7618 - val_loss: 22.3792\n",
            "Epoch 90/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 18.1872roc-auc_val: 0.7943\n",
            "47/47 [==============================] - 14s 301ms/step - loss: 18.0996 - val_loss: 22.3593\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.8298 - val_loss: 22.3668\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.5382 - val_loss: 22.3553\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.8025 - val_loss: 22.3445\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.9438 - val_loss: 22.3137\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.1941 - val_loss: 22.2987\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.8928 - val_loss: 22.3349\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.7555 - val_loss: 22.3289\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.7095 - val_loss: 22.3273\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.8858 - val_loss: 22.3192\n",
            "Epoch 100/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 17.4490roc-auc_val: 0.7951\n",
            "47/47 [==============================] - 14s 303ms/step - loss: 17.4184 - val_loss: 22.3242\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 18.0918 - val_loss: 22.3375\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.9207 - val_loss: 22.3714\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.9171 - val_loss: 22.3703\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.4530 - val_loss: 22.3758\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.5288 - val_loss: 22.3735\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.7856 - val_loss: 22.3726\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.4391 - val_loss: 22.4018\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.5752 - val_loss: 22.3755\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.3262 - val_loss: 22.3889\n",
            "Epoch 110/1000\n",
            "36/47 [=====================>........] - ETA: 0s - loss: 17.1742roc-auc_val: 0.7955\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 17.1706 - val_loss: 22.3617\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.6683 - val_loss: 22.3767\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.6714 - val_loss: 22.3016\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.5086 - val_loss: 22.3111\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.3861 - val_loss: 22.3300\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.6711 - val_loss: 22.3292\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.4412 - val_loss: 22.3325\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.4257 - val_loss: 22.3188\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.3152 - val_loss: 22.3198\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.3788 - val_loss: 22.3090\n",
            "Epoch 120/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 17.3898roc-auc_val: 0.7964\n",
            "47/47 [==============================] - 14s 300ms/step - loss: 17.2872 - val_loss: 22.3087\n",
            "Epoch 121/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.2232 - val_loss: 22.3325\n",
            "Epoch 122/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.2826 - val_loss: 22.3598\n",
            "Epoch 123/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 16.8692 - val_loss: 22.3766\n",
            "Epoch 124/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.5229 - val_loss: 22.3896\n",
            "Epoch 125/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.1070 - val_loss: 22.3935\n",
            "Epoch 126/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.1052 - val_loss: 22.3817\n",
            "Epoch 127/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.0819 - val_loss: 22.3252\n",
            "Epoch 128/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.0679 - val_loss: 22.3768\n",
            "Epoch 129/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 16.9480 - val_loss: 22.3839\n",
            "Epoch 130/1000\n",
            "38/47 [=======================>......] - ETA: 0s - loss: 17.1915roc-auc_val: 0.7969\n",
            "47/47 [==============================] - 14s 307ms/step - loss: 17.1589 - val_loss: 22.3678\n",
            "Epoch 131/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 16.8843 - val_loss: 22.3762\n",
            "Epoch 132/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 17.2450 - val_loss: 22.3953\n",
            "Epoch 133/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 17.1808 - val_loss: 22.3890\n",
            "Epoch 134/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 17.1693 - val_loss: 22.3908\n",
            "Epoch 135/1000\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 16.9741 - val_loss: 22.3838\n",
            "Epoch 136/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 16.9678 - val_loss: 22.3764\n",
            "Epoch 137/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 16.7513 - val_loss: 22.4051\n",
            "Epoch 138/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 16.7624 - val_loss: 22.3708\n",
            "Epoch 139/1000\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 16.5686 - val_loss: 22.4096\n",
            "Epoch 140/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 16.7192roc-auc_val: 0.7973\n",
            "47/47 [==============================] - 14s 301ms/step - loss: 16.6981 - val_loss: 22.4171\n",
            "Epoch 141/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 16.8649 - val_loss: 22.3962\n",
            "Epoch 142/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 16.5806 - val_loss: 22.3905\n",
            "Epoch 143/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 16.7553 - val_loss: 22.3819\n",
            "Epoch 144/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 16.7330 - val_loss: 22.4145\n",
            "Epoch 145/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 16.8468 - val_loss: 22.4124\n",
            "Epoch 1/1000\n",
            "88/88 [==============================] - 1s 16ms/step - loss: 51.1175 - val_loss: 22.8502\n",
            "Epoch 2/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 41.0144 - val_loss: 22.7770\n",
            "Epoch 3/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 36.8916 - val_loss: 23.0616\n",
            "Epoch 4/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 34.5168 - val_loss: 22.9163\n",
            "Epoch 5/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 32.6638 - val_loss: 23.1317\n",
            "Epoch 6/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 31.3305 - val_loss: 22.7326\n",
            "Epoch 7/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 30.2741 - val_loss: 22.6150\n",
            "Epoch 8/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 29.6352 - val_loss: 22.5684\n",
            "Epoch 9/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 28.9536 - val_loss: 22.5816\n",
            "Epoch 10/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 28.2634roc-auc_val: 0.7745\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 28.1567 - val_loss: 22.5611\n",
            "Epoch 11/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 27.2538 - val_loss: 22.4433\n",
            "Epoch 12/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 26.4024 - val_loss: 22.4245\n",
            "Epoch 13/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.9778 - val_loss: 22.3693\n",
            "Epoch 14/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 25.5771 - val_loss: 22.2951\n",
            "Epoch 15/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 25.2113 - val_loss: 22.2507\n",
            "Epoch 16/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.6732 - val_loss: 22.2388\n",
            "Epoch 17/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 24.7459 - val_loss: 22.2332\n",
            "Epoch 18/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 23.9785 - val_loss: 22.1510\n",
            "Epoch 19/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 23.5930 - val_loss: 22.1000\n",
            "Epoch 20/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 23.3822roc-auc_val: 0.7818\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 23.3822 - val_loss: 22.1078\n",
            "Epoch 21/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 23.2857 - val_loss: 22.1437\n",
            "Epoch 22/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 23.1932 - val_loss: 21.9557\n",
            "Epoch 23/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 23.0883 - val_loss: 21.8657\n",
            "Epoch 24/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.5990 - val_loss: 21.7415\n",
            "Epoch 25/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 22.6839 - val_loss: 21.6961\n",
            "Epoch 26/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 22.7218 - val_loss: 21.6058\n",
            "Epoch 27/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.9893 - val_loss: 21.6185\n",
            "Epoch 28/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 22.0270 - val_loss: 21.6112\n",
            "Epoch 29/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.6675 - val_loss: 21.6118\n",
            "Epoch 30/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 21.6404roc-auc_val: 0.7915\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 21.6174 - val_loss: 21.5294\n",
            "Epoch 31/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.3628 - val_loss: 21.5457\n",
            "Epoch 32/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.1284 - val_loss: 21.5775\n",
            "Epoch 33/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 21.3894 - val_loss: 21.5184\n",
            "Epoch 34/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.1853 - val_loss: 21.4931\n",
            "Epoch 35/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 21.2023 - val_loss: 21.4742\n",
            "Epoch 36/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.8764 - val_loss: 21.5037\n",
            "Epoch 37/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 20.6948 - val_loss: 21.4578\n",
            "Epoch 38/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.5761 - val_loss: 21.4664\n",
            "Epoch 39/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.4534 - val_loss: 21.4287\n",
            "Epoch 40/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 20.5570roc-auc_val: 0.7951\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 20.6794 - val_loss: 21.4133\n",
            "Epoch 41/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 20.3121 - val_loss: 21.3655\n",
            "Epoch 42/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9905 - val_loss: 21.3255\n",
            "Epoch 43/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 20.3213 - val_loss: 21.3021\n",
            "Epoch 44/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 20.2695 - val_loss: 21.3251\n",
            "Epoch 45/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.9378 - val_loss: 21.2704\n",
            "Epoch 46/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 20.0317 - val_loss: 21.2588\n",
            "Epoch 47/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.8439 - val_loss: 21.2488\n",
            "Epoch 48/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.7291 - val_loss: 21.2366\n",
            "Epoch 49/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.6454 - val_loss: 21.2255\n",
            "Epoch 50/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 19.8723roc-auc_val: 0.799\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 19.7717 - val_loss: 21.2136\n",
            "Epoch 51/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.5301 - val_loss: 21.2109\n",
            "Epoch 52/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.4708 - val_loss: 21.1933\n",
            "Epoch 53/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.4105 - val_loss: 21.1736\n",
            "Epoch 54/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.0975 - val_loss: 21.1536\n",
            "Epoch 55/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.4510 - val_loss: 21.1633\n",
            "Epoch 56/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.3477 - val_loss: 21.1609\n",
            "Epoch 57/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.3601 - val_loss: 21.1831\n",
            "Epoch 58/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.1076 - val_loss: 21.1773\n",
            "Epoch 59/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.3442 - val_loss: 21.1750\n",
            "Epoch 60/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 19.2140roc-auc_val: 0.8014\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 19.2140 - val_loss: 21.1839\n",
            "Epoch 61/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 19.0239 - val_loss: 21.1382\n",
            "Epoch 62/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 19.1629 - val_loss: 21.1365\n",
            "Epoch 63/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 18.6986 - val_loss: 21.1179\n",
            "Epoch 64/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 19.0680 - val_loss: 21.1186\n",
            "Epoch 65/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 18.8679 - val_loss: 21.1015\n",
            "Epoch 66/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.7902 - val_loss: 21.1176\n",
            "Epoch 67/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.7167 - val_loss: 21.0774\n",
            "Epoch 68/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.6199 - val_loss: 21.0582\n",
            "Epoch 69/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 18.6060 - val_loss: 21.0532\n",
            "Epoch 70/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 18.7327roc-auc_val: 0.8033\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 18.6288 - val_loss: 21.0600\n",
            "Epoch 71/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 18.6944 - val_loss: 21.0462\n",
            "Epoch 72/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.6166 - val_loss: 21.0766\n",
            "Epoch 73/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.5744 - val_loss: 21.0636\n",
            "Epoch 74/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.6555 - val_loss: 21.0406\n",
            "Epoch 75/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.1984 - val_loss: 21.0692\n",
            "Epoch 76/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.5164 - val_loss: 21.0682\n",
            "Epoch 77/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.2305 - val_loss: 21.0563\n",
            "Epoch 78/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.6943 - val_loss: 21.0595\n",
            "Epoch 79/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.1948 - val_loss: 21.0293\n",
            "Epoch 80/1000\n",
            "86/88 [============================>.] - ETA: 0s - loss: 18.6287roc-auc_val: 0.805\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 18.5849 - val_loss: 21.0054\n",
            "Epoch 81/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.5277 - val_loss: 21.0014\n",
            "Epoch 82/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.1584 - val_loss: 20.9696\n",
            "Epoch 83/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.2191 - val_loss: 20.9608\n",
            "Epoch 84/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.2853 - val_loss: 20.9762\n",
            "Epoch 85/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.2007 - val_loss: 20.9702\n",
            "Epoch 86/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.2647 - val_loss: 20.9624\n",
            "Epoch 87/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.3232 - val_loss: 20.9658\n",
            "Epoch 88/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.9198 - val_loss: 20.9601\n",
            "Epoch 89/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 18.0415 - val_loss: 20.9631\n",
            "Epoch 90/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 18.0566roc-auc_val: 0.8062\n",
            "88/88 [==============================] - 12s 140ms/step - loss: 18.1209 - val_loss: 20.9618\n",
            "Epoch 91/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.8975 - val_loss: 20.9424\n",
            "Epoch 92/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 18.0800 - val_loss: 20.9385\n",
            "Epoch 93/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 18.0339 - val_loss: 20.9298\n",
            "Epoch 94/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 18.0533 - val_loss: 20.9252\n",
            "Epoch 95/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.9982 - val_loss: 20.9142\n",
            "Epoch 96/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.8862 - val_loss: 20.9151\n",
            "Epoch 97/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.8289 - val_loss: 20.9033\n",
            "Epoch 98/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.8290 - val_loss: 20.9066\n",
            "Epoch 99/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.8037 - val_loss: 20.9073\n",
            "Epoch 100/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 17.6941roc-auc_val: 0.8074\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 17.6700 - val_loss: 20.9091\n",
            "Epoch 101/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.6401 - val_loss: 20.8963\n",
            "Epoch 102/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.7031 - val_loss: 20.8930\n",
            "Epoch 103/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.8594 - val_loss: 20.9017\n",
            "Epoch 104/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.7138 - val_loss: 20.8795\n",
            "Epoch 105/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.6989 - val_loss: 20.8714\n",
            "Epoch 106/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.6779 - val_loss: 20.8676\n",
            "Epoch 107/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.6368 - val_loss: 20.8712\n",
            "Epoch 108/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.7273 - val_loss: 20.8622\n",
            "Epoch 109/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.4789 - val_loss: 20.8726\n",
            "Epoch 110/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 17.4241roc-auc_val: 0.8086\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 17.4736 - val_loss: 20.8643\n",
            "Epoch 111/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.3766 - val_loss: 20.8728\n",
            "Epoch 112/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.4613 - val_loss: 20.8559\n",
            "Epoch 113/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.3795 - val_loss: 20.8639\n",
            "Epoch 114/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.3636 - val_loss: 20.8674\n",
            "Epoch 115/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.1931 - val_loss: 20.8657\n",
            "Epoch 116/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.3872 - val_loss: 20.8721\n",
            "Epoch 117/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.3389 - val_loss: 20.8645\n",
            "Epoch 118/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.4631 - val_loss: 20.8377\n",
            "Epoch 119/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.5246 - val_loss: 20.8354\n",
            "Epoch 120/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 17.3542roc-auc_val: 0.8097\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 17.3354 - val_loss: 20.8302\n",
            "Epoch 121/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.4480 - val_loss: 20.8235\n",
            "Epoch 122/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.5153 - val_loss: 20.8115\n",
            "Epoch 123/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.1746 - val_loss: 20.8217\n",
            "Epoch 124/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.5089 - val_loss: 20.8288\n",
            "Epoch 125/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.1124 - val_loss: 20.8242\n",
            "Epoch 126/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.0588 - val_loss: 20.8258\n",
            "Epoch 127/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.2685 - val_loss: 20.8062\n",
            "Epoch 128/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.2610 - val_loss: 20.8051\n",
            "Epoch 129/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.1154 - val_loss: 20.8053\n",
            "Epoch 130/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 17.2427roc-auc_val: 0.8106\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 17.2169 - val_loss: 20.8073\n",
            "Epoch 131/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.2314 - val_loss: 20.8059\n",
            "Epoch 132/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.2826 - val_loss: 20.8057\n",
            "Epoch 133/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.1358 - val_loss: 20.8046\n",
            "Epoch 134/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.1568 - val_loss: 20.7953\n",
            "Epoch 135/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.9544 - val_loss: 20.7900\n",
            "Epoch 136/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.1042 - val_loss: 20.7798\n",
            "Epoch 137/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 17.2028 - val_loss: 20.7810\n",
            "Epoch 138/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.9942 - val_loss: 20.7773\n",
            "Epoch 139/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.9666 - val_loss: 20.7802\n",
            "Epoch 140/1000\n",
            "84/88 [===========================>..] - ETA: 0s - loss: 16.9083roc-auc_val: 0.8113\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 16.9099 - val_loss: 20.7931\n",
            "Epoch 141/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.9021 - val_loss: 20.7870\n",
            "Epoch 142/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 17.0420 - val_loss: 20.8022\n",
            "Epoch 143/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.9532 - val_loss: 20.7965\n",
            "Epoch 144/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.9689 - val_loss: 20.7843\n",
            "Epoch 145/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.7770 - val_loss: 20.7830\n",
            "Epoch 146/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.9558 - val_loss: 20.8069\n",
            "Epoch 147/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.9268 - val_loss: 20.7998\n",
            "Epoch 148/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.9043 - val_loss: 20.7736\n",
            "Epoch 149/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.9125 - val_loss: 20.7830\n",
            "Epoch 150/1000\n",
            "83/88 [===========================>..] - ETA: 0s - loss: 16.9081roc-auc_val: 0.8118\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 16.8671 - val_loss: 20.7902\n",
            "Epoch 151/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.8117 - val_loss: 20.7919\n",
            "Epoch 152/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.7916 - val_loss: 20.7736\n",
            "Epoch 153/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.8774 - val_loss: 20.7687\n",
            "Epoch 154/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.7984 - val_loss: 20.7649\n",
            "Epoch 155/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.7179 - val_loss: 20.7654\n",
            "Epoch 156/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.8211 - val_loss: 20.7729\n",
            "Epoch 157/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.6730 - val_loss: 20.7859\n",
            "Epoch 158/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.7371 - val_loss: 20.7742\n",
            "Epoch 159/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.6900 - val_loss: 20.7713\n",
            "Epoch 160/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 16.9050roc-auc_val: 0.8122\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 16.8663 - val_loss: 20.7719\n",
            "Epoch 161/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.7047 - val_loss: 20.7644\n",
            "Epoch 162/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.7182 - val_loss: 20.7605\n",
            "Epoch 163/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.8317 - val_loss: 20.7465\n",
            "Epoch 164/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.6195 - val_loss: 20.7480\n",
            "Epoch 165/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.6300 - val_loss: 20.7458\n",
            "Epoch 166/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.7662 - val_loss: 20.7374\n",
            "Epoch 167/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.8340 - val_loss: 20.7300\n",
            "Epoch 168/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.6252 - val_loss: 20.7443\n",
            "Epoch 169/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.6804 - val_loss: 20.7398\n",
            "Epoch 170/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.4551roc-auc_val: 0.8128\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 16.4551 - val_loss: 20.7320\n",
            "Epoch 171/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.6259 - val_loss: 20.7384\n",
            "Epoch 172/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.5556 - val_loss: 20.7475\n",
            "Epoch 173/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.5024 - val_loss: 20.7516\n",
            "Epoch 174/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.6348 - val_loss: 20.7436\n",
            "Epoch 175/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.7160 - val_loss: 20.7397\n",
            "Epoch 176/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.3821 - val_loss: 20.7297\n",
            "Epoch 177/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.5615 - val_loss: 20.7233\n",
            "Epoch 178/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4634 - val_loss: 20.7410\n",
            "Epoch 179/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.7177 - val_loss: 20.7265\n",
            "Epoch 180/1000\n",
            "85/88 [===========================>..] - ETA: 0s - loss: 16.4711roc-auc_val: 0.8132\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 16.4685 - val_loss: 20.7366\n",
            "Epoch 181/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4504 - val_loss: 20.7202\n",
            "Epoch 182/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.5069 - val_loss: 20.7289\n",
            "Epoch 183/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4315 - val_loss: 20.7403\n",
            "Epoch 184/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4051 - val_loss: 20.7273\n",
            "Epoch 185/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3934 - val_loss: 20.7361\n",
            "Epoch 186/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3588 - val_loss: 20.7310\n",
            "Epoch 187/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4640 - val_loss: 20.7364\n",
            "Epoch 188/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4833 - val_loss: 20.7229\n",
            "Epoch 189/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.2983 - val_loss: 20.7324\n",
            "Epoch 190/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 16.6409roc-auc_val: 0.8134\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 16.5909 - val_loss: 20.7158\n",
            "Epoch 191/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.5673 - val_loss: 20.7170\n",
            "Epoch 192/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.5209 - val_loss: 20.7084\n",
            "Epoch 193/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3345 - val_loss: 20.7191\n",
            "Epoch 194/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4316 - val_loss: 20.7287\n",
            "Epoch 195/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3514 - val_loss: 20.7289\n",
            "Epoch 196/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3036 - val_loss: 20.7299\n",
            "Epoch 197/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4328 - val_loss: 20.7230\n",
            "Epoch 198/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3700 - val_loss: 20.7258\n",
            "Epoch 199/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4588 - val_loss: 20.7397\n",
            "Epoch 200/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 16.5700roc-auc_val: 0.8138\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 16.5162 - val_loss: 20.7192\n",
            "Epoch 201/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4203 - val_loss: 20.7096\n",
            "Epoch 202/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3888 - val_loss: 20.7130\n",
            "Epoch 203/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3367 - val_loss: 20.7157\n",
            "Epoch 204/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3495 - val_loss: 20.7143\n",
            "Epoch 205/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.5334 - val_loss: 20.7233\n",
            "Epoch 206/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.2768 - val_loss: 20.7250\n",
            "Epoch 207/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.5746 - val_loss: 20.7192\n",
            "Epoch 208/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.1482 - val_loss: 20.7246\n",
            "Epoch 209/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4781 - val_loss: 20.7153\n",
            "Epoch 210/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 16.1919roc-auc_val: 0.8141\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 16.1658 - val_loss: 20.7105\n",
            "Epoch 211/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.2577 - val_loss: 20.7008\n",
            "Epoch 212/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0739 - val_loss: 20.7035\n",
            "Epoch 213/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.2241 - val_loss: 20.7084\n",
            "Epoch 214/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.2865 - val_loss: 20.7082\n",
            "Epoch 215/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0979 - val_loss: 20.7202\n",
            "Epoch 216/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.2411 - val_loss: 20.7180\n",
            "Epoch 217/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.4423 - val_loss: 20.7147\n",
            "Epoch 218/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.2015 - val_loss: 20.7133\n",
            "Epoch 219/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3411 - val_loss: 20.7204\n",
            "Epoch 220/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.2387roc-auc_val: 0.8143\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 16.2387 - val_loss: 20.7209\n",
            "Epoch 221/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.1010 - val_loss: 20.7169\n",
            "Epoch 222/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3054 - val_loss: 20.7188\n",
            "Epoch 223/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0632 - val_loss: 20.7264\n",
            "Epoch 224/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.1468 - val_loss: 20.7120\n",
            "Epoch 225/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.1391 - val_loss: 20.7194\n",
            "Epoch 226/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.1364 - val_loss: 20.7310\n",
            "Epoch 227/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3366 - val_loss: 20.7158\n",
            "Epoch 228/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.1720 - val_loss: 20.7172\n",
            "Epoch 229/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.1220 - val_loss: 20.7127\n",
            "Epoch 230/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 16.2426roc-auc_val: 0.8146\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 16.1890 - val_loss: 20.7103\n",
            "Epoch 231/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.1069 - val_loss: 20.7094\n",
            "Epoch 232/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9849 - val_loss: 20.7054\n",
            "Epoch 233/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0334 - val_loss: 20.7140\n",
            "Epoch 234/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.2020 - val_loss: 20.7112\n",
            "Epoch 235/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0092 - val_loss: 20.7023\n",
            "Epoch 236/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.2228 - val_loss: 20.7012\n",
            "Epoch 237/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.1167 - val_loss: 20.7074\n",
            "Epoch 238/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.3134 - val_loss: 20.7072\n",
            "Epoch 239/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0853 - val_loss: 20.7086\n",
            "Epoch 240/1000\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.9970roc-auc_val: 0.8148\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 15.9970 - val_loss: 20.7074\n",
            "Epoch 241/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.1159 - val_loss: 20.6977\n",
            "Epoch 242/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9928 - val_loss: 20.7149\n",
            "Epoch 243/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0705 - val_loss: 20.7171\n",
            "Epoch 244/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0668 - val_loss: 20.7117\n",
            "Epoch 245/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9704 - val_loss: 20.7016\n",
            "Epoch 246/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8125 - val_loss: 20.7085\n",
            "Epoch 247/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8794 - val_loss: 20.7027\n",
            "Epoch 248/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8757 - val_loss: 20.7128\n",
            "Epoch 249/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 15.9954 - val_loss: 20.6955\n",
            "Epoch 250/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 15.9641roc-auc_val: 0.8151\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 15.9510 - val_loss: 20.7091\n",
            "Epoch 251/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 15.8929 - val_loss: 20.7073\n",
            "Epoch 252/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 15.9639 - val_loss: 20.7145\n",
            "Epoch 253/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 15.9044 - val_loss: 20.7179\n",
            "Epoch 254/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 16.0627 - val_loss: 20.7173\n",
            "Epoch 255/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8830 - val_loss: 20.6928\n",
            "Epoch 256/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9798 - val_loss: 20.6987\n",
            "Epoch 257/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9763 - val_loss: 20.6940\n",
            "Epoch 258/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9243 - val_loss: 20.7025\n",
            "Epoch 259/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9459 - val_loss: 20.6974\n",
            "Epoch 260/1000\n",
            "76/88 [========================>.....] - ETA: 0s - loss: 15.7477roc-auc_val: 0.8154\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 15.7824 - val_loss: 20.6962\n",
            "Epoch 261/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7560 - val_loss: 20.7072\n",
            "Epoch 262/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9971 - val_loss: 20.7028\n",
            "Epoch 263/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0161 - val_loss: 20.7079\n",
            "Epoch 264/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7638 - val_loss: 20.6993\n",
            "Epoch 265/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0656 - val_loss: 20.6974\n",
            "Epoch 266/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8399 - val_loss: 20.6944\n",
            "Epoch 267/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9352 - val_loss: 20.6958\n",
            "Epoch 268/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7585 - val_loss: 20.6933\n",
            "Epoch 269/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7613 - val_loss: 20.6972\n",
            "Epoch 270/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 15.7203roc-auc_val: 0.8157\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 15.7143 - val_loss: 20.6944\n",
            "Epoch 271/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7637 - val_loss: 20.7031\n",
            "Epoch 272/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8559 - val_loss: 20.7051\n",
            "Epoch 273/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8283 - val_loss: 20.6959\n",
            "Epoch 274/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8380 - val_loss: 20.7068\n",
            "Epoch 275/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8410 - val_loss: 20.7044\n",
            "Epoch 276/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8166 - val_loss: 20.7061\n",
            "Epoch 277/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 16.0167 - val_loss: 20.7089\n",
            "Epoch 278/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7670 - val_loss: 20.6949\n",
            "Epoch 279/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8766 - val_loss: 20.7014\n",
            "Epoch 280/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 15.7792roc-auc_val: 0.8157\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 15.7685 - val_loss: 20.7000\n",
            "Epoch 281/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7227 - val_loss: 20.6879\n",
            "Epoch 282/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8330 - val_loss: 20.6915\n",
            "Epoch 283/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8578 - val_loss: 20.7024\n",
            "Epoch 284/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9840 - val_loss: 20.6969\n",
            "Epoch 285/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.9455 - val_loss: 20.6990\n",
            "Epoch 286/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7090 - val_loss: 20.6933\n",
            "Epoch 287/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6921 - val_loss: 20.6907\n",
            "Epoch 288/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7376 - val_loss: 20.7057\n",
            "Epoch 289/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7397 - val_loss: 20.7135\n",
            "Epoch 290/1000\n",
            "79/88 [=========================>....] - ETA: 0s - loss: 15.7544roc-auc_val: 0.8159\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 15.8534 - val_loss: 20.7044\n",
            "Epoch 291/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8388 - val_loss: 20.6954\n",
            "Epoch 292/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6775 - val_loss: 20.6925\n",
            "Epoch 293/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7854 - val_loss: 20.6977\n",
            "Epoch 294/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8284 - val_loss: 20.6863\n",
            "Epoch 295/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7568 - val_loss: 20.6854\n",
            "Epoch 296/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6550 - val_loss: 20.6802\n",
            "Epoch 297/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8134 - val_loss: 20.6849\n",
            "Epoch 298/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7878 - val_loss: 20.6866\n",
            "Epoch 299/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7648 - val_loss: 20.6851\n",
            "Epoch 300/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 15.8009roc-auc_val: 0.8162\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 15.8043 - val_loss: 20.6969\n",
            "Epoch 301/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6235 - val_loss: 20.6952\n",
            "Epoch 302/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8928 - val_loss: 20.6969\n",
            "Epoch 303/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6950 - val_loss: 20.6911\n",
            "Epoch 304/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5807 - val_loss: 20.6892\n",
            "Epoch 305/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.7644 - val_loss: 20.6774\n",
            "Epoch 306/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6981 - val_loss: 20.6721\n",
            "Epoch 307/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6613 - val_loss: 20.6845\n",
            "Epoch 308/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6690 - val_loss: 20.6732\n",
            "Epoch 309/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6673 - val_loss: 20.6883\n",
            "Epoch 310/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 15.4748roc-auc_val: 0.8165\n",
            "88/88 [==============================] - 13s 147ms/step - loss: 15.4535 - val_loss: 20.6867\n",
            "Epoch 311/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6810 - val_loss: 20.6786\n",
            "Epoch 312/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4823 - val_loss: 20.6879\n",
            "Epoch 313/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.8044 - val_loss: 20.6737\n",
            "Epoch 314/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5473 - val_loss: 20.6712\n",
            "Epoch 315/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5614 - val_loss: 20.6694\n",
            "Epoch 316/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5616 - val_loss: 20.6829\n",
            "Epoch 317/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6243 - val_loss: 20.6739\n",
            "Epoch 318/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6362 - val_loss: 20.6756\n",
            "Epoch 319/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6976 - val_loss: 20.6756\n",
            "Epoch 320/1000\n",
            "77/88 [=========================>....] - ETA: 0s - loss: 15.5165roc-auc_val: 0.8165\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 15.5337 - val_loss: 20.6904\n",
            "Epoch 321/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 15.6283 - val_loss: 20.6925\n",
            "Epoch 322/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6497 - val_loss: 20.6813\n",
            "Epoch 323/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 15.7251 - val_loss: 20.6790\n",
            "Epoch 324/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5597 - val_loss: 20.6820\n",
            "Epoch 325/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5583 - val_loss: 20.6751\n",
            "Epoch 326/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4359 - val_loss: 20.6779\n",
            "Epoch 327/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5929 - val_loss: 20.6841\n",
            "Epoch 328/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4377 - val_loss: 20.6909\n",
            "Epoch 329/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5005 - val_loss: 20.6900\n",
            "Epoch 330/1000\n",
            "87/88 [============================>.] - ETA: 0s - loss: 15.5952roc-auc_val: 0.8166\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 15.5743 - val_loss: 20.6969\n",
            "Epoch 331/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6500 - val_loss: 20.6952\n",
            "Epoch 332/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6511 - val_loss: 20.6994\n",
            "Epoch 333/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6595 - val_loss: 20.7170\n",
            "Epoch 334/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5261 - val_loss: 20.7019\n",
            "Epoch 335/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6731 - val_loss: 20.7052\n",
            "Epoch 336/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5421 - val_loss: 20.7090\n",
            "Epoch 337/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5188 - val_loss: 20.6916\n",
            "Epoch 338/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4490 - val_loss: 20.6945\n",
            "Epoch 339/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4904 - val_loss: 20.7031\n",
            "Epoch 340/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 15.3904roc-auc_val: 0.8166\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 15.4357 - val_loss: 20.6989\n",
            "Epoch 341/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.3512 - val_loss: 20.6973\n",
            "Epoch 342/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4721 - val_loss: 20.6991\n",
            "Epoch 343/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4237 - val_loss: 20.7055\n",
            "Epoch 344/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.6207 - val_loss: 20.7014\n",
            "Epoch 345/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5056 - val_loss: 20.6982\n",
            "Epoch 346/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 15.4103 - val_loss: 20.7048\n",
            "Epoch 347/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4730 - val_loss: 20.7045\n",
            "Epoch 348/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.3320 - val_loss: 20.6894\n",
            "Epoch 349/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4037 - val_loss: 20.6981\n",
            "Epoch 350/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 15.5723roc-auc_val: 0.8168\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 15.5207 - val_loss: 20.6948\n",
            "Epoch 351/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4649 - val_loss: 20.7024\n",
            "Epoch 352/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5250 - val_loss: 20.6987\n",
            "Epoch 353/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4364 - val_loss: 20.6938\n",
            "Epoch 354/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.5738 - val_loss: 20.7023\n",
            "Epoch 355/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4450 - val_loss: 20.6980\n",
            "Epoch 356/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4859 - val_loss: 20.7065\n",
            "Epoch 357/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4029 - val_loss: 20.7058\n",
            "Epoch 358/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4130 - val_loss: 20.6988\n",
            "Epoch 359/1000\n",
            "88/88 [==============================] - 1s 9ms/step - loss: 15.4347 - val_loss: 20.7047\n",
            "Epoch 360/1000\n",
            "78/88 [=========================>....] - ETA: 0s - loss: 15.3562roc-auc_val: 0.8168\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 15.3565 - val_loss: 20.7025\n",
            "Epoch 361/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.3298 - val_loss: 20.7026\n",
            "Epoch 362/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.3074 - val_loss: 20.7058\n",
            "Epoch 363/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4350 - val_loss: 20.7141\n",
            "Epoch 364/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.2527 - val_loss: 20.7035\n",
            "Epoch 365/1000\n",
            "88/88 [==============================] - 1s 8ms/step - loss: 15.4961 - val_loss: 20.7055\n",
            "Epoch 1/1000\n",
            "47/47 [==============================] - 1s 26ms/step - loss: 52.7518 - val_loss: 25.3457\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 43.4749 - val_loss: 25.7563\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 39.5896 - val_loss: 25.4910\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 37.6087 - val_loss: 24.9128\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 34.3860 - val_loss: 24.5933\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 33.0271 - val_loss: 24.6159\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.7395 - val_loss: 24.4187\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 31.1730 - val_loss: 24.2485\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 30.0464 - val_loss: 23.8684\n",
            "Epoch 10/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 29.7226roc-auc_val: 0.7557\n",
            "47/47 [==============================] - 12s 252ms/step - loss: 29.3463 - val_loss: 23.9573\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 28.5729 - val_loss: 23.6903\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.8380 - val_loss: 23.6650\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 27.6713 - val_loss: 23.8300\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.6877 - val_loss: 23.7102\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.4767 - val_loss: 23.5731\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 26.4184 - val_loss: 23.5628\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 25.3855 - val_loss: 23.5587\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 25.7289 - val_loss: 23.4814\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 25.0169 - val_loss: 23.4365\n",
            "Epoch 20/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 25.3840roc-auc_val: 0.7695\n",
            "47/47 [==============================] - 12s 251ms/step - loss: 24.9438 - val_loss: 23.2967\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 24.4714 - val_loss: 23.4482\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 23.9881 - val_loss: 23.3458\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 24.0251 - val_loss: 23.3302\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 23.5040 - val_loss: 23.2244\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 23.4753 - val_loss: 23.1873\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 23.0268 - val_loss: 23.1753\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 22.9114 - val_loss: 23.1203\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 22.5037 - val_loss: 23.0388\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 22.5941 - val_loss: 23.0524\n",
            "Epoch 30/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 22.9394roc-auc_val: 0.7752\n",
            "47/47 [==============================] - 12s 250ms/step - loss: 22.7696 - val_loss: 23.0244\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 21.9478 - val_loss: 23.0483\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 22.0944 - val_loss: 22.9341\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 22.5264 - val_loss: 22.9574\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 21.6270 - val_loss: 22.8813\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 22.0442 - val_loss: 22.8562\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 21.7990 - val_loss: 22.8926\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 21.6372 - val_loss: 22.8677\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 21.3097 - val_loss: 22.7948\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 21.1892 - val_loss: 22.7437\n",
            "Epoch 40/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 21.3915roc-auc_val: 0.7814\n",
            "47/47 [==============================] - 12s 254ms/step - loss: 21.0644 - val_loss: 22.7805\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.6004 - val_loss: 22.7621\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 21.4310 - val_loss: 22.7543\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.9664 - val_loss: 22.7532\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.6200 - val_loss: 22.7318\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.3175 - val_loss: 22.7059\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.3311 - val_loss: 22.6919\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.3458 - val_loss: 22.6713\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.3098 - val_loss: 22.6236\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.6218 - val_loss: 22.6365\n",
            "Epoch 50/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 19.9505roc-auc_val: 0.7847\n",
            "47/47 [==============================] - 12s 259ms/step - loss: 19.7795 - val_loss: 22.6425\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.1725 - val_loss: 22.6644\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 20.2685 - val_loss: 22.6656\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.9422 - val_loss: 22.6543\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.6828 - val_loss: 22.6145\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.7745 - val_loss: 22.6242\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.6872 - val_loss: 22.6311\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.4208 - val_loss: 22.6452\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.5696 - val_loss: 22.6176\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.2940 - val_loss: 22.6074\n",
            "Epoch 60/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 19.8327roc-auc_val: 0.787\n",
            "47/47 [==============================] - 12s 252ms/step - loss: 19.6537 - val_loss: 22.6078\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.3794 - val_loss: 22.5845\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.6463 - val_loss: 22.6024\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.2256 - val_loss: 22.6343\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.2992 - val_loss: 22.6067\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.9076 - val_loss: 22.5814\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.2818 - val_loss: 22.6175\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 19.0761 - val_loss: 22.5967\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.7642 - val_loss: 22.6087\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.6838 - val_loss: 22.5911\n",
            "Epoch 70/1000\n",
            "40/47 [========================>.....] - ETA: 0s - loss: 19.0481roc-auc_val: 0.7891\n",
            "47/47 [==============================] - 12s 252ms/step - loss: 18.9771 - val_loss: 22.5581\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.9271 - val_loss: 22.5668\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.7846 - val_loss: 22.5968\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.9505 - val_loss: 22.6029\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.6315 - val_loss: 22.5856\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.5073 - val_loss: 22.6015\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.8781 - val_loss: 22.5858\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.5267 - val_loss: 22.6030\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.4451 - val_loss: 22.5948\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.3526 - val_loss: 22.5788\n",
            "Epoch 80/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 18.3657roc-auc_val: 0.7905\n",
            "47/47 [==============================] - 12s 253ms/step - loss: 18.3038 - val_loss: 22.6101\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.9179 - val_loss: 22.6085\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.3117 - val_loss: 22.5696\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.0134 - val_loss: 22.5862\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.7207 - val_loss: 22.5689\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.5664 - val_loss: 22.5837\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.1182 - val_loss: 22.5744\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.2812 - val_loss: 22.5815\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.0811 - val_loss: 22.6162\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.1377 - val_loss: 22.6651\n",
            "Epoch 90/1000\n",
            "39/47 [=======================>......] - ETA: 0s - loss: 17.7194roc-auc_val: 0.791\n",
            "47/47 [==============================] - 12s 253ms/step - loss: 17.7984 - val_loss: 22.6335\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.6903 - val_loss: 22.6296\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.1793 - val_loss: 22.6497\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.4088 - val_loss: 22.6539\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.1008 - val_loss: 22.6484\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.7662 - val_loss: 22.6773\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.6920 - val_loss: 22.6659\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 18.1764 - val_loss: 22.6325\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.6381 - val_loss: 22.6170\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.7350 - val_loss: 22.6216\n",
            "Epoch 100/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 17.7791roc-auc_val: 0.7916\n",
            "47/47 [==============================] - 12s 252ms/step - loss: 17.6884 - val_loss: 22.6082\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.4543 - val_loss: 22.6293\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.7870 - val_loss: 22.6311\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.5416 - val_loss: 22.5863\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.5909 - val_loss: 22.5863\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.5701 - val_loss: 22.5827\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.2322 - val_loss: 22.5787\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.8894 - val_loss: 22.6078\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.1982 - val_loss: 22.5771\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 1s 12ms/step - loss: 17.4644 - val_loss: 22.5632\n",
            "Epoch 110/1000\n",
            "37/47 [======================>.......] - ETA: 0s - loss: 17.4907roc-auc_val: 0.793\n",
            "47/47 [==============================] - 12s 253ms/step - loss: 17.5120 - val_loss: 22.5864\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 1s 14ms/step - loss: 17.3724 - val_loss: 22.6081\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.5547 - val_loss: 22.6350\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.5046 - val_loss: 22.6153\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.3698 - val_loss: 22.6037\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.2351 - val_loss: 22.6221\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.2832 - val_loss: 22.6092\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.2756 - val_loss: 22.6167\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.1391 - val_loss: 22.6346\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 1s 13ms/step - loss: 17.0272 - val_loss: 22.6425\n",
            "Epoch 120/1000\n",
            "47/47 [==============================] - ETA: 0s - loss: 17.2912roc-auc_val: 0.7891\n",
            "47/47 [==============================] - 12s 261ms/step - loss: 17.2912 - val_loss: 22.6464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnpeTPNLkiCP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "outputId": "8197e91b-127a-49ff-f7e1-9ec377cc7c40"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "for i in dk.keys():\n",
        "  sns.distplot(dk[i])\n",
        "  plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6S7Pv_rc-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}